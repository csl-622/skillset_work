<?xml version="1.0" encoding="utf-8"?>
<posts>
  <row Id="1" PostTypeId="1" CreationDate="2015-08-04T18:05:05.010" Score="7" ViewCount="155" Body="&lt;p&gt;I saw a &lt;a href=&quot;https://www.youtube.com/watch?v=7YWTtCsvgvg&quot; rel=&quot;nofollow&quot;&gt;video&lt;/a&gt; on YouTube where a person uses a transparant, knotted pyramid to display &quot;holograms&quot; using their smartphone. (I have not been able to reproduce this yet, so I can't tell for sure if it works).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering, given a model, which transformations I should apply to get the different projections needed to produce this effect?&lt;/p&gt;&#xA;" OwnerUserId="21" LastEditorUserId="30" LastEditDate="2015-08-08T11:28:35.067" LastActivityDate="2015-08-08T11:28:35.067" Title="What transformations should I apply to a model to produce this 3D effect?" Tags="&lt;projections&gt;&lt;transformations&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2" PostTypeId="1" AcceptedAnswerId="11" CreationDate="2015-08-04T18:10:11.117" Score="10" ViewCount="150" Body="&lt;p&gt;The convention in graphics is that performing fewer state changes is better than performing more state changes (switching shaders, binding buffers, binding textures, etc.). For textures, it is faster to render many polygons using a single atlas (for rendering sprites/text) than to individually bind a new texture for each polygon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does this hold true if I am continually painting to a texture via &lt;code&gt;glTexSubImage2D&lt;/code&gt;? I have a stream of data coming in (over a network) that undergoes processing and is then painted to a texture one row at a time. The data is presented visually in an endless scroll.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would I be better off painting to one texture rendered on one large rectangle (scrolling the painted data into view)? The idea here is that I would have one (or two) textures bound at any given time while I just continue to paint to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or should I paint lots of little rectangles (only revealing the rectangle when painting is done)? I assume I would bind one texture per rectangle.&lt;/p&gt;&#xA;" OwnerUserId="36" LastEditorUserId="36" LastEditDate="2015-08-04T18:21:59.540" LastActivityDate="2015-08-04T19:19:15.453" Title="Is modifying a texture (painting on it) considered a &quot;state change&quot;?" Tags="&lt;opengl&gt;&lt;texture&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="3" PostTypeId="1" AcceptedAnswerId="10" CreationDate="2015-08-04T18:13:47.903" Score="18" ViewCount="342" Body="&lt;p&gt;Sometimes I use vector graphics, simply because they look just &lt;em&gt;slightly&lt;/em&gt; nicer in some cases, and other times, I use bitmap/raster graphics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was wondering, are there any significant performance differences between these two options?&lt;/p&gt;&#xA;" OwnerUserId="17" LastEditorUserId="17" LastEditDate="2016-04-25T15:46:40.090" LastActivityDate="2016-04-25T15:46:40.090" Title="Performance of vector graphics versus bitmap or raster graphics" Tags="&lt;texture&gt;&lt;bitmap-graphics&gt;&lt;vector-graphics&gt;&lt;performance&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="5" />
  <row Id="4" PostTypeId="1" AcceptedAnswerId="29" CreationDate="2015-08-04T18:15:49.023" Score="10" ViewCount="151" Body="&lt;p&gt;I just implemented some interpolated texture sampling by sampling the 4x4 nearest pixels then doing Lagrange interpolation across the x axis to get four values to use Lagrange interpolation on across the y axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this the same as bicubic interpolation or is it different?  Or are there different kinds of bicubic interpolation, and this is just one of them perhaps?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Webgl Shadertoy implementation here and relevant GLSL (WebGL) code below: &lt;a href=&quot;https://www.shadertoy.com/view/MllSzX&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/MllSzX&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;float c_textureSize = 64.0;&#xA;&#xA;float c_onePixel = 1.0 / c_textureSize;&#xA;float c_twoPixels = 2.0 / c_textureSize;&#xA;&#xA;float c_x0 = -1.0;&#xA;float c_x1 =  0.0;&#xA;float c_x2 =  1.0;&#xA;float c_x3 =  2.0;&#xA;&#xA;//=======================================================================================&#xA;vec3 CubicLagrange (vec3 A, vec3 B, vec3 C, vec3 D, float t)&#xA;{&#xA;    return&#xA;        A * &#xA;        (&#xA;            (t - c_x1) / (c_x0 - c_x1) * &#xA;            (t - c_x2) / (c_x0 - c_x2) *&#xA;            (t - c_x3) / (c_x0 - c_x3)&#xA;        ) +&#xA;        B * &#xA;        (&#xA;            (t - c_x0) / (c_x1 - c_x0) * &#xA;            (t - c_x2) / (c_x1 - c_x2) *&#xA;            (t - c_x3) / (c_x1 - c_x3)&#xA;        ) +&#xA;        C * &#xA;        (&#xA;            (t - c_x0) / (c_x2 - c_x0) * &#xA;            (t - c_x1) / (c_x2 - c_x1) *&#xA;            (t - c_x3) / (c_x2 - c_x3)&#xA;        ) +       &#xA;        D * &#xA;        (&#xA;            (t - c_x0) / (c_x3 - c_x0) * &#xA;            (t - c_x1) / (c_x3 - c_x1) *&#xA;            (t - c_x2) / (c_x3 - c_x2)&#xA;        );&#xA;}&#xA;&#xA;//=======================================================================================&#xA;vec3 BicubicTextureSample (vec2 P)&#xA;{&#xA;    vec2 pixel = P * c_textureSize + 0.5;&#xA;&#xA;    vec2 frac = fract(pixel);&#xA;    pixel = floor(pixel) / c_textureSize - vec2(c_onePixel/2.0);&#xA;&#xA;    vec3 C00 = texture2D(iChannel0, pixel + vec2(-c_onePixel ,-c_onePixel)).rgb;&#xA;    vec3 C10 = texture2D(iChannel0, pixel + vec2( 0.0        ,-c_onePixel)).rgb;&#xA;    vec3 C20 = texture2D(iChannel0, pixel + vec2( c_onePixel ,-c_onePixel)).rgb;&#xA;    vec3 C30 = texture2D(iChannel0, pixel + vec2( c_twoPixels,-c_onePixel)).rgb;&#xA;&#xA;    vec3 C01 = texture2D(iChannel0, pixel + vec2(-c_onePixel , 0.0)).rgb;&#xA;    vec3 C11 = texture2D(iChannel0, pixel + vec2( 0.0        , 0.0)).rgb;&#xA;    vec3 C21 = texture2D(iChannel0, pixel + vec2( c_onePixel , 0.0)).rgb;&#xA;    vec3 C31 = texture2D(iChannel0, pixel + vec2( c_twoPixels, 0.0)).rgb;    &#xA;&#xA;    vec3 C02 = texture2D(iChannel0, pixel + vec2(-c_onePixel , c_onePixel)).rgb;&#xA;    vec3 C12 = texture2D(iChannel0, pixel + vec2( 0.0        , c_onePixel)).rgb;&#xA;    vec3 C22 = texture2D(iChannel0, pixel + vec2( c_onePixel , c_onePixel)).rgb;&#xA;    vec3 C32 = texture2D(iChannel0, pixel + vec2( c_twoPixels, c_onePixel)).rgb;    &#xA;&#xA;    vec3 C03 = texture2D(iChannel0, pixel + vec2(-c_onePixel , c_twoPixels)).rgb;&#xA;    vec3 C13 = texture2D(iChannel0, pixel + vec2( 0.0        , c_twoPixels)).rgb;&#xA;    vec3 C23 = texture2D(iChannel0, pixel + vec2( c_onePixel , c_twoPixels)).rgb;&#xA;    vec3 C33 = texture2D(iChannel0, pixel + vec2( c_twoPixels, c_twoPixels)).rgb;    &#xA;&#xA;    vec3 CP0X = CubicLagrange(C00, C10, C20, C30, frac.x);&#xA;    vec3 CP1X = CubicLagrange(C01, C11, C21, C31, frac.x);&#xA;    vec3 CP2X = CubicLagrange(C02, C12, C22, C32, frac.x);&#xA;    vec3 CP3X = CubicLagrange(C03, C13, C23, C33, frac.x);&#xA;&#xA;    return CubicLagrange(CP0X, CP1X, CP2X, CP3X, frac.y);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="56" LastEditorUserId="231" LastEditDate="2015-08-10T05:07:39.443" LastActivityDate="2015-08-10T10:40:04.633" Title="Is a cubic Lagrange interpolation tensor product the same as bicubic interpolation?" Tags="&lt;texture&gt;&lt;interpolation&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="5" PostTypeId="1" AcceptedAnswerId="87" CreationDate="2015-08-04T18:25:11.677" Score="10" ViewCount="128" Body="&lt;p&gt;I have an OpenGL application which uses stencil tests quite extensively to render irregular shapes (a bit like a simple 2-D &lt;a href=&quot;https://en.wikipedia.org/wiki/Constructive_solid_geometry&quot;&gt;CSG&lt;/a&gt;). If I could find out how many fragments passed the stencil test and were actually rendered, this would be very helpful in simplifying some computations down the line. Specifically, it would allow me to determine the area of the rendered shape for free instead of having to approximate it with a Monte Carlo simulation later on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that there is a similar concept for primitives emitted from the geometry shader, called &lt;a href=&quot;https://www.opengl.org/wiki/Transform_Feedback&quot;&gt;transform feedback&lt;/a&gt;. I'd like to know if a similar concept exists for fragments and the stencil test.&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2015-08-07T09:06:45.063" Title="Is it possible to find out how many fragments made it through the stencil test?" Tags="&lt;opengl&gt;&lt;stencil-test&gt;" AnswerCount="2" CommentCount="7" FavoriteCount="1" />
  <row Id="6" PostTypeId="1" AcceptedAnswerId="18" CreationDate="2015-08-04T18:31:07.350" Score="16" ViewCount="476" Body="&lt;p&gt;I've heard that the quality of a monte carlo ray tracer (based on path tracing algorithms) is much more realistic than a distributed (stochastic) engine. I try to understand why, but I'm just at the beginning. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to dive into this topic and understand the basics, can someone point me into the right direction? What part of the algorithm leads into a more realistic render result? &lt;/p&gt;&#xA;" OwnerUserId="18" LastEditorUserId="18" LastEditDate="2015-08-11T09:22:47.723" LastActivityDate="2015-08-11T09:22:47.723" Title="Why does monte carlo ray tracing perform better than distributed ray tracing?" Tags="&lt;raytracing&gt;&lt;algorithm&gt;&lt;monte-carlo&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="3" />
  <row Id="8" PostTypeId="2" ParentId="3" CreationDate="2015-08-04T18:37:00.853" Score="8" Body="&lt;p&gt;There might be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Less technical answer:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're building a website or another application where you have nothing to do with the graphics programming then the answer is probably yes. The underlying APIs will try to guess how to render them and cache them efficiently. However, as your application runs and the API sometimes guesses incorrectly it may have to re-render things and affect performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More technical:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Keep in mind that unless you're using one of the newest GPUs and a library for drawing the vector paths on the GPU then it's all bitmap textures being rendered by the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll consider the typical case where vector graphics are rendered to textures. Here the performance would depend on your toolchain, whether your application is dynamically creating textures from the vector assets, and whether the graphics are viewed at various zoom levels. There are two issues involved: resources and texture generation. If you're only displaying the graphics at a static size then I would say there is no difference and perhaps your toolchain can convert the assets into bitmap graphics before runtime. However, if they are being displayed in various sizes or in a 3D world then you'll need mip mapped textures which take more memory. They'll take a lot of memory if you really want to see their fidelity 'up close' with a larger texture. Having your application dynamically create the larger textures only when necessary will save memory but is costly at runtime and will affect performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope this helps.&lt;/p&gt;&#xA;" OwnerUserId="59" LastActivityDate="2015-08-04T18:37:00.853" CommentCount="0" />
  <row Id="9" PostTypeId="1" AcceptedAnswerId="14" CreationDate="2015-08-04T18:39:29.303" Score="12" ViewCount="250" Body="&lt;p&gt;A few years ago I tried to implement &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html&quot;&gt;this GPU Gem&lt;/a&gt; in OpenGL to generate 3D procedural terrain using &lt;a href=&quot;https://en.wikipedia.org/wiki/Marching_cubes&quot;&gt;Marching Cubes&lt;/a&gt;. The article suggests to implement Marching Cubes in a geometry shader to maximum efficiency. This means I need to run the shader once for every voxel in the domain, and it'll generate the entire geometry in that cell.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One problem I stumbled upon was how to get the geometry shader running without actually having anything to render outside of that shader. My solution (which seemed rather hacky) was to render a point in each cell, discard it with the geometry shader and emit my triangles instead. I never found a proper solution and this workaround remained in the final code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, is there anyway to tell OpenGL to start a rendering pass from the geometry shader without any input geometry? Or will I always have to send some dummy points to the GPU to get things running.&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2015-08-04T19:20:23.330" Title="Is there any way to generate primitives in a geometry shader without any input geometry?" Tags="&lt;opengl&gt;&lt;geometry-shader&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="10" PostTypeId="2" ParentId="3" CreationDate="2015-08-04T18:40:58.597" Score="13" Body="&lt;p&gt;As TheBuzzSaw said, it does depend on lots of things, including implementations of the rasterized graphics vs the vector graphics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some high performance vector graphics methods that are rendered using traditionally rasterization methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Loop and Blinn show how to render a vector graphics quadratic bezier curve by rendering a single triangle, and using the texture coordinates in a pixel shader to say whether a pixel is above or below the curve:&#xA;&lt;a href=&quot;http://www.msr-waypoint.net/en-us/um/people/cloop/LoopBlinn05.pdf&quot;&gt;http://www.msr-waypoint.net/en-us/um/people/cloop/LoopBlinn05.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic idea is that you set your triangle corner positions to be the 3 control point positions, and you set the texture coordinates at each corner to be (0,0), (0.5,0) and (1,1) respectively.  In your shader, if the interpolated texture coordinate (x*x-y) is &amp;lt; 0, the pixel is underneath the curve, else it's above the curve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see a faux implementation of it on shadertoy here:&#xA;&lt;a href=&quot;https://www.shadertoy.com/view/4tj3Dy&quot;&gt;https://www.shadertoy.com/view/4tj3Dy&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the second method, here's a method from Valve, where distances to a shape are stored in a texture, instead of pixel data, allowing vector graphics to be drawn by using texture sampling.  The decoding is so simple, it could be implemented even on fixed function hardware using only an alpha test!&#xA;&lt;a href=&quot;http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf&quot;&gt;http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To give you an idea of how well the second technique works, this 1024x768 mustache image was generated from a 64x32 source image that had a single color channel! (aka 2KB uncompressed)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/wlC0y.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wlC0y.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also wrote some stuff about it on my blog:&#xA;&lt;a href=&quot;http://blog.demofox.org/2014/06/30/distance-field-textures/&quot;&gt;http://blog.demofox.org/2014/06/30/distance-field-textures/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is some sample OpenCL code to show just how simple it is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float alpha = read_imagef(tex3dIn, g_textureSampler, textureCoords).w;&#xA;float3 color = (alpha &amp;lt; 0.5f) ? (float3)(1.0f) : (float3)(0.0f);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Both of these techniques are super fast, and blur the line between vector and rasterized graphics a bit.  They are rendered using rasterization techniques, but have zooming / scaling properties like vector graphics techniques.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-04T18:58:30.893" LastActivityDate="2015-08-04T18:58:30.893" CommentCount="4" />
  <row Id="11" PostTypeId="2" ParentId="2" CreationDate="2015-08-04T18:44:21.417" Score="11" Body="&lt;p&gt;Updating an area of memory in the graphics device (a texture, buffer, and the like) is not quite the same as changing a rendering state. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What makes a render state change expensive is the amount of work the driver has to do to validate the new state(s) and reorder the pipeline. This will most likely also incur some synchronization between CPU and graphics device. However, the amount of data transferred between the devices should be small for a state change (probably just a few commands).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a texture/buffer update on the other hand, the main cost lies in the data transfer itself. In theory, unless you're reading the texture data back to the CPU after the update, there should be no synchronization or pipeline stalls. However, another aspect should be considered: API overhead. Even if the amount of data you're sending to the graphics device is small, if you do that often enough, eventually the cost of communicating with the driver/device will become greater then the cost of the data transfer. That's another reason why batching is so important when optimizing a renderer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in your case, the best approach, it seems to me, would be to keep a system-memory copy of the texture that you update whenever new data arrives. Set a dirty flag and consolidate as much updates as possible into one &lt;code&gt;glTexSubImage&lt;/code&gt; for the whole texture (or a large sequential portion of it). You can also play with &lt;a href=&quot;https://www.opengl.org/wiki/Pixel_Buffer_Object&quot;&gt;Pixel Buffer Objects&lt;/a&gt; and try to do asynchronous data transfer to reduce pipeline stalls as much as possible. If you can implement some kind of double buffering, then you can write to one copy of the texture while the other is being rendered with. &lt;a href=&quot;http://www.songho.ca/opengl/gl_pbo.html#unpack&quot;&gt;This tutorial&lt;/a&gt; explores that scenario. That's my intuitive approach, I'd try to reduce the number of API calls and &quot;batch&quot; the texture updates. That being said, this is very speculative, and you'd have to profile and compare that to other approaches, like doing several small updates, to know for sure which is the most performant in your usage case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a side note, this presentation by NVidia is also relevant and provides a lot of good insights: &lt;a href=&quot;http://gdcvault.com/play/1020791/&quot;&gt;Approaching Zero Driver Overhead in OpenGL&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="54" LastEditDate="2015-08-04T19:19:15.453" LastActivityDate="2015-08-04T19:19:15.453" CommentCount="1" />
  <row Id="12" PostTypeId="1" AcceptedAnswerId="21" CreationDate="2015-08-04T18:50:16.343" Score="16" ViewCount="332" Body="&lt;p&gt;Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Stencil_buffer&quot;&gt;states&lt;/a&gt; that a stencil buffer is some arbitrary buffer a shader can use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it hints that it's used for clipping, or otherwise &quot;tightly binding&quot; the depth and pixel buffers, slightly contradicting itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does the stencil buffer &lt;em&gt;really&lt;/em&gt; do, and how is it practically used in modern applications?&lt;/p&gt;&#xA;" OwnerUserId="71" LastActivityDate="2015-08-09T17:31:32.827" Title="What is a stencil buffer?" Tags="&lt;stencil-buffer&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="2" />
  <row Id="13" PostTypeId="2" ParentId="6" CreationDate="2015-08-04T18:57:38.277" Score="4" Body="&lt;p&gt;In &lt;strong&gt;Distributed ray tracing&lt;/strong&gt;, You stochastically sample many rays in directions which &lt;em&gt;may or may not&lt;/em&gt; be preferred by the BRDF. Whereas, in &lt;strong&gt;Monte Carlo ray tracing&lt;/strong&gt; or simply path tracing, you sample only one ray &lt;em&gt;in a direction preferred by the BRDF&lt;/em&gt;. So, there are two obvious advantages Path Tracing would have:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Computationally less expensive. Which means with the same computing power you have the freedom of calculating over more object hits as compared to distributed ray tracing where there are multiple rays.&lt;/li&gt;&#xA;&lt;li&gt;Less noise. Distributed ray tracing samples rays in directions that might not be preferred by the BRDF, therefore introducing unwanted artifacts. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;And so, path tracing would give you better results.&lt;/p&gt;&#xA;" OwnerUserId="77" LastActivityDate="2015-08-04T18:57:38.277" CommentCount="2" />
  <row Id="14" PostTypeId="2" ParentId="9" CreationDate="2015-08-04T18:58:03.393" Score="14" Body="&lt;p&gt;No, there is not really a way to do that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A geometry shader invocation requires an input primitive and generates 0 or more output primitives. &lt;strong&gt;Without an input primitive, there is not really a way to actually &lt;em&gt;invoke&lt;/em&gt; the geometry shader&lt;/strong&gt;. Of course you can stretch the limits of the geometry shader's maximum number of output primitives for each input primitive (don't know the practical limits right now, should be on the order of thousands maybe). So you maybe could generate 1024 triangles for each point, but you always have to have &lt;em&gt;some&lt;/em&gt; input primitives.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you &lt;em&gt;don't need&lt;/em&gt; however is an actual notion of geometry. You don't really have to render 3D points at any reasonable location, they could as well have just some abstract index or texture coordinates or whatever as attributes and not necessarily a meaningful 3D position. &lt;strong&gt;Noone dictates what attributes your vertices have. And you can even render vertices &lt;em&gt;without any&lt;/em&gt; attributes&lt;/strong&gt;. But you have to render &lt;em&gt;some&lt;/em&gt; primitives to invoke the geometry shader on them, even if those primitives don't have any actual attributes (how you then compute your output geometry in the geometry shader is a different question, though).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what you actually did, rendering a point for each grid cell and generating the marching cubes triangles for that cell from it, is exactly the straight-forward approach. Of course what attributes this cell contains is up to you, it could be a 3D position, a texcoord into a 3D texture, whatever, but those are the grid cells you render. Purely semantically spoken, you don't actually &quot;discard&quot; those points and then &quot;replace&quot; them by triangles, you &quot;convert&quot; each point into a set of triangles. That's exactly what the geometry shader is for and there's nothing &quot;hacky&quot; or &quot;improper&quot; about it. &lt;strong&gt;Noone says the geometry shader has to generate the same output primitive type as was input to be &quot;proper&quot;&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;What you can do to achieve a largely input-less way of rendering your voxel grid (and this might be what you were actually asking for) would be to just draw a set of attributeless points. This means you don't need any attribute arrays at all and just disable them all and invoke a simple &lt;code&gt;glDrawArrays&lt;/code&gt; with the number of cells you need. Then in the vertex shader or the geometry shader you can generate the necessary 3D grid cell index with a little index magic from the input vertex ID (i.e. &lt;code&gt;gl_VertexID&lt;/code&gt;, which is the only information you have) and then compute your marching cubes geometry from a lookup into the 3D volume texture (or whatever datastructure).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in retrospect I should relativate my statement from the beginning: &lt;strong&gt;You can't generate primitives without any input &lt;em&gt;primitives&lt;/em&gt;, but you can generate them without any input &lt;em&gt;geometry&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;&#xA;" OwnerUserId="6" LastEditorUserId="6" LastEditDate="2015-08-04T19:20:23.330" LastActivityDate="2015-08-04T19:20:23.330" CommentCount="0" />
  <row Id="15" PostTypeId="1" AcceptedAnswerId="1508" CreationDate="2015-08-04T18:58:36.040" Score="16" ViewCount="192" Body="&lt;p&gt;In 1978 Edwin Catmull and Jim Clark defined the recursive subdivision process that bears their names, and although those principles are applicable still today, what advances have occurred as far as optimization and accuracy?&lt;/p&gt;&#xA;" OwnerUserId="70" LastActivityDate="2015-09-16T06:05:56.157" Title="What subdivision algorithm advances have occurred since Catmull-Clark?" Tags="&lt;subdivision&gt;" AnswerCount="1" CommentCount="10" />
  <row Id="16" PostTypeId="2" ParentId="12" CreationDate="2015-08-04T19:18:52.277" Score="5" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;A stencil buffer contains per-pixel integer data which is used to add&#xA;  more control over which pixels are rendered.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;A stencil buffer operates similarly to a depth buffer. So similarly,&#xA;  that stencil data is stored in a depth buffer. While depth data&#xA;  determines which pixel is closest to the camera, stencil data can be&#xA;  used as a more general purpose per-pixel mask for saving or discarding&#xA;  pixels. To create the mask, use a stencil function to compare a&#xA;  reference stencil value -- a global value -- to the value in the&#xA;  stencil buffer each time a pixel is rendered.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/bb976074.aspx&quot; rel=&quot;nofollow&quot;&gt;https://msdn.microsoft.com/en-us/library/bb976074.aspx&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The general idea is that you can set a value for each pixel and then set the operation (&lt;code&gt;GREATER&lt;/code&gt;, &lt;code&gt;SMALLER&lt;/code&gt;, &lt;code&gt;EQUAL&lt;/code&gt; etc), when this evaluates to true the pixel is shaded and shown.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One application is realtime reflection, when rendering the reflection texture the stencil buffer is used to create a mask for where the reflection is applied. For example only apply the reflection to the glass material shaded triangles. It can also be used to create arbitrary two dimensional masks for the whole framebuffer.&lt;/p&gt;&#xA;" OwnerUserId="64" LastEditorUserId="71" LastEditDate="2015-08-04T19:30:54.387" LastActivityDate="2015-08-04T19:30:54.387" CommentCount="0" />
  <row Id="18" PostTypeId="2" ParentId="6" CreationDate="2015-08-04T19:33:04.573" Score="19" Body="&lt;p&gt;The term &quot;distributed ray tracing&quot; was originally coined by Robert Cook in &lt;a href=&quot;http://artis.inrialpes.fr/Enseignement/TRSA/CookDistributed84.pdf&quot;&gt;this 1984 paper&lt;/a&gt;. His observation was that in order to perform anti-aliasing in a ray-tracer, the renderer needs to perform spatial upsampling - that is, to take more samples (i.e. shoot more rays) than the number of pixels in the image and combine their results. One way to do this is to shoot multiple rays within a pixel and average their color values, for example. However, if the renderer is already tracing multiple rays per pixel anyway to obtain an anti-aliased image, then these rays can also be &quot;distributed&quot; among additional dimensions than just the pixel position to sample effects that could not be captured by a single ray. The important bit is that this comes without any additional cost on top of spatial upsampling, since you're already tracing those additional rays anyway. For example, if you shoot multiple rays within your pixel to compute an anti-aliased result, you can get motion blur absolutely for free if you also use a different time value for each ray (or soft shadows if they connect to a different point on the light source, or depth of field if they use a different starting point on the aperture, etc.).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Monte Carlo ray tracing is a term that is slightly ambiguous. In most cases, it refers to rendering techniques that solve the &lt;a href=&quot;http://www.dca.fee.unicamp.br/~leopini/DISCIPLINAS/IA725/ia725-12010/kajiya-SIG86-p143.pdf&quot;&gt;rendering equation&lt;/a&gt;, introduced by Jim Kajiya in 1986, using Monte Carlo integration. Practically all modern rendering techniques that solve the rendering equation, such as path tracing, bidirectional path tracing, progressive photon mapping and VCM, can be classified as Monte Carlo ray tracing techniques. The idea of Monte Carlo integration is that we can compute the integral of any function by randomly choosing points in the integration domain and averaging the value of the function at these points. At a high level, in Monte Carlo ray tracing we can use this technique to integrate the amount of light arriving at the camera within a pixel in order to compute the pixel value. For example, a path tracer does this by randomly picking a point within the pixel to shoot the first ray, and then continues to randomly pick a direction to continue on the surface it lands on, and so forth. We could also randomly pick a position on the time axis if we want to do motion blur, or randomly pick a point on the aperture if wanted to do depth of field, or...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this sounds very similar to distributed ray tracing, that's because it is! We can think of distributed ray tracing as a very informal description of a Monte Carlo algorithm that samples certain effects like soft shadows. Cook's paper lacks the mathematical framework to really reason about it properly, but you could certainly implement distributed ray tracing using a simple Monte Carlo renderer. It's worth noting that distributed ray tracing lacks any description of global illumination effects, which are naturally modeled in the rendering equation (it should be mentioned that Kajiya's paper was published two years after Cook's paper).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can think of Monte Carlo ray tracing as being a more general version of distributed ray tracing. Monte Carlo ray tracing contains a general mathematical framework that allows you to handle practically any effect, including those mentioned in the distributed ray tracing paper.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These days, &quot;distributed ray tracing&quot; is not really a term that's used to refer to the original algorithm. More often you will hear it in conjunction with &quot;distribution effects&quot;, which are simply effects such as motion blur, depth of field or soft shadows that cannot be handled with a single-sample raytracer.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-04T19:33:04.573" CommentCount="1" />
  <row Id="19" PostTypeId="1" AcceptedAnswerId="26" CreationDate="2015-08-04T19:40:54.970" Score="20" ViewCount="335" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Perlin_noise&quot;&gt;Perlin noise&lt;/a&gt; is one of the most popular procedural noise functions. Perlin later developed &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex_noise&quot;&gt;Simplex noise&lt;/a&gt; which improves on some of the shortcomings of Perlin noise, notably its inefficiency in higher dimensions and directional artefacts (Wikipedia lists five advantages of Simplex noise). Still, Perlin noise appears to be widely used. I can imagine that the main reason is that Simplex noise is conceptually much more difficult to understand, but by now there should be enough implementations so that you don't have to re-implement it yourself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does Perlin noise have any advantages over Simplex noise? When picking a noise function, would I ever choose Perlin instead of Simplex?&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2015-08-04T21:35:01.103" Title="Does the original Perlin noise ever have any advantage over Simplex noise?" Tags="&lt;noise&gt;&lt;procedural-generation&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="5" />
  <row Id="20" PostTypeId="2" ParentId="12" CreationDate="2015-08-04T19:44:03.353" Score="10" Body="&lt;p&gt;The stencil buffer is an unsigned integer buffer, usually 8-bit nowadays, where you can fill per-pixel infomation as you wish based on the use of various operations (&lt;a href=&quot;https://www.opengl.org/wiki/Stencil_Test#Stencil_operations&quot;&gt;OpenGL Ops here for example&lt;/a&gt;) following a stencil test. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The stencil test is simply a per-pixel operation in which the current pixel stencil value is tested against the content of the stencil buffer. You set the condition to which the stencil test is passed ( e.g. ALWAYS, LESS etc. ). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can decide what happens to the stencil value depending on the outcome of the test (in OpenGL using the operation I linked you above) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The stencil buffer has quite a few uses, the first ones that come to mind being:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It can be used as a mask to selectively render certain part of the framebuffer and discard the rest. This, for example, is useful for reflections, but also lots more. When dealing with multiple resolution rendering it can be used to mark up edges that need to be rendered at an higher res. There are plenty of masking usages really. &lt;/li&gt;&#xA;&lt;li&gt;A very simple overdraw filter for example. You can set the stencil test to always pass and always increment the value. The value you end up with in the stencil buffer represents your overdraw. [*] &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In the past it has been used also for shadowing techniques such as Shadow volume.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The poor stencil buffer is often quite underestimated, but can be a good weapon in a graphics programmer's armory. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;[*] Value to be read back in a post-process using the right SRV! &lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: &#xA;Also, worth mentioning that in Shader Model 5.1 (so D3D11.3 and D3D12) you have access to the stencil reference value via &lt;code&gt;SV_StencilRef&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-04T20:21:59.477" LastActivityDate="2015-08-04T20:21:59.477" CommentCount="0" />
  <row Id="21" PostTypeId="2" ParentId="12" CreationDate="2015-08-04T19:45:33.780" Score="16" Body="&lt;p&gt;The stencil buffer definition by Wikipedia is indeed not great, it focuses too much on the details of modern implementations (OpenGL). I find &lt;a href=&quot;https://en.wikipedia.org/wiki/Stencil_(disambiguation)&quot; rel=&quot;nofollow&quot;&gt;the disambiguated version&lt;/a&gt; easier to understand:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;A stencil is a template used to draw or paint identical letters, symbols, shapes, or patterns every time it is used. The design produced by such a template is also called a stencil.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;That's what stencil meant before Computer Graphics. If you type &lt;em&gt;stencil&lt;/em&gt; on Google Images, this is one of the first results:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/bHD7z.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bHD7z.jpg&quot; alt=&quot;Stencil mask&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, it is simply a mask or pattern that can be used to &quot;paint&quot; the negative of the pattern onto something.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;em&gt;stencil buffer&lt;/em&gt; works in the exact same way. One can fill the stencil buffer with a selected pattern by doing a stencil render pass, then set the appropriate stencil function which will define how the pattern is to be interpreted on subsequent drawings, then render the final scene. Pixels that fall into the rejected areas of the stencil mask, according to the compare function, are not drawn.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;When it comes to implementing the stencil buffer, sometimes it is indeed coupled with the depth buffer. Most graphics hardware uses a 1 byte (8 bits) stencil buffer, which is enough for most applications. Depth buffers are usually implemented using 3 bytes (24 bits), which again is normally enough for most kinds of 3D rendering. So it is only logical to pack the 8 bits of the stencil buffer with the other 24 of the depth buffer, making it possible to store each depth + stencil pixel into a 32 bit integer. That's what Wikipedia meant by:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The depth buffer and stencil buffer often share the same area in the RAM of the graphics hardware.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;One application in which the stencil buffer used to be king was for shadow rendering, in a technique called &lt;a href=&quot;https://en.wikipedia.org/wiki/Shadow_volume&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;shadow volumes&lt;/em&gt;&lt;/a&gt;, or sometimes also appropriately called &lt;em&gt;stencil shadows&lt;/em&gt;. This was a very clever use of the buffer, but nowadays most of the rendering field seems to have shifted towards depth-based shadow maps.&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="231" LastEditDate="2015-08-09T17:31:32.827" LastActivityDate="2015-08-09T17:31:32.827" CommentCount="4" />
  <row Id="22" PostTypeId="2" ParentId="3" CreationDate="2015-08-04T19:48:16.840" Score="6" Body="&lt;p&gt;There are a few ways of rendering vector graphics. As TheBuzzSaw mentions, NVIDIA has an extension that can render general paths quite quickly (but of course it only works on NVIDIA GPUs). And Alan Wolfe mentions the implicit surface methods (Loop-Blinn/distance fields), which define a function which says whether you're inside or outside a shape, and color the pixels based on that function.  Another method is stencil-and-cover, where you render the path into a stencil buffer and use the even-odd count to determine whether the path covers a pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, however, the tradeoff is that rendering raster will be faster, but is more susceptible to aliasing (even distance fields break down at low and high scales). Rendering paths requires a lot of setup but in theory can be scaled to any resolution.&lt;/p&gt;&#xA;" OwnerUserId="1" LastActivityDate="2015-08-04T19:48:16.840" CommentCount="0" />
  <row Id="23" PostTypeId="1" AcceptedAnswerId="25" CreationDate="2015-08-04T20:05:43.820" Score="12" ViewCount="298" Body="&lt;p&gt;I have a point cloud that is being rendered to the screen. Each point has its position and color as well as an ID.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was asked to render the IDs for each point to a texture so I created a FBO and attached two textures, one for color and one for depth. I created the necessary VAO and VBO for this off-screen rendering and uploaded for each point its position and ID.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the rendering to the FBO is done, I read the pixels of the color-texture with &lt;code&gt;glReadPixels()&lt;/code&gt; to see what the values are, but they seem to be all cleared out, i.e., the value they have is the same as &lt;code&gt;glClearColor()&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way I can debug what it is being render to the color texture of my FBO? Any tips that you may provide are very welcomed.&lt;/p&gt;&#xA;" OwnerUserId="116" LastEditorUserId="16" LastEditDate="2015-08-07T15:58:52.093" LastActivityDate="2015-08-07T15:58:52.093" Title="How can I debug what is being rendered to a Frame Buffer Object in OpenGL?" Tags="&lt;opengl&gt;&lt;debugging&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="24" PostTypeId="2" ParentId="1" CreationDate="2015-08-04T20:08:24.347" Score="5" Body="&lt;p&gt;Assuming your optimal viewing angle is parallel to the surface of the display and the pyramid is made from faces that are 45 degrees to its virtual (non-existant) base, it's actually just a simple non-transformed image (besides the reflection).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1:1 projection. No transformations. No scaling. &lt;/p&gt;&#xA;" OwnerUserId="20" LastActivityDate="2015-08-04T20:08:24.347" CommentCount="2" />
  <row Id="25" PostTypeId="2" ParentId="23" CreationDate="2015-08-04T20:15:17.410" Score="18" Body="&lt;p&gt;Generally to see what is being rendered in the various steps of your pipeline I'd suggest the use of a tool for frame analysis. These usually provide you with a view on the content of each buffer for each API call and this can help you in your situation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A very good one is &lt;a href=&quot;https://github.com/baldurk/renderdoc&quot;&gt;Renderdoc&lt;/a&gt; which is both completely free and opensource. Also it is actively supported. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another one is &lt;a href=&quot;https://software.intel.com/en-us/gpa/details&quot;&gt;Intel GPA&lt;/a&gt; that unfortunately, according to its webpage, support up to OGL 3.3 Core. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just for the sake of adding one more, I used to use &lt;a href=&quot;http://www.gremedy.com/&quot;&gt;gDEBugger&lt;/a&gt;, but it has passed long since the last update. This has evolved into &lt;a href=&quot;http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/&quot;&gt;AMD CodeXL&lt;/a&gt; that unfortunately I never used.&lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-04T21:18:32.987" LastActivityDate="2015-08-04T21:18:32.987" CommentCount="3" />
  <row Id="26" PostTypeId="2" ParentId="19" CreationDate="2015-08-04T21:35:01.103" Score="20" Body="&lt;p&gt;To directly answer the question: Simplex noise is &lt;a href=&quot;https://www.google.com/patents/US6867776&quot;&gt;patented&lt;/a&gt;, whereas Perlin noise is not. Other than that, Simplex noise has many advantages that are already mentioned in your question, and apart from the slightly increased implementation difficulty, it is the better algorithm of the two.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe the reason why many people still pick Perlin noise is simply because it's more widely known. It's also worth noting that Perlin noise is very frequently confused with a combination of &lt;a href=&quot;https://en.wikipedia.org/wiki/Value_noise&quot;&gt;value noise&lt;/a&gt; and Fractal Brownian Motion (FBM).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Perlin noise, Simplex noise and value noise are all methods for synthesizing coherent noise. On the other hand, FBM (sometimes called &quot;FBM noise&quot;), is what is used when adding multiple layers of noise on top of each other at different scales to obtain more complex functions. The combination of FBM and value noise is simple to implement and can be very useful for terrain synthesis, procedural clouds and friends, and it is quite popular. However, it tends to be mistakenly labelled Perlin noise, misleadingly adding to its popularity.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-04T21:35:01.103" CommentCount="3" />
  <row Id="27" PostTypeId="1" CreationDate="2015-08-04T21:47:25.783" Score="14" ViewCount="208" Body="&lt;p&gt;Both Vulkan and DirectX12 are claimed to be usable in a thread-safe manner. People seem to be excited about that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this considered such a huge feature? The &quot;real&quot; processing gets thrown over the memory bridge on a separate processing unit anyway. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also if it is so big, why is it not until now that a thread safe Graphics API came out?&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-08-05T08:07:39.323" Title="Why is thread safety such a huge deal for Graphics APIs?" Tags="&lt;directx12&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="0" />
  <row Id="28" PostTypeId="2" ParentId="27" CreationDate="2015-08-04T22:20:07.923" Score="9" Body="&lt;p&gt;The main gain would be that it would be easier to divide CPU tasks into multiple threads, without having to solve all the difficult issues with accessing the graphics API. Normally you either would have to make the context current (which might have bad performance implications) or provide a queue and call the graphics api in a single thread. I don't think that any performance is gained this way, because the GPU indeed processes them sequentially anyway, but it makes the developer job a lot easier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason that it was not done until now probably is because directx and opengl were created in a time where multithreading was not really apparent. Also the Khronos board is very conservative in changing the API. Their view on Vulkan is also that it will coexist next to OpenGL, because both serve different purposes. It probably was not until recently that paralism became so important, as consumers get access to more and more processors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I don't mean that no performance is gained from doing work in multiple CPUs, it is not useful to split your calls into multiple threads to create textures/shaders faster. Rather the performance is gained due to having more processors busy and keeping the gpu busy with things to perform.&lt;/p&gt;&#xA;" OwnerUserId="64" LastEditorUserId="64" LastEditDate="2015-08-04T22:29:59.053" LastActivityDate="2015-08-04T22:29:59.053" CommentCount="1" />
  <row Id="29" PostTypeId="2" ParentId="4" CreationDate="2015-08-04T23:00:18.923" Score="8" Body="&lt;p&gt;It turns out that no, while you can use bicubic Lagrange interpolation for bicubic texture sampling, it isn't the highest quality option, and probably not actually likely to be used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cubic hermite splines are a better tool for the job.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lagrange interpolation will make a curve that passes through the data points, thus preserving C0 continuity, but hermite splines preserve the derivatives at the edges while also passing through the data points, thus preserving C1 continuity and looking much better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This question has some great info on cubic hermite splines:&#xA;&lt;a href=&quot;http://dsp.stackexchange.com/questions/18265/bicubic-interpolation&quot;&gt;http://dsp.stackexchange.com/questions/18265/bicubic-interpolation&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the cubic hermite version of the code I posted in the question:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;//=======================================================================================&#xA;vec3 CubicHermite (vec3 A, vec3 B, vec3 C, vec3 D, float t)&#xA;{&#xA;    float t2 = t*t;&#xA;    float t3 = t*t*t;&#xA;    vec3 a = -A/2.0 + (3.0*B)/2.0 - (3.0*C)/2.0 + D/2.0;&#xA;    vec3 b = A - (5.0*B)/2.0 + 2.0*C - D / 2.0;&#xA;    vec3 c = -A/2.0 + C/2.0;&#xA;    vec3 d = B;&#xA;&#xA;    return a*t3 + b*t2 + c*t + d;&#xA;}&#xA;&#xA;//=======================================================================================&#xA;vec3 BicubicHermiteTextureSample (vec2 P)&#xA;{&#xA;    vec2 pixel = P * c_textureSize + 0.5;&#xA;&#xA;    vec2 frac = fract(pixel);&#xA;    pixel = floor(pixel) / c_textureSize - vec2(c_onePixel/2.0);&#xA;&#xA;    vec3 C00 = texture2D(iChannel0, pixel + vec2(-c_onePixel ,-c_onePixel)).rgb;&#xA;    vec3 C10 = texture2D(iChannel0, pixel + vec2( 0.0        ,-c_onePixel)).rgb;&#xA;    vec3 C20 = texture2D(iChannel0, pixel + vec2( c_onePixel ,-c_onePixel)).rgb;&#xA;    vec3 C30 = texture2D(iChannel0, pixel + vec2( c_twoPixels,-c_onePixel)).rgb;&#xA;&#xA;    vec3 C01 = texture2D(iChannel0, pixel + vec2(-c_onePixel , 0.0)).rgb;&#xA;    vec3 C11 = texture2D(iChannel0, pixel + vec2( 0.0        , 0.0)).rgb;&#xA;    vec3 C21 = texture2D(iChannel0, pixel + vec2( c_onePixel , 0.0)).rgb;&#xA;    vec3 C31 = texture2D(iChannel0, pixel + vec2( c_twoPixels, 0.0)).rgb;    &#xA;&#xA;    vec3 C02 = texture2D(iChannel0, pixel + vec2(-c_onePixel , c_onePixel)).rgb;&#xA;    vec3 C12 = texture2D(iChannel0, pixel + vec2( 0.0        , c_onePixel)).rgb;&#xA;    vec3 C22 = texture2D(iChannel0, pixel + vec2( c_onePixel , c_onePixel)).rgb;&#xA;    vec3 C32 = texture2D(iChannel0, pixel + vec2( c_twoPixels, c_onePixel)).rgb;    &#xA;&#xA;    vec3 C03 = texture2D(iChannel0, pixel + vec2(-c_onePixel , c_twoPixels)).rgb;&#xA;    vec3 C13 = texture2D(iChannel0, pixel + vec2( 0.0        , c_twoPixels)).rgb;&#xA;    vec3 C23 = texture2D(iChannel0, pixel + vec2( c_onePixel , c_twoPixels)).rgb;&#xA;    vec3 C33 = texture2D(iChannel0, pixel + vec2( c_twoPixels, c_twoPixels)).rgb;    &#xA;&#xA;    vec3 CP0X = CubicHermite(C00, C10, C20, C30, frac.x);&#xA;    vec3 CP1X = CubicHermite(C01, C11, C21, C31, frac.x);&#xA;    vec3 CP2X = CubicHermite(C02, C12, C22, C32, frac.x);&#xA;    vec3 CP3X = CubicHermite(C03, C13, C23, C33, frac.x);&#xA;&#xA;    return CubicHermite(CP0X, CP1X, CP2X, CP3X, frac.y);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is a picture showing the difference between sampling methods.  From left to right: Nearest Neighbor, Bilinear, Lagrange Bicubic, Hermite Bicubic&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/h0Ot0.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/h0Ot0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="231" LastEditDate="2015-08-10T10:40:04.633" LastActivityDate="2015-08-10T10:40:04.633" CommentCount="8" />
  <row Id="30" PostTypeId="2" ParentId="23" CreationDate="2015-08-04T23:43:22.107" Score="6" Body="&lt;p&gt;In addition to cifz's response, another way to visualise FBOs which doesn't cost very much code is to use glBlitFramebuffer() to transfer pixels from a framebuffer to a window.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// XXX WARNING: Untested code follows&#xA;&#xA;// Blit from fbo...&#xA;glBindFramebuffer(GL_READ_FRAMEBUFFER, fbo);&#xA;// ...to the front buffer.&#xA;glBindFramebuffer(GL_WRITE_FRAMEBUFFER, GL_FRONT);&#xA;&#xA;GLsizei HalfWindowWidth = (GLsizei)(WindowWidth / 2.0f);&#xA;GLsizei HalfWindowHeight = (GLsizei)(WindowHeight / 2.0f);&#xA;&#xA;// Blit attachment 0 to the lower-left quadrant of the window&#xA;glReadBuffer(GL_COLOR_ATTACHMENT0);&#xA;glBlitFramebuffer(0, 0, FboWidth, FboHeight,&#xA;                  0, 0, HalfWindowWidth, HalfWindowHeight,&#xA;                  GL_COLOR_BUFFER_BIT, GL_LINEAR);&#xA;&#xA;// Blit attachment 1 to the lower-right quadrant of the window&#xA;glReadBuffer(GL_COLOR_ATTACHMENT1);&#xA;glBlitFramebuffer(0, 0, FboWidth, FboHeight,&#xA;                  HalfWindowWidth, 0, WindowWidth, HalfWindowHeight,&#xA;                  GL_COLOR_BUFFER_BIT, GL_LINEAR);&#xA;&#xA;// ...and so on. You can switch FBOs if you have more than one to read from.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are a few obvious &quot;gotchas&quot;, in that HDR buffers probably won't visualise the way you expect them to, you probably can't &quot;see&quot; depth/stencil buffers in the obvious way, and if the size of the FBO doesn't match the area being blitted to, the magnification/minification method might be extremely naive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But as quick 'n dirty hacks go, it's pretty good.&lt;/p&gt;&#xA;" OwnerUserId="159" LastEditorUserId="159" LastEditDate="2015-08-05T00:26:44.483" LastActivityDate="2015-08-05T00:26:44.483" CommentCount="0" />
  <row Id="31" PostTypeId="1" AcceptedAnswerId="48" CreationDate="2015-08-05T00:01:57.733" Score="9" ViewCount="94" Body="&lt;p&gt;There are 2 main factors that seem to lead to &lt;a href=&quot;https://en.wikipedia.org/wiki/Digital_artifact&quot; rel=&quot;nofollow&quot;&gt;digital artifacts&lt;/a&gt; when creating JPEG images: Aliasing and Compression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Converting a PNG with characters on it to a JPEG or placing vector images over a photograph, will result in pixelization on their edges. The antialiasing generally creates a kind of blur around them, but if the image is &lt;a href=&quot;https://en.wikipedia.org/wiki/Lossy_compression&quot; rel=&quot;nofollow&quot;&gt;lossy compressed&lt;/a&gt;, part of the details are also lost, therefore the blur and the pixelization may become less noticeable.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Is this the right thing to do? I.e. since a lossless compression generates a highly detailed image, the artifacts resulting from aliasing will be more noticeable, so a balance may be found by using the right compression, although compromising the image quality.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Edit&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I just saved this JPEG in mspaint (3.46KB):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uPlhy.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uPlhy.jpg&quot; alt=&quot;Lossless&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the same JPEG with maximum compression (lowest quality, 0.5KB):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5HPZJ.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5HPZJ.jpg&quot; alt=&quot;Lossy&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's yet the same JPEG with 50% compression (notice the difference in size, 1.29KB):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kS88V.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kS88V.jpg&quot; alt=&quot;Half-no-options&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same 50% compression but saved as &quot;progressive JPG&quot;, kept the original EXIF and XMP data, and &quot;tried to save with original JPG quality&quot; (you can notice that there are no grey pixels around, 2.96KB):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/i5A7q.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/i5A7q.jpg&quot; alt=&quot;Half-options&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And finally the same as before with &lt;strong&gt;chroma subsampling disabled&lt;/strong&gt; (same file size, 2.96KB):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/V8FMk.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/V8FMk.jpg&quot; alt=&quot;Half-no-chroma&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2015-08-05T10:55:35.593" LastActivityDate="2015-08-05T10:55:35.593" Title="When creating a JPEG, how can I minimize the occurence of artifacts?" Tags="&lt;image&gt;&lt;compression&gt;&lt;artifacts&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="32" PostTypeId="1" AcceptedAnswerId="33" CreationDate="2015-08-05T01:13:26.897" Score="8" ViewCount="228" Body="&lt;p&gt;From my basic understanding, a Vertex Buffer Object works something like this (pseudo code):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Normally, if one wanted to say, draw a square, one could issue line drawing commands.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;line (0, 0) -&amp;gt; (1, 0)&#xA;line (1, 0) -&amp;gt; (1, 1)&#xA;line (1, 1) -&amp;gt; (0, 1)&#xA;line (0, 1) -&amp;gt; (0, 0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Using a VBO, if I understand correctly, would load the vertices into a VBO.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;define VBO&#xA;load (0,0) -&amp;gt; VBO&#xA;load (1,0) -&amp;gt; VBO&#xA;load (1,1) -&amp;gt; VBO&#xA;load (0,1) -&amp;gt; VBO&#xA;load (0,0) -&amp;gt; VBO&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then you can issue one drawing command.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;draw VBO vertices&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While I understand how VBOs work, I don't know why they improve performance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do they improve performance?&lt;/p&gt;&#xA;" OwnerUserId="17" LastEditorUserId="231" LastEditDate="2016-05-28T15:15:22.093" LastActivityDate="2016-05-28T15:15:22.093" Title="Why do Vertex Buffer Objects improve performance?" Tags="&lt;performance&gt;&lt;rendering&gt;&lt;vertex-buffer-object&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="2" />
  <row Id="33" PostTypeId="2" ParentId="32" CreationDate="2015-08-05T01:34:47.203" Score="9" Body="&lt;p&gt;In general, when you render an object in an immediate mode—issuing line drawing commands for instance—you build up a series of commands that you submit to the graphics card to draw. If you're drawing a lot of data, or drawing very frequently, you can waste a lot of time sending this data over and over again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A vertex buffer allows you to produce a single object that you submit to the graphics card once. If you don't need to change your geometry, you can leave it on the graphics card and simply send the graphics card a request to draw that object. Since it avoids the copy every time you draw, there's much less overhead for each draw.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that using a vertex buffer object doesn't always provide a very significant speedup. If you're only drawing the object once per frame, and you're replacing the geometry in between each frame, then you don't get the benefits of avoiding the copy each frame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most of my experience comes from writing programs using graphics APIs like OpenGL, so someone who's messed with the backend of a graphics driver can probably provide a more detailed answer, but I hope this makes things a bit clearer.&lt;/p&gt;&#xA;" OwnerUserId="174" LastActivityDate="2015-08-05T01:34:47.203" CommentCount="0" />
  <row Id="34" PostTypeId="2" ParentId="32" CreationDate="2015-08-05T01:36:09.247" Score="5" Body="&lt;p&gt;By using an immediate mode interface (e.g. old style OpenGL glBegin()/glEnd()/glVertex()) you're effectively drip feeding data to the driver one piece at a time. It then has to take that single piece of data, reformat it and pass it on to the hardware (which these days means putting it into a command buffer).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By using a vertex buffer object, you're providing a (hopefully) large block of data to the driver ahead of when it needs to be used. It can perform a number of optimizations (reformatting, placing into video memory) as well as not having to feed the GPU piecemeal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice if you're only drawing a small number of primitives then it probably won't make much difference, however if you're drawing a multi-million triangle mesh then VBOs in video memory are the way to go.&lt;/p&gt;&#xA;" OwnerUserId="144" LastActivityDate="2015-08-05T01:36:09.247" CommentCount="0" />
  <row Id="35" PostTypeId="2" ParentId="5" CreationDate="2015-08-05T02:23:49.107" Score="2" Body="&lt;p&gt;If what you are interested in is the area, you could downsize the stencil buffer until you reach one pixel and deduce that area from its color.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Steps would be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Copy the stencil to a texture, using a format with enough precision.&lt;/li&gt;&#xA;&lt;li&gt;Load a shader that outputs a color proportional to the number of texels with a given color.&lt;/li&gt;&#xA;&lt;li&gt;Ping-pong between to framebuffers to reduce the size by half until reaching one pixel.&lt;/li&gt;&#xA;&lt;li&gt;The color of the pixel is the percentage of the viewport covered by the area: just multiply it by the area of the viewport.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-08-05T02:23:49.107" CommentCount="0" />
  <row Id="36" PostTypeId="2" ParentId="32" CreationDate="2015-08-05T02:55:44.850" Score="9" Body="&lt;p&gt;There are two steps that make the VBO more efficient than immediate mode.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Immediate mode&lt;/strong&gt; (&lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glBegin.xml&quot; rel=&quot;nofollow&quot;&gt;glBegin/glEnd&lt;/a&gt;, &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glVertex.xml&quot; rel=&quot;nofollow&quot;&gt;glVertex*&lt;/a&gt;, etc.) means that at each frame, you spoon feed the vertices, attribute per attribute (position, normal, color, etc.), to the driver, which then reformats them and finally sends the whole package as a command to the GPU. That a lot of function calls per vertex on each frame.&lt;br/&gt;&#xA;(Note that &lt;a href=&quot;https://www.opengl.org/wiki/History_of_OpenGL#Deprecation_Model&quot; rel=&quot;nofollow&quot;&gt;immediate mode is deprecated since OpenGL 3.0&lt;/a&gt;, and is entirely &lt;a href=&quot;https://www.opengl.org/registry/doc/glspec32.core.20091207.pdf&quot; rel=&quot;nofollow&quot;&gt;removed from 3.2&lt;/a&gt;.)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;By using &lt;strong&gt;vertex arrays&lt;/strong&gt; (see &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glDrawArrays.xhtml&quot; rel=&quot;nofollow&quot;&gt;glDrawArrays&lt;/a&gt;, &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glDrawElements.xhtml&quot; rel=&quot;nofollow&quot;&gt;glDrawElements&lt;/a&gt;, &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glVertexPointer.xml&quot; rel=&quot;nofollow&quot;&gt;glVertexPointer&lt;/a&gt;, etc.), you can give the driver the whole thing at once and save it the burden of reformatting the vertices. You're effectively replacing several function calls per vertex by just a handful of calls for the whole mesh. But you still need to do that once a frame.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.opengl.org/wiki/Vertex_Specification#Vertex_Buffer_Object&quot; rel=&quot;nofollow&quot;&gt;Vertex Buffer Object&lt;/a&gt;&lt;/strong&gt;, or &lt;strong&gt;VBO&lt;/strong&gt; (see &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glGenBuffers.xhtml&quot; rel=&quot;nofollow&quot;&gt;glGenBuffers&lt;/a&gt;, &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glBindBuffer.xhtml&quot; rel=&quot;nofollow&quot;&gt;glBindBuffer&lt;/a&gt;, etc.) go one step further and store the data on the GPU side: you send it only once, and then just refer to it by a handle. You save bandwidth by not sending the same data over and over at each frame.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="182" LastEditorUserId="182" LastEditDate="2016-04-08T05:37:52.483" LastActivityDate="2016-04-08T05:37:52.483" CommentCount="0" />
  <row Id="37" PostTypeId="1" AcceptedAnswerId="46" CreationDate="2015-08-05T03:23:24.500" Score="13" ViewCount="317" Body="&lt;p&gt;Programmers are supposed to have a fairly good idea of the cost of certain operations: for example the cost of an instruction on CPU, the cost of a L1, L2, or L3 cache miss, the cost of a LHS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When it comes to graphics, I realize I have little to no idea what they are. I have in mind that if we order them by cost, state changes are something like:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Shader uniform change.&lt;/li&gt;&#xA;&lt;li&gt;Active vertex buffer change.&lt;/li&gt;&#xA;&lt;li&gt;Active texture unit change.&lt;/li&gt;&#xA;&lt;li&gt;Active shader program change.&lt;/li&gt;&#xA;&lt;li&gt;Active frame buffer change.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;But that is a very rough rule of thumb, it might not even be correct, and I have no idea what are the orders of magnitude. If we try to put units, ns, clock cycles or number of instructions, how much are we talking about?&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="17" LastEditDate="2015-08-05T03:51:09.420" LastActivityDate="2015-08-05T06:06:19.940" Title="What is the cost of changing state?" Tags="&lt;performance&gt;&lt;gpu&gt;&lt;optimisation&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="38" PostTypeId="1" AcceptedAnswerId="78" CreationDate="2015-08-05T03:41:52.363" Score="17" ViewCount="236" Body="&lt;p&gt;Tiled rendering is used in modern mobile GPU architectures to increase the coherency of memory access by subdividing image space into a regular grid of small (e.g., 32x32 pixel) tiles. Information is scarce on the types of data structures used to track the primitives that are associated with each tile, considering that arbitrarily many primitives may overlap any given tile. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the perspective of a driver developer, what data structures are commonly used to represent the primitive sets that belong to a tile, and are such structures dynamically allocated/resized according to the geometry that overlaps a particular tile?&lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2015-08-05T22:23:19.080" Title="Data structures for tile-based (deferred) rendering" Tags="&lt;tile-based-rendering&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="4" />
  <row Id="39" PostTypeId="1" AcceptedAnswerId="41" CreationDate="2015-08-05T03:58:22.973" Score="26" ViewCount="971" Body="&lt;p&gt;I've read that blur is done in real time graphics by doing it on one axis and then the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've done a bit of convolution in 1d in the past but am not super comfortable with it, nor know what to convolve in this case exactly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain in plain terms how a 2d Gaussian Blur of an image is done?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also heard that the radius of the Blur can impact performance, is that due to having to do a larger convolution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-11-09T04:32:44.627" Title="How is Gaussian Blur Implemented?" Tags="&lt;texture&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="8" />
  <row Id="40" PostTypeId="2" ParentId="39" CreationDate="2015-08-05T04:32:27.193" Score="10" Body="&lt;p&gt;In general, a convolution is performed by taking the integral of the product of two functions in a sliding window, but if you're not from a math background, that's not a very helpful explanation, and certainly won't give you a useful intuition for it. More intuitively, a convolution allows multiple points in an input signal to affect a single point on an output signal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since you're not super comfortable with convolutions, let's first review  what a convolution means in a discrete context like this, and then go over a simpler blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In our discrete context, we can multiply our two signals by simply multiplying each corresponding sample. The integral is also simple to do discretely, we just add up each sample in the interval we're integrating over. One simple discrete convolution is computing a moving average. If you want to take the moving average of 10 samples, this can be thought of as convolving your signal by a distribution 10 samples long and 0.1 tall, each sample in the window first gets multiplied by 0.1, then all 10 are added together to produce the average. This also reveals an interesting and important distinction, when you're blurring with a convolution, the distribution that you use should sum to 1.0 over all its samples, otherwise it will increase or decrease the overall brightness of the image when you apply it. If the distribution for our average had been 1 over its whole interval, then the total signal would be 10x brighter after the convolution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that we've looked at convolutions, we can move on to blurs. A Gaussian blur is implemented by convolving an image by a Gaussian distribution. Other blurs are generally implemented by convolving the image by other distributions. The simplest blur is the box blur, and it uses the same distribution we described above, a box with unit area. If we want to blur a 10x10 area, then we multiply each sample in the box by 0.01, and then sum them all together to produce the center pixel. We still need to ensure that the total sum of all the samples in our blur distribution are 1.0 to make sure the image doesn't get brighter or darker.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A Gaussian blur follows the same broad procedure as a box blur, but it uses a more complex formula to determine the weights. The distribution can be computed based on the distance from the center &lt;code&gt;r&lt;/code&gt;, by evaluating $$\frac{e^{-x^2/2}}{\sqrt{2\pi}}$$ The sum of all the samples in a Gaussian will eventually be approximately 1.0 if you sample every single pixel, but the fact that a Gaussian has infinite support (it has values everywhere) means that you need to use a slightly modified version that sums to 1.0 using only a few values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course both of these processes can be very expensive if you perform them on a very large radius, since you need to sample a lot of pixels in order to compute the blur. This is where the final trick comes in: both a Gaussian blur and a box blur are what's called a &quot;separable&quot; blur. This means that if you perform the blur along one axis, and then perform it along the other axis, it produces the exact same result as if you'd performed it along both axes at the same time. This can be tremendously important. If your blur is 10px across, it requires 100 samples in the naive form, but only 20 when separated. The difference only gets bigger, since the combined blur is $O(n^2)$, while the separated form is $O(n)$.&lt;/p&gt;&#xA;" OwnerUserId="174" LastEditorUserId="231" LastEditDate="2015-11-07T23:21:50.137" LastActivityDate="2015-11-07T23:21:50.137" CommentCount="2" />
  <row Id="41" PostTypeId="2" ParentId="39" CreationDate="2015-08-05T04:46:43.567" Score="32" Body="&lt;p&gt;In convolution, two mathematical functions are combined to produce a third function. In image processing functions are usually called kernels. A kernel is nothing more than a (square) array of pixels (a small image so to speak). Usually, the values in the kernel add up to one. This is to make sure noe energy is added or removed from the image after the operation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, a Gaussian kernal (used for Gaussian blur) is a square array of pixels where the pixel values correspond to the values of a Gaussian curve (in 2d).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Qc4Mq.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Qc4Mq.gif&quot; alt=&quot;Image linked from http://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each pixel in the image gets multiplied by the Gaussian kernel. This is done by placing the center pixel of the kernel on the image pixel and multiplying the values in the original image with the pixels in the kernel that overlap. The values resulting from these multiplications are added up and that result is used for the value at the destination pixel. Looking at the image, you would multiply the value at (0,0) in the input array by the value a (i) in the kernel array, the value at (1,0) in the input array by the value a (h) in the kernel array, etc. and then add all these values to get the value for (1,1) at the output image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/bRN2c.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bRN2c.jpg&quot; alt=&quot;Image linked from http://www.songho.ca/dsp/convolution/convolution.html&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer your second question first, the larger the kernel, the more expensive the operation. So, the larger the radius of the blur, the longer the operation will take.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer your first question, as explained above, convolution can be done by multiplying each input pixel with the entire kernel. However, if the kernel is symmetrical (wich a Gaussian kernel is) you can also multiply each axis (x and y) independently, which will decrease the total number of multiplications. In proper mathematical terms, if a matrix is separable it can be decomposed into (M×1) and (1×N) matrices. For the Gaussian kernel above this means you can also use the following kernels:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\frac1{256}\cdot\begin{bmatrix}&#xA;1&amp;amp;4&amp;amp;6&amp;amp;4&amp;amp;1\\&#xA;4&amp;amp;16&amp;amp;24&amp;amp;16&amp;amp;4\\&#xA;6&amp;amp;24&amp;amp;36&amp;amp;24&amp;amp;6\\&#xA;4&amp;amp;16&amp;amp;24&amp;amp;16&amp;amp;4\\&#xA;1&amp;amp;4&amp;amp;6&amp;amp;4&amp;amp;1&#xA;\end{bmatrix}&#xA;=&#xA;\frac1{256}\cdot\begin{bmatrix}&#xA;1\\4\\6\\4\\1&#xA;\end{bmatrix}\cdot\begin{bmatrix}&#xA;1&amp;amp;4&amp;amp;6&amp;amp;4&amp;amp;1&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You would now multiply each pixel in the input image with both kernels and add the resulting values to get the value for the output pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more information on how to see if a kernel is separable, follow this &lt;a href=&quot;http://stackoverflow.com/questions/5886529/how-can-i-determine-if-my-convolution-is-separable&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: the two kernels shown above use slightly different values. This is because the (sigma) parameter used for the Gaussian curve to create these kernels were slightly different in both cases. For an explanation on which parameters influence the shape of the Gaussian curve and thus the values in the kernel follow this &lt;a href=&quot;http://dev.theomader.com/gaussian-kernel-calculator/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: in the second image above it says the kernel that is used is flipped. This of course only makes any difference if the kernel you use is not symmetric. The reason why you need to flip the kernel has to do with the mathematical properties of the convolution operation (see &lt;a href=&quot;http://songho.ca/dsp/convolution/convolution.html&quot;&gt;link&lt;/a&gt; for a more in depth explanation on convolution). Simply put: if you would not flip the kernel, the result of the convolution operation will be flipped. By flipping the kernel, you get the correct result.&lt;/p&gt;&#xA;" OwnerUserId="194" LastEditorUserId="137" LastEditDate="2015-11-03T17:34:07.373" LastActivityDate="2015-11-03T17:34:07.373" CommentCount="3" />
  <row Id="42" PostTypeId="2" ParentId="39" CreationDate="2015-08-05T05:11:19.993" Score="12" Body="&lt;p&gt;Here is the best article I've read on the topic:&#xA;&lt;a href=&quot;http://rastergrid.com/blog/2010/09/efficient-gaussian-blur-with-linear-sampling/&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Efficient Gaussian blur with linear sampling&lt;/em&gt;&lt;/a&gt;. It addresses all your questions and is really accessible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the layman very short explanation: Gaussian is a function with the nice property of being separable, which means that a 2D Gaussian function can be computed by combining two 1D Gaussian functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for a $n \times n$ size ($O(n^2)$), you only need to evaluate $2 \times n$ values ($O(n)$), which is significantly less. If your operation consists in reading a texture element (commonly called a &lt;em&gt;&quot;tap&quot;&lt;/em&gt;), it is good news: less taps is cheaper because a texture fetch has a cost.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's why blurring algorithms use that property by doing two passes, one to blur horizontally by gathering the $n$ horizontal pixels, and one to blur vertically by gathering the $n$ vertical pixels. The result is the final blurred pixel color.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="174" LastEditDate="2015-11-09T04:32:25.737" LastActivityDate="2015-11-09T04:32:25.737" CommentCount="0" />
  <row Id="43" PostTypeId="2" ParentId="37" CreationDate="2015-08-05T05:38:47.300" Score="17" Body="&lt;p&gt;The actual cost of any particular state change varies with so many factors that a general answer is nigh impossible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, every state change can potentially have both a CPU-side cost and a GPU-side cost. The CPU cost may, depending on your driver and graphics API, be paid entirely on the main thread or partially on a background thread.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second, the GPU cost may depend on the amount of work in flight. Modern GPUs are very pipelined and love to get lots of work in flight at once, and the biggest slowdown you can get is from stalling the pipeline so that everything that's currently in flight must retire before the state changes. What can cause a pipeline stall? Well, it depends on your GPU!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The thing you actually need to know to understand the performance here is: what does the driver and GPU need to do to process your state change? This of course depends on your GPU, and also on details that ISVs often don't share publicly. However, there are some &lt;strong&gt;general principles&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GPUs generally are split into a frontend and a backend. The frontend handles a stream of commands generated by the driver, while the backend does all the real work. Like I said before, the backend loves to have lots of work in flight, but it needs some information to store information about that work (perhaps filled in by the frontend). If you kick enough small batches and use up all the silicon keeping track of the work, then the frontend will have to stall even if there's lots of unused horsepower sitting around. So a principle here: &lt;strong&gt;the more state changes (and small draws), the more likely you are to starve the GPU backend&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While a draw is actually being processed, you're basically just running shader programs, which are doing memory accesses to fetch your uniforms, your vertex buffer data, your textures, but also the control structures that tell the shader units where your vertex buffers and your textures are. And the GPU has caches in front of those memory accesses as well. So whenever you throw new uniforms or new texture/buffer bindings at the GPU, it'll likely suffer a cache miss the first time it has to read them. Another principle: &lt;strong&gt;most state changes will cause a GPU cache miss.&lt;/strong&gt; (This is most meaningful when you are managing constant buffers yourself: if you keep constant buffers the same between draws, then they are more likely to stay in cache on the GPU.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A big part of the cost for state changes for shader resources is the CPU side. Whenever you set a new constant buffer, the driver is most likely copying the contents of that constant buffer into a command stream for the GPU. If you set a single uniform, the driver is very likely turning that into a big constant buffer behind your back, so it has to go look up the offset for that uniform in the constant buffer, copy the value in, then mark the constant buffer as dirty so it can get copied into the command stream before the next draw call. If you bind a new texture or vertex buffer, the driver is probably copying a control structure for that resource around. Also, if you're using a discrete GPU on a multitasking OS, the driver needs to track every resource you use and when you start using it so that the kernel's GPU memory manager can guarantee that the memory for that resource is resident in the GPU's VRAM when the draw happens. Principle: &lt;strong&gt;state changes make the driver shuffle memory around to generate a minimal command stream for the GPU.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you change the current shader, you're probably causing a GPU cache miss (they have an instruction cache too!). In principle, the CPU work should be limited to putting a new command in the command stream saying &quot;use the shader.&quot; In reality, though, there's a whole mess of shader compilation to deal with. GPU drivers very often lazily compile shaders, even if you've created the shader ahead of time. More relevant to this topic, though, some states are not supported natively by the GPU hardware and are instead compiled into the shader program. One popular example is vertex formats: these may be compiled into the vertex shader instead of being separate state on the chip. So if you use vertex formats that you haven't used with a particular vertex shader before, you may now be paying a bunch of CPU cost to patch the shader and copy the shader program up to the GPU. Additionally, the driver and shader compiler may conspire to do all sorts of things to optimize the execution of the shader program. This might mean optimizing the memory layout of your uniforms and resource control structures so that they are nicely packed into adjacent memory or shader registers. So when you change shaders, it may cause the driver to look at everything you have already bound to the pipeline and repack it into an entirely different format for the new shader, and then copy that into the command stream. Principle: &lt;strong&gt;changing shaders can cause a lot of CPU memory shuffling.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Frame buffer changes are probably the most implementation-dependent, but are generally pretty expensive on the GPU. Your GPU may not be able to handle multiple draw calls to different render targets at the same time, so it may need to stall the pipeline between those two draw calls. It may need to flush caches so that the render target can be read later. It may need to resolve work that it has postponed during the drawing. (It is very common to be accumulating a separate data structure along with depth buffers, MSAA render targets, and more. This may need to be finalized when you switch away from that render target. If you are on a GPU that is tile-based, like many mobile GPUs, a fairly large amount of actual shading work might need to be flushed when you switch away from a frame buffer.) Principle: &lt;strong&gt;changing render targets is expensive on the GPU.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure that's all very confusing, and unfortunately it's hard to get too specific because details are often not public, but I'm hoping it's a half-decent overview of some of the things that are actually going on when you call some state changing function in your favorite graphics API.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-05T05:38:47.300" CommentCount="0" />
  <row Id="44" PostTypeId="1" AcceptedAnswerId="53" CreationDate="2015-08-05T05:47:39.463" Score="11" ViewCount="223" Body="&lt;p&gt;I've heard a lot of people working on VR talk about scanline racing and that it's supposed  to help improve latency for motion-to-photon. However, it isn't clear to me how this can be done with OpenGL. Could someone explain how scanline racing works, and how it can be implemented on modern GPUs.&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-08-05T09:04:59.070" Title="What is &quot;Scanline Racing&quot;" Tags="&lt;virtual-reality&gt;&lt;scanline&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="45" PostTypeId="2" ParentId="27" CreationDate="2015-08-05T05:55:08.537" Score="5" Body="&lt;p&gt;Modern GPUs generally have a single frontend section that processes an entirely linear stream of commands from the CPU. Whether this is a natural hardware design or if it simply evolved out of the days when there was a single CPU core generating commands for the GPU is debatable, but it's the reality for now. So if you generate a single linear stream of stateful commands, of course it makes sense to generate that stream linearly on a single thread on the CPU! Right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Well, modern GPUs also generally have a very flexible unified backend that can work on lots of different things at once. Generally speaking, the GPU works on vertices and pixels at fairly fine granularity. There's not a whole lot of difference between a GPU processing 1024 vertices in one draw and 512+512 vertices in two different draws.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That suggests a fairly natural way to do less work: instead of throwing huge number of vertices at the GPU in a single draw call, split your model into sections, do cheap coarse culling on those sections, and submit each chunk individually if it passes the culling test. If you do it at the right granularity you should get a nice speedup!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, in the current graphics API reality, draw calls are extremely expensive on the CPU. A simplified explanation of why: state changes on the GPU may not directly correspond to graphics API calls, so many graphics API calls simply set some state inside the driver, and the draw call that would depend on this new state goes and looks at all the state that is marked as having changed since the last draw, writes it into the command stream for the GPU, then actually initiates the draw. This is all work that is done in an attempt to get a lean and mean command stream for the GPU frontend unit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What this boils down to is that you have a &lt;strong&gt;budget for draw calls which is entirely imposed by the overhead of the driver&lt;/strong&gt;. (I think I heard that these days you can get away with about 5,000 per frame for a 60 FPS title.) You can increase that by a large percentage by building this command stream in parallel chunks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are other reasons too (for example, asynchronous timewarp for VR latency improvements), but this is a big one for graphics-bound games and other drawcall-heavy software (like 3D modeling packages).&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-05T05:55:08.537" CommentCount="0" />
  <row Id="46" PostTypeId="2" ParentId="37" CreationDate="2015-08-05T06:06:19.940" Score="14" Body="&lt;p&gt;The most data I've seen is on the relative expense of various state changes is from Cass Everitt and John McDonald's &lt;a href=&quot;https://www.youtube.com/watch?v=-bCeNzgiJ8I&amp;amp;list=PLckFgM6dUP2hc4iy-IdKFtqR9TeZWMPjm&quot;&gt;talk on reducing OpenGL API overhead&lt;/a&gt; from January 2014.  Their talk included this slide (at 31:55):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JgrSc.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JgrSc.jpg&quot; alt=&quot;Relative costs of state changes&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The talk doesn't give any more info on how they measured this (or even whether they're measuring CPU or GPU cost, or both!).  But at least it dovetails with the conventional wisdom: render target and shader program changes are the most expensive, uniform updates the least, with vertex buffers and texture changes somewhere in the middle.  The rest of their talk also has a lot of interesting wisdom about reducing state-change overhead.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T06:06:19.940" CommentCount="1" />
  <row Id="48" PostTypeId="2" ParentId="31" CreationDate="2015-08-05T07:23:16.503" Score="9" Body="&lt;p&gt;JPEG compression involves three main steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chroma_subsampling&quot;&gt;Chroma subsampling&lt;/a&gt;. The image is converted from RGB into YCbCr color space, in which the &lt;em&gt;luma&lt;/em&gt; or brightness (Y) is stored separately from the &lt;em&gt;chroma&lt;/em&gt; or color components, Cb and Cr.  The Y component is kept at full resolution, but Cb and Cr are downsampled, typically to half resolution on each axis.  This exploits the fact that the human visual system is more sensitive to fine details of brightness than of color.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quantization_%28image_processing%29#Frequency_quantization_for_image_compression&quot;&gt;Frequency quantization.&lt;/a&gt; The Y, Cb, and Cr images are converted into a frequency representation, by breaking them up into 8x8 blocks and applying the &lt;a href=&quot;https://en.wikipedia.org/wiki/Discrete_cosine_transform&quot;&gt;discrete cosine transform&lt;/a&gt; (a variant of the Fourier transform) to each block.  The result is a matrix of numbers that describe the amplitudes of different spatial frequencies in the block.  These numbers can then be quantized (rounded off to a chosen number of bits of precision). Different levels of quantization are used for different frequencies, exploiting our visual system's relatively lower sensitivity to high frequencies.  This is where the JPEG encoder's quality setting comes into play: lower qualities use coarser quantization.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_encoding&quot;&gt;Entropy coding&lt;/a&gt;. The  quantized DCT values are passed through an entropy coder, which losslessly compresses the bit stream by using fewer bits to represent the more common values, sort of like a zip file.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Steps 1 and 2 are the lossy ones, and each produces its own type of artifacts (though there is some overlap).  Chroma subsampling tends to blur sharp edges between regions of different colors.  This is particularly visible in vector art, where brightly colored shapes pick up vicious jaggies around their edges. Frequency quantization blurs fine details in general, and also creates block-shaped artifacts at low quality settings, because the DCT is done on a block-by-block basis.  It is particularly visible on text.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why JPEG usually isn't used for images containing vector graphics or text&amp;mdash;its compression algorithms are poorly suited for those cases, although they work well for photographs and other images with complex texture and not-too-sharp edges.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T07:23:16.503" CommentCount="1" />
  <row Id="49" PostTypeId="2" ParentId="27" CreationDate="2015-08-05T08:07:39.323" Score="6" Body="&lt;p&gt;There's a lot of work needed on the CPU to set up a frame for the GPU, and a good chunk of that work is inside the graphics driver.  Prior to DX12 / Vulkan, that graphics driver work was essentially forced to be single-threaded by the design of the API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The hope is that DX12 / Vulkan lift that restriction, allowing driver work to be performed in parallel on multiple CPU threads within a frame.  This will enable more efficient use of multicore CPUs, allowing game engines to push more complex scenes without becoming CPU-bound.  That's the hope&amp;mdash;whether it will be realized in practice is something we'll have to wait to see over the next few years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To elaborate a bit: the output of a game engine renderer is a stream of DX/GL API calls that describe the sequence of operations to render a frame.  However, there's a great distance between the stream of API calls and the actual binary command buffers that the GPU hardware consumes.  The driver has to &quot;compile&quot; the API calls into the GPU's machine language, so to speak.  That isn't a trivial process&amp;mdash;it involves a lot of translation of API concepts into low-level hardware realities, validation to make sure the GPU is never set into an invalid state, wrangling memory allocations and data, tracking state changes to issue the correct low-level commands, and so on and on.  The graphics driver is responsible for all this stuff.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In DX11 / GL4 and earlier APIs, this work is typically done by a single driver thread.  Even if you call the API from multiple threads (which you can do using DX11 deferred command lists, for example), it just adds some work to a queue for the driver thread to chew through later.  One big reason for this is the state tracking I mentioned before.  Many of the hardware-level GPU configuration details require knowledge of the current graphics pipeline state, so there's no good way to break up the command list into chunks that can be processed in parallel&amp;mdash;each chunk would have to know exactly what state it should start with, even though the previous chunk hasn't been processed yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's one of the big things that changed in DX12 / Vulkan.  For one thing, they incorporate almost all the graphics pipeline state into one object, and for another (at least in DX12) when you start creating a command list you &lt;em&gt;must&lt;/em&gt; provide an initial pipeline state; the state isn't inherited from one command list to the next.  In principle, this allows the driver not to have to know anything about previous command lists before it can start compiling&amp;mdash;and that in turn allows the application to break up its rendering into parallelizable chunks, producing fully-compiled command lists, which can then be concatenated together and sent to the GPU with a minimum of fuss.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, there are many other changes in the new APIs, but as far as multithreading goes, that's the most important part.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T08:07:39.323" CommentCount="0" />
  <row Id="50" PostTypeId="1" AcceptedAnswerId="52" CreationDate="2015-08-05T08:13:38.827" Score="6" ViewCount="202" Body="&lt;p&gt;In traditional computer graphics, most 3D models are rendered by rasterizing or ray tracing against a subdivided mesh of primitives, usually triangles or quads. &lt;a href=&quot;https://docs.unrealengine.com/latest/INT/Engine/Rendering/LightingAndShadows/RayTracedDistanceFieldShadowing/index.html&quot;&gt;More recently&lt;/a&gt;, some real-time techniques have been predicated on ray-tracing against signed distance fields (SDFs). Intuitively, these distance fields cannot be the same triangles and quads of traditional rendering and must be something like 3D geometric primitives (cubes, spheres, etc.) in order to get proper shading. Is this true? If not, can complex scenes of &quot;traditional&quot; 3D models be represented using SDFs? If it is true, then how do artists create high-detail models used in SDF renderers?&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-08-05T08:40:01.477" Title="Asset creation for signed distance field rendering?" Tags="&lt;raytracing&gt;&lt;signed-distance-field&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="51" PostTypeId="1" AcceptedAnswerId="62" CreationDate="2015-08-05T08:19:21.280" Score="14" ViewCount="375" Body="&lt;p&gt;I would like to generate procedural noise on the surface of a sphere (e.g. to procedurally generate planets or marble-textured balls). Of course, I could just take a standard noise algorithm and map it onto the sphere, but this has all the issues of projecting a plane onto a sphere, like distortions on the poles or elsewhere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suppose I could generate volume noise and &quot;cut out&quot; the sphere, but this seems unnecessarily inefficient - and if the noise has some grid-based artefacts, these would still not appear uniformly on the sphere. Furthermore, at least in the case of Simplex noise, cutting 2D slices out of 3D noise generally looks different than generating 2D noise right away.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to avoid these issues, e.g. by generating noise natively on the sphere? The noise should have at least the quality of &lt;a href=&quot;https://en.wikipedia.org/wiki/Perlin_noise&quot;&gt;Perlin noise&lt;/a&gt;, ideally that of &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex_noise&quot;&gt;Simplex noise&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2015-11-03T16:14:57.213" Title="How can I generate procedural noise on a sphere?" Tags="&lt;projections&gt;&lt;noise&gt;&lt;procedural-generation&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="3" />
  <row Id="52" PostTypeId="2" ParentId="50" CreationDate="2015-08-05T08:40:01.477" Score="7" Body="&lt;p&gt;Signed distance fields are popular in minimal graphics applications, such as the demo scene, where interesting objects can be synthesized from few simple analytic primitives such as spheres or cubes. However, signed distance fields are not restricted to these simple objects, and they do not necessarily need to be designed by a human. For example, you can synthesize the analytic signed distance field of a triangle mesh without any artist interaction - the SDF is just the signed distance to the closest triangle after all, which can be easily computed. This allows you to keep using your traditional mesh pipeline while also using SDFs in the background.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of the Unreal Engine, the SDF is precomputed automatically once for all static meshes and is then sampled into a low-resolution 3D texture. It can then be cheaply evaluated everywhere using a simple texture lookup, allowing them to do soft shadows and similar at runtime. &lt;a href=&quot;http://www.nvidia.com/content/pdf/gdc2011/epic.pdf&quot;&gt;This Unreal presentation&lt;/a&gt; at GDC 2011 mentions the 3D textures briefly (slide 27).&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-05T08:40:01.477" CommentCount="2" />
  <row Id="53" PostTypeId="2" ParentId="44" CreationDate="2015-08-05T09:04:59.070" Score="11" Body="&lt;p&gt;When your GPU displays a new frame on the screen, it transfers the image over the HDMI cable (or whatever kind) in a process called &quot;scanout&quot;.  The pixels are sent out in linear order, usually left-to-right and top-to-bottom.  The process is timed so that it takes most of the duration of a refresh interval to do this.  For instance, at 60Hz, one frame is ~17 ms.  Each scanout will take probably around 15-16 ms, with 1-2 ms of vblank in between (the exact values vary according to the display and video mode).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Traditionally, rendering is double-buffered, which means there are two buffers stored in GPU memory: one that is currently being scanned out (&quot;front buffer&quot;), and one that is being rendered to (&quot;back buffer&quot;).  Each frame, the two are swapped.  The GPU never renders to the same buffer that's being scanned out, which prevents artifacts due to potentially seeing parts of an incomplete frame.  However, a side effect of this is increased latency, since each frame may sit around in the buffer for several ms before it starts being scanned out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VR is very latency-sensitive, so this isn't desirable. An alternative approach is to render directly to the front buffer, but time things out very carefully so that you have rendered each line of the image shortly before the scanout gets there. That's called &quot;scanline racing&quot; or &quot;racing the beam&quot; (the &quot;beam&quot; harkening back to the CRT days of yore). This more or less requires that you render the image in scanline order, i.e. the same order that the pixels get scanned out.  It doesn't literally have to be rendered one line at a time&amp;mdash;it could be rendered in thin strips a few pixels high, but it does have to be done in order, as you can't go back and edit pixels that have already been scanned out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a lot of disadvantages to this approach; it has very stringent performance requirements, has to be timed very carefully against vsync, and it greatly complicates the rendering process. But in principle it can shave milliseconds off your latency, which why VR folks are interested in it.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T09:04:59.070" CommentCount="3" />
  <row Id="54" PostTypeId="1" AcceptedAnswerId="60" CreationDate="2015-08-05T09:30:30.980" Score="14" ViewCount="447" Body="&lt;p&gt;Image filtering operations such as blurs, SSAO, bloom and so forth are usually done using pixel shaders and &quot;gather&quot; operations, where each pixel shader invocation issues a number of texture fetches to access the neighboring pixel values, and computes a single pixel's worth of the result.  This approach has a theoretical inefficiency in that many redundant fetches are done: nearby shader invocations will re-fetch many of the same texels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way to do it is with compute shaders. These have the potential advantage of being able to share a small amount of memory across a group of shader invocations. For instance, you could have each invocation fetch one texel and store it in shared memory, then calculate the results from there.  This might or might not be faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is under what circumstances (if ever) is the compute-shader method &lt;em&gt;actually&lt;/em&gt; faster than the pixel-shader method?  Does it depend on the size of the kernel, what kind of filtering operation it is, etc.?  Clearly the answer will vary from one model of GPU to another, but I'm interested in hearing if there are any general trends.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T14:30:50.343" Title="When is a compute shader more efficient than a pixel shader for image filtering?" Tags="&lt;performance&gt;&lt;rendering&gt;&lt;compute-shader&gt;&lt;pixel-shader&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="55" PostTypeId="2" ParentId="39" CreationDate="2015-08-05T09:42:12.257" Score="8" Body="&lt;p&gt;The most important thing to consider when implementing the Gaussian blur is, as others have pointed out, to separate the 2D convolution filter into two 1D convolutions because it brings the complexity down from $O(n^2)$ to $O(n)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But there are two more tricks you might want to consider in an actual implementation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The filter has a certain radius and because of that, at the very borders, you will need to calculate with pixels which fall outside of the image. In such case, you could try one of the following: for the outside pixels you simply take the last possible value (i.e. the pixel at the very border, as in &lt;code&gt;max(x, 0)&lt;/code&gt;. Or you could &quot;reflect&quot; the image towards outside (as in &lt;code&gt;x &amp;lt; 0 ? -x : x&lt;/code&gt;). Or you could simply stop at the border but then you would need to adjust the denominator in convolution filter so that it sums up to 1. For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\operatorname{sum}&#xA;\frac{1}{256}&#xA;\begin{bmatrix}&#xA;1 &amp;amp; 4  &amp;amp; 6  &amp;amp; 4  &amp;amp; 1 \\&#xA;4 &amp;amp; 16 &amp;amp; 24 &amp;amp; 16 &amp;amp; 4 \\&#xA;6 &amp;amp; 24 &amp;amp; 36 &amp;amp; 24 &amp;amp; 6 \\&#xA;4 &amp;amp; 16 &amp;amp; 24 &amp;amp; 16 &amp;amp; 4 \\&#xA;1 &amp;amp; 4  &amp;amp; 6  &amp;amp; 4  &amp;amp; 1 \\&#xA;\end{bmatrix}&#xA;=&#xA;\operatorname{sum}&#xA;\frac{1}{225}&#xA;\begin{bmatrix}&#xA;0 &amp;amp; 0  &amp;amp; 0  &amp;amp; 0  &amp;amp; 0 \\&#xA;0 &amp;amp; 16 &amp;amp; 24 &amp;amp; 16 &amp;amp; 0 \\&#xA;0 &amp;amp; 24 &amp;amp; 36 &amp;amp; 16 &amp;amp; 0 \\&#xA;0 &amp;amp; 16 &amp;amp; 24 &amp;amp; 16 &amp;amp; 0 \\&#xA;0 &amp;amp; 0  &amp;amp; 0  &amp;amp; 0  &amp;amp; 0 \\&#xA;\end{bmatrix}&#xA;= 1.&#xA;$$&#xA;Another trick concerns how to calculate the actual coefficients of the kernel. Obviously you could try to implement the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_function&quot; rel=&quot;nofollow&quot;&gt;Gaussian function&lt;/a&gt; but much faster way is to observe that the 1D kernel reassembles &lt;a href=&quot;https://en.wikipedia.org/wiki/Pascal%27s_triangle&quot; rel=&quot;nofollow&quot;&gt;Pascal's triangle&lt;/a&gt;. For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     1&#xA;    1 1&#xA;   1 2 1&#xA;  1 3 3 1&#xA;[1 4 6 4 1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="141" LastEditorUserId="174" LastEditDate="2015-11-09T04:32:44.627" LastActivityDate="2015-11-09T04:32:44.627" CommentCount="0" />
  <row Id="56" PostTypeId="1" AcceptedAnswerId="59" CreationDate="2015-08-05T10:50:13.490" Score="6" ViewCount="130" Body="&lt;p&gt;In GLSL, perspective correct interpolation of vertex attributes is the default setting - one can disable it for specific vertex attributes by using the &lt;em&gt;noperspective&lt;/em&gt; qualifier. Other than in post-processing shaders, I've never seen the perspective correct interpolation disabled - are there any other use cases? Also, does it even make a difference, performance-wise?&lt;/p&gt;&#xA;" OwnerUserId="215" LastActivityDate="2015-08-05T14:03:44.697" Title="When to disable perspective correct interpolation ( noperspective )" Tags="&lt;opengl&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="57" PostTypeId="1" CreationDate="2015-08-05T11:25:51.847" Score="11" ViewCount="244" Body="&lt;p&gt;How can I distribute points over an implicit surface, to concentrate them more densely in areas of higher curvature?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've considered adding points randomly and rejecting points not required based on the curvature, but I'd like to know if there is a better approach giving a more even distribution over areas of similar curvature, while still giving the higher density required in high curvature regions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking specifically at using these points for a triangulation of the surface, and I don't want to create more triangles than I need for relatively flat parts.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;This will be applied to shapes with a known derivative so the curvature at a given point can be calculated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This does not need to be a real-time approach.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T11:30:46.930" LastActivityDate="2015-11-05T17:09:56.003" Title="How can I concentrate points in areas of higher curvature?" Tags="&lt;algorithm&gt;&lt;triangulation&gt;" AnswerCount="3" CommentCount="6" FavoriteCount="1" />
  <row Id="58" PostTypeId="1" AcceptedAnswerId="77" CreationDate="2015-08-05T11:31:47.607" Score="8" ViewCount="61" Body="&lt;p&gt;I want to model rays with a continuous range of frequencies so that I can get raytraced images with colour separation on refraction. I can model a light source with a specified frequency distribution by using the distribution to affect the probability of a random ray lying in a given frequency range, or alternatively I can choose frequencies from a uniform random distribution and make the brightness of each ray proportional to the frequency distribution at its particular frequency. I see the first as more physically accurate, but I suspect the second will give images that look &quot;finished&quot; with fewer rays. Is this intuitive suspicion correct? Are there any features that will be lost from the image with the second approach? Is there a way to get some of the speed increase without compromising the image?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-05T22:04:17.097" Title="What are the side effects of biasing brightness in continuous spectrum raytracing?" Tags="&lt;raytracing&gt;&lt;color&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="59" PostTypeId="2" ParentId="56" CreationDate="2015-08-05T14:03:44.697" Score="8" Body="&lt;p&gt;Use cases are only limited by your imagination! &lt;code&gt;noperspective&lt;/code&gt; means that the attribute is interpolated across the triangle as though the triangle was completely flat on the surface of the screen. You can do &lt;a href=&quot;http://orbit.dtu.dk/fedora/objects/orbit:55459/datastreams/file_3735323/content&quot;&gt;antialiased wireframe rendering&lt;/a&gt; with this: output a screen-space distance to the nearest edge as a &lt;code&gt;noperspective&lt;/code&gt; varying and use that as coverage in the pixel shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or if you're doing non-photorealistic rendering and want a pattern in screen-space like &lt;a href=&quot;https://en.wikipedia.org/wiki/Halftone&quot;&gt;halftoning&lt;/a&gt;, you can enable &lt;code&gt;noperspective&lt;/code&gt; on your UVs used for texturing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make a performance difference? Probably, but you probably won't notice (with the potential exception of less powerful graphics hardware). Most GPUs are composed of a series of pipeline stages that execute in parallel, and in some sense you only pay the cost for the most expensive stage. If rasterization is the most limiting part for you, then you may see a difference from the divisions that you're skipping per-pixel. I would guess that is most likely when rendering a shadow map or a depth prepass, but those also have the fewest attributes to interpolate.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-05T14:03:44.697" CommentCount="0" />
  <row Id="60" PostTypeId="2" ParentId="54" CreationDate="2015-08-05T14:30:50.343" Score="8" Body="&lt;p&gt;An architectural &lt;em&gt;advantage&lt;/em&gt; of compute shaders for image processing is that they skip the &lt;a href=&quot;https://en.wikipedia.org/wiki/Render_output_unit&quot;&gt;ROP&lt;/a&gt; step. It's very likely that writes from pixel shaders go through all the regular blending hardware even if you don't use it. Generally speaking compute shaders can write directly to memory, so you may avoid a bottleneck that you would otherwise have. I've heard of fairly sizable performance wins attributed to this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An architectural &lt;em&gt;disadvantage&lt;/em&gt; of compute shaders is that the GPU no longer knows which work unit retires to which set of pixels. If you are using the pixel shading pipeline, the GPU has the opportunity to pack work into a warp/wavefront that write to an area of the render target which is contiguous in memory (which may be &lt;a href=&quot;https://en.wikipedia.org/wiki/Z-order_curve&quot;&gt;Z-order tiled&lt;/a&gt; or something like that for performance reasons). If you are using a compute pipeline, the GPU may no longer kick work in optimal batches, leading to more bandwidth use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may be able to turn that altered warp/wavefront packing into an advantage again, though, if you know that your particular operation has a substructure that you can exploit by packing related work into the same thread group. Like you said, you could in theory give the sampling hardware a break by sampling one value per lane and putting the result in groupshared memory for other lanes to access without sampling. Whether this is a win depends on how expensive your groupshared memory is: if it's cheaper than the lowest-level texture cache, then this may be a win, but there's no guarantee of that. GPUs already deal pretty well with highly local texture fetches (by necessity).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have an intermediate stages in the operation where you want to share results, it may make more sense to use groupshared memory (since you can't fall back on the texture sampling hardware without having actually written out your intermediate result to memory). Unfortunately you also can't depend on having results from any other thread group, so the second stage would have to limit itself to only what is available in the same tile. I think the canonical example here is computing the average luminance of the screen for auto-exposure. I could also imagine combining texture upsampling with some other operation (since upsampling, unlike downsampling and blurs, doesn't depend on any values outside a given tile).&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-05T14:30:50.343" CommentCount="2" />
  <row Id="61" PostTypeId="1" AcceptedAnswerId="63" CreationDate="2015-08-05T14:56:21.613" Score="12" ViewCount="492" Body="&lt;p&gt;The &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/fwidth.xhtml&quot;&gt;OpenGL documentation&lt;/a&gt; states that fwidth &lt;code&gt;returns the sum of the absolute value of derivatives in x and y&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does this mean in less mathematical terms, and is there a way to visualize it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on my understanding of the function, &lt;code&gt;fwidth(p)&lt;/code&gt; has access to the value of &lt;code&gt;p&lt;/code&gt; in neighboring pixels. How does this work on the GPU without drastically impacting performance, and does it work reliably and uniformly across all pixels?&lt;/p&gt;&#xA;" OwnerUserId="88" LastActivityDate="2015-08-18T12:17:27.583" Title="What is fwidth and how does it work?" Tags="&lt;opengl&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="2" />
  <row Id="62" PostTypeId="2" ParentId="51" CreationDate="2015-08-05T14:56:45.127" Score="14" Body="&lt;p&gt;I'd consider just going with 3D noise and evaluating it on the surface of the sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For gradient noise which is naturally in the domain of the surface of the sphere, you need a regular pattern of sample points on the surface that have natural connectivity information, with roughly equal area in each cell, so you can interpolate or sum adjacent values. I wonder if something like a &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1256/qj.05.227/epdf&quot; rel=&quot;nofollow&quot;&gt;Fibonacci grid&lt;/a&gt; might work:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sQQb3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sQQb3.png&quot; alt=&quot;Fibonacci grid on the surface of a sphere&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't chewed through the math to determine how much work it would be to figure out the indices of and distance to your four neighbors (I don't even know if you end up having four well-defined neighbors in all cases), and I suspect it may be less efficient than simply using 3D noise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Someone else has chewed through the math! See this new paper on &lt;a href=&quot;http://lgdv.cs.fau.de/uploads/publications/spherical_fibonacci_mapping.pdf&quot; rel=&quot;nofollow&quot;&gt;Spherical Fibonacci Mapping&lt;/a&gt;. It seems that it would be straightforward to adapt it to sphere noise.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If you are rendering a sphere, not just evaluating noise on the surface of a sphere, and are fine with tessellating your sphere to the resolution of your noise lattice, you can create a &lt;a href=&quot;https://en.wikipedia.org/wiki/Geodesic_grid&quot; rel=&quot;nofollow&quot;&gt;geodesic grid&lt;/a&gt; on the surface of the sphere (a subdivided icosahedron, usually):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/pjVBA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pjVBA.png&quot; alt=&quot;Geodesic sphere&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each vertex of the sphere can have a randomly generated gradient for gradient noise. To get this information to the pixel shader (unless you want straightforward interpolation like value noise), you may need a technique like this article's &lt;a href=&quot;http://codeflow.org/entries/2012/aug/02/easy-wireframe-display-with-barycentric-coordinates/&quot; rel=&quot;nofollow&quot;&gt;wireframe rendering with barycentric coordinates&lt;/a&gt;: do unindexed rendering, with each vertex containing the barycentric coordinates of that vertex in the triangle. You can then read from &lt;code&gt;SV_PrimitiveID&lt;/code&gt; (or the OpenGL equivalent) in the pixel shader, read the three noise gradients from the vertices based on what triangle you are on, and use whatever noise calculation you like using the interpolated barycentric coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think the most difficult part of this method is coming up with a scheme to map your triangle ID to three samples in order to look up the noise values at each vertex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need multiple octaves of noise or noise at a finer resolution than your sphere model, you may be able to do a coarse geodesic grid with vertices and do a few levels of subdivision in the pixel shader. i.e. from the barycentric coordinates, figure out which subdivided triangle you &lt;em&gt;would&lt;/em&gt; be in if the mesh was further tessellated, and then figure out what the primitive ID and barycentric coordinates would be for that triangle.&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2015-11-03T16:14:57.213" LastActivityDate="2015-11-03T16:14:57.213" CommentCount="3" />
  <row Id="63" PostTypeId="2" ParentId="61" CreationDate="2015-08-05T15:10:50.933" Score="12" Body="&lt;p&gt;Pixel screen-space derivatives &lt;em&gt;do&lt;/em&gt; drastically impact performance, but they impact performance whether you use them or not, so from a certain point of view they're free!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every GPU in recent history packs a quad of four pixels together and puts them in the same warp/wavefront, which essentially means that they're running right next to each other on the GPU, so accessing values from them is very cheap. Because warps/wavefronts are run in lockstep, the other pixels will also be at exactly the same place in the shader as you are, so the value of &lt;code&gt;p&lt;/code&gt; for those pixels will just be sitting in a register waiting for you. These other three pixels will &lt;em&gt;always&lt;/em&gt; be executed, even if their results will be thrown away. So a triangle that covers a single pixel will always shade four pixels and throw away the results of three of them, just so that these derivative features work!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is considered an acceptable cost (for current hardware) because it isn't just functions like &lt;code&gt;fwidth&lt;/code&gt; that use these derivatives: every single texture sample does as well, in order to pick what mipmap of your texture to read from. Consider: if you are very close to a surface, the UV coordinate you are using to sample the texture will have a very small derivative in screen space, meaning you need to use a larger mipmap, and if you are farther the UV coordinate will have a larger derivative in screen space, meaning you need to use a smaller mipmap.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as what it means in less mathematical terms: &lt;code&gt;fwidth&lt;/code&gt; is equivalent to &lt;code&gt;abs(dFdx(p)) + abs(dFdy(p))&lt;/code&gt;. &lt;code&gt;dFdx(p)&lt;/code&gt; is simply the difference between the value of &lt;code&gt;p&lt;/code&gt; at pixel x+1 and the value of &lt;code&gt;p&lt;/code&gt; at pixel x, and similarly for &lt;code&gt;dFdy(p)&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-05T15:10:50.933" CommentCount="3" />
  <row Id="64" PostTypeId="2" ParentId="61" CreationDate="2015-08-05T15:13:45.657" Score="8" Body="&lt;p&gt;In entirely technical terms, &lt;code&gt;fwidth(p)&lt;/code&gt; is defined as&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;fwidth(p) := abs(dFdx(p)) + abs(dFdy(p))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And &lt;code&gt;dFdx(p)&lt;/code&gt;/&lt;code&gt;dFdy(p)&lt;/code&gt; are the partial derivates of the value &lt;code&gt;p&lt;/code&gt; with respect to the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; screen dimensions. So they denote how the value of &lt;code&gt;p&lt;/code&gt; behaves when going one pixel to the right (&lt;code&gt;x&lt;/code&gt;) or one pixel up (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how can they be practically computed? Well, if you know the neighbour pixels' values for &lt;code&gt;p&lt;/code&gt;, you can just compute those derivates as direct &lt;strong&gt;finite differences&lt;/strong&gt; as an approximation for their actual mathematical derivatives (which might not have an exact analytical solution at all):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dFdx(p) := p(x+1) - p(x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But of course now you may ask, how do we even know the values of &lt;code&gt;p&lt;/code&gt; (which could afterall be any arbitrarily computed value inside the shader program) for the neighbouring pixels? How do we compute those values without incurring major overhead by doing the whole shader computation two (or three) times?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Well, you know what, those neighbouring values are computed anyway, since for the neighbouring pixel you also run a fragment shader. So all that you need is access to this neighbouring fragment shader invocation when run for the neighbouring pixel. But it's even easier, because those neighbouring values are also computed at the exact same time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Modern rasterizers call fragment shaders in larger tiles of more than one neighbouring pixels. At the smallest those would be a 2x2 grid of pixels. And for each such a pixel block the fragment shader is invoked for each pixel and those invocations run &lt;strong&gt;in perfectly parallel lock-step&lt;/strong&gt; so that all computations are done in the exact same order and at the exact same time for each of those pixels in the block (which is also why branching in the fragment shader, while not deadly, should be avoided if possible, since each invocation of a block would have to explore every branch that is taken by at least one of the invocations, even if it just throws away the results afterwards, as also adressed in the answers to &lt;a href=&quot;http://computergraphics.stackexchange.com/q/259/6&quot;&gt;this related question&lt;/a&gt;). So at any moment, a fragment shader theoretically has access to its neighbouring pixels' fragment shader values. And while you don't have direct access to those values, you have access to values computed from them, like the derivative functions &lt;code&gt;dFdx&lt;/code&gt;, &lt;code&gt;dFdy&lt;/code&gt;, &lt;code&gt;fwidth&lt;/code&gt;, ...&lt;/p&gt;&#xA;" OwnerUserId="6" LastEditorUserId="6" LastEditDate="2015-08-18T12:17:27.583" LastActivityDate="2015-08-18T12:17:27.583" CommentCount="0" />
  <row Id="65" PostTypeId="1" AcceptedAnswerId="75" CreationDate="2015-08-05T15:36:21.923" Score="8" ViewCount="108" Body="&lt;p&gt;Using a gaussian distribution of points on an image plane to calculate a pixel value, what radius/standard deviation will give the most information in the final image? Too large a radius gives a blurred image, and too small a radius neglects information that is smaller than a pixel so that it does not contribute to the final image. Where is the optimal compromise? Is there a single answer to this question or are there circumstances under which it can vary?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm thinking of this in relation to raytracing but I imagine it will apply equally to things like downsizing an image. Where the answers would differ, I am interested in what applies when sampling a continuous image plane, so that the positions of pixels in a larger image cannot be used to determine an optimal radius.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-05T16:25:31.173" LastActivityDate="2016-03-25T21:41:43.557" Title="What is the optimal radius of gaussian distribution for determining pixel colour?" Tags="&lt;sampling&gt;&lt;pixels&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="66" PostTypeId="1" AcceptedAnswerId="70" CreationDate="2015-08-05T16:03:41.260" Score="8" ViewCount="187" Body="&lt;p&gt;I know that depth of field involves blurring.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's some great information about how to do a Gaussian blur in the question &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/39/how-is-gaussian-blur-implemented&quot;&gt;How is Gaussian Blur Implemented?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But, other than that, how is depth of field implemented?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the rules about how you blur each pixel, and how do you handle the case of when there is a pixel that wants to blur a lot next to a pixel that doesn't want to blur as much?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also heard of the &quot;circle of confusion&quot; but have no idea what that is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain DOF in a plain, easy to understand way?&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="71" LastEditDate="2015-08-05T19:59:00.067" LastActivityDate="2015-08-05T21:07:18.557" Title="How is Depth of Field Implemented?" Tags="&lt;blur&gt;&lt;post-processing&gt;&lt;depth-of-field&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" />
  <row Id="67" PostTypeId="1" CreationDate="2015-08-05T16:28:50.313" Score="11" ViewCount="331" Body="&lt;p&gt;Most computer monitors and televisions have a rectangular array of pixels arranged on a square (or nearly square) lattice. Would a hexagonal lattice give better image quality for the same number of pixels? In other words, would the same amount of memory allow storage of more detail?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an intuitive feeling that (at least slightly) more detail should be possible from the same number of pixels with a hexagonal lattice because (for a fixed area image) the average distance to the nearest pixel centre will be lower than with a square lattice. I'd like to see the difference defined more concretely.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Even if the answer is &quot;yes&quot;, I don't expect monitor manufacturers to suddenly start making hexagonally arranged pixels. However, I ask the question because I wonder whether there would be anything to be gained from storing images as hexagonal lattices of pixels, even if they are translated to square lattices of pixels for display purposes.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-05T19:57:24.753" LastActivityDate="2016-03-25T21:25:36.990" Title="Could a hexagonal pixel array store an image more efficiently?" Tags="&lt;pixels&gt;&lt;memory&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="1" />
  <row Id="68" PostTypeId="1" AcceptedAnswerId="72" CreationDate="2015-08-05T16:33:33.160" Score="15" ViewCount="219" Body="&lt;p&gt;Do different monitors (including mobile screens) still use significantly different gamma functions when displaying colour images? Is there a standardised way to represent colour that can then be translated according to the gamma of the monitor, or are they all sufficiently similar to preclude the need for this nowadays?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-11-03T12:45:56.093" Title="Is gamma still important to take into account?" Tags="&lt;color&gt;&lt;gamma&gt;&lt;compatibility&gt;" AnswerCount="2" CommentCount="10" FavoriteCount="1" />
  <row Id="69" PostTypeId="1" AcceptedAnswerId="71" CreationDate="2015-08-05T16:40:58.147" Score="9" ViewCount="57" Body="&lt;p&gt;If rendering an image in 2D, adding depth of field effects (blurring objects further from the focal distance) adds realism and draws the eye to the object of the image. With a 3D (i.e. stereo) image, looking at an object in the image at a given depth will makes objects at all other depths defocused (not blurred, but incorrectly aligned by the eyes, giving a double image). This means that if depth of field effects are used, there will be conflicting results: looking at an object that is at a different depth will cause that depth to be the only depth not having a double image, but it is also a depth that is blurred. This gives the object a property of being focused upon, and a property of not being focused upon. In a 3d still image, are depth of field effects detrimental to the acceptance of the image by the eye, or are there ways around this?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-05T16:56:09.080" LastActivityDate="2015-08-05T17:39:24.690" Title="Is depth of field incongruous in a 3D still image?" Tags="&lt;depth-of-field&gt;&lt;3d&gt;&lt;stereo-rendering&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="70" PostTypeId="2" ParentId="66" CreationDate="2015-08-05T17:03:21.057" Score="18" Body="&lt;p&gt;The depth of field is a characteristic of a camera lens setting, although the name &quot;Depth of Field&quot; is commonly used to describe the effect caused by such characteristic. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Camera lenses can only perfectly focus on one single point, but there is a distance for which the image will still look reasonably sharp. Such distance is what actually the Depth Of Field is. This distance is variable depending on various factors, but let's say for the sake of brevity that the easiest way to adjust it is by changing your camera aperture. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ezhSb.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ezhSb.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the circle of confusion it is helpful to look a pic from wikipedia: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/g7MnD.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/g7MnD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In all of the three cases a single point is &quot;projected&quot; onto our image plane, but as you can see that is at different distances from the lens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically because, as I said, not everything is perfectly in focus (that is the focal point is on the image plane), a point can be projected to our image plane not to a single point but to an area. This, as it is clear from the pic, is because the focal point is either before or after the image plane. This &quot;circle&quot; that is created on the image plane is what is called the circle of confusion. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now from this should be clear that varying the depth of field you vary the CoC size, hence it can be used as a measure of the DoF itself and an intuitive one when thinking in terms of blurring. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As per how it is implemented in rendering there are really lots of methods out there. Usually in games is done as a post process using the depth information to split your scene in different &quot;planes&quot;; for example a focus plane that does not need to be blurred, a near plane and a far/background plane that need to be blurred. Once you blurred your planes you can composite them back together to get your final image. &#xA;How much you blur the various planes it is dependent on the effect you want to achieve. Advanced system usually implement this and other &quot;lenses&quot; effect using appropriate parameters coming from the photography world like the aperture. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that there are many ways to implement the effect, the one I described is just one way that also has variations (for example performing the whole thing at lower res). The limit is dependent on what your target is and how much you budgeted for this effect. You can go from adopting the &quot;3 planes&quot; method at extremely low res to computing the circle of confusion for each pixel and applying the consequent ad-hoc blur to that pixel. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT: The comment from @NathanReed needs to be more in evidence as part of the answer: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Matt Pettineo's blog posts &lt;a href=&quot;https://mynameismjp.wordpress.com/2011/02/28/bokeh/&quot;&gt;How To Fake Bokeh&lt;/a&gt; and &lt;a href=&quot;https://mynameismjp.wordpress.com/2011/04/19/bokeh-ii-the-sequel/&quot;&gt;Bokeh II: The Sequel&lt;/a&gt; are great introductions to how to practically implement post-process DoF and address the typical artifacts you get from it. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-05T21:07:18.557" LastActivityDate="2015-08-05T21:07:18.557" CommentCount="1" />
  <row Id="71" PostTypeId="2" ParentId="69" CreationDate="2015-08-05T17:39:24.690" Score="6" Body="&lt;p&gt;In traditional stereo 3D, I don't believe that there is a way to make a fixed focal plane feel natural to the viewer. When looking at an out-of-focus object in stereo 3D, the object remains out-of-focus, causing conflicting cues. The lens in the eye tries to adjust to bring the object into focus, but of course it won't succeed, causing eye strain and headaches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there is hope outside stereo 3D: Lightfield displays, such as &lt;a href=&quot;https://research.nvidia.com/publication/near-eye-light-field-displays&quot;&gt;this nvidia prototype&lt;/a&gt;, go a different route. In stereo 3D, the light in the scene is already captured by two virtual (or physical) cameras, &quot;baking&quot; in the focal plane. Head-mounted displays like Oculus Rift then attempt to tape two displays in front of your eyes in such a way that the retina receives the exact same image that was captured by the camera. Lightfield displays go a different route: Instead of capturing two images ahead of time, they reproduce the entire 4D light field in front of your eyes, allowing your eyes to capture the image as if they were sitting directly inside the virtual scene. This has a number of benefits, including much smaller and lighter hardware as well as giving your eyes the ability to refocus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there is a way to make lightfield displays technically and commercially viable, then I believe they can remove the need for depth of field and fixed focal planes entirely and make VR feel a whole lot more comfortable for the viewer. However, it is likely not possible to construct lightfield screens, so televisions and cinemas won't be able to use this technology.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-05T17:39:24.690" CommentCount="0" />
  <row Id="72" PostTypeId="2" ParentId="68" CreationDate="2015-08-05T17:53:18.857" Score="13" Body="&lt;p&gt;Yes, while many screens and OS operations are using a gamma of 2.2 your hardware and computation result still need to be corrected. There are also special mediums such as broadcast TV's that have a different gamma. Sensor equipment like cameras are mostly linear so they need to be adjusted accordingly. Besides this the user can set their system to whatever gamma they like. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In computer graphics primary reason to account for gamma is that your computation is most likely done in linear space* or summing light contributions becomes unnecessarily complicated. In any case gamma is really a simplification of things, you'd be much much better of doing profile to profile conversions if its possible to invest the computational time to do so.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Mathematical explanation&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;This may be a simpler to understand. Your monitor has a display error of $g(x)$ which is no problem if your data is conforming to $g(x)$, however if you want to calculate:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ a+b+c+d $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and $a$, $b$, $c$, $d$, have the error of $g(x)$ you would actually need to calculate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ g(g^{-1}(a)+g^{-1}(b)+g^{-1}(c)+g^{-1}(d)) $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You would need to do this for each sub element over and over again. Therefore instead of running the transform each time you transform all once to linear and then once back. &lt;/p&gt;&#xA;&#xA;&lt;h2&gt;And finally a artistic reason&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;It may just look better with a different gamma. Many games have a gamma adjustment for this reason.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* Worst case is that you think your computations as linear but do not compensate for the monitors nonlinear output charachteristics.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="16" LastEditDate="2015-11-03T12:45:56.093" LastActivityDate="2015-11-03T12:45:56.093" CommentCount="3" />
  <row Id="73" PostTypeId="2" ParentId="57" CreationDate="2015-08-05T18:15:47.057" Score="7" Body="&lt;p&gt;The idea i would try to apply would be the following, i make the example for the curve but it should be straightforward the application for the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say we have a curve $\gamma$ uniformly parametrized, let's say the parameter of the curve is $s$. Your goal is to sample point corresponding to value of $s$ such that the curvature is high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you get the magnitude of the curvature $c$ this will be function of $s$ too, so if you normalize the function $|c|$ you will get a probability distribution. If you get the integral of such distribution you will have the cumulative distribution, let's call the cumulative function $C(s)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The sampling problem from a distribution given the cumulative function is well known, so basically once you have sampled a set of value $s_0,s_1,\dots,s_n$ such value will be related to the points of interest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The application of this method to the case of surface should be straight since basically you have a two dimensional cumulative distribution function, but the sampling problem is exactly the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just to give some detail... basically sampling from a distribution given the cumulative function involves two steps. &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;take a random value in the interval $[0,1]$, let's say $k$ &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;solve the equation $C(s) = k$.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This approach is exact, of course it is expensive, but if you like such approach you can work on optimization.&lt;/p&gt;&#xA;" OwnerUserId="228" LastEditorUserId="127" LastEditDate="2015-11-05T17:09:56.003" LastActivityDate="2015-11-05T17:09:56.003" CommentCount="5" />
  <row Id="74" PostTypeId="2" ParentId="67" CreationDate="2015-08-05T18:53:15.353" Score="8" Body="&lt;p&gt;Here's my take on it.  A pixel is not a square, and it isn't even a rectangle.  A pixel is a point (infinitely small) that has a color associated with it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only way I personally have ever seen pixels viewed (interpreted) by a display is to use &quot;nearest neighbor&quot; sampling where the pixels were on a rectangular grid, which means that the color of any given space on a display is the color of the pixel that it is nearest to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a fancy way of saying &quot;pixels are rectangular and laid out on a grid&quot;, but stay with me on this :P&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a result, image formats have their pixels stored in a grid as well, with the assumption that nearest neighbor in a grid will also be used to display it.  For instance, many images will have anti aliasing built into them so that they will look good when displayed on a &quot;nearest neighbor grid&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Interactive applications (games), may use textures that are not meant to be displayed as is on nearest neighbor grids so are kind of an exception to that rule.  They do this because as part of their execution, they do anti aliasing, bilinear texture sampling, etc, so that whatever picture they push out to the display will look good, when the display shows it as a nearest neighbor grid!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, getting closer to your question: would a hexagonal grid have any advantages?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think that yes, it would!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First off, I think nearest neighbor would look better.  I don't have any real proof of that sorry, but the hexagon more closely approximates a circle, and since it isn't a regular grid of data, i think your eye is getting a better distribution of data.  Sorry, that is a little hand wavey.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think the big part of why it would look better though is that linear filtering would be taking information from 6 neighbors instead of 4, and would be interpolating on 3 axis instead of 2.  More information from less regularly spaced samples than a grid gives you = better resulting image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Doing cubic interpolation would also be better than cubic interpolation on a grid, so the quality scales up as you scale up the quality of your algorithm too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as whether it stores data more efficiently, the fact that it can do better filtering with less data means to me that yes, it could store data more efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And i guess lastly... maybe you could use these properties to your advantage.  Maybe you could have an image format stored in a lower res hexagonal format, and then before you needed to display the image at runtime, you could use sampling algorithms to convert it back to a grid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe there'd even be a good way to do this efficiently in a pixel shader, so it would use less memory at runtime too?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's an interesting idea (:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS - how cool would it be to have an analog display (no individual pixels, but a continuous colored surface) that looked at the pixel data you sent not as rectangles, but instead as sample points on a continuous surface.  Maybe a bit out there though....&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-05T18:53:15.353" CommentCount="2" />
  <row Id="75" PostTypeId="2" ParentId="65" CreationDate="2015-08-05T20:52:43.003" Score="6" Body="&lt;p&gt;I'm not sure that there's a truly &lt;em&gt;optimal&lt;/em&gt; radius&amp;mdash;it's going to be a subjective matter based on what the image looks like.  As you say, too large a radius results in blurring and too small a radius results in aliasing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I like to set sigma = 0.5 px, so that the overall radius is about 1.5 px (since the Gaussian has the majority of its weight within ±3 sigma of its mean).  In my experience that gives a good trade-off between blurring and aliasing, but that's just my taste, not based on any objective considerations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way, as part of &lt;a href=&quot;http://www.reedbeta.com/blog/2014/11/15/antialiasing-to-splat-or-not/&quot;&gt;a blog post on antialiasing&lt;/a&gt; I wrote last year (which was based on an answer I posted on the previous incarnation of this site!), I tested a variety of antialiasing kernels against a synthetic test image and came out with 0.5 px Gaussian as my subjective favorite.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T20:52:43.003" CommentCount="1" />
  <row Id="76" PostTypeId="2" ParentId="65" CreationDate="2015-08-05T20:58:28.940" Score="2" Body="&lt;p&gt;In my opinion and experience i don't think there exists an univocal answer... since basically in literature you can easily find example of adaptive filters too (i.e. of variable size). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think the actual answer should be related to the both context of applications (i.e. hardware or sofware, real time or not) and kind of scene you're going to synthesize (some scenes usually involves different kind of aliasing when are synthetized (i use this general term on purpose)). Basically computer graphics is the study of algorithms and data structure for image synthesis, and such definition isn't stricly related to any kind of application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course an important factor is even the goal to be achieved by a filtering process (i.e. not necessary an excessive blurring could be bad...).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're speaking of &quot;nice to see&quot; i think you could agree with me when i say that there's no specific measure of &quot;pleasant image&quot;.&lt;/p&gt;&#xA;" OwnerUserId="228" LastActivityDate="2015-08-05T20:58:28.940" CommentCount="0" />
  <row Id="77" PostTypeId="2" ParentId="58" CreationDate="2015-08-05T22:04:17.097" Score="8" Body="&lt;p&gt;Generally, uniformly-weighted samples with a variable distribution (importance sampling) gives lower variance in the final average than uniformly-distributed samples with variable weights.  This is a common rule of thumb in Monte Carlo raytracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, another thing to consider is that you'll eventually be converting the images to RGB for display (I assume).  So a potential problem might be that if a light source has very little energy in the blue part of the spectrum, for instance, then you'll put few samples in the blue frequencies, and the blue channel of the final RGB image could end up excessively noisy compared to the other channels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way to resolve this might be to consider the product of the light source's spectrum with the RGB color-matching curves used to generate the output.  You could normalize the three against each other to ensure you get enough samples in all three channels, but still distribute the samples to the most important frequencies for each channel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On balance, I suspect that simply using a uniform frequency distribution of samples will be simpler and give good results as long as the light source spectra are fairly smooth.  But if you have spectra with sharp spikes (e.g. LEDs, lasers, fluorescent lamps) then spectral importance sampling will probably be necessary.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-05T22:04:17.097" CommentCount="0" />
  <row Id="78" PostTypeId="2" ParentId="38" CreationDate="2015-08-05T22:23:19.080" Score="11" Body="&lt;p&gt;That blog post that John mentions is a pretty good start (if I do say so myself!), but there's a bit of extra detail that might be helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the PowerVR architecture, the intermediate data structure -- variously called the primitive list or parameter buffer (PB) -- that stores the per-tile data, after all vertex shading and the tiling process is complete, is actually mostly generated and managed by the hardware, rather than the driver.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The PB's in-memory structures are physically split into two. First, blocks of transformed vertex data, including vertex attributes. The blocks are compressed, and as you can imagine they're just packed and compressed floating point data for the most part. The second in-memory structure is the tiling data, which is effectively a list of lists.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The top level list in that data structure is called a region, and it can encode a set of tiles rather than single tile at a time, for a given primitive block. A region is therefore a set of locations of screen tiles, tile states, and then a list of the compressed blocks that hold geometry in that region. Regions are what the rasteriser works on, and you can imagine that empty tiles are just automatically skipped, although there's a good reason in some cases for the rasteriser to visit empty regions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The memory the GPU uses for the PB is dynamically allocated in all modern PowerVR implementations. A pointer to that memory is provided by the driver, and the driver, with the GPU's help, will size it as required. That mechanism is a tradeoff between having to reallocate frequently and minimising the amount of allocated PB space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Modern GPUs try really hard to minimise memory indirection, but walking the PB to feed the rasterisation stage is one of those cases where it's really difficult and there's no other choice. Thankfully the pointer chasing wraps up large blocks that cache well and are streamed into the core.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other architectures don't work exactly the same as PowerVR, because part of the reason the PB is the way it is in our architecture is to help the fully deferred pixel shading concept we implement, but the general concept applies to all other tilers in the mobile space that I'm aware of.&lt;/p&gt;&#xA;" OwnerUserId="259" LastActivityDate="2015-08-05T22:23:19.080" CommentCount="0" />
  <row Id="79" PostTypeId="2" ParentId="23" CreationDate="2015-08-06T04:25:20.727" Score="9" Body="&lt;p&gt;I tested several applications/APIs/libraries to get data from FBOs in applications which use gbuffer for example. After months of suffering I discovered &lt;a href=&quot;https://apitrace.github.io/&quot;&gt;apitrace&lt;/a&gt; available on Github to debug OpenGL, Direct3D and WebGL. I used in Windows and Linux without problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that could be useful for you.&lt;/p&gt;&#xA;" OwnerUserId="262" LastEditorUserId="260" LastEditDate="2015-08-06T04:28:51.917" LastActivityDate="2015-08-06T04:28:51.917" CommentCount="0" />
  <row Id="80" PostTypeId="2" ParentId="68" CreationDate="2015-08-06T06:12:42.647" Score="9" Body="&lt;p&gt;The de facto standard color space for digital images these days is &lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB&quot;&gt;sRGB&lt;/a&gt;.  sRGB is a good default assumption if working with a display whose exact color space is not known (i.e. most random displays someone might run your app on), or images whose color space encoding is not known (i.e. most random image files you might encounter).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The sRGB standard defines the &lt;a href=&quot;https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_xy_chromaticity_diagram_and_the_CIE_xyY_color_space&quot;&gt;CIE chromaticity&lt;/a&gt; of the pure red, green, and blue primaries and the white point&amp;mdash;in other words, it defines what those primaries and white should perceptually look like relative to pure wavelengths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;sRGB also defines a gamma curve that's used for encoding the RGB values.  The gamma curve is the part that graphics programmers are usually concerned with, as we have to convert colors back and forth between sRGB and linear to do lighting math physically-correctly.  All modern GPUs have sRGB support built in: they can automatically apply the gamma transformations in hardware when sampling a texture, or writing a pixel value to a render target.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as monitors are concerned, with a high-quality one it should be possible to calibrate its settings (or it may come pre-calibrated) so that its output matches sRGB as faithfully as possible.  In case the monitor itself can't do it, a limited amount of color correction can also be done on the GPU during scan-out; there are some small hardware lookup tables that the RGB values are mapped through before being sent out over the wire.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You might also come across &lt;a href=&quot;https://en.wikipedia.org/wiki/Rec._709&quot;&gt;Rec. 709&lt;/a&gt;, which is the standard color space for HDTVs; it's very similar to sRGB, using the same primaries and white point, but a slightly different gamma curve.  Some high-end monitors use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Adobe_RGB_color_space&quot;&gt;Adobe RGB&lt;/a&gt; color space, which is somewhat wider-gamut than sRGB; photographers tend to like those because they more faithfully represent what photos will look like when printed.  The next generation of HDR TVs coming out (hopefully) in the next few years will use &lt;a href=&quot;https://en.wikipedia.org/wiki/Rec._2020&quot;&gt;Rec. 2020&lt;/a&gt;, which has a huge gamut and requires 10 or 12 bits per component rather than 8.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So to come back to your question whether you need to worry about different monitors having different gamma: not much.  For gaming and general PC graphics you can pretty much assume sRGB, and figure that if the user really cares about color accuracy, they'll have a good, calibrated monitor.  If you're producing software for photographers or print media, or for next-gen HDR video standards, then you might have to start worrying about wide-gamut color spaces.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-06T06:12:42.647" CommentCount="1" />
  <row Id="81" PostTypeId="1" AcceptedAnswerId="82" CreationDate="2015-08-06T06:34:33.160" Score="12" ViewCount="358" Body="&lt;p&gt;If you read papers about subsurface scattering, you'll frequently come across references to something called the &quot;dipole approximation&quot;.  This term seems to go back to the paper &lt;a href=&quot;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&quot;&gt;A Practical Model for Subsurface Light Transport&lt;/a&gt; by Henrik Wann Jensen et al, but this paper is pretty difficult to understand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain in relatively simple terms what the dipole approximation is and how it's used in rendering subsurface scattering?&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-10T21:56:10.297" Title="What is the &quot;dipole approximation&quot; for subsurface scattering?" Tags="&lt;rendering&gt;&lt;subsurface-scattering&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="5" />
  <row Id="82" PostTypeId="2" ParentId="81" CreationDate="2015-08-06T07:31:40.703" Score="18" Body="&lt;p&gt;The assumption underlying such model is the same as lots of other models for skin rendering; the subsurface scattering can be approximated as a diffusion phenomenon. This is good because in highly scattering media, the distribution of light loses dependency from the angle and tends to isotropy. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The dipole approximation is a formulation for the resolution of such diffusion problem in an analytical fashion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically they start by approximating the BSSRDF as a multiple scattering and single scattering component. The multiple scattering is then defined as: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/3OivX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3OivX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $F_t$ are Fresnel terms and $R$ is the diffusion profile expressed as function of the distance between the entry and exit point.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This $R$ is referred to as diffusion profile and they formulate this profile via a dipole approximation. The contribution of the incoming light ray is considered to be the one of two virtual sources: one negative beneath the surface and one positive above it (that's why dipole) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/CJQLF.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CJQLF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here in the picture r is the $\|x_i - x_o\|$ above.  The contribution of those light sources is dependent on various factors such as the distance of the light from the surface, scattering coefficient etc. (See below for a more detailed description of the formula itself). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This model only account for multiple scattering events, but that's fine enough for skin. It must be noticed though that for some translucent materials (e.g. smoke and marble) the single scattering is fundamental. That paper propose a single scattering formulation, but is expensive. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The diffusion profile is usually approximated for real-time application as a series of gaussian blurs (like in the seminal works of D'Eon et al. in GPU Gems 3 then used for the Jimenez's SSSSS) so to make it practical for real time scenarios. In &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch14.html&quot; rel=&quot;nofollow&quot;&gt;this wonderful paper&lt;/a&gt; there are details on such approximation. &#xA;A picture from that paper show actually how good is this formulation: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/0GcPn.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0GcPn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As side note the dipole approximation assumes that the material is semi-infinite, however this assumption doesn’t hold with thin slabs and multi-layered material such as the skin. Building on the dipole work, Donner and Jensen [2005] proposed the multi-pole approximation that accounts for the dipole problems. &#xA;With this model instead of a single dipole, the authors use a set of them to describe the scattering phenomenon. In such formulation the reflectance and transmittance profiles can be obtained by summing up the contribution of the different dipoles involved&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT: &#xA;I am putting here the answers to a couple of @NathanReed 's questions in the comment section:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Even with the diffusion profile approximation, the BSSRDF model still requires integrating over a radius of nearby points on the surface to gather incoming light, correct? How is that accomplished in, say, a path tracer? Do you have to build some data structure so you can sample points on the surface nearby a given point?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The BSSRDF approximation still need to be integrated over a certain area, yes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the paper linked they used a Montecarlo ray-tracer randomly sampling around a point with a density defined as: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\sigma_{tr}e^{-\sigma_{tr}d}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where that sigma value is the effective extinction coefficient defined below ( it is dependent on scattering and absorbing coefficient, which are properties of the material) and d is the distance to the point we are evaluating. This density is this way defined because the diffusion term has exponential fall-off. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://graphics.ucsd.edu/~henrik/papers/fast_bssrdf/fast_bssrdf.pdf&quot; rel=&quot;nofollow&quot;&gt;[Jensen and Buhler 2002]&lt;/a&gt; they proposed an acceleration technique. One of the main concepts was to decouple the sampling from the evaluation of the diffusion term. This way they perform a hierarchical evaluation of the information computed during the sampling phase to cluster together distant samples when it comes to evaluating the diffusion. The implementation described in the paper uses an octree as structure. &#xA;This technique, according to the paper, is order of magnitude faster than the full Monte Carlo integration.&lt;br&gt;&#xA;Unfortunately I never got myself into an off-line implementation, so I can't help more than this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the real-time sum-of-Gaussians approximations the correct radius is implicitly set when defining the variance of the Gaussian blurs that need to be applied.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Why one positive and one negative light? Is the goal for them to cancel each other in some way?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Yes, the dipole source method (which dates way before Jensen's paper) is such defined to satisfy some boundary condition. Specifically the fluence must be zero at a certain extrapolated boundary that has a distance from the surface of $2AD$ where &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/rOdqD.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/rOdqD.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Being $F_{dr}$ the fresnel reflectivity of the slab considered and that sigma value is the reduced extinction coefficient described below. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT2: I have expanded (a tiny bit) some of the concepts in this answer in a blog post: &lt;a href=&quot;http://bit.ly/1Q82rqT&quot; rel=&quot;nofollow&quot;&gt;http://bit.ly/1Q82rqT&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;For those who are not scared by lots of greek letters in a formula, here's an extract from my thesis where the reflectance profile is briefly described in each term: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xtQG8.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xtQG8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2016-02-10T21:56:10.297" LastActivityDate="2016-02-10T21:56:10.297" CommentCount="8" />
  <row Id="83" PostTypeId="1" CreationDate="2015-08-06T13:41:45.743" Score="7" ViewCount="72" Body="&lt;p&gt;&lt;a href=&quot;http://computergraphics.stackexchange.com/questions/68/is-gamma-still-important-to-take-into-account&quot;&gt;Trichoplax question&lt;/a&gt; aroused my curiosity and &lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/11445/gamma-vs-brightness-any-difference&quot;&gt;the answers in this question&lt;/a&gt; also reminded me why I sometimes use different gamma &quot;amounts&quot; to enhance images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, is it possible to determine the amount of gamma applied to an image by knowing its source (the original image)? I.e. can the mathematical formula be applied to compare two images and determine the difference in gamma &quot;amount&quot;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: which of the following have a different gamma amount (I'll give you a non-edible cookie if you find out, and sorry trichoplax for snatching &lt;a href=&quot;http://i.stack.imgur.com/iukw5.png?s=328&amp;amp;g=1&quot;&gt;your ball&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/msMpT.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/msMpT.png&quot; alt=&quot;t_1&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/qMPrK.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qMPrK.png&quot; alt=&quot;t_2&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/5GsqI.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5GsqI.png&quot; alt=&quot;t_3&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/NxJLB.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/NxJLB.png&quot; alt=&quot;t_4&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-11-03T14:47:58.327" Title="Brightness and contrast VS Gamma, is it possible to identify the gamma &quot;correction amount&quot;?" Tags="&lt;color&gt;&lt;gamma&gt;&lt;brightness&gt;&lt;image-comparison&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="84" PostTypeId="2" ParentId="83" CreationDate="2015-08-06T14:04:38.327" Score="6" Body="&lt;p&gt;If I get correctly what you are asking you basically just need to find the G in this equation: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$Image_{out} = Image_{in}^G$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This could be easily solved as &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$G = \frac{\log{Image_{out}}}{\log{Image_{in}}}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because usually gamma is applied in a uniform fashion on the image, you can just pick any two non zero pixel values (one for source and one for destination) to find out the gamma value applied.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: As @ChristianRau pointed out, if you don't actually know if the image has been modified with gamma correction, you have to take an higher amount of samples and trying to fit a gamma function on those. If a fit to a gamma function results in too many significant outliers, then probably gamma correction wasn't the function applied. &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="137" LastEditDate="2015-11-03T14:47:58.327" LastActivityDate="2015-11-03T14:47:58.327" CommentCount="4" />
  <row Id="85" PostTypeId="1" AcceptedAnswerId="86" CreationDate="2015-08-06T14:53:38.463" Score="9" ViewCount="131" Body="&lt;p&gt;Bicubic sampling is pretty good for up sampling an image and making it larger,  but is it a good choice for down sampling as well? Are there better choices?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-06T18:02:02.100" Title="Algorithms for down sampling an image?" Tags="&lt;texture&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="86" PostTypeId="2" ParentId="85" CreationDate="2015-08-06T15:07:23.013" Score="10" Body="&lt;p&gt;When Sean and I wrote &lt;a href=&quot;https://github.com/nothings/stb/blob/master/stb_image_resize.h&quot;&gt;stb_image_resize&lt;/a&gt; we chose Mitchell for downsizing. Mitchell is similar to Cubic, you can read about the cubic class of sampling filters in &lt;a href=&quot;http://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf&quot;&gt;Mitchell Netravali 1988&lt;/a&gt;. They are all pretty similar and will get you very similar results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't find any written record between Sean and I of why we decided to go with Mitchell, but if memory serves we just resampled a bunch of images and used the algorithm that we thought looked best. I wouldn't say that there is one authoritative or best filter, you should use the one that looks best on your data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: Like joojaa says, a windowed sinc filter is also good, if not quite as cheap. You can find some implementations &lt;a href=&quot;https://code.google.com/p/imageresampler/source/browse/trunk/resampler.cpp#266&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="125" LastEditorUserId="125" LastEditDate="2015-08-06T18:02:02.100" LastActivityDate="2015-08-06T18:02:02.100" CommentCount="4" />
  <row Id="87" PostTypeId="2" ParentId="5" CreationDate="2015-08-06T19:04:26.033" Score="9" Body="&lt;p&gt;One possible approach could be the use of Hardware Occlusion Query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can use the facts that, by specification, the Stencil Test is executed before the depth test, and only the fragments that pass the depth test are counted by the Occlusion Query.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple example (not tested) would be like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    GLuint samples_query = 0;&#xA;    GLuint samples_passed = 0;&#xA;    glGenQueries(1, &amp;amp;samples_query);&#xA;    // Initialize your buffers and textures ...&#xA;    glEnable(GL_DEPTH_TEST);&#xA;    glEnable(GL_STENCIL_TEST);&#xA;&#xA;    // Set up the values on the stencil buffer ...&#xA;&#xA;    // Now we count the fragments that pass the stencil test&#xA;    glDepthFunc(GL_ALWAYS); // Set up the depth test to always pass&#xA;    glBeginQuery(GL_SAMPLES_PASSED, samples_query);&#xA;    // Render your meshes here&#xA;    glEndQuery(GL_SAMPLES_PASSED);&#xA;    glGetQueryObjectuiv(samples_query, GL_QUERY_RESULT, &amp;amp;samples_passed);&#xA;    // samples_passed holds the number of fragments that passed the stencil test (if any)&#xA;&#xA;    // Release your resources ...&#xA;    glDeleteQueries(1, &amp;amp;samples_query);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that the call to obtain the number of samples will call forcibly the flush of the pipeline and wait for the query to finish.&#xA;If you need a more asynchronous approach you can query wether the occlusion query is done or not by using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    GLuint query_done = 0;&#xA;    glGetQueryObjectuiv(samples_query, GL_QUERY_RESULT_AVAILABLE, &amp;amp;query_done);&#xA;    if (query_done != 0)&#xA;        // Your query result is ready&#xA;    else&#xA;        // Maybe check the next frame?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="281" LastEditorUserId="281" LastEditDate="2015-08-07T09:06:45.063" LastActivityDate="2015-08-07T09:06:45.063" CommentCount="0" />
  <row Id="88" PostTypeId="1" AcceptedAnswerId="108" CreationDate="2015-08-06T21:21:07.490" Score="3" ViewCount="114" Body="&lt;p&gt;Start with a precalculated &quot;ray cloud&quot; - the starting point and direction of a large number of rays, most of which will not contribute to the image. The image plane's position and orientation are then specified, and the task is to search the ray cloud for rays that intersect the image plane without occlusion by other objects, and output the resulting pixel values. No ray reflections would be required, as all of the work has already been done in generating the ray cloud. The only step left is to convert the rays to an image. This means that a single ray cloud can be used to generate images from arbitrary view angles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could I go about accelerating this process using a GPU? The decision for each ray of whether it intersects the image plane is parallelisable as these decisions are independent, but each requires access to the objects in the scene as some rays will reach some potential image planes but not others. The final step of sampling the relevant rays across the pixels of the image requires each ray to contribute to a number of the surrounding pixels. Is this also parallelisable using a GPU?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;To clarify, &lt;a href=&quot;http://computergraphics.stackexchange.com/users/310/richiesams&quot;&gt;RichieSams&lt;/a&gt;' understanding is correct:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Start with a set of rays pointing in arbitrary directions.&lt;/li&gt;&#xA;&lt;li&gt;Find which of these rays intersect with the image plane, taking into account occlusion. &lt;/li&gt;&#xA;&lt;li&gt;Calculate the colour of each pixel based on the rays that hit the image plane.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Ideally I would like to be able to accelerate this process sufficiently to move around a scene in real time, given a precomputed ray cloud over the whole scene.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T12:14:14.533" LastActivityDate="2015-08-11T12:14:14.533" Title="Can I accelerate rendering an image from a ray cloud using a GPU?" Tags="&lt;raytracing&gt;&lt;gpu&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="89" PostTypeId="1" AcceptedAnswerId="195" CreationDate="2015-08-06T21:41:28.387" Score="10" ViewCount="143" Body="&lt;p&gt;I want to render realistic images of water in an orbiting space habitat. The image does not need to be generated in real time, although I wouldn't want it to take weeks either. I'm looking for an approach that can generate realistic images in hours or days.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The habitat is cylindrical with the curved inner surface being the living space. Rotation of the cylinder about it's axis provides an approximation of gravity. I'm not looking for details of simulating the physics of this, just the rendering of an image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The specific aspect I want to know about is polarisation. Light reflected from the surface of water is polarised, leaving the light that passed into the water polarised perpendicularly to the reflected light. Ignoring this effect and simply modelling the proportions of light that are reflected and transmitted works reasonably well when there is only one water surface, but if the cylindrical habitat has bodies of water that take up large proportions of the curved surface then a given ray will make multiple reflections at a wide range of different angles. This means the proportion of light that is reflected will depend on the polarisation angle previously applied to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there existing approaches that incorporate such effects that could give realistic images of multiple reflections from a curved water surface? They would also need to model refraction with polarisation. The water will be shallow in places so I'm expecting polarised refraction to influence the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If not, could I adapt an existing ray tracer or would this need an approach starting from scratch?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for realism in order to discover unexpected effects, not just to pass as realistic to a casual observer. Obviously most observers (including me) won't know the effects to look for since they aren't familiar from everyday life, so I'm looking for &quot;reasonably physically correct&quot; rather than just &quot;convincing&quot;.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-11T05:45:18.720" Title="Polarising reflection and refraction for a wrap around water surface" Tags="&lt;raytracing&gt;&lt;physically-based&gt;&lt;reflection&gt;&lt;refraction&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="90" PostTypeId="1" AcceptedAnswerId="92" CreationDate="2015-08-06T21:47:38.553" Score="8" ViewCount="112" Body="&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Double-slit_experiment&quot;&gt;Young's double slit experiment&lt;/a&gt; is very simple to set up and simple to explain, but it is an example of both diffraction and interference, neither of which are modelled by conventional raytracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is straightforward to render an approximation of the result using textures, but that would require knowing in advance what the result should be. For an arbitrary set up, where the number and arrangement of slits is not known in advance, are there any existing algorithms for modelling the effect in order to generate the correct resulting image?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If not, what would a model need to include in order to accurately produce these effects? Can ray tracing be adapted to use rays carrying additional information, or would an entirely new approach be required?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="30" LastEditDate="2015-08-07T16:52:10.167" LastActivityDate="2015-08-07T16:52:10.167" Title="Modelling Young's double slit experiment" Tags="&lt;raytracing&gt;&lt;algorithm&gt;&lt;physically-based&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="91" PostTypeId="1" AcceptedAnswerId="113" CreationDate="2015-08-06T22:03:19.767" Score="7" ViewCount="95" Body="&lt;p&gt;Is it realistic to render a super-high resolution image over an array of 3 by 3 or 5 by 5 (for example) stacked screens? Could I do this by combining several different GPUs, or would the only way be to use a GPU per screen and simply divide the image to be rendered?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally I'd like to be able to use fewer GPUs than screens. Even though one GPU is insufficient I'm hoping I won't need n GPUs for n screens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this is possible I'd also need to know whether this requires using the same make and model of GPU or whether unrelated ones could still share processing.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-07T08:46:51.577" LastActivityDate="2015-08-08T07:46:26.633" Title="Can I use several GPUs for a grid multi screen image?" Tags="&lt;gpu&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="92" PostTypeId="2" ParentId="90" CreationDate="2015-08-06T22:12:49.037" Score="8" Body="&lt;p&gt;It is in fact possible to augment a ray tracer to make it capable of simulating wave effects. The paper &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.369.2189&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Rendering Wave Effects with Augmented Light Field&#xA;&lt;/a&gt; describes one way to do it. In summary, they introduce a framework called Augmented Light Field that allows them to model wave effects with a ray-based representation. In this framework, rays can carry negative radiance in addition to positive radiance; intuitively, rays carrying negative radiance can &quot;subtract&quot; light from surfaces that it cannot reach because of interference effects, for example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't implemented it personally, and I can't speak for its implementation complexity, but I believe it is a good starting point.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-06T22:12:49.037" CommentCount="0" />
  <row Id="93" PostTypeId="2" ParentId="88" CreationDate="2015-08-07T04:01:49.707" Score="2" Body="&lt;p&gt;It sounds like the tricky part is that last step where each ray affects neighboring pixels. I would consider an approach where you do the first steps as normal (parallelized as on the CPU) without the last step, and then you do that last bit in a new pass. I'm not sure exactly how your ray contributions work so it's tough to say, but it seems like you should be able to use the results from the first part as input and then do your operation in a second pass.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blur is done in a similar manner. The blur is done in one dimension, and then the output of that pass is used as the input of the second pass to blur the other dimension. The result is that the value of some pixel ends up contributing to neighboring pixels, so it's similar to your problem.&lt;/p&gt;&#xA;" OwnerUserId="125" LastActivityDate="2015-08-07T04:01:49.707" CommentCount="0" />
  <row Id="94" PostTypeId="1" AcceptedAnswerId="111" CreationDate="2015-08-07T05:02:41.923" Score="8" ViewCount="103" Body="&lt;p&gt;Say I have render targets 0 through N and RT 0 happens to have in its fourth component an alpha channel specified by a material or mask or something.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to have the hardware compositor blend render targets 1 through N using the alpha of the first render target?&lt;/p&gt;&#xA;" OwnerUserId="87" LastActivityDate="2015-08-08T06:30:49.560" Title="Is it possible to alpha blend multiple render targets using a specified alpha?" Tags="&lt;opengl&gt;&lt;directx11&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="95" PostTypeId="1" CreationDate="2015-08-07T07:38:01.290" Score="19" ViewCount="183" Body="&lt;p&gt;When rendering two overlapping co-planar surfaces, a common issue is &quot;z-fighting&quot;, where the renderer can't decide which of the two surfaces is closer to the camera, giving visual artifacts in the area of overlap.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The standard solution to this is to give the surfaces a slight offset when designing the model.  Is there any other solution?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2016-05-20T13:51:31.210" Title="Avoiding z-fighting with coincident surfaces" Tags="&lt;rendering&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="1" />
  <row Id="96" PostTypeId="1" AcceptedAnswerId="101" CreationDate="2015-08-07T08:24:33.827" Score="17" ViewCount="1065" Body="&lt;p&gt;When writing non-trivial shaders (just as when writing any other piece of non-trivial code), people make mistakes.&lt;sup&gt;[citation needed]&lt;/sup&gt; However, I can't just debug it like any other code - you can't just attach gdb or the Visual Studio debugger after all. You can't even do printf debugging, because there's no form of console output. What I usually do is render the data I want to look at as colour, but that is a very rudimentary and amateurish solution. I'm sure people have come up with better solutions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I actually debug a shader? Is there a way to step through a shader? Can I look at the execution of the shader on a specific vertex/primitive/fragment?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(This question is specifically about how to debug shader code akin to how one would debug &quot;normal&quot; code, not about debugging things like state changes.)&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2016-01-26T11:23:57.880" Title="How can I debug GLSL shaders?" Tags="&lt;opengl&gt;&lt;glsl&gt;&lt;debugging&gt;" AnswerCount="4" CommentCount="4" FavoriteCount="3" />
  <row Id="97" PostTypeId="2" ParentId="96" CreationDate="2015-08-07T09:39:47.633" Score="5" Body="&lt;p&gt;While it doesn't seem to be possible to actually step through an OpenGL shader, it is possible to get the compilation results.&lt;br&gt;&#xA;The following is taken from the &lt;a href=&quot;https://github.com/googlesamples/cardboard-java/blob/master/CardboardSample/src/main/java/com/google/vrtoolkit/cardboard/samples/treasurehunt/MainActivity.java&quot;&gt;Android Cardboard Sample&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;while ((error = GLES20.glGetError()) != GLES20.GL_NO_ERROR) {&#xA;    Log.e(TAG, label + &quot;: glError &quot; + error);&#xA;    throw new RuntimeException(label + &quot;: glError &quot; + error);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If your code compiles properly, then you have little choice but to try out a different way of communicating the program's state to you. You could signal that a part of the code was reached by, for example, changing the color of a vertex or using a different texture. Which is awkward, but seems to be the only way for now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: For WebGL, I am looking at &lt;a href=&quot;http://benvanik.github.io/WebGL-Inspector/&quot;&gt;this project&lt;/a&gt;, but I've only just found it... can't vouch for it. &lt;/p&gt;&#xA;" OwnerUserId="21" LastActivityDate="2015-08-07T09:39:47.633" CommentCount="1" />
  <row Id="98" PostTypeId="1" AcceptedAnswerId="105" CreationDate="2015-08-07T10:27:48.687" Score="8" ViewCount="149" Body="&lt;p&gt;I'm looking to use my GPU for non-graphical calculations (artificial life simulations) but ideally I would like to leave this running for weeks at a time, 24 hours a day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there anything I should take into account before trying this? Is a GPU subject to overheating or shortening of its lifespan by continuous use?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What about the computer in general (it's a laptop in my case, but I'm interested in the differences in case it's worth me getting a desktop computer for this)? Will running the GPU non-stop place any strain on connected parts of the computer? Are there known problems that result from this?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-07T18:57:35.670" Title="Is long term continuous use of GPGPU safe for my GPU?" Tags="&lt;gpu&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="99" PostTypeId="2" ParentId="98" CreationDate="2015-08-07T10:43:06.360" Score="4" Body="&lt;p&gt;No, and yes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You're well advised to stay away from stress testing your GPU, or CPU, or any tool for that matter that can be exhausted. In this case it's your GPU, and it definitely can be exhausted.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, if you are using a GPU as a GPU, it's much like using a car as a car. If you use it appropriately it will last a long time and do it's functions, vice versa for inappropriate use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the GPU a lot will put strain on the motherboard, i.e. physical connectors, PCI-e bus, etc. But so will using it once a week for a year.&#xA;Overheating will only be a problem if your simulation is spending most if not all of the GPU's resources and a fault occurs in the cooling.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;it's a laptop in my case, &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I definitely would not run a laptop more than 12 hours a day without expecting a fast and hot demise.&lt;/p&gt;&#xA;" OwnerUserId="296" LastActivityDate="2015-08-07T10:43:06.360" CommentCount="0" />
  <row Id="100" PostTypeId="1" AcceptedAnswerId="109" CreationDate="2015-08-07T12:37:50.097" Score="18" ViewCount="558" Body="&lt;p&gt;I often find myself copy-pasting code between several shaders. This includes both certain computations or data shared between all shaders in a single pipeline, and common computations which all of my vertex shaders need (or any other stage).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, that's horrible practice: if I need to change the code anywhere, I need to make sure I change it everywhere else.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an accepted best practice for keeping &lt;a href=&quot;https://en.wikipedia.org/wiki/Don%27t_repeat_yourself&quot;&gt;DRY&lt;/a&gt;? Do people just prepend a single common file to all their shaders? Do they write their own rudimentary C-style preprocessor which parses &lt;code&gt;#include&lt;/code&gt; directives? If there are accepted patterns in the industry, I'd like to follow them.&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2015-08-10T19:16:27.473" Title="Sharing code between multiple GLSL shaders" Tags="&lt;glsl&gt;" AnswerCount="4" CommentCount="7" FavoriteCount="5" />
  <row Id="101" PostTypeId="2" ParentId="96" CreationDate="2015-08-07T13:14:59.983" Score="16" Body="&lt;p&gt;As far as I know there are no tools that allows you to steps through code in a shader (also, in that case you would have to be able to select just a pixel/vertex you want to &quot;debug&quot;, the execution is likely to vary depending on that). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I personally do is a very hacky &quot;colourful debugging&quot;. So I sprinkle a bunch of dynamic branches with &lt;code&gt;#if DEBUG / #endif&lt;/code&gt; guards that basically say &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#if DEBUG&#xA;if( condition ) &#xA;    outDebugColour = aColorSignal;&#xA;#endif&#xA;&#xA;.. rest of code .. &#xA;&#xA;// Last line of the pixel shader&#xA;#if DEBUG&#xA;OutColor = outDebugColour;&#xA;#endif&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So you can &quot;observe&quot; debug info this way. I usually do various tricks like lerping or blending between various &quot;colour codes&quot; to test various more complex events or non-binary stuff. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this &quot;framework&quot; I also find useful to have a set of fixed conventions for common cases so that if I don't have to constantly go back and check what colour I associated with what. &#xA;The important thing is have a good support for hot-reloading of shader code, so you can almost interactively change your tracked data/event and switch easily on/off the debug visualization. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If need to debug something that you cannot display on screen easily, you can always do the same and use one frame analyser tool to inspect your results. I've listed a couple of them &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl/25#25&quot;&gt;as answer of this other question.&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obv, it goes without saying that if I am not &quot;debugging&quot; a pixel shader or compute shader, I pass this &quot;debugColor&quot; info throughout the pipeline without interpolating it (in GLSL with &lt;code&gt; flat &lt;/code&gt; keyword ) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, this is very hacky and far from proper debugging, but is what I am stuck with not knowing any proper alternative.&lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-08T07:38:06.277" LastActivityDate="2015-08-08T07:38:06.277" CommentCount="1" />
  <row Id="102" PostTypeId="2" ParentId="100" CreationDate="2015-08-07T14:26:32.683" Score="7" Body="&lt;p&gt;I generally just use the fact that glShaderSource(...) accepts an array of strings as it's input.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use a json-based shader definition file, which specifies how a shader (or a program to be more correct) is composed, and there I specify the preprocessor defines I may need, the uniforms it uses, the vertex/fragment shaders file, and all the additional &quot;dependency&quot; files.&#xA;These are just collections of functions that gets appended to the source before the actual shader source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just to add, AFAIK, the Unreal Engine 4 uses an #include directive that gets parsed and append all the relevant files, before the compilation, as you were suggesting.&lt;/p&gt;&#xA;" OwnerUserId="281" LastActivityDate="2015-08-07T14:26:32.683" CommentCount="0" />
  <row Id="103" PostTypeId="1" AcceptedAnswerId="112" CreationDate="2015-08-07T14:42:38.660" Score="7" ViewCount="75" Body="&lt;p&gt;Apparently bicubic pixel interpolation is good for scaling up or down an image (in real time or not).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it recommended to use a lowpass filter before downsizing though, or does the bicubic sampling handle aliasing problems at all?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-08T07:11:05.800" Title="Do you need to use a lowpass filter before downsizing an image?" Tags="&lt;texture&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="104" PostTypeId="2" ParentId="96" CreationDate="2015-08-07T18:39:26.880" Score="8" Body="&lt;p&gt;There is also &lt;a href=&quot;https://github.com/GLSL-Debugger/GLSL-Debugger&quot;&gt;GLSL-Debugger&lt;/a&gt;. It is a debugger used to be known as &quot;GLSL Devil&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Debugger itself is super handy not only for GLSL code, but for OpenGL itself as well. You have the ability to jump between draw calls and break on Shader switches. It also shows you error messages communicated by OpenGL back to the application itself.&lt;/p&gt;&#xA;" OwnerUserId="307" LastActivityDate="2015-08-07T18:39:26.880" CommentCount="0" />
  <row Id="105" PostTypeId="2" ParentId="98" CreationDate="2015-08-07T18:57:35.670" Score="6" Body="&lt;p&gt;Running a GPU at full capacity will reduce its lifespan through &lt;a href=&quot;https://en.wikipedia.org/wiki/Electromigration&quot;&gt;electromigration&lt;/a&gt;; the speed at which the chips are damaged depends on how hot it is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A desktop computer has enough room that the designer can put in a cooling system to handle the worst-case thermal situation.  Running your GPU at continuous full load won't overheat it too badly, and you can expect it to last several years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A laptop, on the other hand, is strongly space-constrained.  Typically, the cooling system is designed to handle brief bursts of heat interspersed with long periods of near-idle operation.  You may not be &lt;em&gt;able&lt;/em&gt; to run it at full load 24/7: the BIOS or operating system will slow things down to keep from overheating.  In any case, running at full capacity will likely cause things to burn out in a year or less.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-07T18:57:35.670" CommentCount="0" />
  <row Id="106" PostTypeId="2" ParentId="100" CreationDate="2015-08-07T19:11:21.303" Score="4" Body="&lt;p&gt;I don't think there is a common convention, but if I'd take a guess, I'd say that almost everyone implements some simple form of textual inclusion as a preprocessing step (an &lt;code&gt;#include&lt;/code&gt; extension), because it is very easy to do so. (In JavaScript/WebGL, you can do that with a simple regular-expression, for example). The upside of this is that you can perform the preprocessing in an offline step for &quot;release&quot; builds, when the shader code no longer needs to be changed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact, an indication that this approach is common is the fact that an ARB extension was introduced for that: &lt;a href=&quot;https://www.opengl.org/registry/specs/ARB/shading_language_include.txt&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;GL_ARB_shading_language_include&lt;/code&gt;&lt;/a&gt;.&#xA;I'm not sure if this became a core feature by now or not, but the extension was written against OpenGL 3.2.&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="54" LastEditDate="2015-08-07T23:09:16.677" LastActivityDate="2015-08-07T23:09:16.677" CommentCount="1" />
  <row Id="107" PostTypeId="2" ParentId="95" CreationDate="2015-08-07T20:33:07.803" Score="8" Body="&lt;p&gt;If the surfaces are exactly co-planer, your fate is up to the FPU gods; you'll more than likely have Z-fighting. If the triangles are &lt;em&gt;identical&lt;/em&gt; and you do the &lt;em&gt;exact&lt;/em&gt; same math to each, you &lt;em&gt;will&lt;/em&gt; end up with the same Z-values for both. But again, this will only happen if your math operations are identical for both. (Since, in general, FPU operations are not commutative)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One possible solution is to not use the Z-buffer at all. Rather you can use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Painter%27s_algorithm&quot;&gt;Painter's Algorithm&lt;/a&gt;. Granted, this comes with all the problems of the painters algorithm as well. But, it would solve the Z-fighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your example case of Screen Space Decals (SSDs), the common solution is to use an offset, aka a simple thin cube. See Warhammer 40k's &lt;a href=&quot;http://www.slideshare.net/blindrenderer/screen-space-decals-in-warhammer-40000-space-marine-14699854&quot;&gt;presentation&lt;/a&gt; about SSDs for reference. Or Bart Wronski's &lt;a href=&quot;http://bartwronski.com/2015/03/12/fixing-screen-space-deferred-decals/&quot;&gt;post&lt;/a&gt; which addresses some other issues with decals, but also links to a few other presentations about SSDs&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2015-08-09T18:44:10.750" LastActivityDate="2015-08-09T18:44:10.750" CommentCount="4" />
  <row Id="108" PostTypeId="2" ParentId="88" CreationDate="2015-08-08T01:15:07.103" Score="3" Body="&lt;p&gt;I'm not quite sure I fully understand the problem, this is what I think you're asking:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You start with a set of rays pointing a all sorts of directions&lt;/li&gt;&#xA;&lt;li&gt;You want to know which of these rays intersect with the image plane, taking into account occlusion. &lt;/li&gt;&#xA;&lt;li&gt;I'm going to assume these rays carry some kind of color information along with them. IE. when they hit the image plane, they will accumulate color.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With that goal in mind, I would do the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create 2 image buffers&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;For accumulated color&lt;/li&gt;&#xA;&lt;li&gt;To keep track of the number of hits&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;Clear the accumulation buffer to black, and the hitCount buffer to all zeros&lt;/li&gt;&#xA;&lt;li&gt;Create a kernel per ray&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use classic ray tracing math to find the nearest intersection&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Here you can do anything from a simple 'iterate through all the object in the scene' to something like BVHs, or kd-trees.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;If the closest intersection is the image plane, use the WorldView matrix for the image plane to transform the intersection point from world coordinates to view space coordinates.&lt;/li&gt;&#xA;&lt;li&gt;Use the x, y coordinates of the image space intersection point to get the pixel coordinates of the image buffer&lt;/li&gt;&#xA;&lt;li&gt;Add the color of the ray to the corresponding pixel in the accumulation buffer&lt;/li&gt;&#xA;&lt;li&gt;Increment the value of the corresponding pixel in the hitCount buffer&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For 4 and 5 you could also use a filter in order to apply the color to multiple pixels, rather than just one. In that case, you would add a value less than 1 to the hitCount buffer&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;On the CPU, or in another pass per pixel: &#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Divide the accumulated color buffer by the hitCount buffer&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't see any problem with doing these steps on the GPU. Any of the compute languages should handle it fine. You might get some divergence in the actual ray tracing code, but it should be ok. The biggest potential problem is that you have to store the entire scene in VRAM.&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2015-08-08T01:15:07.103" CommentCount="0" />
  <row Id="109" PostTypeId="2" ParentId="100" CreationDate="2015-08-08T01:33:56.490" Score="10" Body="&lt;p&gt;There's a bunch of a approaches, but none is perfect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's possible to share code by using &lt;code&gt;glAttachShader&lt;/code&gt; to combine shaders, but this doesn't make it possible to share things like struct declarations or &lt;code&gt;#define&lt;/code&gt;-d constants. It does work for sharing functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some people like to use the array of strings passed to &lt;code&gt;glShaderSource&lt;/code&gt; as a way to prepend common definitions before your code, but this has some disadvantages:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;It's harder to control what needs to be included from within the shader (you need a separate system for this.)&lt;/li&gt;&#xA;&lt;li&gt;It means the shader author cannot specify the GLSL &lt;code&gt;#version&lt;/code&gt;, due to the following statement in the GLSL spec:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The &lt;strong&gt;#version&lt;/strong&gt; directive must occur in a shader before anything else, except for comments and white space.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Due to this statement, &lt;code&gt;glShaderSource&lt;/code&gt; cannot be used to prepend text before the &lt;code&gt;#version&lt;/code&gt; declarations. This means that the &lt;code&gt;#version&lt;/code&gt; line needs to be included in your &lt;code&gt;glShaderSource&lt;/code&gt; arguments, which means that your GLSL compiler interface needs to somehow be told what version of GLSL is expected to be used. Additionally, not specifying a &lt;code&gt;#version&lt;/code&gt; will make the GLSL compiler default to using GLSL version 1.10. If you want to let shader authors specify the &lt;code&gt;#version&lt;/code&gt; within the script in a standard way, then you need to somehow insert &lt;code&gt;#include&lt;/code&gt;-s after the &lt;code&gt;#version&lt;/code&gt; statement. This could be done by explicitly parsing the GLSL shader to find the &lt;code&gt;#version&lt;/code&gt; string (if present) and make your inclusions after it, but having access to an &lt;code&gt;#include&lt;/code&gt; directive might be preferable to control more easily when those inclusions need to be made. On the other hand, since GLSL ignores comments before the &lt;code&gt;#version&lt;/code&gt; line, you could add metadata for includes within comments at the top of your file (yuck.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question now is: Is there a standard solution for &lt;code&gt;#include&lt;/code&gt;, or do you need to roll your own preprocessor extension?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is the &lt;code&gt;GL_ARB_shading_language_include&lt;/code&gt; extension, but it has some drawbacks:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;It is only supported by NVIDIA (&lt;a href=&quot;http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_ARB_shading_language_include&quot; rel=&quot;nofollow&quot;&gt;http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_ARB_shading_language_include&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;It works by specifying the include strings ahead of time. Therefore, before compiling, you need to specify that the string &lt;code&gt;&quot;/buffers.glsl&quot;&lt;/code&gt; (as used in &lt;code&gt;#include &quot;/buffers.glsl&quot;&lt;/code&gt;) corresponds to the contents of the file &lt;code&gt;buffer.glsl&lt;/code&gt; (which you have loaded previously).&lt;/li&gt;&#xA;&lt;li&gt;As you may have noticed in point (2), your paths need to start with &lt;code&gt;&quot;/&quot;&lt;/code&gt;, like Linux-style absolute paths. This notation is generally unfamiliar to C programmers, and means you can't specify relative paths.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;A common design is to implement your own &lt;code&gt;#include&lt;/code&gt; mechanism, but this can be tricky since you also need to parse (and evaluate) other preprocessor instructions like &lt;code&gt;#if&lt;/code&gt; in order to properly handle conditional compilation (like header guards.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you implement your own &lt;code&gt;#include&lt;/code&gt;, you also have some liberties in how you want to implement it:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;You could pass strings ahead of time (like &lt;code&gt;GL_ARB_shading_language_include&lt;/code&gt;).&lt;/li&gt;&#xA;&lt;li&gt;You could specify an include callback (this is done by DirectX's D3DCompiler library.)&lt;/li&gt;&#xA;&lt;li&gt;You could implement a system that always reads directly from the filesystem, as done in typical C applications.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;As a simplification, you can automatically insert header guards for each include in your preprocessing layer, so your processor layer looks like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (#include and not_included_yet) include_file();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(Credit to Trent Reed for showing me the above technique.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In conclusion&lt;/strong&gt;, there exists no automatic, standard, and simple solution. In a future solution, you could use some SPIR-V OpenGL interface, in which case the GLSL to SPIR-V compiler could be outside of the GL API. Having the compiler outside the OpenGL runtime greatly simplifies implementing things like &lt;code&gt;#include&lt;/code&gt; since it's a more appropriate place to interface with the filesystem. I believe the current widespread method is to just implement a custom preprocessor that works in a way any C programmer should be familiar with.&lt;/p&gt;&#xA;" OwnerUserId="311" LastEditorUserId="311" LastEditDate="2015-08-10T19:16:27.473" LastActivityDate="2015-08-10T19:16:27.473" CommentCount="0" />
  <row Id="110" PostTypeId="2" ParentId="100" CreationDate="2015-08-08T05:31:58.103" Score="3" Body="&lt;p&gt;Some people have already pointed out that &lt;code&gt;glShaderSource&lt;/code&gt; can take an array of strings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On top of that, in GLSL the compilation (&lt;code&gt;glShaderSource&lt;/code&gt;, &lt;code&gt;glCompileShader&lt;/code&gt;) and linking (&lt;code&gt;glAttachShader&lt;/code&gt;, &lt;code&gt;glLinkProgram&lt;/code&gt;) of shader is separate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have used that in some projects to split shaders between the specific part, and the parts common to most shaders, which is then compiled and shared with all shader programs. This works and is not difficult to implement: you just have to maintain a dependency list.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In terms of maintainability though, I am not sure it is a win. The observation was the same, let's try to factorize. While it indeed avoids repetition, the overhead of the technique feels significant. Moreover, the final shader is more difficult to extract: you cannot just concatenate the shader sources, as the declarations end in an order that some compilers will reject, or will be duplicated. So it makes it more difficult to do a quick shader test in a separate tool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end this technique addresses some DRY issues, but it's far from ideal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side topic, I am not sure if this approach has any impact in terms of compilation time; I have read that some drivers only really compile the shader program on linking, but I haven't measured.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-08-08T05:31:58.103" CommentCount="2" />
  <row Id="111" PostTypeId="2" ParentId="94" CreationDate="2015-08-08T06:30:49.560" Score="4" Body="&lt;p&gt;As far as I'm aware there's no way in either DX or GL to re-use RT 0's alpha for all the blending operations.  Unfortunately, it doesn't seem to be something that's supported by hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can configure different blend modes for each render target, or enable blending for some and disable for others; however, if blending is enabled for a render target, it always uses its own alpha.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's also a mode called &quot;dual-source blending&quot; (see &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb205120.aspx#dual_source&quot; rel=&quot;nofollow&quot;&gt;DX11 doc&lt;/a&gt; and &lt;a href=&quot;https://www.opengl.org/wiki/Blending#Dual_Source_Blending&quot; rel=&quot;nofollow&quot;&gt;OGL doc&lt;/a&gt;), which allows you to specify the alpha for blending with an entirely separate output from the pixel shader, not the render target's alpha channel.  However, this mode only works with one render target on current hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So as far as I can tell, the only options for blending several render targets with the same alpha are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Output the same alpha on all render targets (sacrificing the ability to store other values to the alpha channel as you would in deferred shading, for instance).&lt;/li&gt;&#xA;&lt;li&gt;Repeat the rendering in a separate pass for each render target, using dual-source blending.&lt;/li&gt;&#xA;&lt;li&gt;Use UAVs / image load-store to execute the blending in the pixel shader (only works if geometry is non-self-overlapping in screen space, because there's no protection against race conditions; also probably kinda slow).&lt;/li&gt;&#xA;&lt;li&gt;On hardware that supports it, &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn914601.aspx&quot; rel=&quot;nofollow&quot;&gt;DX11.3/DX12 Rasterizer-Order Views&lt;/a&gt;, &lt;a href=&quot;https://www.opengl.org/registry/specs/NV/fragment_shader_interlock.txt&quot; rel=&quot;nofollow&quot;&gt;NV_fragment_shader_interlock&lt;/a&gt;, or &lt;a href=&quot;https://www.opengl.org/registry/specs/INTEL/fragment_shader_ordering.txt&quot; rel=&quot;nofollow&quot;&gt;INTEL_fragment_shader_ordering&lt;/a&gt; (the latter also exposed by AMD GPUs).  These are three names for the same thing: basically a &quot;critical section&quot; in the pixel shader that lets you read-modify-write a texture atomically with respect to other pixel shader invocations. It essentially enables arbitrary programmable blending, but it's probably fairly slow, and only available on recent hardware.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-08T06:30:49.560" CommentCount="3" />
  <row Id="112" PostTypeId="2" ParentId="103" CreationDate="2015-08-08T07:11:05.800" Score="7" Body="&lt;p&gt;If the downsampling pass is properly designed, it will effectively perform low-pass filtering as part of the downsampling.  There is no need for a separate low-pass filter operation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially, when you downsample, you are performing a filter over the source (high-res) image pixels, but only evaluating it at the locations of the destination (low-res) pixels.  The footprint of this filter needs to be approximately the spacing between destination pixels, to avoid missing information by skipping over in-between source pixels.  But that means the filter footprint will be several source pixels wide, so it will effectively low-pass the source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, let's suppose you downsample an image by exactly 10x on each axis.  With a box filter (for example's sake), you would set each destination pixel to the average of a 10x10 box of source pixels.  That would wipe out any features smaller than 10px, so it's effectively a low-pass filter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You mention bicubic interpolation; we have to make a distinction between filtering and interpolation here.  Interpolation is appropriate for upsampling, not downsampling.  Bicubic interpolation works by fitting a bicubic spline patch to a 4x4 neighborhood of pixels, then evaluating the patch at interpolated points. While it may work well enough for downsampling images by a small factor (up to 2x or so), it will fail if you go much further than that.  For instance, if downsampling by 10x as in the previous example, you can see that bicubic will miss the majority of the source pixels, and the result may be quite aliased.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, bicubic &lt;em&gt;filtering&lt;/em&gt; is just standard filtering, using a kernel that's a bicubic function (as opposed to a box, triangle, Gaussian, Lanczos, etc. kernel).  The Mitchell-Netravali kernel is the classic example of this type.  If used for downsampling, the kernel should be sized appropriately for the destination pixel spacing as discussed earlier, and you would sum over all the pixels in the footprint, not just a 4x4 or other fixed-size neighborhood.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-08T07:11:05.800" CommentCount="0" />
  <row Id="113" PostTypeId="2" ParentId="91" CreationDate="2015-08-08T07:46:26.633" Score="3" Body="&lt;p&gt;Yes, it's certainly possible to do this kind of thing.  First of all, you can typically connect 3 or 4 displays to each GPU, and (with an appropriate motherboard) you can have up to 4 GPUs per machine.  That would give you 16 screens.  With multiple machines, you could have even more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rendering to multiple displays from a single GPU is no big deal.  You can create a single window that stretches across all the displays, render to it, and it should &quot;just work&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rendering the same scene on multiple GPUs is a bit trickier.  One way is to create a separate rendering context on each GPU, and essentially repeat all the rendering&amp;mdash;re-upload all buffers and textures to each GPU, and re-submit the draw calls to each one every frame.  Another possibility is to use the &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn933253.aspx&quot; rel=&quot;nofollow&quot;&gt;built-in multi-GPU support in DX12&lt;/a&gt;, which allows you to manage several GPUs in one rendering context.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-08T07:46:26.633" CommentCount="0" />
  <row Id="115" PostTypeId="5" CreationDate="2015-08-08T18:28:16.773" Score="0" Body="&lt;p&gt;Transformations are the mathematical operations that can be applied to an object (often a set of vertices that belong together) to move it, rotate it, or otherwise transform it.&lt;br&gt;&#xA;Transformations are usually applied using a matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Transformations are not necessarily bijective; data may be lost, for example, in a projection from a 3D object to it's 2D representation on a screen.&lt;/p&gt;&#xA;" OwnerUserId="21" LastEditorUserId="21" LastEditDate="2015-08-11T04:40:02.740" LastActivityDate="2015-08-11T04:40:02.740" CommentCount="0" />
  <row Id="116" PostTypeId="4" CreationDate="2015-08-08T18:28:16.773" Score="0" Body="Transformations are mathematical operations that can be applied to an object to change its scale, position and orientation. " OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-02-18T02:08:04.167" LastActivityDate="2016-02-18T02:08:04.167" CommentCount="0" />
  <row Id="117" PostTypeId="1" AcceptedAnswerId="118" CreationDate="2015-08-08T18:33:55.790" Score="6" ViewCount="99" Body="&lt;p&gt;How would one do a color separation if there are more than 3 color primaries, or the primaries are nonstandard. In Standard  CMYK conversion K is relatively easy conceptually to figure out. Its just a constant value off all the channels. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would one construct a CMY + orange for instance. Or in the case of a led array with say Red, Yellow, Blue and Violet for example. How would one approach this problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm fine with a conceptual answer no need to do LAB to end result conversion for example. Though that would be nice.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="316" LastEditDate="2015-08-09T07:16:48.483" LastActivityDate="2015-11-03T13:53:01.553" Title="How to do a color separation with more than 3 primary colors" Tags="&lt;color&gt;&lt;color-separation&gt;&lt;conversion&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="118" PostTypeId="2" ParentId="117" CreationDate="2015-08-08T19:51:13.280" Score="5" Body="&lt;p&gt;The mathematical approach to this is to represent the other colors/light sources in terms of your standard primaries in the color space. Represent your target color and available light sources as vectors with, say, the RGB color space as the basis. Figuring out how to display your target color using your available sources then becomes a &lt;a href=&quot;http://math.stackexchange.com/a/340991/166490&quot;&gt;change of basis&lt;/a&gt;, a standard linear algebra operation, where the target basis consists of your LEDs/nonstandard primaries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the dimensions of the source and destination bases are different, the change of basis matrix won't be square and thus won't be invertible. In that case, you need to solve a system of linear equations, expressed in matrix form:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$c_{RGB} = [r, y, b, v] \cdot c_{RYBV}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $c_{RGB}$ and $c_{RYBV}$ are the coordinate vectors of your desired color expressed in terms of RGB and custom primaries, respectively, and $r$, $y$, $b$ and $v$ are your custom primaries (basis vectors) in RGB coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Solving this system is another common linear algebra operation, and there are a variety of numerical and symbolical algorithms to do so. The system is over-constrained, so solutions will likely have a free variable, however since we can't physically produce negative light, nor do the primaries have infinite dynamic range, for most practical purposes each coefficient will have to be limited to be within the $[0, 1]$ range. This will render many colors unrepresentable under certain sets of primaries.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="16" LastEditDate="2015-11-03T13:53:01.553" LastActivityDate="2015-11-03T13:53:01.553" CommentCount="9" />
  <row Id="119" PostTypeId="1" AcceptedAnswerId="120" CreationDate="2015-08-09T01:57:50.783" Score="8" ViewCount="513" Body="&lt;p&gt;Temporal anti aliasing (and other temporal algorithms) work by matching pixels this frame with pixels from the last frame and then using that information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I get that you can use the last and current frame matrices along with motion vector information to match the pixels between frames.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I don't get though is how do you know whether the reprojected pixel is valid or not?  For instance the old pixel may now be hidden behind a different object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it by color only? If so, how are animated textures or changing light conditions handled?&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-09T03:34:13.070" LastActivityDate="2015-08-09T06:39:28.673" Title="How does temporal reprojection work?" Tags="&lt;post-processing&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="120" PostTypeId="2" ParentId="119" CreationDate="2015-08-09T06:39:28.673" Score="3" Body="&lt;p&gt;One strategy mentioned in &lt;a href=&quot;http://advances.realtimerendering.com/s2014/epic/TemporalAA.pptx&quot; rel=&quot;nofollow&quot;&gt;Brian Karis's talk about TAA&lt;/a&gt; is neighborhood clamping. The general idea is that, for the previous frame's pixel to be valid, its color should be in the color range found in the neighborhood (say 3x3 pixels) of the current pixel this frame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This rejects history from changing light conditions, which is probably what you want anyway if you don't want moving shadows to produce ghosting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Animated textures, depending on the speed of the animation, could also be handled with a motion vector, if you have a predictable UV mapping or can guess reasonably well.)&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-09T06:39:28.673" CommentCount="4" />
  <row Id="121" PostTypeId="1" AcceptedAnswerId="128" CreationDate="2015-08-09T08:29:04.527" Score="9" ViewCount="355" Body="&lt;p&gt;Alpha blending can be turned on to make surfaces transparent, like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glDisable(GL_DEPTH_TEST); //or glDepthMask(GL_FALSE)? depth tests break blending&#xA;glEnable(GL_BLEND);&#xA;glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But this only works if objects are rendered in back to front order. Otherwise things in the background appear in front of closer objects, like the floor in the image below. For particles and GUI elements sorting would be OK, but for triangle meshes it seems like it'd be too much effort and slow, as discussed here: &lt;a href=&quot;https://www.opengl.org/wiki/Transparency_Sorting&quot;&gt;https://www.opengl.org/wiki/Transparency_Sorting&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the common methods to deal with this? I know this is quite broad and am not after in-depth implementation details, just a brief description of some approaches and what might be involved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/G3Ybm.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/G3Ybm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="198" LastEditorUserId="100" LastEditDate="2015-08-09T09:40:54.873" LastActivityDate="2015-08-11T16:46:35.160" Title="What are some methods to render transparency in OpenGL" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;transparency&gt;" AnswerCount="3" CommentCount="2" />
  <row Id="122" PostTypeId="1" AcceptedAnswerId="137" CreationDate="2015-08-09T08:46:52.920" Score="6" ViewCount="73" Body="&lt;p&gt;I'm looking for an algorithm that can identify edges across which colour is changing sharply, rather than just finding changes in brightness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this just a matter of using a different colour space with existing edge detection algorithms, or is there a better approach?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-09T17:03:58.693" Title="How can I detect edges between different colours of the same brightness?" Tags="&lt;color&gt;&lt;algorithm&gt;&lt;edge-detection&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="123" PostTypeId="1" AcceptedAnswerId="135" CreationDate="2015-08-09T08:56:08.447" Score="7" ViewCount="88" Body="&lt;p&gt;I'm aware of glFog, which blends between the fog and surface colour based on distance to the camera, like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2pERbm.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2pERbm.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The equations are even given in the &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glFog.xml&quot;&gt;docs&lt;/a&gt;.&#xA;But the fog is applied immediately in front of the camera. I'm trying to render a water surface and want it to have some depth, for example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uEXcwm.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uEXcwm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the moment I'm just rendering the water surface with alpha blending, as below. What I need is to have the fog based on the distance from the water surface to the bottom of the water. How can I get this distance? For the moment I'm assuming the camera will always be above the water.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/GsYC5.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/GsYC5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="198" LastEditorUserId="38" LastEditDate="2015-08-09T14:27:04.993" LastActivityDate="2015-08-09T14:27:04.993" Title="Rendering fog underneath water" Tags="&lt;opengl&gt;&lt;fog&gt;&lt;depth-map&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="124" PostTypeId="2" ParentId="121" CreationDate="2015-08-09T08:57:55.630" Score="5" Body="&lt;p&gt;One option is to use depth peeling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially, one processes the scene a set number of times (say, &lt;code&gt;n&lt;/code&gt; times) in order to determine the closest, second-closest, all the way to &lt;code&gt;n&lt;/code&gt;th-closest fragments of the scene. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This processing is done by first applying a regular depth test to the entire scene (which naturally returns the closest surface). One then uses the result of the depth test to filter out the first layer, by ignoring everything with a lesser depth than returned in the depth test.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Applying the depth test again will then return the second layer. Repeat as needed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have the layers, you can just draw all the layers in reverse order (assuming you kept track of the RGBA colors for each layer), blending normally, since the layers are in front-to-back order.&lt;/p&gt;&#xA;" OwnerUserId="170" LastActivityDate="2015-08-09T08:57:55.630" CommentCount="2" />
  <row Id="125" PostTypeId="1" AcceptedAnswerId="152" CreationDate="2015-08-09T08:58:52.810" Score="6" ViewCount="58" Body="&lt;p&gt;Raytracing an image can be performed in parallel by calculating the colour of different pixels on different machines. However, this still requires each machine to have access to the entire scene to be rendered. Is there a way of also subdividing the scene so that different machines have access to different parts of it but can still calculate their assigned pixels correctly?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I cannot imagine a way in which this could be possible in general. Are there any particular circumstances in which this can be achieved?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-09T22:09:49.603" Title="Can I parallelise the scene for a raytraced image, rather than just the pixels?" Tags="&lt;raytracing&gt;&lt;scene-description&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="126" PostTypeId="1" AcceptedAnswerId="153" CreationDate="2015-08-09T09:07:37.560" Score="8" ViewCount="101" Body="&lt;p&gt;If the scene to be raytraced cannot be stored in memory, then without adding more RAM to the machine it seems unrealistic to render it in a practical time span, due to the need to load different parts of the scene from disk potentially several times per pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way around this? I'm trying to think of some way of performing a large number of the calculations involving a particular subset of the scene all at once, to reduce the number of times it needs to be loaded into memory. Is there any other way of improving the speed in such a case?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-09T22:47:46.967" Title="How can I raytrace a scene that does not fit into memory?" Tags="&lt;raytracing&gt;&lt;memory&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="127" PostTypeId="2" ParentId="125" CreationDate="2015-08-09T09:07:40.690" Score="3" Body="&lt;p&gt;In the general case, no.  Reflection, refraction, and shadows can all generate secondary rays that could go anywhere, requiring the entire scene to be available.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the special case of rendering with no secondary rays, you can quickly generate a vista buffer (essentially a mapping of your acceleration hierarchy to the screen) to figure out which objects are potentially visible from each pixel and distribute portions of the scene accordingly.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-09T09:07:40.690" CommentCount="0" />
  <row Id="128" PostTypeId="2" ParentId="121" CreationDate="2015-08-09T09:31:04.153" Score="8" Body="&lt;p&gt;A set of techniques to avoid explicit ordering go under the name of Order Independent Transparency (OIT for short). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are lots of OIT techniques. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Historically one is &lt;strong&gt;Depth Peeling&lt;/strong&gt;. In this approach you first render the front-most fragments/pixels, then you find the closest to the one found in the previous step and so forth, going on with as many &quot;layer&quot; as you need. &#xA;It is called depth peeling because at each pass you &quot;peel&quot; one layer of depth. All your layer can be then normally recombined from back to front. &#xA;To implement this algorithm you need to have a copy of the depth buffer. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another set of techniques are the blendend OIT ones. One the most recent and interesting one is the &lt;strong&gt;Weighted Blended OIT&lt;/strong&gt; proposed by &lt;a href=&quot;http://jcgt.org/published/0002/02/09/&quot;&gt;McGuire and Bavoil&lt;/a&gt;. It basically apply a weighted sum for all the surfaces that occupies a given a fragment.  The weighting scheme they propose is based on camera-space Z (as an approximation to occlusion) and opacity.&lt;br&gt;&#xA;The idea is that if you can reduce the problem to a weighted sum, you don't really care about ordering. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other than the original paper, a great resource for implementation details and problems of Weighted Blended OIT is in &lt;a href=&quot;https://mynameismjp.wordpress.com/2014/02/03/weighted-blended-oit/&quot;&gt;Matt Pettineo's blog&lt;/a&gt;.  As you can read from his post this technique is not a silver bullet. The main problem is that the weighting scheme is central and it needs to be tuned according to your scene/content. &#xA;From his experiments, whilst the technique seems to work fine for relatively low and medium opacity, it is failing when opacity approaches 1 and so could not be used from materials where big part of the surface is opaque (he makes the example of foliage). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, all come down to how you tune your depth-weights and finding the ones that fit perfectly your use-cases is not necessarily trivial. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for what is needed to for the Weighted Blended OIT, nothing more than two extra render targets. One that you fill with the premultiplied alpha color ( color * alpha) and alpha, both weighted accordingly. The other one for the weights only.  &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-09T10:01:50.560" LastActivityDate="2015-08-09T10:01:50.560" CommentCount="0" />
  <row Id="129" PostTypeId="1" AcceptedAnswerId="131" CreationDate="2015-08-09T10:15:53.817" Score="7" ViewCount="150" Body="&lt;p&gt;To render a scene with a single light source using phong shading, one can calculate the final color of each fragment passed into the fragment shader based on the ambient/diffuse/specular components of both the material and the light source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can be easily extended to accommodate for multiple light sources by adding together the results of applying each individual light source onto the fragment as such:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;final_color = (0, 0, 0, 1)&#xA;for each light:&#xA;    final_color += apply_light(material, light)&#xA;final_color = clamp(final_color, (0,0,0,1), (1,1,1,1))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, with a very large number of light sources, this process is quite slow; with &lt;code&gt;N&lt;/code&gt; lights, this approach requires calculations for phong shading to be done &lt;code&gt;N&lt;/code&gt; times per fragment. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better approach to rendering scenes with a very large number of light sources (hundreds, thousands, etc.)?&lt;/p&gt;&#xA;" OwnerUserId="170" LastActivityDate="2015-08-09T11:36:54.697" Title="Efficient rendering with many light sources" Tags="&lt;rendering&gt;&lt;shader&gt;&lt;lighting&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" />
  <row Id="130" PostTypeId="1" AcceptedAnswerId="202" CreationDate="2015-08-09T10:18:03.970" Score="8" ViewCount="141" Body="&lt;p&gt;I am trying to implement microfacet BRDF model. I am reading &lt;a href=&quot;http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr_v2.pdf&quot;&gt;Sebastien Lagarde's slides&lt;/a&gt;. I implemented formulas to my code but i think result image is wrong. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yellow is base color of material. Specular color is red to see properly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Fragment Shader&#xA;#version 330 core&#xA;&#xA;in vec3 Position;&#xA;in vec2 TexCoord0;&#xA;in vec3 Normal;&#xA;in vec3 Tangent;&#xA;out vec4 FinalColor;&#xA;&#xA;uniform vec3 uCameraPosition; // init value: vec3(0, 0, 5)&#xA;&#xA;#define PI 3.1415926f&#xA;#define EPSILON 10e-5f&#xA;#define saturate(value) clamp(value, 0.0f, 1.0f);&#xA;&#xA;float BRDF_Lambert(float NdotL)&#xA;{&#xA;    return NdotL;&#xA;}&#xA;&#xA;// Got these BRDF formulas Moving Frostbite to PBR slide by Sebastien Lagarde &amp;amp; Charles de Rousiers &#xA;// http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr_v2.pdf&#xA;float BRDF_D_GGX(float NdotH, float Roughness)&#xA;{&#xA;    float Roughness2 = Roughness * Roughness;&#xA;    float f = (NdotH * Roughness2 - NdotH) * NdotH + 1.0f;&#xA;    return Roughness2 / (f * f + EPSILON);&#xA;}&#xA;&#xA;&#xA;float BRDF_F_FresnelSchlick(float LdotH, float F0)&#xA;{&#xA;    float f = F0 + (1.0f - F0) * pow((1.0f - LdotH), 5);&#xA;    return f;&#xA;}&#xA;&#xA;float BRDF_G_SmithGGXCorrelated(float NdotL, float NdotV, float Roughness)&#xA;{&#xA;    float Roughness2 = Roughness * Roughness;&#xA;    float GV = NdotL * sqrt((-NdotV * Roughness2 + NdotV) * NdotV + Roughness2);&#xA;    float GL = NdotV * sqrt((-NdotL * Roughness2 + NdotL) * NdotL + Roughness2);&#xA;&#xA;    return 0.5f / (GV + GL + EPSILON);&#xA;}&#xA;&#xA;float BRDF_Specular(float NdotV, float NdotL, float NdotH, float LdotH, float Roughness, float F0)&#xA;{&#xA;    float D = BRDF_D_GGX(NdotH, Roughness);&#xA;    float F = BRDF_F_FresnelSchlick(LdotH, F0);&#xA;    float G = BRDF_G_SmithGGXCorrelated(NdotL, NdotV, Roughness);&#xA;    return (D * F * G) / PI;&#xA;}&#xA;&#xA;void main()&#xA;{&#xA;    FinalColor = vec4(1.0f, 1.0f, 1.0f, 1.0f);&#xA;    vec4 BaseColor = vec4(1.0f, 1.0f, 0.0f, 1.0f);&#xA;    vec4 SpecularColor = vec4(1.0f, 0.0f, 0.0f, 1.0f);&#xA;    vec3 LightDirection = normalize(vec3(0, 4, 4));&#xA;    vec3 ViewDirection = normalize(Position - uCameraPosition);&#xA;    vec3 HalfVector = normalize(ViewDirection + LightDirection);&#xA;    float Roughness = 0.9f; // [0.04 - 0.1f] -&amp;gt; Dielectric, [0.7, 1.0f] -&amp;gt; Metallic&#xA;&#xA;    float RefractiveIndex = 0.27049f; // RI for Gold materials. I got this from http://refractiveindex.info/&#xA;    float F0 = pow(((1.0f - RefractiveIndex) / (1.0f + RefractiveIndex)), 2);&#xA;&#xA;    float NdotL = saturate(dot(LightDirection, Normal));&#xA;    float NdotV = abs(dot(ViewDirection, Normal)) + EPSILON; // Avoid artifact - Ref: SIGGRAPH14 - Moving Frosbite to PBR&#xA;    float LdotH = saturate(dot(LightDirection, HalfVector));&#xA;    float NdotH = saturate(dot(Normal, HalfVector));&#xA;&#xA;    float DiffuseFactor = BRDF_Lambert(NdotL);&#xA;    float SpecularFactor = BRDF_Specular(NdotV, NdotL, NdotH, LdotH, Roughness, F0);&#xA;&#xA;    FinalColor = BaseColor * DiffuseFactor + SpecularColor * SpecularFactor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vwRZ8.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vwRZ8.png&quot; alt=&quot;Wrong result&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Roughness = 0.2f;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/89aAW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/89aAW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Roughness = 0.04f;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/SlEtb.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SlEtb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="83" LastEditorUserId="83" LastEditDate="2015-08-09T20:20:02.463" LastActivityDate="2015-08-11T07:53:30.297" Title="Trying to implement Microfacet BRDF but my result images are wrong" Tags="&lt;rendering&gt;&lt;glsl&gt;&lt;shading&gt;&lt;specular&gt;" AnswerCount="1" CommentCount="7" FavoriteCount="3" />
  <row Id="131" PostTypeId="2" ParentId="129" CreationDate="2015-08-09T11:23:55.003" Score="10" Body="&lt;p&gt;Yes, but you need a paradigm shift. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you are accustomed is called forward rendering. You submit your geometry and then you proceed immediately with the shading pass. In the basic forward rendering you can either loop inside the shader for each light or perform one pass per light and blend the result together (with additive blending). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But things have evolved quite a lot. Enters: &lt;strong&gt;Deferred Rendering&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that are so many variants that describe them all in details will take way more than acceptable for an answer here. So here I am just going to describe the gist of Deferred shading, there are plenty of other resources that you can easily find using google, hopefully after reading this you'll have the right keywords to find what you need. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic idea is to defer the shading to after down the pipeline. You have two main steps: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Render your geometry and all the information needed for shading into multiple render targets. This means that typically in a basic implementation you would have a depth buffer, a buffer containing the normals of your geometry and the albedo colour. You'll soon find that you need other information of the materials (e.g. roughness, &quot;metallic&quot; factor etc.). &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;This image from wikipedia shows three buffer (color, depth and normals) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/fmaeF.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fmaeF.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again, the amount, type and content of the buffers used varies quite a lot among different projects. You will find the set of buffers with the name of GBuffers.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;After this is the time of applying the actual lighting. During the lighting pass for each light you want to draw a light volume that depends on the type of light:&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For a directional light you render a full screen quad. &lt;/li&gt;&#xA;&lt;li&gt;For a point light you render a sphere where the radius is based on the attenuation of your point light. &lt;/li&gt;&#xA;&lt;li&gt;For a spot-light you render a cone which dimensions again depend on the characteristics of your light. &lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In the pixel shader of this pass you pass your GBuffers and perform your lighting and shading using the information in them. This way you process only the pixels affected by each of the lights having a sensible speed-up if compared to the classical forward rendering. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It has also various disadvantages, most notably the handling of transparent  objects and higher consumption of bandwidth and video memory. But also it is trickier to handle various models for materials. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You have other side-advantages (as having lots of info ready for post-processing) and is also pretty easy to implement. But this is not the coolest thing around for lots of lights anymore. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Newer techniques are for example &lt;strong&gt;Tiled rendering&lt;/strong&gt; ones. The main idea of those is to subdivide the scene in screen space &quot;tiles&quot; and assign to each tile the lights affecting it. This exists both in a deferred and forward fashion.  These techniques lead to some problems when you have various depth discontinuities in a tile, but is generally faster than the classical deferred and it solves various problems of it. For example, among the advantages, with tiled deferred you read the GBuffers once per lit fragment and pixels in the same tile coherently process the same lights. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further evolution on this side is the &lt;strong&gt;&lt;a href=&quot;http://www.cse.chalmers.se/~uffe/clustered_shading_preprint.pdf&quot;&gt;Clustered shading&lt;/a&gt;&lt;/strong&gt; which is conceptually similar to the tiled based approaches, having instead of screen space tiles, clusters with a 3D extent. This method handles better the depth discontinuities problem and generally performs better than the tiled methods. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT NOTE:&lt;/strong&gt; I have described the basics of the deferred shading. There are multiple variations, optimizations and improvements around so I urge you to experiment with a simple version and then do some researches on other techniques such the one I mentioned above. &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-08-09T11:36:54.697" LastActivityDate="2015-08-09T11:36:54.697" CommentCount="1" />
  <row Id="132" PostTypeId="2" ParentId="122" CreationDate="2015-08-09T11:28:28.467" Score="2" Body="&lt;p&gt;Color difference is a difference in brightness, in some color channel. So while the sum of individual channels stay the same, there is a intensity difference if there is a color difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the basic algorithms do not really specify how you should hadle the multi channel data. So there is nothing wrong with using a different color space. On the other hand nothing says you can not apply the convolution on all channels and then pick the result that is most contrasting.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-08-09T11:28:28.467" CommentCount="0" />
  <row Id="133" PostTypeId="2" ParentId="126" CreationDate="2015-08-09T11:38:54.363" Score="6" Body="&lt;p&gt;If you organize your scene in a spatial structure (the usual way being a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bounding_volume_hierarchy&quot;&gt;Bounding Volume Hierarchy&lt;/a&gt;), you can use a sort of virtual scene (I am making up this term, in reference to &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gameworks/events/gdc14/GDC_14_Real%20Virtual%20Texturing%20-%20Taking%20Advantage%20of%20DirectX%2011.2%20Tiled%20Resources.pdf&quot;&gt;virtual textures&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A memory manager would keep only a limited number of bounding boxes loaded at a time, and abstract the operation consisting in retrieving one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This way, a box would be loaded only as needed: when a ray hits a bounding box, the box get loaded to resolve the collision. Later on when another box needs to be loaded, the unused one is deleted to make room for the new one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With all these boxes getting loaded and deleted, the ray coherency would be a major factor in speed. I suppose a further improvement could be to defer loading, by reordering rays to treat first the boxes that are already loaded.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-08-09T11:38:54.363" CommentCount="1" />
  <row Id="134" PostTypeId="2" ParentId="126" CreationDate="2015-08-09T11:39:04.653" Score="1" Body="&lt;p&gt;What you do is you load triangles into memory from disk based on what has been hit previously. You can begin with triangles in close proximity first. The reasoning is that in one area the rays are likely to hit same triangles repeatedly. And eventually you will be somewhat efficient. (For this reason it is a good idea to cache last hit triangle in occlusion tracing that dont care of order)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second you store the triangles in a spatial tree that allows you to do quick searching from the disk, to renew what portions you are having in memory by proximity. So load only branches that will be in the way of the ray. If its some kind of voxel tree, like a octree you can even sort secondary rays and solve them by coherence. A BSP tree is also somewhat good at pruning areas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are cases where this fails but its reasonably efficient in most scene buckets if your not rendering noise...&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-09T11:59:09.313" LastActivityDate="2015-08-09T11:59:09.313" CommentCount="0" />
  <row Id="135" PostTypeId="2" ParentId="123" CreationDate="2015-08-09T11:52:38.473" Score="4" Body="&lt;p&gt;You can ray trace the distance to the second surface. This may be conceptually easiest, but not necessarily fastest method*.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But there is a neat trick, you can multi pass render a depth map to the water surface from the camera. The distance in water is now pixel depth minus the depth map depth. This does not work in all situations such as when you exit water and re-enter later. But should work in many cases, like the one depicted. You can even use same technique for Boolean operations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/UcYKY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/UcYKY.png&quot; alt=&quot;dmap&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Render depth map, consult depth map when shading the bottom of your pond.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* It might be in certain cases.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-09T12:54:20.610" LastActivityDate="2015-08-09T12:54:20.610" CommentCount="0" />
  <row Id="136" PostTypeId="2" ParentId="125" CreationDate="2015-08-09T11:54:03.197" Score="1" Body="&lt;p&gt;Short answer: no, because you don't know which rays will end up hitting which element of the scene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you knew, you could, which brings the following idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could first do a fast pre-render (with a low resolution and trivial materials), to approximate the answer to the question &lt;em&gt;&quot;Which elements need to be loaded for each area of the final render&quot;&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With this information, you could then create the jobs on the different machines while having them load only the elements needed for their area. It would be an approximation though, and it doesn't sound easy to implement.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my best guess, although it is not an answer to your question, would be to load the scene on demand and have the different machines work with the same dataset.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-08-09T11:54:03.197" CommentCount="0" />
  <row Id="137" PostTypeId="2" ParentId="122" CreationDate="2015-08-09T12:13:20.517" Score="3" Body="&lt;p&gt;I think you can convert from the RGB space to the HSV one, or whatever color space has the HUE in a single channel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Take the HUE channel, and make the edge detection on that one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here a simple Matlab script to achieve the result.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I = imread('image.png');&#xA;hsv = rgb2hsv(I);&#xA;hue = hsv(:,:,1);&#xA;edges = edge(uint8(hue),'sobel');&#xA;imshow(edges);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="316" LastEditorUserId="316" LastEditDate="2015-08-09T17:03:58.693" LastActivityDate="2015-08-09T17:03:58.693" CommentCount="0" />
  <row Id="138" PostTypeId="1" CreationDate="2015-08-09T12:39:35.540" Score="7" ViewCount="210" Body="&lt;p&gt;Quaternions (a four-dimensional extension of complex numbers) can used to represent rotation and scaling of a 3D vector, and the application of a quaternion onto a 3D vector involves two quaternion multiplications, thus requiring fewer operations than multiplication by the corresponding transformation matrix. However, linear and affine transformation matrixes are often used instead, especially in shader code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When is it appropriate and preferable (due to speed, stability, etc.) to use a quaternion to represent scaling and rotation in three dimensions, instead of the corresponding transformation matrix?&lt;/p&gt;&#xA;" OwnerUserId="170" LastActivityDate="2015-08-09T17:22:51.250" Title="When should quaternions be used to represent rotation and scaling in 3D?" Tags="&lt;transformations&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="1" />
  <row Id="140" PostTypeId="2" ParentId="138" CreationDate="2015-08-09T13:16:41.053" Score="2" Body="&lt;p&gt;Matrices offer more possible transforms than quaternions, it is possible to skew, mirror and non uniformly scale the matrix. There is nothing that states you can not make your engine do just quaternion based transforms, if you have no need for the additional transform features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Matrices are just very convenient when you need to build spaces where you know the basis vectors. Such as when doing projections into orthographic. Also doing perspective transform in a matrix space is easy. Matrices are superior when it comes to projecting stuff.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a way matrices are usually used because they represent the most common denomination and aren't too complicated to master and understand. The benefits of standardization far outweighs the benefit you get form a custom workflow. Its well known how to do the matrix operations. Whereas quats are not something most get immediate introduction to in uni. Just ask around how many know how to invert a quaternion, whereas you dont find many students in higher education who dont know how to invert a matrix. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that graphics cards also have dedicated pipes for matrix operations.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-09T13:30:27.103" LastActivityDate="2015-08-09T13:30:27.103" CommentCount="1" />
  <row Id="141" PostTypeId="2" ParentId="138" CreationDate="2015-08-09T13:27:07.007" Score="2" Body="&lt;p&gt;A quaternion can only represent uniform scaling and rotation so if you need anything else you would need to add something to represent that. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Translation can be done with a single additional vec3 (or using &lt;a href=&quot;https://en.wikipedia.org/wiki/Dual_quaternion&quot; rel=&quot;nofollow&quot;&gt;dual quaternions&lt;/a&gt;). However non-uniform scaling and sheering is represented better by a mat4. Projection transforms (essentially non-uniform scaling and swapping z and w) cannot be represented by a quaternion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quaternions have a major advantage when interpolating. The slerp is most easily calculated using quaternions. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Applying a quaternion (or dual quaternion) is not built into the GPU so you would need to implement that using the vector operations. Most quaternion libraries assume that you won't be using the quaternion to represent scale so that's something to look out for.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-08-09T16:44:50.557" LastActivityDate="2015-08-09T16:44:50.557" CommentCount="0" />
  <row Id="143" PostTypeId="2" ParentId="138" CreationDate="2015-08-09T15:58:55.490" Score="2" Body="&lt;p&gt;(A lot of information here I shamelessly borrowed from joojaa's and ratchet freak's answers, with some notes of my own.)&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Matrix Advantages&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Non uniform scaling and rotation, skewing, projection&lt;/li&gt;&#xA;&lt;li&gt;Translation (unless using dual quaternions)&lt;/li&gt;&#xA;&lt;li&gt;Native hardware support&lt;/li&gt;&#xA;&lt;li&gt;Quaternions often require transcendental functions to construct&lt;/li&gt;&#xA;&lt;li&gt;Easier to understand&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;Quaternion Advantages&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Transforming a vector requires fewer operations (Or not - See John's answer)&lt;/li&gt;&#xA;&lt;li&gt;Transforming by another quat requires many fewer operations&lt;/li&gt;&#xA;&lt;li&gt;Quaternions occupy 4 floats, (8 if it's a dual) but Matrices occupy 9-16 floats&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If you know you're only going to be doing uniform rigid body transforms, a vector/quat pair is usually a solid win on a 3x4 matrix in terms of storage space (vector/quat: 7 or 8 floats vs mat3x4: 12 floats) and processing speed. If Quaternions are still mystical voodoo to you, try &lt;a href=&quot;https://www.youtube.com/playlist?list=PLW3Zl3wyJwWNWsJIPZrmY19urkYHXOH3N&quot; rel=&quot;nofollow&quot;&gt;this web series on them&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="125" LastEditorUserId="125" LastEditDate="2015-08-09T17:20:16.970" LastActivityDate="2015-08-09T17:20:16.970" CommentCount="0" />
  <row Id="144" PostTypeId="2" ParentId="138" CreationDate="2015-08-09T17:16:24.560" Score="3" Body="&lt;p&gt;I want to start with misconceptions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Modern GPUs (NVIDIA for quite a while, and AMD since Southern Islands) do not meaningfully support vector/matrix operations natively in hardware. They are vector architectures in a different direction: each component of a vector (x, y, z) are generally 32- or 64-valued, containing values for each element in a lane. So a 3D dot product is not usually an instruction, it is a multiply and two multiply-adds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, counting primitive operations like multiply-add, transforming a vector by a quaternion is more expensive than transforming a vector by a matrix. Transforming a vector by a 3x3 matrix is 3 multiplies and 6 multiply-adds, and transforming a vector by a quaternion is two quaternion multiplies, each of which consist of 4 multiplies and 12 multiply-adds. (You can get less naïve than this—&lt;a href=&quot;https://molecularmusings.wordpress.com/2013/05/24/a-faster-quaternion-vector-multiplication/&quot; rel=&quot;nofollow&quot;&gt;here's a writeup on a faster way&lt;/a&gt;—but it's still not as cheap as multiplying a vector by a matrix.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, performance is not always determined simply by counting the number of ALU operations it performs. Quaternions require less space than the equivalent matrix (assuming you are only doing pure rotation/scale), and that means less storage space and less memory traffic. This is often important in animation (which is conveniently also often where the nice interpolation properties of quaternions show up).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other than that:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Matrices use more space because they support more operations. A 3x3 matrix can contain nonuniform scale, skew, reflection, and orthogonal projection.&lt;/li&gt;&#xA;&lt;li&gt;Matrices can be naturally thought of as basis vectors, and easily constructed from those vectors.&lt;/li&gt;&#xA;&lt;li&gt;Multiplying one quaternion by another (composing two rotations) is less operations than multiplying one matrix by another.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2015-08-09T17:22:51.250" LastActivityDate="2015-08-09T17:22:51.250" CommentCount="0" />
  <row Id="145" PostTypeId="1" AcceptedAnswerId="156" CreationDate="2015-08-09T18:05:02.017" Score="11" ViewCount="77" Body="&lt;p&gt;Literature on rendering volumetric materials and effects tends to use a lot of mathematical physics terminology. Let's say that I have a decent handle on the concepts involved in surface rendering. What concepts do I need to understand for volumetric rendering? (Both real-time and offline rendering.)&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;What exactly is meant by light scattering in the context of volumetric rendering? (And why is it split into in-scattering and out-scattering?)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What is the relationship between transmission, attenuation, and absorption?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What is a phase function and how does it play into volumetric rendering? (In particular, the Henyey-Greenstein phase function.)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What is the Beer-Lambert law and how is it related to light scattering?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Basically, how do I make sense out of diagrams like this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TZ5vY.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TZ5vY.png&quot; alt=&quot;Confusing diagram&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-09T23:37:56.080" Title="Volumetric rendering fundamental concepts and terminology" Tags="&lt;volumetric&gt;&lt;atmospherics&gt;&lt;scattering&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="146" PostTypeId="1" AcceptedAnswerId="147" CreationDate="2015-08-09T19:33:44.087" Score="8" ViewCount="138" Body="&lt;p&gt;Forward rendering is the process of computing a radiance value for a surface fragment directly from input geometry and lighting information. Deferred rendering splits that process into two steps: first producing a screen-space buffer containing material properties (a geometry buffer, or G-buffer) built by rasterizing the input geometry, and second producing a radiance value for each pixel by combining the G-buffer with lighting information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Deferred rendering is often presented as an optimization of forward rendering. One explanation is that lighting is fairly expensive and if you have any overdraw then you are lighting pixels that will never be seen on screen, whereas if you store material properties into a G-buffer and light afterwards, you are only lighting a pixel that will actually appear on-screen. Is this actually an advantage of deferred, given that you can also do a depth pre-pass and then do a forward rendering pass with depth test set to &lt;code&gt;D3D11_COMPARISON_EQUAL&lt;/code&gt; or &lt;code&gt;GL_EQUAL&lt;/code&gt; or the equivalent?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Deferred rendering also has the potential to schedule better on the GPU. Splitting one large warp/wavefront into a smaller geometry wavefront and then smaller lighting wavefronts later improves occupancy (more wavefronts in flight simultaneously). But you also end up with a lot more bandwidth use (writing out a large number of channels to the G-buffer, then reading them back in during lighting). Obviously the specifics here depend a lot on your GPU, but what are the general principles?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there other practical performance considerations when deciding between forward and deferred rendering? (Assume that we can use variations of each technique if necessary: i.e. we can compare tiled forward to tiled deferred as well.)&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-09T19:33:44.087" Title="What is the performance tradeoff between forward and deferred rendering?" Tags="&lt;performance&gt;&lt;deferred-rendering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="147" PostTypeId="2" ParentId="146" CreationDate="2015-08-09T19:33:44.087" Score="8" Body="&lt;p&gt;It is possible to avoid overdraw from opaque objects even with forward rendering by doing a depth pre-pass and using that information to reject any pixel that is not actually visible. However, depending on the vertex cost of your scene, a depth pre-pass may add an unacceptable amount of performance overhead. Additionally, rendering using the pixel shading pipeline of the GPU means that you don't pay a cost per pixel that is rendered, you pay a cost per 2x2 pixel &lt;em&gt;quad&lt;/em&gt; that is rendered. So even doing a depth pre-pass still causes triangle edges to waste work shading pixels that will be discarded.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GPU scheduling is a complex topic, and the tradeoff between forward and deferred does not boil down simply to &quot;runs faster but uses more bandwidth.&quot; If you have two equally cheap operations that run in sequence and each use the same number of resources, there's no reason to split them into separate shaders: two small wavefronts that each use X resources don't fundamentally work better than a single longer wavefront that also uses X resources. If you have a cheap operation and an expensive operation to run in sequence, though, it may benefit from splitting into separate shaders: the shader in general will reserve the maximum amount of resources that it might use at any point. It's conceivable that forward rendering may not be able to use all the bandwidth of your GPU because there are so few wavefronts in flight that it cannot issue enough operations to saturate the bandwidth. But if you &lt;em&gt;are&lt;/em&gt; bandwidth limited, there may be no advantage to deferred rendering (since it will probably use more bandwidth).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional performance concern is that forward rendering supports different material types (different BRDFs, say) simply by using a different shader for that object. A straightforward deferred renderer needs to handle different material types in a different way (potentially a branch in the shader), since work is no longer grouped into warps/wavefronts coherently depending on the object being rendered. This can be mitigated with a tiled renderer—if only specific areas of the screen use an alternate material type (say, for hair), then you can use the shader variation with a material type branch only for tiles that contain any pixels with that material.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-09T19:33:44.087" CommentCount="0" />
  <row Id="148" PostTypeId="1" AcceptedAnswerId="150" CreationDate="2015-08-09T20:02:43.317" Score="5" ViewCount="102" Body="&lt;p&gt;Antialiasing of 2D shapes boils down to computing the fraction of a pixel that is covered by the shape. For simple non-overlapping shapes, this is not too difficult: clip the shape against the pixel rectangle and calculate the resulting shape's area. But it becomes more difficult if multiple shapes overlap the same pixel. Simply summing areas can cause the computed coverage to be too high, if it neglects the amount that one shape covers another shape. For example, see &lt;a href=&quot;http://nothings.org/gamedev/rasterize/&quot;&gt;the Limitations section of this article on font rendering&lt;/a&gt;. You might also be in a situation where the two curves come from different objects with different colors (so it's not about the total coverage for the union of the two shapes, but the coverage of each one separately).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would this be computed if you cared about complete accuracy? Even trickier, how do you compute coverage accurately for overlapping non-polygonal shapes like curves? Is there some point when you have no choice but to fall back to multisampling or stochastic techniques?&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-14T07:36:33.963" Title="How do I accurately compute coverage of overlapping analytical curves?" Tags="&lt;antialiasing&gt;&lt;curve&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="149" PostTypeId="1" CreationDate="2015-08-09T20:27:41.297" Score="14" ViewCount="329" Body="&lt;p&gt;It is common knowledge that hair simulation and rendering is particularly challenging and in fact rare are the examples in games that propose believable hairs. It is pretty easy to imagine how hard it is to actually simulate the high amount of fibres and to simulate the various scattering events that may happen between so many strands, let alone the fact that each of them is translucent. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone formalize what are the main challenges in hair rendering, especially for real time scenarios? How these are overcome by state-of-art techniques? Are there some de-facto standard theoretical models used? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To clarify, for real time I don't necessarily mean in a game context. &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="36" LastEditDate="2015-08-10T07:13:13.427" LastActivityDate="2015-08-14T22:40:35.007" Title="How does state of the art real time hair rendering work?" Tags="&lt;shading&gt;&lt;real-time&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="5" />
  <row Id="150" PostTypeId="2" ParentId="148" CreationDate="2015-08-09T21:07:03.407" Score="5" Body="&lt;p&gt;There is really no good way of doing this efficiently analytically for all corner cases. Most or all commercial 2D renderers that attempt to do analytic coverage calculation make predictable errors that multisampling methods do not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A typical problem is two overlapping shapes that share the same edge. The common situation is that alpha channels sum up to a too thick alpha edge that aliases slightly. Or if shapes are differently colored the system confuses what color the background is. This is extremely annoying.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Tfs4k.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: The rendering engine confuses the coverage and makes a thin white outline where no outline should be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second perfect coverage amounts to box filtering. We can certainly do better. Considering that there are so many special corner cases that would require boolean operations on the shapes to do right, super sampling is still superior. In fact the coverage estimates may be used to concentrate sampling where it's most likely needed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The situation could be simplified to polygons at sub pixel levels then the discrete analytical solution could be solved. But this at the expense of flexibility. For example its not out of the question that future vector systems might want to allow for variable width blurred lines which pose a problem for analytic solutions, as do other variably colored objects.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;How to do it analytically&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/CihEg.png&quot; alt=&quot;Analytic scene&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Suppose you have this scene, exploded view on right&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now you can not just do this analytically, each piece separately and then merge the data. Because it results in wrong data. See alpha blending would let the blue shine trough the gaps if you did so.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you have to so is split the scene up so that each shape eliminates what is under the other:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rJu1X.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: You need to cut the underlying surfaces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now if everything is opaque then this is all straight forward. just calculate the area of each piece and multiply that by color and sum them together. Now you can use something like &lt;a href=&quot;http://stackoverflow.com/questions/10039679/how-can-i-calculate-the-area-of-a-bezier-curve&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This all breaks down if your individual shapes aren't opaque off course but even that can be done at some level.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Remember:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AA calculation need to be done in linear color space, and converted back to use space.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-14T07:36:33.963" LastActivityDate="2015-08-14T07:36:33.963" CommentCount="2" CommunityOwnedDate="2015-08-09T21:07:03.407" />
  <row Id="151" PostTypeId="1" AcceptedAnswerId="168" CreationDate="2015-08-09T22:06:50.547" Score="10" ViewCount="918" Body="&lt;p&gt;After so much reading about transformations it is time to implement a trackball for my app.&#xA;I understand I have to create a vector from the origin to where the mouse is clicked and then another from the origin to where the mouse is released.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, do I have to transfor the (x,y) pixel coords to world-coords or should I just do everything in image space (considering image space is the 2D projection of the scene measured in pixels)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Richie Sams' answer is a very good one. However, I think I'm following a slightly different approach, please correct me if I'm wrong or I'm misunderstanding something.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my application I have a &lt;code&gt;SimplePerspectiveCamera&lt;/code&gt; class that receives the &lt;code&gt;position&lt;/code&gt; of the camera, the &lt;code&gt;position of the target&lt;/code&gt; we are looking at, the &lt;code&gt;up&lt;/code&gt; vector, the &lt;code&gt;fovy&lt;/code&gt;, &lt;code&gt;aspectRatio&lt;/code&gt;, &lt;code&gt;near&lt;/code&gt; and &lt;code&gt;far&lt;/code&gt; distances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With those I build my View and Projection matrices. Now, If I want to zoom in/out I update the field of view and update my projection matrix. If I want to pan I move the position of the camera and look at by the delta the mouse produces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, for rotations I can use either angle-axis transformation or quaternions. For this, I save the pixel-coords where the mouse was pressed and then when the mouse moves I also save the pixel-coords.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each pair of coords I can compute the Z-value given the formula for a sphere, i.e., sqrt(1-x^2-y^2), then compute to vectors that go from the &lt;code&gt;target&lt;/code&gt; to &lt;code&gt;PointMousePressed&lt;/code&gt; and from &lt;code&gt;target&lt;/code&gt; to &lt;code&gt;PointMouseMoved&lt;/code&gt;, do cross product to get the axis of rotation and use any method to compute the new camera position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, my biggest doubt is that the (x,y,z) values are given in pixel-coords, and when computing the vectors I'm using &lt;code&gt;target&lt;/code&gt; which is a point in world-coords. Isn't this mixing of coordinate system affecting the result of the rotation I'm trying to do?&lt;/p&gt;&#xA;" OwnerUserId="116" LastEditorUserId="116" LastEditDate="2015-08-12T00:55:29.510" LastActivityDate="2016-02-09T06:19:09.250" Title="How to implement a trackball in OpenGL?" Tags="&lt;opengl&gt;&lt;trackball&gt;" AnswerCount="2" CommentCount="6" FavoriteCount="1" />
  <row Id="152" PostTypeId="2" ParentId="125" CreationDate="2015-08-09T22:09:49.603" Score="6" Body="&lt;p&gt;Yes, such a thing is possible. There are a few different ways of doing it, but the basic idea is to split up the scene into chunks and assign these chunks to different machines. Each machine then traces rays locally within their assigned chunk.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The tricky bit is of course when rays leave a chunk, in which case machines need to synchronize and exchange rays that cross the local chunk boundaries. A recent paper from this year's EGSR, &lt;a href=&quot;http://vc.cs.ovgu.de/files/publications/2014/Guenther_2014_CGF.pdf&quot;&gt;Distributed Out-of-Core&#xA;Stochastic Progressive Photon Mapping&lt;/a&gt;, describes how to do this efficiently for Progressive Photon Mapping, although other light transport methods might be feasible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although such distributed ray tracing is possible, it is certainly not the most efficient form of parallelization, and non-uniform lighting directly translates into non-uniform distribution of the workload. It is really only beneficial when the size of the scene far exceeds the memory of a single machine.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-09T22:09:49.603" CommentCount="0" />
  <row Id="153" PostTypeId="2" ParentId="126" CreationDate="2015-08-09T22:47:46.967" Score="10" Body="&lt;p&gt;If the scene does not entirely fit into memory, you are entering the field of out-of-core rendering. There are essentially two approaches here: a) Generate your scene on-demand b) Load your scene on-demand&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The former approach aligns well with most animation workflows, where models are heavily subdivided using e.g. Catmull-Clark and can become very memory-intensive, but the base meshes themselves easily fit into memory. Pixar have a few papers about this (e.g. &lt;a href=&quot;http://www.pixar.com/companyinfo/research/per/eg03.pdf&quot;&gt;Ray Differentials and Multiresolution Geometry Caching&#xA;for Distribution Ray Tracing in Complex Scenes&lt;/a&gt;), but the gist of it is that models are only subdivided when they are hit by a ray, and only subdivided as much as is reasonable for such a ray (e.g. diffuse interreflection need less accuracy than mirror reflections). The rest is handled by a geometry cache, which keeps the subdivided models in memory and hopefully makes the process efficient by a good eviction strategy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As long as all your base meshes comfortably fit into memory, you can easily go out-of-core and render meshes at subdivision levels that would never fit into memory. The geometry cache also scales nicely with the amount of memory you have, allowing you to weigh RAM vs. render times. This was also used in &lt;a href=&quot;http://graphics.pixar.com/library/RayTracingCars/paper.pdf&quot;&gt;Cars&lt;/a&gt; I believe.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second approach is more general and does not rely on heavy use of subdivision. Instead, it relies on the fact that your scene was most likely made by an artist and already comes partitioned into reasonably small objects that fit into memory individually. The idea is then to keep two hierarchies (kD-tree or bounding volume hierarchy): A top-level hierarchy that only stores bounding boxes of the objects in your scene, and a low-level hierarchy that stores the actual geometry. There is one such low-level hierarchy for each object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this approach, you ideally already store a bounding box along with each object on disk. As the scene is loaded, you only build the top-level hierarchy initially, meaning you only have to look at the bounding boxes and not the geometry. You then start tracing rays and traverse them through the hierarchy. Whenever a ray hits a leaf node in the top-level hierarchy (i.e. it hits the bounding box of an object), that object is loaded into memory and its low-level hierarchy is built. The ray then continues down into tracing that object. Combined with an object cache that keeps as much of the low-level hierarchy in memory as possible, this can perform reasonably well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first benefit of such an approach is that objects that are never hit are never loaded, meaning that it automatically adapts to the visibility in your scene. The second benefit is that if you are tracing many rays, you don't have to load an object immediately as it is hit by a ray; instead, you can hold that ray and wait until enough rays have hit that object, amortizing the load over multiple ray hits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also combine this approach with a ray sorting algorithm such as &lt;a href=&quot;https://disney-animation.s3.amazonaws.com/uploads/production/publication_asset/70/asset/Sorted_Deferred_Shading_For_Production_Path_Tracing.pdf&quot;&gt;Sorted Deferred Shading for Production Path Tracing&lt;/a&gt; to avoid thrashing due to incoherent rays. The mentioned paper describes the architecture of Disney's Hyperion renderer, used for Big Hero 6 I believe, so it most likely can handle scenes at production scale.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-09T22:47:46.967" CommentCount="2" />
  <row Id="154" PostTypeId="1" AcceptedAnswerId="157" CreationDate="2015-08-09T23:11:11.807" Score="6" ViewCount="59" Body="&lt;p&gt;I'd like to be able to render a large population of small independently moving objects in real time. They may move in a swarm-like manner, but their relative positions will not be coherent - their position may change arbitrarily within a swarm and swarms may break up and reform at any point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What approach to building a bounding volume hierarchy would best suit this situation? Is there a way to maintain a hierarchy which is sub-optimal but good enough, that only requires a partial update each frame? Or is there a way of building a hierarchy from scratch each frame that is fast enough for smooth animation?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of objects will be too large to render without a hierarchy, but for the same reason I expect building the hierarchy to be time consuming. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Following the comment from John Calsbeek, if my focus on bounding volume hierarchies is misguided, and there is a better space partitioning approach for this situation, please answer accordingly. I'm looking for something that can deal with what I describe, including anything I haven't thought of.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-09T23:32:10.547" LastActivityDate="2015-08-09T23:50:26.667" Title="How can I create a bounding volume hierarchy for constantly moving objects?" Tags="&lt;real-time&gt;&lt;bounding-volume-hierarchy&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="155" PostTypeId="2" ParentId="154" CreationDate="2015-08-09T23:26:06.717" Score="2" Body="&lt;p&gt;You could try to simply make the bounding volumes a bit larger than necessary so that the objects don't cross their boundaries on every move but then again, you would have to rebuild the structure now-and-then anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or, there is &lt;a href=&quot;https://en.wikipedia.org/wiki/Bounding_interval_hierarchy&quot; rel=&quot;nofollow&quot;&gt;Bounding interval hierarchy&lt;/a&gt; which tries to address precisely this scenario.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or, the paper by Ingo Wald, Solomon Boulos and Peter Shirley titled &lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/wald07_packetbvh.pdf&quot; rel=&quot;nofollow&quot;&gt;Ray Tracing Deformable Scenes Using Dynamic Bounding Volume Hierarchies&lt;/a&gt; might be of interest.&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2015-08-09T23:36:06.820" LastActivityDate="2015-08-09T23:36:06.820" CommentCount="0" />
  <row Id="156" PostTypeId="2" ParentId="145" CreationDate="2015-08-09T23:37:56.080" Score="6" Body="&lt;p&gt;When I was first reading about all of this I stumbled upon this &lt;a href=&quot;http://www.scratchapixel.com/old/lessons/3d-advanced-lessons/volume-rendering/volume-rendering-for-artists/&quot;&gt;link&lt;/a&gt; which helped me better understand this large subject.  Also &lt;a href=&quot;http://www.cs.dartmouth.edu/~wjarosz/publications/dissertation/chapter4.pdf&quot;&gt;this&lt;/a&gt; goes into some more detail on things mentioned here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Light scattering is a natural phenomenon which arises when light interacts with particles distributed in a media as it travels through it.  From &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_scattering&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Light scattering can be thought of as the deflection of a ray from a straight path, for example by irregularities in the propagation medium, particles, or in the interface between two media&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In computer graphics there are models that have been developed to simulate the effect of light traversing volume objects from an entry point (&lt;em&gt;Point A&lt;/em&gt;) to an exit point (&lt;em&gt;Point B&lt;/em&gt;).  As the light travels from &lt;em&gt;A&lt;/em&gt; to &lt;em&gt;B&lt;/em&gt; it is changed due to interactions with particles and these interactions are often referred to as &lt;strong&gt;Absorption&lt;/strong&gt;, &lt;strong&gt;Out Scattering&lt;/strong&gt; and &lt;strong&gt;In Scattering&lt;/strong&gt;.  Often you will see these split into two groups; &lt;em&gt;Transmittance&lt;/em&gt; (Absorption and Out Scattering) which I like to think of as 'light lost' and &lt;em&gt;In-Scattering&lt;/em&gt; ('light gained').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Absorption&lt;/strong&gt; is basically incident light energy that is transformed into some other form of energy and therefore 'lost'.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Transmittance&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Transmittance describes how light reflected behind a volume will be attenuated due to Absorption as it travels through a medium from &lt;em&gt;A&lt;/em&gt; to &lt;em&gt;B&lt;/em&gt;.  This is usually calculated with the Beer-Lambert law which relates the attenuation of light to the properties of the material through which it is travelling.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the light travels through the medium there is a chance that the photons can be scattered away from their incident direction and therefore not make it to the eye of the observer and this is referred to as Out-Scattering.  In most models the Transmittance equation is changed slightly to introduce the concept of Out-Scattering. &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;In Scattering&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Above we have seen how light can be lost due to photons been scattered away from the viewing direction.  At the same time light can be scattered back into the viewing direction as it is travelling from &lt;em&gt;A&lt;/em&gt; to &lt;em&gt;B&lt;/em&gt; and this is called In-Scattering. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Particle In-Scattering itself is a pretty complex topic but basically you can split it into Isotropic and Anisotropic scattering.  Modelling Anisotropic scattering would take a considerable amount of time so usually in computer graphics this is simplified by using a Phase Function which describes the amount of light from the incident light direction that is scattered into the viewing direction as it travels from &lt;em&gt;A&lt;/em&gt; to &lt;em&gt;B&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One commonly used non-isotropic Phase Function is called the Henyey-Greenstein phase function which can model Backward and Forward scattering.  It usually has a single parameter, g ∈ [−1,1], that determines the relative strength of forward and backward scattering.&lt;/p&gt;&#xA;" OwnerUserId="329" LastActivityDate="2015-08-09T23:37:56.080" CommentCount="0" />
  <row Id="157" PostTypeId="2" ParentId="154" CreationDate="2015-08-09T23:50:26.667" Score="2" Body="&lt;p&gt;Consider using spatial hashing, especially if your objects are similarly sized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, divide your world into uniformly-sized grid cells (2D and 3D are both valid possibilities depending on the amount of vertical motion). Each update, assign your object to each bin that it overlaps—if the cells are decently sized relative to the objects, most objects should end up in a single bin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each bin is inserted into a hash table, with the key being the coordinates of the bin. (You can also think of it as a hash table with multiple values for the same key, and inserting an object once for every cell that it overlaps.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's no hierarchy to rebuild in this scheme, which makes it well suited for dynamic scenes. You can still test the cell's dimensions against the frustum or against occluders at a coarse level and discard many objects at once. Also, it's easier to manage this structure incrementally—you can keep the hash table the same from frame to frame and only move objects from one bin to another when they cross the boundary of a cell.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-09T23:50:26.667" CommentCount="0" />
  <row Id="158" PostTypeId="2" ParentId="149" CreationDate="2015-08-10T01:11:41.027" Score="6" Body="&lt;p&gt;&lt;em&gt;Sorry for the bad quality of my answer. I do not have access to a computer currently and editing from my phone is not a straightforward task. In particular I would love to be able to paste images.&lt;/em&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would say that the main challenges of simulating hair are:  &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;replicating their very specific reaction to lighting (as a material)  &lt;/li&gt;&#xA;&lt;li&gt;replicating their volumetric property (as a geometry)  &lt;/li&gt;&#xA;&lt;li&gt;animating them in a realistic manner  &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here is a list of references I gathered on the subject, ordered chronologically &lt;em&gt;(mainly about the rendering part)&lt;/em&gt;:  &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;JAMES T. KAJIYA, TIMOTHY L. KAY, &lt;a href=&quot;https://www.cs.drexel.edu/~david/Classes/CS586/Papers/p271-kajiya.pdf&quot; rel=&quot;nofollow&quot;&gt;Rendering Fur With Three Dimensional Textures&lt;/a&gt;, 1989  &lt;/li&gt;&#xA;&lt;li&gt;JEROME LENGYEL, EMIL PRAUN, ADAM FINKELSTEIN, HUGUES HOPPE, &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/hoppe/fur.pdf&quot; rel=&quot;nofollow&quot;&gt;Real-Time Fur over Arbitrary Surfaces&lt;/a&gt;, 2000  &lt;/li&gt;&#xA;&lt;li&gt;STEPHEN R. MARSCHNER, HENRIK WANN JENSEN, MIKE CAMMARANO, &lt;a href=&quot;http://www.graphics.stanford.edu/papers/hair/hair-sg03final.pdf&quot; rel=&quot;nofollow&quot;&gt;Light Scattering from Human Hair Fibers&lt;/a&gt;, 2003  &lt;/li&gt;&#xA;&lt;li&gt;ARMIN BRUDERLIN, SUNIL HADAP, TAE-YONG KIM, NADIA MAGNENAT-THALMANN , ULRICH NEUMANN, YIZHOU YU, STEVE WORLEY, &lt;a href=&quot;http://www.graphics.stanford.edu/courses/cs448-05-winter/papers/course34.pdf&quot; rel=&quot;nofollow&quot;&gt;Photorealistic Hair Modeling, Animation, and Rendering&lt;/a&gt;, Siggraph 2003 Course  &lt;/li&gt;&#xA;&lt;li&gt;THORSTEN SCHEUERMANN, &lt;a href=&quot;http://bookzz.org/book/487747/b3254e&quot; rel=&quot;nofollow&quot;&gt;Hair Rendering and Shading&lt;/a&gt;, ShaderX3, 2004  &lt;/li&gt;&#xA;&lt;li&gt;THORSTEN SCHEUERMANN, &lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Scheuermann_HairRendering.pdf&quot; rel=&quot;nofollow&quot;&gt;Hair Rendering and Shading&lt;/a&gt;, ATI / GDC 2004  &lt;/li&gt;&#xA;&lt;li&gt;THORSTEN SCHEUERMANN, &lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Scheuermann_HairSketch.pdf&quot; rel=&quot;nofollow&quot;&gt;Practical Real-Time Hair Rendering and Shading&lt;/a&gt;, ATI / Siggraph 2004  &lt;/li&gt;&#xA;&lt;li&gt;THORSTEN SCHEUERMANN, &lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Scheuermann_HairSketchSlides.pdf&quot; rel=&quot;nofollow&quot;&gt;Practical Real-Time Hair Rendering and Shading&lt;/a&gt;, ATI / Siggraph 2004 (slides)  &lt;/li&gt;&#xA;&lt;li&gt;MARTIN KOSTER, JORG HABER, HANS-PETER SEIDEL, &lt;a href=&quot;http://resources.mpi-inf.mpg.de/FAM/publ/cgi2004.pdf&quot; rel=&quot;nofollow&quot;&gt;Real-Time Rendering of Human Hair using Programmable Graphics Hardware&lt;/a&gt;, 2004  &lt;/li&gt;&#xA;&lt;li&gt;HUBERT NGUYEN, WILLIAM DONNELLY, &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter23.html&quot; rel=&quot;nofollow&quot;&gt;Hair Animation and Rendering in the Nalu Demo&lt;/a&gt;, GPU Gems 2 (Chapter 23), 2005  &lt;/li&gt;&#xA;&lt;li&gt;LENA PETROVIC, MARK HENNE, JOHN ANDERSON, &lt;a href=&quot;http://graphics.pixar.com/library/Hair/paper.pdf&quot; rel=&quot;nofollow&quot;&gt;Volumetric Methods for Simulation and Rendering of Hair&lt;/a&gt;, Pixar 2006  &lt;/li&gt;&#xA;&lt;li&gt;CHRISTOPHE HERY, RAVI RAMAMOORTHI, &lt;a href=&quot;http://graphics.pixar.com/library/ImportanceSamplingHair/paper.pdf&quot; rel=&quot;nofollow&quot;&gt;Importance Sampling of Reflections from Hair Fibers&lt;/a&gt;, Pixar 2007  &lt;/li&gt;&#xA;&lt;li&gt;SARAH TARIQ, LOUIS BAVOIL, &lt;a href=&quot;http://www.nvidia.com/object/siggraph-2008-hair.html&quot; rel=&quot;nofollow&quot;&gt;Real-Time Hair Rendering on the GPU&lt;/a&gt;, Siggraph 2008. &lt;/li&gt;&#xA;&lt;li&gt;ARNO ZINKE, CEM YUKSEL, ANDREAS WEBER, JOHN KEYSER, &lt;a href=&quot;http://research.cs.tamu.edu/keyser/Papers/Siggraph08_dualscattering.pdf&quot; rel=&quot;nofollow&quot;&gt;Dual Scattering Approximation for Fast Multiple Scattering in Hair&lt;/a&gt;, Siggraph 2008  &lt;/li&gt;&#xA;&lt;li&gt;IMAN SADEGHI, HEATHER PRITCHETT, HENRIK WANN JENSEN, RASMUS TAMSTORF, &lt;a href=&quot;http://graphics.ucsd.edu/~henrik/papers/artist_hair.pdf&quot; rel=&quot;nofollow&quot;&gt;An Artist Friendly Hair Shading System&lt;/a&gt;, 2010  &lt;/li&gt;&#xA;&lt;li&gt;EUGENE D’EON, GUILLAUME FRANCOIS, MARTIN HILL, JOE LETTERI, JEAN-MARIE AUBRY, &lt;a href=&quot;http://www.eugenedeon.com/project/an-energy-conserving-hair-reflectance-model/&quot; rel=&quot;nofollow&quot;&gt;An Energy-Conserving Hair Reflectance Model&lt;/a&gt;, 2011  &lt;/li&gt;&#xA;&lt;li&gt;XUAN YU, JASON C. YANG, JUSTIN HENSLEY, TAKAHIRO HARADA, JINGYI YU, &lt;a href=&quot;http://www.eecis.udel.edu/~xyu/publications/frameworkhair124.pdf&quot; rel=&quot;nofollow&quot;&gt;A Framework for Rendering Complex Scattering Effects on Hair&lt;/a&gt;, 2012  &lt;/li&gt;&#xA;&lt;li&gt;JIAWEI OU, FENG XIE, PARASHAR KRISHNAMACHARI, FABIO PELLACINI, &lt;a href=&quot;http://pellacini.di.uniroma1.it/publications/ishair12/ishair12-paper.pdf&quot; rel=&quot;nofollow&quot;&gt;ISHair: Importance Sampling for Hair Scattering&lt;/a&gt;, 2012  &lt;/li&gt;&#xA;&lt;li&gt;SARAH INVERNIZZI, &lt;a href=&quot;http://nccastaff.bournemouth.ac.uk/jmacey/MastersProjects/MSc13/07/MSc_thesis_On_physically_based_hair_rendering.pdf&quot; rel=&quot;nofollow&quot;&gt;On Physically Based Hair Rendering&lt;/a&gt;, 2013  &lt;/li&gt;&#xA;&lt;li&gt;WOLFGANG ENGEL, &lt;a href=&quot;http://www.slideshare.net/WolfgangEngel/hair-intombraider-final&quot; rel=&quot;nofollow&quot;&gt;Hair Rendering in Tomb Raider&lt;/a&gt;, 2013  &lt;/li&gt;&#xA;&lt;li&gt;EUGENE D’EON, STEVE MARSCHNER, JOHANNES HANIKA, &lt;a href=&quot;https://cg.ivd.kit.edu/publications/pubhanika/2013_hairbrief.pdf&quot; rel=&quot;nofollow&quot;&gt;Importance Sampling for Physically-Based Hair Fiber Models&lt;/a&gt;, 2013  &lt;/li&gt;&#xA;&lt;li&gt;TIMOTHY MARTIN, WOLFGANG ENGEL, NICOLAS THIBIEROZ, JASON YANG, AND JASON LACROIX, &lt;a href=&quot;http://bookzz.org/book/2373940/c298ee&quot; rel=&quot;nofollow&quot;&gt;TressFX: Advanced Real-Time Hair Rendering&lt;/a&gt;, GPU Pro 5, 2015  &lt;/li&gt;&#xA;&lt;li&gt;LEONID PEKELIS, CHRISTOPHE HERY, RYUSUKE VILLEMIN, JUNYI LING, &lt;a href=&quot;http://graphics.pixar.com/library/DataDrivenHairScattering/paper.pdf&quot; rel=&quot;nofollow&quot;&gt;A Data-Driven Light Scattering Model for Hair&lt;/a&gt;, Pixar 2015  &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I did not read all of these material yet, but here is a little of what I remember (I am more interested by the real-time solutions) :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Kajiya-kay laid the basis for realistic hair / fur rendering  &lt;/li&gt;&#xA;&lt;li&gt;Marschner proposed an improved model, and noted that hair has two distinct specular highlights (one white due to direct reflection, the other colored and sparkling due to reflection inside of the hair fiber)  &lt;/li&gt;&#xA;&lt;li&gt;Scheuermann proposed approximations to simulate the behaviour described by Marschner, approximations suited for real-time (using tricks such as two separate specular highlights, artist-designed specular offset and exponent, noise textures). He also proposed using three rendering passes (opaque, transparent back and front) of polygons to simulate the &quot;volumetric&quot; qualities of hair  &lt;/li&gt;&#xA;&lt;li&gt;Most of the games I heard of seem to use Scheuermann approaches for hair shading. However, this is not an energy-conserving solution. Nevertheless, I think d'Eon attempts to fix this problem in the 2011 paper.&lt;br&gt;&#xA;An other topic of interest is the use of tessellation, that has also been used recently to replace the polygonal approach and simulate hair strands individually.     &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The offline rendering world attempts to solve more advanced challenges. I think most of them are described in the references above.  &lt;/p&gt;&#xA;" OwnerUserId="110" LastEditorUserId="110" LastEditDate="2015-08-14T22:40:35.007" LastActivityDate="2015-08-14T22:40:35.007" CommentCount="0" />
  <row Id="159" PostTypeId="1" CreationDate="2015-08-10T01:52:02.447" Score="10" ViewCount="96" Body="&lt;p&gt;The iris (the colourful ring surrounding the pupil of the eye) is covered in a layer of water, and appears to have opaque elements embedded in transparent and translucent elements. What effects do I need to model in order for the iris to appear realistic close up (the iris taking up over 20% of the image area)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is sub surface scattering necessary or is transparency sufficient? Is it necessary to take into account light from within the eye that entered via the pupil, or can the back of the iris be regarded as completely opaque? Are there other effects that I haven't considered?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking to produce still images offline - the approach does not need to work in real time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I've tried covering the eye in a thin transparent film to simulate a water layer, and treating the iris as a section of a transparent ball with coloured transparent radially arranged strands, offset as they are layered front to back. To prevent unwanted effects from light that gets all the way through, I have backed the iris with a matt black sphere section. This still seems to give an eye that looks somewhat artificial (inorganic) and disconnected though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to make this work with geometric primitives such as spheres, cones and cylinders, but I'm open to an approach using triangle meshes if that opens up new possibilities.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T13:47:10.233" LastActivityDate="2015-09-23T16:24:11.350" Title="Which effects do I need to take into account for the iris of the eye?" Tags="&lt;photo-realistic&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="160" PostTypeId="1" AcceptedAnswerId="164" CreationDate="2015-08-10T02:02:02.570" Score="13" ViewCount="95" Body="&lt;p&gt;Close up, rain can be modeled as transparent balls of water with appropriate motion blur. This seems impractical for large volumes, which would be necessary for scenes of rain in the distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At distances for which the human eye cannot resolve individual raindrops, how can the effect of rain-filled atmosphere be modeled? This does not need to be efficient enough for real time use, but it does need to allow multiple frames to be produced offline for animation without jarring discontinuities or flickering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What approaches are available for this?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="198" LastEditDate="2015-08-10T09:12:33.097" LastActivityDate="2015-08-10T09:12:33.097" Title="How can I model distant rain?" Tags="&lt;atmospherics&gt;&lt;offline&gt;&lt;animation&gt;&lt;fog&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="161" PostTypeId="1" AcceptedAnswerId="163" CreationDate="2015-08-10T06:26:07.000" Score="9" ViewCount="542" Body="&lt;p&gt;A lot of ShaderToy demos share the Ray Marching algorithm to render the scene, but they are often written with a very compact style and i can't find any straightforward examples or explanation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So what is Ray Marching? Some comments suggests that it is a variation of Sphere Tracing. What are the computational advantages of a such approach?&lt;/p&gt;&#xA;" OwnerUserId="316" LastActivityDate="2015-08-10T13:44:05.627" Title="What is Ray Marching? Is Sphere Tracing the same thing?" Tags="&lt;raytracing&gt;&lt;demoscene&gt;&lt;raymarching&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="162" PostTypeId="2" ParentId="161" CreationDate="2015-08-10T07:34:19.733" Score="7" Body="&lt;p&gt;Ray marching is an iterative ray intersection test in which you step along a ray and test for intersections, normally used to find intersections with &lt;em&gt;solid&lt;/em&gt; geometry, where inside/outside tests are fast.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JhSP2.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JhSP2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;br/&gt;&lt;sup&gt;Images from &lt;a href=&quot;http://artis.imag.fr/Publications/2006/BD06/relief05.pdf&quot;&gt;Rendering Geometry with Relief Textures&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A fixed step size is pretty common if you really have no idea where an intersection may occur, but sometimes root finding methods such as a binary or secant search are used instead. Often a fixed step size is used to find the first intersection, followed by a binary search. I first came across ray marching in per-pixel displacement mapping techniques. &lt;a href=&quot;http://www.inf.ufrgs.br/~oliveira/pubs_files/Policarpo_Oliveira_RTM_multilayer_I3D2006.pdf&quot;&gt;Relief Mapping of Non-Height-Field Surface Details&lt;/a&gt; is a good read!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/msLZp.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/msLZp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's commonly used with space leaping, an acceleration technique where some preprocessing gives a safety distance that you can move along the ray without intersecting geometry, or better yet, without intersecting and then leaving geometry so that you miss it. For example, cone step mapping, and relaxed cone step mapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strike&gt;Sphere tracing may refer to an implicit ray-sphere intersection test,&lt;/strike&gt; but it's also the name of a space leaping technique by John Hart, as @joojaa mentions, and used by William Donnelly (&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter08.html&quot;&gt;&lt;em&gt;Per-Pixel Displacement Mapping with Distance Functions&lt;/em&gt;&lt;/a&gt;), where a 3D texture encodes spheres radii in which no geometry exists.&lt;/p&gt;&#xA;" OwnerUserId="198" LastEditorUserId="198" LastEditDate="2015-08-10T07:41:52.513" LastActivityDate="2015-08-10T07:41:52.513" CommentCount="4" />
  <row Id="163" PostTypeId="2" ParentId="161" CreationDate="2015-08-10T07:35:06.200" Score="11" Body="&lt;h3&gt;TL;DR&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;They belong to the same family of solvers, where sphere tracing is one method of ray marching, which is the family name.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Raymarching a definition&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Raymarching is a technique a bit like traditional raytracing where the surface function is not easy to solve (or impossible without numeric iterative methods). In raytracing you just look up the ray intersection, whereas in ray marching you march forward (or back and forth) until you find the intersection, have enough samples or whatever it is your trying to solve. Try to think of it like a newton-raphson method for surface finding, or summing for integrating a varying function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can be useful if you:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Need to render volumetrics that arenot uniform&lt;/li&gt;&#xA;&lt;li&gt;Rendering implicit functions, fractals&lt;/li&gt;&#xA;&lt;li&gt;Rendering other kinds of parametric surfaces where intersection is not known ahead of time, like paralax mapping&lt;/li&gt;&#xA;&lt;li&gt;Etc&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/bc4D5.png&quot; alt=&quot;Traditional raymarching&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Traditional ray marching for surface finding&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Related posts:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://gamedev.stackexchange.com/questions/67719/how-do-raymarch-shaders-work&quot;&gt;how-do-raymarch-shaders-work&lt;/a&gt; (GameDev.SE)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Sphere tracing&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Sphere tracing is one possible Ray marching algorithm. Not all raymarching uses benefit form this method, as they can not be converted into this kind of scheme.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sphere tracing is used for rendering &lt;a href=&quot;https://en.wikipedia.org/wiki/Implicit_surface&quot;&gt;implicit surfaces&lt;/a&gt;. Implicit surfaces are formed at some level of a continuous function. In essence solving the equation&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;F(X,Y,Z) = 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Because of how this function can be solved at each point, one can go ahead and estimate the biggest possible sphere that can fit the current march step (or if not exactly reasonably safely). You then know that next march distance is at least this big. This way you can have adaptive ray marching steps speeding up the process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dFHFY.png&quot; alt=&quot;Sphere tracing has a adaptive step size&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Sphere tracing* in action note how the step size is adaptive&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more info see: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://graphics.cs.illinois.edu/sites/default/files/zeno.pdf&quot;&gt;Sphere tracing: a geometric method for the antialiased ray tracing of implicit surfaces&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;* Perhaps in 2d it's should be called circle tracing :)&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-10T13:44:05.627" LastActivityDate="2015-08-10T13:44:05.627" CommentCount="0" />
  <row Id="164" PostTypeId="2" ParentId="160" CreationDate="2015-08-10T08:09:19.900" Score="7" Body="&lt;p&gt;When you think about rain in the distance (generally), you will have rain at several different depths from the camera, some close up, some very far away, all of which will look slightly different as you won't be able to focus on all of them. But the effect of them all layering on top of each other as they go off into the distance is what helps give the look you're aiming for, you need to recreate that depth and layering to really give the look of rain further in the distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Particle emitters are the general method of creating rain, as usually they use very small meshes, or even just a texture sprite of a drop. This keeps them pretty efficient - to a degree as the more particles you use and have on-screen at a time will change how heavy or resource dependant the effect becomes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generally you would only use large detailed meshes for rain, when you have a close up of a puddle and wanted to animate the splash, or even a close up of the single drop for some reason.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can be combined with distance fog to further increase the sense of depth in your scene by occluding the objects in the far distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again you could use another particle emitter to create dark, heavy clouds that go off into the distance, as having clouds in the scene will help to sell the look.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A lot of this will depend on what software you're using, as well as its intended platform, as different software have different solutions but particles and fog are pretty commonplace.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;tldr: Particle Effects &amp;amp; Distance Fog&lt;/p&gt;&#xA;" OwnerUserId="118" LastActivityDate="2015-08-10T08:09:19.900" CommentCount="2" />
  <row Id="165" PostTypeId="1" CreationDate="2015-08-10T09:39:33.647" Score="8" ViewCount="157" Body="&lt;p&gt;I recently had an &lt;a href=&quot;http://superuser.com/q/930036/264276&quot;&gt;issue with sub-pixel anti-aliasing of text&lt;/a&gt;, which produces very harsh colours and made me wonder how it is meant to be done properly:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/SQWSI.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SQWSI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I've drawn some cases of a black tile covering thirds of pixels below.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/CE16y.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CE16y.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The colours match the ones I'm seeing, however when I look at properly antialiased text, the result is not nearly as bright and distracting:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4Vrfz.png&quot; alt=&quot;from www.lagom.nl&quot;&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I assume there has to be a balance between a good light intensity and the right colour. What methods are used for sub-pixel anti-aliasing that give such good results?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've rendered a white teapot to an image with 3x width and with multisampling. Below I compare averaging every 3 pixels with assigning each to RGB. The colours still seem overly bright in some cases (especially compared to the example above from &lt;a href=&quot;http://www.lagom.nl/lcd-test/subpixel.php&quot;&gt;here&lt;/a&gt;), not that my phone captures them well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/19ABb.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/19ABb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/RE5Jv.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RE5Jv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BZxjL.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BZxjL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/WT6ci.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/WT6ci.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qveN5m.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qveN5m.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/wPGG7m.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wPGG7m.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;sup&gt;OK, so my monitor needs a little dusting&lt;/sup&gt;&lt;/p&gt;&#xA;" OwnerUserId="198" LastEditorUserId="198" LastEditDate="2015-08-10T14:18:09.920" LastActivityDate="2015-08-10T14:18:09.920" Title="Sub-pixel antialiasing rules" Tags="&lt;antialiasing&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="1" />
  <row Id="166" PostTypeId="1" CreationDate="2015-08-10T10:05:56.743" Score="10" ViewCount="170" Body="&lt;p&gt;How to quickly draw a curved shape?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;by &quot;quickly&quot; I presume one should use hardware facilities as much as possible&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;by &quot;curved&quot; I mean boundaries defined by either quadratic or cubic Bezier curves&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;by &quot;shape&quot; I mean either a &quot;fat&quot; stroke (i.e. more than 1px wide) or even-odd/non-zero filled &quot;2D curved polygon&quot;, possibly with holes (i.e. letter &quot;O&quot;)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm asking because the options I know of have several drawbacks:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;triangulating the shape and sending it to OpenGL - does the most difficult work on CPU and might use too many/few triangles (i.e. wasteful/coarse)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;texture atlas - has to recompute/upload the texture on every change (shape, scale, rotation, ...)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Signed distance field - on large scales the details don't look pretty or has to recompute/upload the texture&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;NV_path_rendering - could be it, if it was not working only on Nvidia's cards&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;OpenVG - could be it, if it was not working only on mobile&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;* It seems to me that OpenVG is not exactly moving forward, to put it mildly. Does anyone know anything about its future prospects? Is it worth at all to keep an eye on in the present day?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;** OpenGL 4+ provides means of on-fly tessellation of polygons. Could it be somehow used to refine the the mesh from the &quot;triangulating&quot; option so that the shape boundary at least wont look &quot;angled&quot;?&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-11-15T12:17:42.913" Title="Hardware-accelerated drawing of curved shapes" Tags="&lt;drawing&gt;&lt;2d&gt;&lt;bezier-curve&gt;" AnswerCount="2" CommentCount="7" />
  <row Id="167" PostTypeId="2" ParentId="165" CreationDate="2015-08-10T10:38:50.383" Score="1" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;What methods are used for sub-pixel anti-aliasing that give such good results?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;You simply rasterize the letters as if they were 3x as wide.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The results differ in the way you rasterize the curves, most notably what kind of anti-aliasing/sampling scheme does it use and whether it makes use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Font_hinting&quot; rel=&quot;nofollow&quot;&gt;font hinting&lt;/a&gt;. For great overview see &lt;a href=&quot;http://www.antigrain.com/research/font_rasterization/&quot; rel=&quot;nofollow&quot;&gt;Texts Rasterization Exposures&lt;/a&gt; by Anti-Grain Geometry (IIRC this is what Chromium/PDFium uses) or &lt;a href=&quot;https://en.wikipedia.org/wiki/Subpixel_rendering&quot; rel=&quot;nofollow&quot;&gt;Subpixel rendering&lt;/a&gt; on Wikipedia.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What the attached image goes, I don't believe it has anything to do with sub-pixel rendering. It could be anything: wrong &lt;a href=&quot;https://en.wikipedia.org/wiki/Pixel_geometry&quot; rel=&quot;nofollow&quot;&gt;pixel geometry&lt;/a&gt;, wrong gamma, wrong color space conversion, ...&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-08-10T10:38:50.383" CommentCount="4" />
  <row Id="168" PostTypeId="2" ParentId="151" CreationDate="2015-08-10T12:07:14.027" Score="11" Body="&lt;p&gt;Assuming you mean a camera that rotates based on mouse movement:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way to implement it is to keep track of the camera position and its rotation in space. Spherical coordinates happen to be convenient for this, since you can represent the angles directly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9kXOr.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9kXOr.png&quot; alt=&quot;Spherical Coordinates Image&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float m_theta;&#xA;float m_phi;&#xA;float m_radius;&#xA;&#xA;float3 m_target;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The camera is located at &lt;em&gt;P&lt;/em&gt; which is defined by m_theta, m_phi, and m_radius. We can rotate and move freely wherever we want by changing those three values. However, we always look at, and rotate around, m_target. m_target is the local origin of the sphere. However, we are free to move this origin wherever we want in world space. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three main camera functions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void Rotate(float dTheta, float dPhi);&#xA;void Zoom(float distance);&#xA;void Pan(float dx, float dy);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In their simplest forms, Rotate() and Zoom() are trivial. The just modify m_theta, m_phi, and m_radius respectively:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void Camera::Rotate(float dTheta, float dPhi) {&#xA;    m_theta += dTheta;&#xA;    m_phi += dPhi;&#xA;}&#xA;&#xA;void Camera::Zoom(float distance) {&#xA;    m_radius -= distance;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Panning is a bit more complicated. A camera pan is defined as moving the camera to the left/right and/or up/down respective to the current camera view. The easiest way we can accomplish this is to convert our current camera view from spherical coordinates to cartesian coordinates. This will give us an &lt;em&gt;up&lt;/em&gt; and &lt;em&gt;right&lt;/em&gt; vectors.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void Camera::Pan(float dx, float dy) {&#xA;    float3 look = normalize(ToCartesian());&#xA;    float3 worldUp = float3(0.0f, 1.0f, 0.0f, 0.0f);&#xA;&#xA;    float3 right = cross(look, worldUp);&#xA;    float3 up = cross(look, right);&#xA;&#xA;    m_target = m_target + (right * dx) + (up * dy);&#xA;}&#xA;&#xA;inline float3 ToCartesian() {&#xA;    float x = m_radius * sinf(m_phi) * sinf(m_theta);&#xA;    float y = m_radius * cosf(m_phi);&#xA;    float z = m_radius * sinf(m_phi) * cosf(m_theta);&#xA;    float w = 1.0f;&#xA;&#xA;    return float3(x, y, z, w);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, first, we convert our spherical coordinate system to cartesian to get our &lt;em&gt;look&lt;/em&gt; vector. Next, we do the vector cross product with the world &lt;em&gt;up&lt;/em&gt; vector, in order to get a &lt;em&gt;right&lt;/em&gt; vector. This is a vector that points directly right of the camera view. Lastly, we do another vector cross product to get the camera &lt;em&gt;up&lt;/em&gt; vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To finish the pan, we move m_target along the &lt;em&gt;up&lt;/em&gt; and &lt;em&gt;right&lt;/em&gt; vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One question you might be asking is: Why convert between cartesian and spherical all the time (you will also have to convert in order to create the View matrix). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Good question. I too had this question and tried to exclusively use cartesian. You end up with problems with rotations. Since floating point operations are not exactly precise, multiple rotations end up accumulating errors, which corresponded to the camera slowly, and unintentionally rolling. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1OFpX.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1OFpX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in the end, I stuck with spherical coordinates. In order to counter the extra calculations, I ended up caching the view matrix, and only calculate it when the camera moves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The last step is to &lt;em&gt;use&lt;/em&gt; this Camera class. Just call the appropriate member function inside your app's MouseDown/Up/Scroll functions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void MouseDown(WPARAM buttonState, int x, int y) {&#xA;    m_mouseLastPos.x = x;&#xA;    m_mouseLastPos.y = y;&#xA;&#xA;    SetCapture(m_hwnd);&#xA;}&#xA;&#xA;void MouseUp(WPARAM buttonState, int x, int y) {&#xA;    ReleaseCapture();&#xA;}&#xA;&#xA;void MouseMove(WPARAM buttonState, int x, int y) {&#xA;    if ((buttonState &amp;amp; MK_LBUTTON) != 0) {&#xA;        if (GetKeyState(VK_MENU) &amp;amp; 0x8000) {&#xA;            // Calculate the new phi and theta based on mouse position relative to where the user clicked&#xA;            float dPhi = ((float)(m_mouseLastPos.y - y) / 300);&#xA;            float dTheta = ((float)(m_mouseLastPos.x - x) / 300);&#xA;&#xA;            m_camera.Rotate(-dTheta, dPhi);&#xA;        }&#xA;    } else if ((buttonState &amp;amp; MK_MBUTTON) != 0) {&#xA;        if (GetKeyState(VK_MENU) &amp;amp; 0x8000) {&#xA;            float dx = ((float)(m_mouseLastPos.x - x));&#xA;            float dy = ((float)(m_mouseLastPos.y - y));&#xA;&#xA;            m_camera.Pan(-dx * m_cameraPanFactor, dy * m_cameraPanFactor);&#xA;        }&#xA;    }&#xA;&#xA;    m_mouseLastPos.x = x;&#xA;    m_mouseLastPos.y = y;&#xA;}&#xA;&#xA;void MouseWheel(int zDelta) {&#xA;    // Make each wheel dedent correspond to a size based on the scene&#xA;    m_camera.Zoom((float)zDelta * m_cameraScrollFactor);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The m_camera*Factor variables are just scale factors that change how quickly your camera rotates/pans/scrolls&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code I have above is a simplified pseudo-code version of the camera system I made for a side project: &lt;a href=&quot;https://github.com/RichieSams/thehalflingproject/blob/master/source/scene/camera.h&quot;&gt;camera.h&lt;/a&gt; and &lt;a href=&quot;https://github.com/RichieSams/thehalflingproject/blob/master/source/scene/camera.cpp&quot;&gt;camera.cpp&lt;/a&gt;. The camera tries to imitate the Maya camera system. The code is free and open source, so feel free to use it in your own project. &lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-02-09T06:19:09.250" LastActivityDate="2016-02-09T06:19:09.250" CommentCount="2" />
  <row Id="169" PostTypeId="1" AcceptedAnswerId="194" CreationDate="2015-08-10T13:00:41.330" Score="8" ViewCount="67" Body="&lt;p&gt;Different screens can have different &lt;a href=&quot;https://en.wikipedia.org/wiki/Pixel_geometry&quot;&gt;pixel geometry&lt;/a&gt;, so that the red, green and blue components are arranged in different patterns. Using sub-pixel rendering to give a higher apparent resolution is only possible if the pixel geometry is known (what will give an improvement in clarity on one type of monitor will make things worse on another).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is particularly relevant if an application needs to run on both a desktop/laptop and a mobile screen, as different pixel geometry is quite common in mobile screens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to determine which geometry the screen uses at runtime, without having to ask the user? I'm interested in whether this is possible in general, but ideally I'd like to know whether this is possible when using JavaScript with WebGL.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-11T05:26:44.280" Title="Can I determine the pixel geometry programmatically?" Tags="&lt;webgl&gt;&lt;javascript&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="170" PostTypeId="1" CreationDate="2015-08-10T13:03:23.800" Score="5" ViewCount="296" Body="&lt;p&gt;It is no secret that according to the &lt;a href=&quot;http://developer.android.com/reference/android/opengl/package-summary.html&quot;&gt;official documentation&lt;/a&gt; extensions are not available under OpenGL ES 2.0. Nevertheless, the &lt;em&gt;glext.h&lt;/em&gt; file present in the NDK platform-include directories makes me think that extensions are indeed available. I know that working with OpenGL under NDK doesn't differ from working with standalone OpenGL. So, if I make something like a JNI bridge between my java engine interface and these extensions, I could use them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is: what architectural solution should I use if I want to use available OpenGL ES extensions on ES2.0 devices?&lt;/p&gt;&#xA;" OwnerUserId="333" LastEditorUserId="231" LastEditDate="2015-08-10T13:08:42.690" LastActivityDate="2015-08-10T13:15:04.140" Title="Using extensions in Android OpenGL ES 2.0" Tags="&lt;opengl&gt;&lt;android&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="171" PostTypeId="2" ParentId="170" CreationDate="2015-08-10T13:15:04.140" Score="3" Body="&lt;p&gt;I have encountered devices that advertise certain extensions via glGetString(GL_EXTENSIONS) and do not expose the entry points. I have also encountered the opposite, the extension was not in the extension strings but the entry points were there and it worked. It depends on whether the vendor has disabled any features in the driver from the GPU provider.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You will need to check both the extension strings and check if the entry point is not NULL via &lt;a href=&quot;https://www.khronos.org/registry/egl/sdk/docs/man/html/eglGetProcAddress.xhtml&quot; rel=&quot;nofollow&quot;&gt;eglGetProcAddress&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apart from that, each extension should conform to the &lt;a href=&quot;https://www.khronos.org/registry/gles/#headers2&quot; rel=&quot;nofollow&quot;&gt;Khronos registry of extensions&lt;/a&gt; so you can include gl2ext.h directly from there if you wanted.&lt;/p&gt;&#xA;" OwnerUserId="135" LastActivityDate="2015-08-10T13:15:04.140" CommentCount="0" />
  <row Id="173" PostTypeId="5" CreationDate="2015-08-10T13:21:38.380" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2015-08-10T13:21:38.380" LastActivityDate="2015-08-10T13:21:38.380" CommentCount="0" />
  <row Id="174" PostTypeId="4" CreationDate="2015-08-10T13:21:38.380" Score="0" Body="For questions involving use of the OpenGL graphics library." OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T09:07:02.583" LastActivityDate="2015-08-11T09:07:02.583" CommentCount="0" />
  <row Id="175" PostTypeId="1" CreationDate="2015-08-10T14:38:40.183" Score="3" ViewCount="57" Body="&lt;p&gt;If I were to analyze or change an &lt;a href=&quot;https://en.wikipedia.org/wiki/ICC_profile&quot; rel=&quot;nofollow&quot;&gt;ICC profile&lt;/a&gt; embedded in an image, how should I proceed?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd also be glad to understand how an &quot;end-user&quot; is able to create these with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tristimulus_colorimeter&quot; rel=&quot;nofollow&quot;&gt;tristimulus colorimeter&lt;/a&gt; or a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectrophotometer&quot; rel=&quot;nofollow&quot;&gt;spectrophotometer&lt;/a&gt; and if/how this new profile can be added to a color capturing device (i.e. a camera).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My curiosity comes from images that were taken by a professional photographer. When I open them in GIMP, there's a pop-up referring &quot;there is an embedded ICC profile&quot; and if I &quot;would like to convert the image to RGB (sRGB built-in)&quot;. I always convert all of them before using them, since the colors are slightly improved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the image as it was provided by the photographer:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Y4TQL.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y4TQL.jpg&quot; alt=&quot;No ICC&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And this is after the embedded ICC profile was applied:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/M1Dt0.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/M1Dt0.jpg&quot; alt=&quot;With ICC&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="157" LastEditorUserId="157" LastEditDate="2015-08-10T14:46:49.100" LastActivityDate="2015-08-16T18:15:33.757" Title="How is an ICC profile embedded or built into an image?" Tags="&lt;color&gt;&lt;color-management&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="176" PostTypeId="1" AcceptedAnswerId="193" CreationDate="2015-08-10T17:43:35.103" Score="6" ViewCount="97" Body="&lt;p&gt;While rendering my scene with OpenGL, I sometimes add an overlay which contains information, settings and a few draggable items. Currently, the overlay has a slightly transparent background to make text easily readable while still allowing the scene to shine through and to let the user see the scene behind/through the overlay.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to replace the transparent background with a translucent background, which (as far as I know) requires the clear image of the scene to be heavily blurred*. And especially on mobile devices (e.g. iPad) with high resolutions and limited processing power, lots of texture lookups and real-time rendering don't work well together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to make real-time translucency feasible on mobile devices like the iPad? Or is there a way to avoid the need to heavily blur the scene in every frame?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As suggested in the comments, here is an image I just found in the Wikipedia that describes the difference between transparency (right column) and translucency (middle column). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iKXOo.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iKXOo.png&quot; alt=&quot;Opacity, translucency and transparency&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Image source: &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Opacity_Translucency_Transparency.svg&quot;&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;*I know there is the two-pass blur (first blur in one and then in the other direction) to reduce texture lookups. But for translucency this still requires quite a large number of texture lookups.&lt;/sup&gt;&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-08-10T20:41:17.287" LastActivityDate="2015-08-11T05:07:01.950" Title="Real-time translucency effect" Tags="&lt;opengl&gt;&lt;real-time&gt;&lt;mobile&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="177" PostTypeId="1" AcceptedAnswerId="179" CreationDate="2015-08-10T19:59:58.817" Score="10" ViewCount="178" Body="&lt;p&gt;I can model ice cubes as slightly misshapen transparent cubes with the refractive index of water, but they don't look convincing. They look like lumps of glass rather than ice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at real ice cubes I can intuitively describe some differences but I don't know what physical properties to change to match them:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ice cubes are wet. Mine look like dry glass.&lt;/li&gt;&#xA;&lt;li&gt;Ice cubes are transparent in places and not in others.&lt;/li&gt;&#xA;&lt;li&gt;Ice cubes often have cracks that are visible despite not separating.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this instance I am trying to model ice cubes on a surface (in air, not floating in water).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What techniques do I need to include in order to increase the realism?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not looking for real time techniques, just to produce still images. I would like the ice to be photorealistic even close up, and to cast realistic caustics and shadows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried using curved edges and coating my ice cubes with a thin layer of transparent material to simulate a melted layer of water, but it doesn't seem to give the impression of being wet. I've also tried embedding a transparent sphere half the size of the cube at its centre, with a fog effect, but it doesn't blend into the cube naturally - it just looks embedded. Even a series of nested spheres with gradually increasing fog still doesn't look right.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T14:03:55.660" LastActivityDate="2015-08-11T23:20:59.580" Title="How can I make my ice cubes look real?" Tags="&lt;refraction&gt;&lt;transparency&gt;&lt;photo-realistic&gt;" AnswerCount="3" CommentCount="4" FavoriteCount="4" />
  <row Id="178" PostTypeId="2" ParentId="177" CreationDate="2015-08-10T20:16:23.327" Score="4" Body="&lt;p&gt;Two big ones you're missing:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fresnel_equations&quot; rel=&quot;nofollow&quot;&gt;Angle-dependent reflection&lt;/a&gt;.  This is one possible cause of your &quot;transparent in places and not in others&quot; effect, and the most likely cause of the missing wetness.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Ice cubes usually have air bubbles trapped inside.  This shows up as a white volumetric haze denser in the center of the cube (for small bubbles) or distinct bubbles (for large ones).  This is the other likely cause of your &quot;transparent in places and not in others&quot;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The techniques for modeling these depend on what rendering method you're using.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-10T20:16:23.327" CommentCount="0" />
  <row Id="179" PostTypeId="2" ParentId="177" CreationDate="2015-08-10T20:17:10.753" Score="3" Body="&lt;p&gt;According to &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_refractive_indices&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;, ice has a slightly lower IOR than non-frozen water, though I don't know how much that difference would affect the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &quot;opaque&quot;-looking parts of an ice cube are caused by clusters of microscopic bubbles formed during freezing. You might be able to model those using geometry, but given the scale and number I suspect that some kind of participating media model would probably be a better fit. (Though I don't know of any.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, remember that most non-submerged ice you'll see is going to very soon form a thin layer of liquid water interfacing it with the air, so this might significantly alter its appearance too.&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-10T20:17:10.753" CommentCount="0" />
  <row Id="180" PostTypeId="2" ParentId="176" CreationDate="2015-08-10T20:30:20.923" Score="4" Body="&lt;p&gt;There's no way around it. If you want the area behind the textboxes to appear blurry... you're gonna have to blur it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way to mitigate the performance cost is to be sure to use a 2-pass separable blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another measure that will probably help (depending on how much of the screen your textboxes cover) is to use scissoring or a stencil test to only compute the blur in the areas of the screen that will later be covered by textboxes. If you do this, be sure to leave an extra vertical padding of at least half the width of your blur kernel, otherwise, the second blurring pass will try to sample uncomputed texels in the intermediate texture, leading to artifacts or darkening of the blur near the edges of the textbox areas. (This isn't an issue for the first pass since you'll be sampling from the original unmasked texture there.)&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-10T20:30:20.923" CommentCount="0" />
  <row Id="181" PostTypeId="1" CreationDate="2015-08-10T20:40:17.313" Score="4" ViewCount="43" Body="&lt;p&gt;I'm currently preparing to implement variance shadow mapping based on &lt;a href=&quot;https://a248.e.akamai.net/f/1890/806/6/http.developer.nvidia.com/GPUGems3/gpugems3_ch08.html&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;. However, one point it makes concerns me:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Rendering only casters (and not receivers) into the variance shadow map is incorrect! For the interpolation to work properly, the receiving surface must be represented in the depth distribution. If it is not, shadow penumbrae will be improperly sized and fail to fade smoothly from light to dark.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This means that the naive method of rendering an object that does not cast a shadow, but can recieve shadows from other objects - simply skip rendering it into the shadow map - will not work. How else can this be accomplished?&lt;/p&gt;&#xA;" OwnerUserId="349" LastActivityDate="2015-08-11T04:49:54.333" Title="How to render an object that recieves shadows but does not cast them in a variance shadow mapping system?" Tags="&lt;opengl&gt;&lt;shadow-mapping&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="183" PostTypeId="1" AcceptedAnswerId="184" CreationDate="2015-08-10T21:39:52.303" Score="4" ViewCount="84" Body="&lt;p&gt;I want to be able to model an opaque liquid being dropped into a transparent liquid with sufficient velocity to cause turbulence and the resulting chaotic mixing. Assuming the two liquids are of the same density and viscosity, how could I model this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This doesn't need to be rendered in real time. I would like the turbulent flow to gradually settle down as the two fluids mix, but it will not be required to interact with other objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried modelling an expanding interface between the two fluids, and accelerating the vertices of the interface based on the local curvature, adding new vertices as they move apart. This seemed to work in the early stages, giving a constant exaggeration of small surface imperfections to create at least the appearance of turbulence, but it ran into problems very early which seemed to result from the interface self-intersecting. I've been trying to think of ways to redirect vertices that are near to causing a surface self-intersection, but I wonder if I would be better off starting from scratch with a more amenable method.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-11T14:45:32.540" LastActivityDate="2015-08-11T14:45:32.540" Title="How can I model ink dropped into still water?" Tags="&lt;simulation&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="184" PostTypeId="2" ParentId="183" CreationDate="2015-08-10T23:21:28.050" Score="10" Body="&lt;p&gt;Simulating drops of ink is probably one of the best applications of the &lt;a href=&quot;http://doc.utwente.nl/56247/1/K227____.pdf&quot;&gt;Vorton Method&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unlike fluid solvers based on the velocity form of the Navier-Stokes equations, which track the density and velocity of a fluid, the Vorton Method tracks vorticity instead. It does so by simulating a large number of vortons, each carrying a small amount of vorticity; you can think of a vorton as being a tiny particle that induces a small spinning vortex around itself. The sum of velocities caused by each vortex defines a velocity field in the fluid, which is used to in turn advect the vortons themselves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vorton methods are difficult to combine with boundary conditions and have a few efficiency caveats, but for fluid flow where intricate turbulance is the key feature and boundaries can be avoided, the Vorton method far outshines e.g. Eulerian grid solvers, which would need a large resolution to simulate turbulence at a comparable scale.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Michael Gourlay has a very accessible &lt;a href=&quot;http://www.mijagourlay.com/fluid&quot;&gt;article series&lt;/a&gt; on the vorton method. Although his articles are targeted at video games, they hold up similarly for off-line simulation. Here's also an &lt;a href=&quot;https://www.youtube.com/watch?v=LD6JfO41JIE&quot;&gt;example video&lt;/a&gt; of a drop of ink simulated using the vorton method.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-10T23:21:28.050" CommentCount="1" />
  <row Id="185" PostTypeId="1" AcceptedAnswerId="1471" CreationDate="2015-08-10T23:43:46.900" Score="15" ViewCount="239" Body="&lt;p&gt;Most descriptions of Monte Carlo rendering methods, such as path tracing or bidirectional path tracing, assume that samples are generated independently; that is, a standard random number generator is used that generates a stream of independent, uniformly distributed numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We know that samples that are not chosen independently can be beneficial in terms of noise. For example, stratified sampling and low-discrepancy sequences are two examples of correlated sampling schemes that almost always improve render times.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there are many cases where the impact of sample correlation is not as clear-cut. For example, Markov Chain Monte Carlo methods such as &lt;a href=&quot;https://graphics.stanford.edu/papers/metro/metro.pdf&quot;&gt;Metropolis Light Transport&lt;/a&gt; generate a stream of correlated samples using a Markov chain; &lt;a href=&quot;http://www.csie.ntu.edu.tw/~cyy/courses/rendering/13fall/lectures/handouts/ManyLight-I.pdf&quot;&gt;many-light methods&lt;/a&gt; reuse a small set of light paths for many camera paths, creating many correlated shadow connections; even &lt;a href=&quot;http://graphics.stanford.edu/~henrik/papers/ewr7/egwr96.pdf&quot;&gt;photon mapping&lt;/a&gt; gains its efficiency from reusing light paths across many pixels, also increasing sample correlation (although in a biased way).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of these rendering methods can prove beneficial in certain scenes, but seem to make things worse in others. It's not clear how to quantify the quality of error introduced by these techniques, other than rendering a scene with different rendering algorithms and eyeballing whether one looks better than the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is: How does sample correlation influence the variance and the convergence of a Monte Carlo estimator? Can we somehow mathematically quantify which kind of sample correlation is better than others? Are there any other considerations that could influence whether sample correlation is beneficial or detrimental (e.g. perceptual error, animation flicker)?&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-09-10T12:27:27.860" Title="How do correlated samples influence the behavior of a Monte Carlo renderer?" Tags="&lt;raytracing&gt;&lt;rendering&gt;&lt;monte-carlo&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="4" />
  <row Id="186" PostTypeId="1" AcceptedAnswerId="187" CreationDate="2015-08-11T00:37:44.757" Score="9" ViewCount="106" Body="&lt;p&gt;I'm reading a &lt;a href=&quot;https://books.google.com.br/books/about/Computer_graphics_for_Java_programmers.html?id=vY5RAAAAMAAJ&amp;amp;redir_esc=y&quot;&gt;book on computer graphics&lt;/a&gt;, and at some point, it shows a 3D model, created from a .dat file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are a few rules for the .dat file creation:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Made of a list of vertex positions / faces&lt;/li&gt;&#xA;&lt;li&gt;Written counter-clockwise&lt;/li&gt;&#xA;&lt;li&gt;Faces end with a dot (.)&lt;/li&gt;&#xA;&lt;li&gt;Fractions are also acceptable&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I want to know if this is the standard, and if not, what is the standard way of defining a 3D object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, if there is a specification, who defined it and where to find it?&lt;/p&gt;&#xA;" OwnerUserId="96" LastActivityDate="2015-08-11T07:10:16.647" Title="Is there a standard specification for 3D graphic representation?" Tags="&lt;3d&gt;" AnswerCount="4" CommentCount="0" FavoriteCount="1" />
  <row Id="187" PostTypeId="2" ParentId="186" CreationDate="2015-08-11T02:49:30.960" Score="8" Body="&lt;p&gt;When talking about file formats, we are talking about persisting some data related to a 3D model/geometry. There is no universal standard on file formats for persisting 3D geometry. There are only a few formats more dominant than others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just as it is with image file formats, the PNGs and JPEGs are the most common formats out there today, but there is no universal agreement between applications on using one or the other. Each app uses the best fit for its purposes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same happens with file formats storing 3D data. Each 3D modeling software will generally have one or a set of preferred formats. Most actually define custom formats that only work with the specific version of the tool. This can happen for many reasons, from simplifying the inner workings of the application, or making loading of files faster, to binding the user to a given tool on purpose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This &lt;code&gt;.dat&lt;/code&gt; format you describe is a custom format created by the authors of the book, which was probably designed with simplicity in mind. It seems to be a text file similar to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wavefront_.obj_file&quot;&gt;Wavefront OBJ&lt;/a&gt; format, which in turn is a very popular format for storing static geometry, though a bit outdated by now. One could say that the &lt;code&gt;.OBJ&lt;/code&gt; format is the &lt;code&gt;.BMP&lt;/code&gt; of 3D model formats.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other popular 3D model formats include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/COLLADA&quot;&gt;Collada&lt;/a&gt; format.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/.3ds&quot;&gt;3DS&lt;/a&gt; format.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/PLY_(file_format)&quot;&gt;PLY&lt;/a&gt; format.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;And many others. More general explanation &lt;a href=&quot;https://en.wikipedia.org/wiki/Polygon_mesh&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note that I said at the beginning that these are formats for storing/persisting a 3D model or geometry on an offline storage. It doesn't mean (and it usually isn't the case) that the applications that use them will store the data internally in memory using the same layout as the file. Normally, this kind of data will undergo a lot of processing after being loaded from a file until it is, for instance, displayed on the screen.&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="54" LastEditDate="2015-08-11T03:01:33.010" LastActivityDate="2015-08-11T03:01:33.010" CommentCount="1" />
  <row Id="188" PostTypeId="2" ParentId="186" CreationDate="2015-08-11T02:57:02.380" Score="3" Body="&lt;p&gt;There is not a single standard that defines the storage of 3D models. The graphics API only cares about triangles, which are defined by vertices and indices indicating which vertices form a triangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many companies have created their own model formats with different features and goals. One of the simplest is the .obj (Wavefront Object) format. Which contains lists of vertex positions, normals and texture coordinates and a list of indices that make up the triangles. It also contains materials, with some basic properties as textures, diffuse and specular colors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More advanced model formats, such as .fbx (FilmBox) used by Autodesk contain additional information required by the modeling software. These might include bones, hierarchies and even more vertex properties used for skinning etc. So there are &quot;standards&quot;, but they are not really unified as every program might require different information. But wavefront is a very good choice, because many modelling applications can export to it and it is easy to write (find) an importer for.&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2015-08-11T02:57:02.380" CommentCount="0" />
  <row Id="189" PostTypeId="2" ParentId="186" CreationDate="2015-08-11T03:00:43.867" Score="5" Body="&lt;p&gt;There is no standard format for 3D models. Some common ones are listed in glampert's answer, more can be found &lt;a href=&quot;http://stackoverflow.com/questions/13600158/recommended-file-formats-and-graphics-libraries-for-importing-3d-model-into-open&quot;&gt;in this SE answer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are designing your own format for your own purposes, (a fairly common thing to do, actually) it's a good first step to look at how you are using your model data. For example, if you are focused on rendering, you can use the format that the graphics library expects. If you want to reduce the load time of your software as much as possible then reducing the amount of parsing and processing you have to do can help. A simple utility that can output a file in the format that your engine expects will mean you can simply map the file into memory and then pass a pointer into your graphics library.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, if rendering is not your main concern and editing is more important to you, you might consider a format more amenable to editing, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Winged_edge&quot;&gt;winged edge&lt;/a&gt;. Using a general purpose model format may suit your purposes at the beginning of a project, but ultimately there are many things you can do with 3D models and each format has its own benefits and drawbacks, so you can choose the one that works best for you.&lt;/p&gt;&#xA;" OwnerUserId="125" LastActivityDate="2015-08-11T03:00:43.867" CommentCount="0" />
  <row Id="190" PostTypeId="2" ParentId="181" CreationDate="2015-08-11T04:49:54.333" Score="1" Body="&lt;p&gt;Not rendering a shadow caster could work but your mileage can vary. When variance is &quot;large&quot; a VSM is not able to exactly localize the occluders, which causes light leaks. Not rendering a large receiver (let say the floor of a room, a road, etc..) can dramatically increase the variance of the depth samples within a region of the shadow map that include the missing receiver(s) (e.g. the receiver depth is replaced by something much more distant, like the far plane).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, if you skip rendering in the VSM some little object that is unlikely to ever be the most distant receiver in the scene, then you have some hope that things will work out (compatibly with the limitations of variance shadow maps). If you can afford the cost I suggest you to use something more robust like exponential variance shadow maps (see this work: &lt;a href=&quot;https://software.intel.com/en-us/articles/sample-distribution-shadow-maps&quot; rel=&quot;nofollow&quot;&gt;https://software.intel.com/en-us/articles/sample-distribution-shadow-maps&lt;/a&gt;).&lt;/p&gt;&#xA;" OwnerUserId="355" LastActivityDate="2015-08-11T04:49:54.333" CommentCount="0" />
  <row Id="191" PostTypeId="5" CreationDate="2015-08-11T04:50:21.863" Score="0" Body="&lt;p&gt;See also:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_tracing_%28graphics%29&quot; rel=&quot;nofollow&quot;&gt;Ray tracing on Wikipedia&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/tagged/raytracing&quot;&gt;Raytracing implementation questions on Stack Overflow&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-19T08:50:06.257" LastActivityDate="2015-11-19T08:50:06.257" CommentCount="0" />
  <row Id="192" PostTypeId="4" CreationDate="2015-08-11T04:50:21.863" Score="0" Body="Questions specific to raytracing (as opposed to scanline rendering), the 3D graphics technique of intersecting rays from the camera with objects in the scene." OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-16T15:47:42.347" LastActivityDate="2015-11-16T15:47:42.347" CommentCount="0" />
  <row Id="193" PostTypeId="2" ParentId="176" CreationDate="2015-08-11T05:07:01.950" Score="4" Body="&lt;p&gt;I think you may want to take another look at the iOS user interface if you consider real-time blurs to be out of range of mobile hardware:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dRT54.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dRT54.png&quot; alt=&quot;iOS control center&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Dt2Sg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dt2Sg.png&quot; alt=&quot;iOS notification center&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blurs are totally in range of mobile hardware. Yes, you need a fairly large number of texture samples for a blur with a large kernel, but the texture samples are also cached very well, and you can use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Separable_filter&quot; rel=&quot;nofollow&quot;&gt;separable blur&lt;/a&gt;, which effectively means that an MxN blur requires M + N samples, not M * N samples. You can also take advantage of hardware bilinear sampling to &lt;a href=&quot;http://rastergrid.com/blog/2010/09/efficient-gaussian-blur-with-linear-sampling/&quot; rel=&quot;nofollow&quot;&gt;reduce the number of samples&lt;/a&gt; that your blur requires.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-11T05:07:01.950" CommentCount="2" />
  <row Id="194" PostTypeId="2" ParentId="169" CreationDate="2015-08-11T05:26:44.280" Score="1" Body="&lt;p&gt;It appears that Microsoft has punted on this in Windows 7:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/KbzW2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KbzW2.png&quot; alt=&quot;ClearType Text Tuner&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the method available in the control panel for selecting what layout ClearType uses.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, it seems that iOS and the Windows modern UI style de-emphasize subpixel antialiasing heavily, due to the prevalence of animations and screen rotations. As a result I expect the OS vendors to not spend a lot of effort trying to figure out the subpixel layout of every screen.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-11T05:26:44.280" CommentCount="0" />
  <row Id="195" PostTypeId="2" ParentId="89" CreationDate="2015-08-11T05:45:18.720" Score="8" Body="&lt;p&gt;The most commonly suggested method seems to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Mueller_calculus&quot;&gt;Mueller calculus&lt;/a&gt;, which boils down to tracking the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stokes_parameters&quot;&gt;Stokes parameters&lt;/a&gt; of a light ray to represent the polarization of light transmitted along that ray. A ray might be unpolarized—Stokes parameters of (1, 0, 0, 0)—or it may be circularly or linearly polarized in various directions, which is a property of light in the aggregate. At the surface light is scattered according to the polarization and the Stokes vector is propagated by multiplying it by the Mueller matrix of the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a &lt;a href=&quot;http://cs.au.dk/~toshiya/mcp.pdf&quot;&gt;writeup by Toshiya Hachisuka&lt;/a&gt; about ray tracing while tracking light polarization. It seems like a good introduction, and there are several references that seem promising. The article argues for &lt;em&gt;direct tracking&lt;/em&gt; of the polarization state of the ray: instead of an aggregate representation, individually tracking the direction and frequency of the two harmonic oscillations of a given light ray. This may have the disadvantage that you need more samples to reproduce the polarization effects accurately, but it may be able to reproduce more effects (in the article, thin-film interference).&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-11T05:45:18.720" CommentCount="0" />
  <row Id="196" PostTypeId="2" ParentId="177" CreationDate="2015-08-11T05:57:28.187" Score="4" Body="&lt;p&gt;I've found that bump mapping when calculating lighting and refraction rays can add a lot to the look of ice. It makes the ice look textured and imperfect, like a melting ice cube would look.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I sort of wonder if maybe animating a bump map could help make it look wet, as water sheets / droplets ran down it's surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The images below look pretty nice, but they would probably look even better with the internal imperfections that other people are talking about.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/t5t5K.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/t5t5K.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4NJbX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4NJbX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sPo4R.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sPo4R.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's my shadertoy where I got the screenshots from: &lt;a href=&quot;https://www.shadertoy.com/view/ldj3zz#&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/ldj3zz#&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-11T23:20:59.580" LastActivityDate="2015-08-11T23:20:59.580" CommentCount="7" />
  <row Id="197" PostTypeId="5" CreationDate="2015-08-11T05:57:44.960" Score="0" Body="&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenGL_Shading_Language&quot; rel=&quot;nofollow&quot;&gt;OpenGL shading language&lt;/a&gt; is a high-level language that can be used to write &lt;a href=&quot;https://en.wikipedia.org/wiki/Shader&quot; rel=&quot;nofollow&quot;&gt;shader programs&lt;/a&gt; that can execute on a GPU.&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2015-08-11T09:06:48.117" LastActivityDate="2015-08-11T09:06:48.117" CommentCount="0" />
  <row Id="198" PostTypeId="4" CreationDate="2015-08-11T05:57:44.960" Score="0" Body="GLSL is the OpenGL shading language. Use this tag for questions which are specifically about shaders written in this language. For generic shader questions use [shader] instead." OwnerUserId="196" LastEditorUserId="16" LastEditDate="2015-08-16T12:49:01.910" LastActivityDate="2015-08-16T12:49:01.910" CommentCount="0" />
  <row Id="199" PostTypeId="2" ParentId="183" CreationDate="2015-08-11T06:25:24.990" Score="0" Body="&lt;p&gt;As an alternative you could also check Jon Stam's paper &lt;a href=&quot;http://dx.doi.org/10.1145/311535.311548&quot; rel=&quot;nofollow&quot;&gt;&quot;Stable Fluids&quot;&lt;/a&gt;, that method is usually called the semi-Lagrangian approach to fluid simulations. You can find several implementations of it. You said that you don't need it to be real time, so this might do the work.&lt;/p&gt;&#xA;" OwnerUserId="168" LastActivityDate="2015-08-11T06:25:24.990" CommentCount="1" />
  <row Id="200" PostTypeId="2" ParentId="186" CreationDate="2015-08-11T07:10:16.647" Score="2" Body="&lt;p&gt;To add to other answers, there are actually two kinds of formats, intermediate and runtime. Intermediate formats (example: COLLADA) are more verbose/slower and are converted between tools ultimately to the custom runtime format that is optimized for a specific application/hardware. Simple applications that do not care much about performance can directly use intermediate formats.&lt;/p&gt;&#xA;" OwnerUserId="67" LastActivityDate="2015-08-11T07:10:16.647" CommentCount="0" />
  <row Id="201" PostTypeId="1" AcceptedAnswerId="206" CreationDate="2015-08-11T07:12:44.110" Score="6" ViewCount="35" Body="&lt;p&gt;I'm working on a program to use randomly-selected images as a desktop background.  Now, not all images are the same shape as a computer monitor.  One idea I had for dealing with this is to set a background color that is perceptually similar to the image.  How would I go about finding this color?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple RGB averaging doesn't take into account nonlinearities such as monitor gamma, and doesn't deal with the human vision system.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-11T15:45:12.683" Title="Finding a color perceptually similar to an image" Tags="&lt;color&gt;&lt;human-vision&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="202" PostTypeId="2" ParentId="130" CreationDate="2015-08-11T07:53:30.297" Score="3" Body="&lt;p&gt;Problem fixed by RichieSams, trichoplax and xpicox. Thanks all of you for the answers. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I lower the roughness, change the color of material and reversed the ViewDirection then finally I start to see proper specular :).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fixed Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 330 core&#xA;&#xA;in vec3 Position;&#xA;in vec2 TexCoord0;&#xA;in vec3 Normal;&#xA;in vec3 Tangent;&#xA;out vec4 FinalColor;&#xA;&#xA;uniform vec3 uCameraPosition;&#xA;&#xA;#define PI 3.1415926f&#xA;#define EPSILON 10e-5f&#xA;#define saturate(value) clamp(value, 0.0f, 1.0f);&#xA;&#xA;float BRDF_Lambert(float NdotL)&#xA;{&#xA;    return NdotL;&#xA;}&#xA;&#xA;// Got these BRDF formulas Moving Frostbite to PBR slide by Sebastien Lagarde &amp;amp; Charles de Rousiers &#xA;// http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr_v2.pdf&#xA;float BRDF_D_GGX(float NdotH, float Roughness)&#xA;{&#xA;    float Roughness2 = Roughness * Roughness;&#xA;    float f = (NdotH * Roughness2 - NdotH) * NdotH + 1.0f;&#xA;    return Roughness2 / (f * f + EPSILON);&#xA;}&#xA;&#xA;&#xA;float BRDF_F_FresnelSchlick(float LdotH, float F0)&#xA;{&#xA;    float f = F0 + (1.0f - F0) * pow((1.0f - LdotH), 5);&#xA;    return f;&#xA;}&#xA;&#xA;float BRDF_G_SmithGGXCorrelated(float NdotL, float NdotV, float Roughness)&#xA;{&#xA;    float Roughness2 = Roughness * Roughness;&#xA;    float GV = NdotL * sqrt((-NdotV * Roughness2 + NdotV) * NdotV + Roughness2);&#xA;    float GL = NdotV * sqrt((-NdotL * Roughness2 + NdotL) * NdotL + Roughness2);&#xA;&#xA;    return 0.5f / (GV + GL + EPSILON);&#xA;}&#xA;&#xA;float BRDF_Specular(float NdotV, float NdotL, float NdotH, float LdotH, float Roughness, float F0)&#xA;{&#xA;    float D = BRDF_D_GGX(NdotH, Roughness);&#xA;    float F = BRDF_F_FresnelSchlick(LdotH, F0);&#xA;    float G = BRDF_G_SmithGGXCorrelated(NdotL, NdotV, Roughness);&#xA;    return (D * F * G) / PI;&#xA;}&#xA;&#xA;void main()&#xA;{&#xA;    vec3 normal = normalize(Normal);&#xA;&#xA;    vec4 BaseColor = vec4(1.0f, 0.0f, 0.0f, 1.0f);&#xA;    vec4 SpecularColor = vec4(1.0f, 1.0f, 1.0f, 1.0f);&#xA;&#xA;    vec3 LightDirection = normalize(vec3(0, 4, 4) - Position);&#xA;    vec3 ViewDirection = normalize(uCameraPosition - Position);&#xA;    vec3 HalfVector = normalize(ViewDirection + LightDirection);&#xA;    float Roughness = 0.04f;&#xA;&#xA;    float RefractiveIndex = 0.24f; // RI for Gold materials. I got this from http://refractiveindex.info/&#xA;    float F0 = pow(((1.0f - RefractiveIndex) / (1.0f + RefractiveIndex)), 2);&#xA;&#xA;    float NdotL = saturate(dot(LightDirection, normal));&#xA;    float NdotV = abs(dot(ViewDirection, normal)) + EPSILON; // Avoid artifact - Ref: SIGGRAPH14 - Moving Frosbite to PBR&#xA;    float LdotH = saturate(dot(LightDirection, HalfVector));&#xA;    float NdotH = saturate(dot(normal, HalfVector));&#xA;&#xA;    float DiffuseFactor = BRDF_Lambert(NdotL);&#xA;    float SpecularFactor = 0.0f;&#xA;    if(DiffuseFactor &amp;gt; 0.0f)&#xA;    {&#xA;        SpecularFactor = BRDF_Specular(NdotV, NdotL, NdotH, LdotH, Roughness, F0);&#xA;    }&#xA;    FinalColor = BaseColor * DiffuseFactor + SpecularColor * SpecularFactor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Final Image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/j7Ft6.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/j7Ft6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="83" LastActivityDate="2015-08-11T07:53:30.297" CommentCount="0" />
  <row Id="203" PostTypeId="1" CreationDate="2015-08-11T11:24:41.203" Score="23" ViewCount="498" Body="&lt;p&gt;In graphics we use RGB and other color spaces as an approximation to the full spectrum of light wavelengths. This evidently works pretty well in general, but are there any reasonably common objects/materials/phenomena, things you might encounter in your everyday life, whose appearance isn't represented well by RGB rendering due to having a complex emission/reflection/absorption spectrum?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While the current answers are focusing mainly on colors outside a given RGB gamut, I'm also interested in hearing if there are examples where, for instance, the color of an object appears &quot;wrong&quot; when rendered in RGB because of an interaction between the light source spectrum and the object's reflection spectrum. In other words, a case where a spectral renderer would give you more correct results.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Credit: I liked this question in the previous private beta so I'm reproducing it here. It was originally asked by &lt;a href=&quot;http://computergraphics.stackexchange.com/users/48/nathan-reed&quot;&gt;Nathan Reed&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-30T20:19:56.160" Title="Are there common materials that aren't represented well by RGB?" Tags="&lt;color&gt;" AnswerCount="4" CommentCount="4" FavoriteCount="5" />
  <row Id="204" PostTypeId="2" ParentId="203" CreationDate="2015-08-11T13:05:13.567" Score="11" Body="&lt;p&gt;I believe the most prominent spectral effect that can't be faithfully reproduced with RGB is &lt;a href=&quot;https://en.wikipedia.org/wiki/Dispersion_(optics)&quot;&gt;dispersion&lt;/a&gt;, caused by dielectrics with spectrally varying index of refraction (usually modelled with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Sellmeier_equation&quot;&gt;Sellmeier equation&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other spectral phenomena are usually caused by wave effects. One example that is encountered in real life every now and then is &lt;a href=&quot;https://en.wikipedia.org/wiki/Thin-film_interference&quot;&gt;thin-film interference&lt;/a&gt;, which is caused by one or more reflective surfaces layered closely on top of each other (e.g. oil slicks, soap bubbles). Another wave effect that can sometimes be observed is &lt;a href=&quot;https://en.wikipedia.org/wiki/Diffraction&quot;&gt;diffraction&lt;/a&gt;, caused e.g. by &lt;a href=&quot;https://en.wikipedia.org/wiki/Diffraction_grating&quot;&gt;diffraction gratings&lt;/a&gt;, which is what causes the funky appearance of CDs.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-11T13:05:13.567" CommentCount="0" />
  <row Id="206" PostTypeId="2" ParentId="201" CreationDate="2015-08-11T15:45:12.683" Score="3" Body="&lt;p&gt;You could try some form of color quantization algorithm, which generally extract the most dominant N colors. The one I've seen referenced most is &lt;a href=&quot;http://www.leptonica.com/papers/mediancut.pdf&quot; rel=&quot;nofollow&quot;&gt;modified median cut quantization&lt;/a&gt; [.pdf], which is based on &lt;a href=&quot;http://dcis.uohyd.ernet.in/~mravi/downloads/CIP/OLD-CIP/Median-cut.doc&quot; rel=&quot;nofollow&quot;&gt;median cut quantization&lt;/a&gt; [.doc]. The benefit to this kind of algorithm is that instead of simply averaging every color in the image, it extracts and discards other highly prominent colors instead of allowing them to pollute the average.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The concept is that color space (RGB space) is partitioned into 3D axis-aligned rectangular regions (the paper calls them vboxes) and iteratively subdivided by splitting vboxes, attempting to leave half of the pixels on each side of the split. The result is color clusters that should correspond to color clusters in the image. The largest color has a strong likelihood of being &quot;perceptually similar&quot; to the image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's a &lt;a href=&quot;http://lokeshdhakar.com/projects/color-thief/&quot; rel=&quot;nofollow&quot;&gt;JavaScript implementation and demo of this algorithm called Color Thief&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-11T15:45:12.683" CommentCount="0" />
  <row Id="207" PostTypeId="2" ParentId="203" CreationDate="2015-08-11T15:46:12.543" Score="6" Body="&lt;p&gt;RGB works because that's how our sensory apparatus works. Ina addition to dispersion, some man made materials and insect bodies sometimes have surfaces that have very tight color bands. These might benefit from a wide spectrum rendering. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However since many of these  effects are pretty localized, you can often get away with making the shader just work weird. This does not work right in reflections and refractions but nobody is likely to notice. Unless you are doing some physics simulation its not really a big deal. But if you design optics this might be a big deal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some materials, like snow, also convert incoming ultraviolet into visible light. Again this kind of effect can usually be handled by shaders/ special light groups.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Butterfly wings are also a curiosity as they manipulate the waves phases and the forms of the incoming light. So if you want to do physics simulation on those then its a big deal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Polarisation of light is also a big factor in insects and water effects.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-08-11T15:46:12.543" CommentCount="0" />
  <row Id="208" PostTypeId="2" ParentId="121" CreationDate="2015-08-11T16:46:35.160" Score="1" Body="&lt;p&gt;The creme de la creme of single-pass no (or few) compromises transparency in OpenGL is an A-buffer. With modern OpenGL, it is possible to implement:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://blog.icare3d.org/2010/06/fast-and-accurate-single-pass-buffer.html&quot; rel=&quot;nofollow&quot;&gt;http://blog.icare3d.org/2010/06/fast-and-accurate-single-pass-buffer.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It avoids the multiple passes of depth peeling and doesn't require onerous sorting.&lt;/p&gt;&#xA;" OwnerUserId="363" LastActivityDate="2015-08-11T16:46:35.160" CommentCount="1" />
  <row Id="209" PostTypeId="1" AcceptedAnswerId="398" CreationDate="2015-08-11T21:54:24.663" Score="8" ViewCount="510" Body="&lt;p&gt;Bresenham's line algorithm is a way of drawing straight lines using only fast integer operations (addition, subtraction, and multiplication by 2).  However, it generates aliased lines.  Is there a similarly fast way to draw antialiased lines?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-09-01T16:16:10.817" Title="Fast antialiased line drawing" Tags="&lt;antialiasing&gt;&lt;pixel-graphics&gt;&lt;line-drawing&gt;" AnswerCount="1" CommentCount="9" FavoriteCount="1" />
  <row Id="210" PostTypeId="1" AcceptedAnswerId="211" CreationDate="2015-08-11T23:25:16.290" Score="6" ViewCount="137" Body="&lt;p&gt;I'm trying to implement ambient occlusion in Python 3 and I'm seeing shadows beneath my reflective spheres but they seem very faint. I'm not sure if that means I've missed something, or if I just have a false impression of how much shadow results from ambient occlusion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The result looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/etDFe.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/etDFe.png&quot; alt=&quot;4 reflective spheres under a uniform white sky&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is modeled as spheres (rather than triangle meshes). The 4 yellowish mirror spheres are hovering just above a very large sphere to approximate a plane, and the whole scene is surrounded by a very large white emissive sphere that provides the ambient sky light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each pixel sample rays are chosen with a Gaussian distribution around the pixel centre and more samples are chosen until the variance is sufficiently low. Rays reflect in the single specular reflection direction from the mirror spheres, and in any direction from the hemisphere of possible directions when hitting the floor. All rays eventually hit the white sky sphere and the colour is determined based on the losses along the way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there some vital step I'm overlooking?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-10-08T21:30:05.937" LastActivityDate="2015-10-09T01:44:16.853" Title="Am I missing something for ambient occlusion?" Tags="&lt;raytracing&gt;&lt;ambient-occlusion&gt;&lt;shadow&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="211" PostTypeId="2" ParentId="210" CreationDate="2015-08-11T23:41:33.870" Score="5" Body="&lt;p&gt;It's a bit hard to tell from your image, but it does look a bit faint. When debugging these kinds of things, it's always useful to strip down your scene as much as possible to remove any unnecessary complexity from the picture. In your case, try only creating a single diffuse sphere that touches the ground in one point. Give the ground and the sphere an albedo of 1. If ambient occlusion is implemented correctly, the point where the sphere touches the ground should have a pixel value of 0; the further away from the sphere you get, the closer the pixel values should go to 1 (not above - make sure nothing gets clamped when you output the image). Make sure to gamma correct your images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The one thing that sticks out from your description is &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Rays reflect [...] in any direction from the hemisphere of possible directions when hitting the floor&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;That by itself is fine, but you need to make sure to multiply the rays by the Lambertian BRDF (i.e. dot(normal, ray)/Pi). Even better is to directly sample from a cosine hemisphere, in which case all factors cancel out. &lt;a href=&quot;http://www.rorydriscoll.com/2009/01/07/better-sampling/&quot;&gt;This blog post&lt;/a&gt; has all the important info in one place.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-08-11T23:41:33.870" CommentCount="3" />
  <row Id="212" PostTypeId="1" AcceptedAnswerId="220" CreationDate="2015-08-12T00:31:52.567" Score="4" ViewCount="128" Body="&lt;p&gt;When computing ray-object intersections against a transformed object, most raytracers apply the inverse transform to each ray and compute the intersection against a non-transformed object.  Wouldn't applying the forward transform, putting the object into a flat world-space, be faster and more efficient, because you only need to compute the transform once?&lt;/p&gt;&#xA;" OwnerUserId="158" LastEditorUserId="6" LastEditDate="2015-08-12T09:31:56.920" LastActivityDate="2015-08-12T14:27:25.487" Title="Why are inverse transformations applied to rays rather than forward transformations to objects?" Tags="&lt;raytracing&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="213" PostTypeId="2" ParentId="212" CreationDate="2015-08-12T00:44:23.180" Score="4" Body="&lt;p&gt;Most ray intersection algorithms can be greatly simplified if you can assume the shape you're intersecting is at the origin, unrotated and with unit size. There are a few exceptions (Like spheres, which have rotational symmetry.) but the more general versions of the algorithms mathematically usually end up basically translating both the shape and ray to the origin before calculating the intersection. With that in mind it's simpler and more consistent to simply transform rays so that the intersection always happens in object-space.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-12T00:54:41.007" LastActivityDate="2015-08-12T00:54:41.007" CommentCount="2" />
  <row Id="214" PostTypeId="1" CreationDate="2015-08-12T00:46:42.240" Score="5" ViewCount="123" Body="&lt;p&gt;&lt;em&gt;I'm trying to learn about raytracing by implementing things in Python 3. I know this is always going to be slower than something like C++, and I know the speed could also be improved by using GPU raytracing, but for this question I'm not looking for general ways of speeding up, but specifically ways of reducing the number of samples required, which will then be useful for any language I may work with in future.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I have a partly formed idea which I'd like to work on, but first I wanted to run this past the experts to see if this is a pre-existing technique so I don't repeat work that has already been done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've searched for &quot;sampling solid angle&quot; and &quot;voronoi sphere sampling&quot; but I can't see any sign of prior work. I'll describe my idea in case it goes by a name I can't think of.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example image&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/FzJUi.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/FzJUi.jpg&quot; alt=&quot;3 spheres on a plane&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is an image of three spheres on a plane (which is actually a very large sphere). One is emissive, one is reflective, and one is matt (as is the floor). Sampling is adaptive so that pixels that quickly reach a stable colour do not take up much time. I limit the total number of samples per pixel to avoid the rendering continuing for too long. Even allowed to run overnight, the resulting image is very grainy, and experimenting with smaller images suggests this size image (1366 by 768) would take weeks to converge with my current approach.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;My idea: concentrating samples along colour boundaries&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I'd like to be able to concentrate samples where they are needed, and to do this adaptively based on previous samples for the same intersection point or pixel. This will give an unknown bias in the distribution of samples, which means taking the average of the samples will give an inaccurate result. Instead I would like to consider the size of the voronoi cell on the surface of the unit hemisphere centred on the point of intersection (for sampling light incident at a point on a matt surface) or on the surface of a small circle (for sampling around a pixel).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assume that all points within that voronoi cell are receiving rays of the same colour as the centre of the voronoi cell. Now an estimate of the average colour can be obtained by weighting according to the area of each voronoi cell. Choosing new samples on the boundary between the two voronoi cells with the greatest difference in colour leads to an improvement in the estimate without needing to sample the entire hemisphere. Samples should end up more densely concentrated in areas of higher colour gradient. Areas of flat colour should end up being ignored once they have a few points near their boundary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The extra complication is that in both cases (sampling from a point on a matt surface, or sampling over a circle around a pixel centre) the simplified approach I have described is roughly equivalent to a large number of samples distributed uniformly. To make this work I would need to be able to bias the average by both the voronoi cell areas &lt;em&gt;and&lt;/em&gt; the required distribution (cosine around the surface normal for a matt surface or gaussian around a pixel centre).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I have some more thinking to do before I could test this idea, but I wanted to check first if this has either already been done, or already ruled out as unworkable.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-09-02T01:54:45.520" Title="Speeding up convergence: am I reinventing the wheel?" Tags="&lt;raytracing&gt;&lt;sampling&gt;&lt;efficiency&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="215" PostTypeId="1" CreationDate="2015-08-12T01:30:57.970" Score="3" ViewCount="92" Body="&lt;p&gt;(Note: This has been cross posted from my &lt;a href=&quot;http://ompf2.com/viewtopic.php?f=15&amp;amp;t=1977&quot; rel=&quot;nofollow&quot;&gt;ompf2 post&lt;/a&gt;.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Recently I've implemented Multiple Importance Sampling for the sampling of surfaces in my ray tracer. I do this by, on each intersection, sampling both a random direction from the BRDF and also sampling a random light, then combining both using the power heuristic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I have only a single light, the results are beautiful. Whereas previously I had this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/rdpdig3.png&quot; alt=&quot;BRDF sampling only&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I have this, with the same amount (1024 per pixel) of samples:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/kgCgBrA.png&quot; alt=&quot;BRDF+Light sampling&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the algorithm seems to work well. The problem is when I add a second light source to the scene, the noise comes back! This is a render with a second light source added:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zVdm7.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zVdm7.png&quot; alt=&quot;BRDF+Light with 2 lights&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am probably using the technique incorrectly. Perhaps I need to sample all light sources at once, instead of randomly picking one like I'm doing now? That doesn't seem like it would scale well as the number of lights go up, however. The implementation was loosely based on pbrt's DirectLightingIntegrator, and I suspect the reason why it doesn't work properly is that I also have multi-bounce indirect lighting. This is my core integration function:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec3 calc_light_incidence(const Scene&amp;amp; scene, Rng&amp;amp; rng, const Ray&amp;amp; ray, int depth) {&#xA;    float ray_weight = 1.0f;&#xA;&#xA;    depth += 1;&#xA;    if (depth &amp;gt; 2) {&#xA;        const float live_probability = 0.75f;&#xA;&#xA;        if (rng.canonical() &amp;gt; live_probability) {&#xA;            return vec3_0;&#xA;        } else {&#xA;            ray_weight = 1.0f / live_probability;&#xA;        }&#xA;    }&#xA;&#xA;    vec3 color = vec3_0;&#xA;&#xA;    const Optional&amp;lt;Intersection&amp;gt; surface_hit = find_nearest_intersection(scene, ray);&#xA;    if (surface_hit) {&#xA;        const vec3 out_dir = -normalized(ray.direction);&#xA;&#xA;        const size_t light_index = (size_t)(rng.canonical() * scene.lights.size());&#xA;        const SceneObject* light = &amp;amp;scene.objects[scene.lights[light_index]];&#xA;        const float light_weight = (float)scene.lights.size();&#xA;&#xA;        // Sample BRDF&#xA;        {&#xA;            const vec3 in_dir = cosine_weighted_point_on_hemisphere(rng.canonical(), rng.canonical(), surface_hit-&amp;gt;normal);&#xA;            const vec3 reflectance = surface_hit-&amp;gt;object-&amp;gt;material.diffuse_brdf(*surface_hit, in_dir, out_dir);&#xA;            const float cos_term = dot(in_dir, surface_hit-&amp;gt;normal);&#xA;&#xA;            const float brdf_pdf = cos_term / pi;&#xA;&#xA;            const Optional&amp;lt;Intersection&amp;gt; light_hit = light-&amp;gt;shape-&amp;gt;intersect(ray_from_surface(*surface_hit, in_dir));&#xA;            const float light_pdf = (light_hit ? light-&amp;gt;shape-&amp;gt;areaPdf(surface_hit-&amp;gt;position, in_dir) : 0.f);&#xA;&#xA;            if (brdf_pdf != 0.f &amp;amp;&amp;amp; reflectance != vec3_0) {&#xA;                const vec3 illuminance = calc_light_incidence(scene, rng, ray_from_surface(*surface_hit, in_dir), depth);&#xA;                color += (1.0f / brdf_pdf) * cos_term * reflectance * illuminance * power_heuristic(brdf_pdf, light_pdf);&#xA;            }&#xA;        }&#xA;&#xA;        // Sample lights&#xA;        {&#xA;            const ShapeSample light_sample = light-&amp;gt;shape-&amp;gt;sampleArea(rng, surface_hit-&amp;gt;position);&#xA;            const vec3 light_vec = light_sample.point - surface_hit-&amp;gt;position;&#xA;            const vec3 in_dir = normalized(light_vec);&#xA;            const float cos_term = vmax(0.f, dot(in_dir, surface_hit-&amp;gt;normal));&#xA;&#xA;            const Optional&amp;lt;Intersection&amp;gt; light_hit = find_nearest_intersection(scene, ray_from_surface(*surface_hit, light_vec));&#xA;            bool occluded = !light_hit || light_hit-&amp;gt;object != light;&#xA;            const float light_pdf = light_weight * light_sample.pdf;&#xA;&#xA;            const float brdf_pdf = cos_term / pi;&#xA;&#xA;            const vec3 reflectance = surface_hit-&amp;gt;object-&amp;gt;material.diffuse_brdf(*surface_hit, in_dir, out_dir);&#xA;            if (!occluded &amp;amp;&amp;amp; light_pdf != 0.f &amp;amp;&amp;amp; reflectance != vec3_0) {&#xA;                const vec3 illuminance = calc_light_incidence(scene, rng, ray_from_surface(*surface_hit, in_dir), depth);&#xA;                const float differential_area = -dot(light_hit-&amp;gt;normal, in_dir) / length_sqr(light_vec);&#xA;                color += light_weight * (1.0f / light_pdf) * differential_area * cos_term * reflectance * illuminance * power_heuristic(light_pdf, brdf_pdf);&#xA;            }&#xA;        }&#xA;&#xA;        if (dot(out_dir, surface_hit-&amp;gt;normal) &amp;gt;= 0.f) {&#xA;            color += surface_hit-&amp;gt;object-&amp;gt;material.emmision-&amp;gt;getValue(*surface_hit);&#xA;        }&#xA;    } else {&#xA;        //color = lerp(mvec3(1.0f, 0.2f, 0.0f), mvec3(0.35f, 0.9f, 1.0f), 1.0f - std::pow(1.0f - vmax(0.0f, dot(ray.direction, vec3_y)), 2)) * 0.5f;&#xA;        color = lerp(mvec3(0.02f, 0.06f, 0.36f), mvec3(0.0f, 0.0f, 0.0f), 1.0f - std::pow(1.0f - vmax(0.0f, dot(ray.direction, vec3_y)), 2)) * 0.5f;&#xA;    }&#xA;&#xA;    return color * ray_weight;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-12T17:10:01.963" LastActivityDate="2015-08-12T17:10:01.963" Title="Multiple Importance Sampling with more than one light" Tags="&lt;raytracing&gt;&lt;importance-sampling&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="2" />
  <row Id="216" PostTypeId="2" ParentId="203" CreationDate="2015-08-12T07:12:46.860" Score="1" Body="&lt;p&gt;Just to add to the excellent suggestions above, it occurred to me that, without an ultraviolet channel, fluorescent materials would be tricky to model.&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2015-08-12T07:12:46.860" CommentCount="4" />
  <row Id="217" PostTypeId="2" ParentId="215" CreationDate="2015-08-12T08:17:24.157" Score="2" Body="&lt;p&gt;It seems that the answer to my question is that my approach inherently can't work. After doing thinking about it some more and researching existing renderers, none seem to implement what I'm doing, and I think the noise comes from contributions from lights other than the one randomly picked will not be estimated with the PDF. To do it correctly I would need to loop through all the lights, which gives me essentially path tracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Note however, that at Mitsuba and PBRT implement a Direct Illumination integrator which uses the approach I do, but only works when excluding indirect illumination, which is also true of my approach.)&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-12T08:17:24.157" CommentCount="1" />
  <row Id="218" PostTypeId="1" AcceptedAnswerId="224" CreationDate="2015-08-12T08:55:30.060" Score="6" ViewCount="72" Body="&lt;p&gt;How can I overlay graphics on particle simulations? For instance if I am representing a liquid or gas with tons of little points how can I make those points look like a liquid or gas? Take Nvidia Flex for instance, they simulate liquids and gases with tons of small points but can render them as realistic liquids and gases instead of small balls.&lt;/p&gt;&#xA;" OwnerUserId="113" LastEditorUserId="6" LastEditDate="2015-08-12T09:32:44.950" LastActivityDate="2015-08-12T17:01:00.427" Title="Overlaying graphics on particle simulation" Tags="&lt;simulation&gt;&lt;physics&gt;&lt;particles&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="219" PostTypeId="2" ParentId="218" CreationDate="2015-08-12T10:17:33.037" Score="-1" Body="&lt;p&gt;A good reference for game physics is &lt;a href=&quot;https://books.google.it/books?id=ynQZBwAAQBAJ&amp;amp;printsec=frontcover&amp;amp;dq=game+physics+eberly&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0CCAQ6AEwAGoVChMIkKr24aSjxwIVwS4aCh3nrg55#v=onepage&amp;amp;q=game%20physics%20eberly&amp;amp;f=false&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; chapter 4 describes the basic free deformable surfaces (nurbs and bspline are of course cited and treated enough well) fluid and gases are instead treated in chapter five (basically the author derivate simplified model of navier stokes equations, suitable for real time applications).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So actually i guess what i've written commenting your post was correct, i.e. combine the physical deformation of the point that controls the shape of the surface with a ffd computations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The book i cited should provide somewhere/somehow on the web source code, and itself it cites some example involving the technique i cited. If that is what you need let me know for a more specific explanation.&lt;/p&gt;&#xA;" OwnerUserId="228" LastActivityDate="2015-08-12T10:17:33.037" CommentCount="6" />
  <row Id="220" PostTypeId="2" ParentId="212" CreationDate="2015-08-12T14:27:25.487" Score="5" Body="&lt;p&gt;Most raytracers do both!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have you ever seen a ray-triangle intersection test where the triangle is transformed so that one vertex is at the origin and the triangle is flat along one of the axes? That would simplify the test, but storing the inverse transformation to apply it to the ray takes more space than the triangle itself. So you could say that almost all ray-triangle tests &lt;em&gt;do&lt;/em&gt; apply the forward transformation to the object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And yet most of the time you are rendering a mesh of many triangles, and &quot;most raytracers&quot; don't transform every vertex in the mesh by the object's transform matrix, they inverse transform the ray into object space. There isn't necessarily a good reason for this—it can definitely be more performant in some cases to preprocess the mesh and put every vertex in world space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But then say you're doing animation studio-scale path tracing. Now your scene geometry might not all fit in RAM at once. It's no longer a performance win to pre-transform all your vertices/control points, because (a) you need one copy of the object per instance, and (b) you need to transform it each time you read it into RAM. When data bound, the cost of two matrix multiplies per ray is insignificant anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Floating point precision is a potential reason as well: when you have an object a long way from the origin, you get more error from floating point positions, and if you pre-transform the object you apply that error in different directions to each vertex, whereas if you inverse transform the ray you have one fixed error amount from the imprecise position of the ray, but your vertices have less error relative to each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although I suspect the real answer to &quot;why do most raytracers inverse transform the ray&quot; is that most raytracers with public source code implement a wide range of geometric primitives, either for educational purposes or as a proof-of-concept, and it's easier to implement one cheap ray inverse transform than N shape forward transforms. (Plus, as has been mentioned, some intersection algorithms are much simpler if the object is in its canonical space.)&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-12T14:27:25.487" CommentCount="0" />
  <row Id="221" PostTypeId="1" AcceptedAnswerId="223" CreationDate="2015-08-12T16:16:49.647" Score="12" ViewCount="119" Body="&lt;p&gt;I know in the not so long ago (5-10 years?) that it was popular / efficient to bake data out into textures and then read the data from the textures, often using the built in texture interpolation to get linear interpolation of the baked out data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that computing time is cheaper compared to texture lookup time, this practice has definitely lessened if not all together disappeared.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, are baked out textures still used for anything?  Does anyone have any usage cases for them in modern architecture?  Does it seem likely they will ever make a come back? (say, if memory technology or basic GPU architecture changes)&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-12T16:42:14.817" Title="Are lookup textures still used for anything?" Tags="&lt;texture&gt;&lt;gpu&gt;&lt;hardware&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="222" PostTypeId="1" AcceptedAnswerId="231" CreationDate="2015-08-12T16:19:26.997" Score="5" ViewCount="118" Body="&lt;p&gt;I've seen mention of tiled raytracing, but haven't been able to find much on google about it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone explain what it is and how it works?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-13T19:05:22.927" Title="What is &quot;tiled&quot; raytracing?" Tags="&lt;raytracing&gt;&lt;tile-based-rendering&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="223" PostTypeId="2" ParentId="221" CreationDate="2015-08-12T16:42:14.817" Score="12" Body="&lt;p&gt;Yes, lookup textures are still used. For example, pre-integrated BRDFs (for ambient lighting, say), or arbitrarily complicated curves baked down to a 1D texture, or a 3D lookup texture for color grading, or a noise texture instead of a PRNG in the shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ALU is generally cheaper than a texture sample, true, but you still have a limited amount of ALU per frame. GPUs are good at latency hiding and small lookup textures are likely to be in the cache. If your function is complicated enough, it may still be worth using a lookup texture.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-12T16:42:14.817" CommentCount="0" />
  <row Id="224" PostTypeId="2" ParentId="218" CreationDate="2015-08-12T16:55:26.340" Score="4" Body="&lt;p&gt;For rendering of gases, I think the usual approach is to simply render each particle as a tiny disc. Gases don't really coalesce into surfaces like liquids do, so this should produce acceptable results. You could perhaps apply a light blur over the gas layer afterwards to soften it and hide the fact that it is made of discrete elements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Liquids, on the other hand, tend to coalesce together to form droplets and smooth surfaces, so you need to derive a surface from the particles somehow. One way to do this is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Metaballs&quot; rel=&quot;nofollow&quot;&gt;Metaballs&lt;/a&gt;, which also display this behavior and can be tweaked to suit different liquids and particle densities. By interpreting each particle as a metaball, you will have an implicit equation representing your liquid surface. To render this implicit surface you can then use an algorithm like &lt;a href=&quot;https://en.wikipedia.org/wiki/Marching_tetrahedra&quot; rel=&quot;nofollow&quot;&gt;Marching Tetrahedra&lt;/a&gt; to convert it to triangles, or utilize &lt;a href=&quot;http://gamedev.stackexchange.com/a/67745/5337&quot;&gt;Ray Marching&lt;/a&gt; to directly render it. (Ray marching can be easily done in realtime in a fragment shader these days.) You can also use this approach for gases if you want a somewhat softer look.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-12T17:01:00.427" LastActivityDate="2015-08-12T17:01:00.427" CommentCount="0" />
  <row Id="225" PostTypeId="1" CreationDate="2015-08-12T19:34:12.640" Score="5" ViewCount="73" Body="&lt;p&gt;There are a number of terms for rendering techniques based on the particle model of light: forward ray-tracing, reverse ray-tracing, ray-casting, ray-marching, and possibly others.  What's the difference between them?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-13T00:20:04.227" Title="Ray-based rendering terms" Tags="&lt;raytracing&gt;&lt;raymarching&gt;&lt;terminology&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2015-08-26T10:41:38.667" />
  <row Id="226" PostTypeId="2" ParentId="225" CreationDate="2015-08-13T00:20:04.227" Score="4" Body="&lt;p&gt;Forward ray-tracing means that rays starts from the light source and are traced towards the eye (light-surface-eye), this is a highly inefficient but accurate method. Reverse raytracing is simply starting in the eye and then trace them towards the light source. These are two classes of ray tracers.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Ray casting is the use of ray-surface intersection tests to solve a variety of problems in computer graphics and computational geometry. The distinction is that ray casting is a rendering algorithm that never recursively traces secondary rays, whereas other ray tracing-based rendering algorithms may do so. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Ray-marching is an implementation of a ray tracer, often used to visualize volumes. Normally ray casting uses a formulae to determine the point of intersection with the surface. In ray marching such a function is not required, instead the intersection is evaluated at several points along the ray. Either these points have a fixed distance between them or every iteration the distance to the next point is based on the distance to the closest surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The difference between those are that ray-tracing is a concept to solve the rendering equation, in the end resulting in a certain color for each pixel. Forward and reverse ray-tracing are classes of ray-tracers, ray-casting is a simple algorithm to do ray-tracing and ray-marching is another algorithm to do ray-tracing. As by the comment many other algorithms have been developed to solve the problem with different advantages and disadvantages.&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2015-08-13T00:20:04.227" CommentCount="1" />
  <row Id="227" PostTypeId="1" AcceptedAnswerId="236" CreationDate="2015-08-13T01:10:15.263" Score="6" ViewCount="231" Body="&lt;p&gt;How are volumetric effects such as smoke, fog, or clouds rendered by a raytracer?  Unlike solid objects, these don't have a well-defined surface to compute an intersection with.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-11-03T13:56:14.140" Title="How are volumetric effects handled in raytracing?" Tags="&lt;raytracing&gt;&lt;volumetric&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="228" PostTypeId="2" ParentId="227" CreationDate="2015-08-13T02:04:11.007" Score="4" Body="&lt;p&gt;One way to do it - which isn't exactly the &quot;go to&quot; solution, but can work nicely, is to find the distance that the ray traveled through the volume and use integration of some density function to calculate how much &quot;stuff&quot; was hit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a link with an example implementation:&#xA;&lt;a href=&quot;http://blog.demofox.org/2014/06/22/analytic-fog-density/&quot; rel=&quot;nofollow&quot;&gt;http://blog.demofox.org/2014/06/22/analytic-fog-density/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-13T03:31:06.013" LastActivityDate="2015-08-13T03:31:06.013" CommentCount="0" />
  <row Id="229" PostTypeId="2" ParentId="222" CreationDate="2015-08-13T02:32:23.337" Score="5" Body="&lt;p&gt;I believe it's where you batch it and do a block of pixels at once, or a &quot;tile&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means calculating the final color for multiple pixels at a time. Imagine breaking up your window of pixels into squares containing the pixels, usually around 2x2 pixels per square. Then simply work through them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This causes speed boosts for lots of different reasons, depending on your implementation and hardware. If you can, check out Ingo Wald's PhD thesis about real time raytracing, he makes a lot of references to batching to get speed using things like AVX (advanced vectoring extensions) in the processor. These provide speed boosts when you calculate the color for a certain amount of similar pixels. It's all cool and definitely worth looking into, and you can read more here: &lt;a href=&quot;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically you represent similar data for similar pixels using a vector of floats and do the same calculations to all at once, saving cycles. Picture Bart Simpson writing out the same words on the chalkboard all at once instead of line-by-line in the Simpsons opening :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also when doing real time raytracing, you may be using the graphics card and sending all the data over the bus is a large bottleneck. You don't want to send all the data over to the card to calculate the value of only one pixel because of the bus overhead - so you want to send a reasonable amount of data to chew on while you work on getting the next chunk ready. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, to me at least, &quot;tiled&quot; raytracing is just &quot;batched&quot; raytracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope this helps.&lt;/p&gt;&#xA;" OwnerUserId="377" LastEditorUserId="377" LastEditDate="2015-08-13T19:05:22.927" LastActivityDate="2015-08-13T19:05:22.927" CommentCount="0" />
  <row Id="230" PostTypeId="1" AcceptedAnswerId="238" CreationDate="2015-08-13T03:32:42.397" Score="6" ViewCount="70" Body="&lt;p&gt;I know how to program directional lights, point lights and spot lights.  Sometimes though, people have lights that are shaped - like bars, or even a torus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does that work from either a ray based or rasterization perspective?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-13T20:48:19.837" Title="How do shaped lights work?" Tags="&lt;lighting&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="231" PostTypeId="2" ParentId="222" CreationDate="2015-08-13T03:36:58.337" Score="6" Body="&lt;p&gt;Tiled raytracing is a way of speeding up rendering.  Instead of rendering pixels on a row-by-row basis down the entire image, you render them in 2D groups (say, 32x32 tiles).  Since adjacent rays tend to intersect the same objects, this improves the cache hit rate at the CPU level.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At a smaller scale, tiling at the 2x2 level lets you use SSE/AVX/etc to compute four rays with a single CPU instruction, further speeding things up.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-13T03:36:58.337" CommentCount="2" />
  <row Id="232" PostTypeId="2" ParentId="230" CreationDate="2015-08-13T04:33:04.053" Score="10" Body="&lt;p&gt;In a path tracer, when tracing a ray to a light with a shape other than a point (&quot;area light&quot; is the usual terminology), you generally select a random point on the surface of the light. When enough samples are taken, this results in both softer specular highlights and softer shadow penumbras: light will be reflected off a surface from a random distribution of incoming directions, and rays cast towards an area light with an obstruction in the way may or may not intersect the obstruction depending on which point on the light was sampled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In real time, approximations to area lights are used. This &lt;a href=&quot;http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf&quot;&gt;Unreal Engine 4 white paper&lt;/a&gt; describes some of the considerations. One possibility is the &quot;representative point&quot; method, which per-pixel selects a point in the light's volume (brightest or nearest) and uses that as though it was a point light. Another option is modifying the roughness that is fed into the specular calculation based on a cone that approximates the area light (in the paper as &quot;Specular D Modification&quot;).&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-13T04:33:04.053" CommentCount="0" />
  <row Id="233" PostTypeId="1" AcceptedAnswerId="244" CreationDate="2015-08-13T05:24:10.437" Score="7" ViewCount="260" Body="&lt;p&gt;Looking at a light probe texture, it looks like a blurry environment map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's the difference between the two, how is a light probe made, and what is the benefit of it being blurry?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-14T05:31:08.123" Title="How is a light probe different than an environmental cube map?" Tags="&lt;lighting&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="234" PostTypeId="2" ParentId="227" CreationDate="2015-08-13T05:50:52.547" Score="3" Body="&lt;p&gt;Depends on the volume efffect. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Uniform volume effects that do not belong do scattering can be simulated by just calculating ray enter and ray exit distances.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Otherwise you need to do integration of the ray path, also known as ray marching. To avoid needing to shoot secondary rays the raymarching is often coupled with some sort of cache, like depthmap, deepmaps, brick maps or voxel clouds for light shadowing, etc. This way you dont necceserily need to march the whole scene. Similar caching is often done to the volume procedural texture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is also possible to convert the texture to surface primitives like boxes, spheres or planes that have some suitable soft edged texture. You can then use normal rendering techniques to solve the volumetric effect. The problem with this is that you usually need lots of primitives. Additionally the shape of the primitive may show up as too uniform sampling.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-13T06:04:53.237" LastActivityDate="2015-08-13T06:04:53.237" CommentCount="3" />
  <row Id="235" PostTypeId="1" AcceptedAnswerId="237" CreationDate="2015-08-13T07:33:20.873" Score="6" ViewCount="43" Body="&lt;p&gt;Say I've got a photograph of a scene, and measurements for part of it (eg. a room where I know the dimensions of the walls).  Assuming a rectilinear lens, how would I go about setting up a virtual camera so that 3D objects are rendered as if they were in the physical scene?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-13T12:25:00.457" Title="Matching a virtual camera to a physical camera" Tags="&lt;rendering&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="236" PostTypeId="2" ParentId="227" CreationDate="2015-08-13T11:51:27.977" Score="6" Body="&lt;h2&gt;Overview&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The appearance of volumes (also called participating media) in nature is caused by tiny particles, such as dust, water droplets or plankton, that are suspended in the surrounding fluid, such as air or water. These particles are solid objects, and light refracts or reflects off of these objects as it would on a normal surface. In theory, participating media could therefore be handled by a traditional ray tracer with only surface intersections.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Statistical Model&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Of course, the &lt;em&gt;sheer number&lt;/em&gt; of these particles makes it infeasible to actually raytrace them individually. Instead, they are approximated with a statistical model: Because the particles are very small, and the distance between the particles is much larger than the particle size, individual interactions of light with the particles can be modeled as statistically independent. Therefore, it is a reasonable approximation to replace the individual particles with continuous quantities that describe the &quot;average&quot; light-particle interaction at that certain region in space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For physically based volumetric light transport, we replace the inconceivably many particles with a continuous participating medium that has two properties: The absorption coefficient, and the scattering coefficient. These coefficients are very convenient for ray tracing, as they allow us to compute the probability of a ray interacting with the medium - that is, the probability of striking one of the particles - as a function of distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The absorption coefficient is denoted $\sigma_a$. Say a ray of light wants to travel $t$ meters inside the participating medium; the probability of making it through unabsorbed -- i.e. not hitting one of the particles and being absorbed by it -- is $e^{-t\cdot\sigma_a}$`. As t increases, we can see that this probability goes to zero, that is, the longer we travel through the medium, the more likely it is to hit something and be absorbed. Very similar things hold for the scattering coefficient $\sigma_s$: The probability of the ray not hitting a particle and being scattered is $e^{-t\cdot\sigma_s}$; that is, the longer we travel through a medium, the more likely it is that we hit a particle and are scattered into a different direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Usually, these two quantities are folded into a single extinction coefficient, $\sigma_t = \sigma_a + \sigma_s$. The probability of traveling $t$ meters through a medium without interacting with it (neither being absorbed or scattered) is then $e^{-t\cdot\sigma_t}$. On the other hand, the probability of interacting with a medium after $t$ meters is $1 - e^{-t\cdot\sigma_t}$.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Rendering with Participating Media&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The way this is used in physically based renderers is as follows: When a ray enters a medium, we probabilistically stop it inside the medium and have it interact with a particle. Importance sampling the interaction probability$1 - e^{-t\cdot\sigma_t}$ yields a distance $t$; this tells us that the ray traveled $t$ meters in the medium before striking a particle, and now one of two things happens: Either the ray gets absorbed by the particle (with probability $\frac{\sigma_a}{\sigma_t}$), or it gets scattered (with probability $\frac{\sigma_s}{\sigma_t}$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How the ray is scattered is described by the &lt;em&gt;phase function&lt;/em&gt; and depends on the nature of the particles; the Rayleigh phase function describes scattering from spherical particles smaller than the wavelength of light (e.g. our atmosphere); the Mie phase function describes scattering from spherical particles of similar size than the wavelength (e.g. water droplets); in graphics, usually the &lt;a href=&quot;http://www.astro.umd.edu/~jph/HG_note.pdf&quot; rel=&quot;nofollow&quot;&gt;Henyey-Greenstein&lt;/a&gt; phase function is used, originally applied to scattering from interstellar dust.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, in graphics, we don't normally render pictures of an infinite medium, but render media inside a scene consisting of hard surfaces as well. In that case, we first fully trace the ray until it hits the next surface, completely ignoring the participating medium; this gives us the distance to the next surface, $t_{Max}$. We then sample an interaction distance $t$ in the medium as described before. If $t \lt t_{Max}$, the ray hit a particle on the way to the next surface and we either absorb it or scatter it. If $t \geq t_{Max}$, the ray made it through unscathed and interacts with the surface as usual.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Outlook&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;This post was only a small introduction to rendering with participating media; among other things, I completely ignored spatially varying coefficients (which you need for clouds, smoke, etc.). &lt;a href=&quot;http://www.cs.cornell.edu/courses/cs6630/2012sp/notes/09volpath.pdf&quot; rel=&quot;nofollow&quot;&gt;Steve Marschner's notes&lt;/a&gt; are a good resource, if you're interested. In general, participating media are very difficult to render efficiently, and you can go a lot more sophisticated than what I described here; there's &lt;a href=&quot;http://www.cs.dartmouth.edu/~wjarosz/publications/jarosz08beam.pdf&quot; rel=&quot;nofollow&quot;&gt;Volumetric Photon Mapping&lt;/a&gt;, &lt;a href=&quot;http://www.cs.dartmouth.edu/~wjarosz/publications/jarosz11progressive.pdf&quot; rel=&quot;nofollow&quot;&gt;Photon Beams&lt;/a&gt;, &lt;a href=&quot;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&quot; rel=&quot;nofollow&quot;&gt;Diffusion approximations&lt;/a&gt;, &lt;a href=&quot;http://www.cs.dartmouth.edu/~wjarosz/publications/georgiev13joint.pdf&quot; rel=&quot;nofollow&quot;&gt;Joint Importance Sampling&lt;/a&gt; and more. There's also interesting work on &lt;a href=&quot;http://www.cs.dartmouth.edu/~wjarosz/publications/meng15granular.pdf&quot; rel=&quot;nofollow&quot;&gt;granular media&lt;/a&gt; that describes what to do when the statistical model breaks down, i.e. particle interactions are no longer statistically independent.&lt;/p&gt;&#xA;" OwnerUserId="79" LastEditorUserId="137" LastEditDate="2015-11-03T13:56:14.140" LastActivityDate="2015-11-03T13:56:14.140" CommentCount="0" />
  <row Id="237" PostTypeId="2" ParentId="235" CreationDate="2015-08-13T12:25:00.457" Score="5" Body="&lt;p&gt;This is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Camera_resectioning&quot;&gt;Camera re-sectioning problem&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We need to have a few points, like the end points of the walls, to take the roles of x&lt;sub&gt;w&lt;/sub&gt;, y&lt;sub&gt;w&lt;/sub&gt; and z&lt;sub&gt;w&lt;/sub&gt; in the following equation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qXr4W.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qXr4W.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here A is defined as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Xs4BR.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Xs4BR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are the values of the camera itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;R and T are defined as the Rotation and Translation matrices for the camera, and are the values we need to solve for. The position, C, of the camera expressed in world coordinates is C = -R&lt;sup&gt;-1&lt;/sup&gt;T = -R&lt;sup&gt;T&lt;/sup&gt; T .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The linked Wikipedia article mentions a few algorithms to solve these equations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example of a practical application of these is also found on &lt;a href=&quot;http://math.stackexchange.com/questions/82602/how-to-find-camera-position-and-rotation-from-a-4x4-matrix&quot;&gt;Math Stack Exchange&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another example, with some code samples that you may want to use, is found &lt;a href=&quot;http://stackoverflow.com/questions/76134/how-do-i-reverse-project-2d-points-into-3d&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="21" LastActivityDate="2015-08-13T12:25:00.457" CommentCount="0" />
  <row Id="238" PostTypeId="2" ParentId="230" CreationDate="2015-08-13T12:38:56.643" Score="2" Body="&lt;p&gt;In my torus light implementation(Unity3D) I use the same real-time method &lt;a href=&quot;http://computergraphics.stackexchange.com/a/232/376&quot;&gt;described in @JohnCalsbeek's answer&lt;/a&gt;. Just treat it the same way you treat your regular point/spot light and calculate a single direction and distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use the torus distance equation from &lt;a href=&quot;http://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm&quot; rel=&quot;nofollow&quot;&gt;Inigo Quilez's site&lt;/a&gt; and&#xA;here's the function I made to get the direction to a torus&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;float3 torusDirection(float3 p) {&#xA;float3 torusRel;&#xA;torusRel.xz = normalize(p.xz-_TorusLocation.xz);&#xA;torusRel.y = 0;&#xA;&#xA;return -normalize(p-(TorusLocation+torusRel*TorusCenterRadius));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Overall I think it works well minus a few areas that look off where the torus loops around &lt;a href=&quot;http://i.stack.imgur.com/gBzMT.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gBzMT.png&quot; alt=&quot;Example scene lit by a torus&quot;&gt;&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The specular hint is also off but @JohnCalsbeek's point of modifying the roughness seems like it should fix that :). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The hardest challenge for me with the light right now is making a forward shader that supports different kinds of area lights and multiple lights. I don't know how I would do this besides just having a whole bunch of shader properties for all the different lights though. Currently I apply the light as a deferred pass so it only works with opaque objects but soon I'll need to make a forward version for transparent objects. That's the real challenge I think, getting multiple area lights in a forward shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here's a &lt;a href=&quot;https://www.shadertoy.com/view/MllSWS&quot; rel=&quot;nofollow&quot;&gt;Shadertoy demo&lt;/a&gt; of a torus light I made to test mine, its based off of BeyondTheStatics shape light demo.&lt;/p&gt;&#xA;" OwnerUserId="376" LastEditorUserId="231" LastEditDate="2015-08-13T20:48:19.837" LastActivityDate="2015-08-13T20:48:19.837" CommentCount="1" />
  <row Id="239" PostTypeId="1" CreationDate="2015-08-13T18:55:38.310" Score="6" ViewCount="86" Body="&lt;p&gt;Given an arbitrary triangle mesh, is it possible to automatically generate a UV mapping for it, and if so, how is it done?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-13T21:25:01.247" Title="Automatic generation of UV maps" Tags="&lt;uv-mapping&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="240" PostTypeId="2" ParentId="239" CreationDate="2015-08-13T19:10:59.400" Score="7" Body="&lt;p&gt;Short answer: Yes, but it won't be pretty.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Explaination: While there &lt;em&gt;are&lt;/em&gt; &lt;a href=&quot;http://knowledge.autodesk.com/support/maya-lt/learn-explore/caas/CloudHelp/cloudhelp/2015/ENU/MayaLT/files/Create-UVs--Automatic-Mapping-htm.html&quot; rel=&quot;nofollow&quot;&gt;algorithms&lt;/a&gt; to automagically UV map polygon soup, the mapping probably wont be ideal. UV mapping is an art really. Choosing where to hide the seams and where to put them to limit stretching and optimize texture space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, there &lt;em&gt;are&lt;/em&gt; lots of tools that can make manual UV mapping a heck of a lot easier. For example: &lt;a href=&quot;http://knowledge.autodesk.com/support/maya-lt/learn-explore/caas/CloudHelp/cloudhelp/2015/ENU/MayaLT/files/Create-UVs--Automatic-Mapping-htm.html&quot; rel=&quot;nofollow&quot;&gt;http://knowledge.autodesk.com/support/maya-lt/learn-explore/caas/CloudHelp/cloudhelp/2015/ENU/MayaLT/files/Create-UVs--Automatic-Mapping-htm.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2015-08-13T21:25:01.247" LastActivityDate="2015-08-13T21:25:01.247" CommentCount="4" />
  <row Id="241" PostTypeId="1" AcceptedAnswerId="1483" CreationDate="2015-08-13T19:20:21.843" Score="7" ViewCount="118" Body="&lt;p&gt;In chromaticity charts the CMYK color space is usually four-sided. I've lately had to read quite a few charts and it suddenly dawned to me that I don't really understand why. It has just not been something I've needed to think about until now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/pfP8P.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: A sample CIE31 chromaticity graph showing the spaces*.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of my working hypotheses is that it is actually a triangle. This triangle just clips for some reason. But there might be other reasons such as non-ideality of the color primaries, or bad plane. So why is it squarish?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* This image is not so clearly a polygon, like it's usually depicted as, but it's the best one I had at hand.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="48" LastEditDate="2015-08-13T19:59:48.217" LastActivityDate="2015-09-13T11:52:18.847" Title="Why is the CMYK space four sided in chromaticity charts" Tags="&lt;color&gt;&lt;color-management&gt;&lt;color-science&gt;" AnswerCount="2" CommentCount="4" FavoriteCount="1" />
  <row Id="242" PostTypeId="2" ParentId="241" CreationDate="2015-08-13T21:08:56.523" Score="3" Body="&lt;p&gt;There are two aspects to this:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The traditional chromaticity graph is actually a two-dimensional slice of a three-dimensional volume.  The slice usually used is one that matches up (more or less) to the fully-saturated primaries of an RGB-based colorspace; the full RGB colorspace is a tetrahedron.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;CMYK is a four-primary colorspace, with primaries that are not linearly independent.  This makes the full volume a complex shape.  Further, at least one (black) and possibly all four of the primaries don't lie on the plane of the 2D chromaticity graph.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The CMYK space is four-sided because it's a 2D slice of a complex 3D shape that (probably) doesn't lie on any of the surfaces of that shape.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-13T21:08:56.523" CommentCount="1" />
  <row Id="243" PostTypeId="1" AcceptedAnswerId="253" CreationDate="2015-08-14T02:19:08.863" Score="4" ViewCount="72" Body="&lt;p&gt;The Gimp has two different options in how you can perform Gaussian blur: &quot;IIR&quot; and &quot;RLE&quot;.  What's the difference between them?&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-15T10:35:37.133" Title="Different types of Gaussian blur?" Tags="&lt;2d&gt;&lt;gaussian-blur&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="244" PostTypeId="2" ParentId="233" CreationDate="2015-08-14T05:31:08.123" Score="8" Body="&lt;p&gt;There are two different common meanings of &quot;light probe&quot; that I'm aware of. Both of them represent the light around a single point in a scene, i.e. what you would see around you in all directions if you were shrunk down to a tiny size and stood at that point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One meaning is a spherical harmonic representation of the light around a point.  &lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_harmonics&quot;&gt;Spherical harmonics&lt;/a&gt; are a collection of functions defined over a spherical domain, which are analogous to sine waves that oscillate a certain number of times around the equator and from pole to pole on the sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spherical harmonics can be used to create a smooth, low-res approximation of any spherical function, by scaling and adding together some number of spherical harmonics&amp;mdash;usually 4 (known as linear, first-degree, or one-band SH) or 9 (called quadratic, second-degree, or two-band SH). This is very compact because you only have to store the scaling factors. For instance, for quadratic SH with RGB data, you only need 9*3 = 27 values per probe. So SH makes a very compact, but also necessarily very soft and blurry, representation of the light around a point. This is suitable for diffuse lighting, and perhaps specular with a high roughness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This screenshot from &lt;a href=&quot;http://simonstechblog.blogspot.com/2014_10_01_archive.html&quot;&gt;Simon's Tech Blog&lt;/a&gt; shows an array of SH light probes spaced throughout a scene, each one showing the indirect lighting received at that point:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xgMCf.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xgMCf.jpg&quot; alt=&quot;SH light probe array&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other currently common meaning of &quot;light probe&quot; is an environment cube-map whose mip levels have been pre-blurred to different extends so it can be used for specular lighting with varying levels of roughness. This image from &lt;a href=&quot;https://seblagarde.wordpress.com/2011/08/17/hello-world/&quot;&gt;Seb Lagarde's blog&lt;/a&gt; shows the basic idea:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/y2ICS.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/y2ICS.jpg&quot; alt=&quot;Prefiltered cubemap mip levels&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The higher-resolution mips (toward the left) are used for highly polished surfaces where you need a detailed reflected image. Toward the right, the lower-res mip levels are increasingly blurred, and are used for reflections from rougher surfaces. In a shader, when sampling this cubemap, you can calculate your requested mip level based on the material roughness, and take advantage of the trilinear filtering hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Both of these types of light probes are used in real-time graphics to approximate indirect lighting. While direct lighting can be calculated in real-time (or at least approximated well for area lights), indirect lighting is usually still baked in an offline preprocess due to its complexity and computational overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Traditionally, the result of the baking process would be lightmaps, but lightmaps only work for diffuse lighting on static geometry, and they take up a lot of memory besides. Baking a bunch of SH light probes (you can afford a lot of them because they're very compact), plus a sparser sprinkling of cubemap light probes, allows you to get decent diffuse and specular indirect lighting on both static and dynamic objects. They're a popular option in games today.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-14T05:31:08.123" CommentCount="4" />
  <row Id="245" PostTypeId="1" AcceptedAnswerId="247" CreationDate="2015-08-14T17:42:22.480" Score="5" ViewCount="69" Body="&lt;p&gt;I've played with real time raytracing (and raymarching etc) quite a bit, but haven't spent that much time on non real time raytracing - for higher quality images or for pre-rendering videos and the like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know one common technique to improve image quality in the non real time case is to just cast A LOT more rays per pixel and average the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other techniques which stand out as good ways to improve image quality in the non real time case, over what you would normally do in the real time case?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-14T17:58:47.153" Title="Non Real Time Raytracing" Tags="&lt;raytracing&gt;&lt;photo-realistic&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="246" PostTypeId="1" AcceptedAnswerId="248" CreationDate="2015-08-14T17:45:21.800" Score="10" ViewCount="177" Body="&lt;p&gt;I have written a small path-tracer after learning and experimenting on &lt;a href=&quot;http://www.kevinbeason.com/smallpt/&quot;&gt;smallpt&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only thing that I did not write (and understood) myself is how the initial rays are computed and fired from the camera. I got the principle right, but I'm looking for some resources describing how to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Compute the initial direction of rays&lt;/li&gt;&#xA;&lt;li&gt;Modelize a real lens (as opposed to pinhole camera), supposedly allowing effects like depth of field ?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;State of the art maths and physics are not required, but OK if explained thoroughly.&lt;/p&gt;&#xA;" OwnerUserId="34" LastActivityDate="2015-08-18T02:02:18.880" Title="How to build a decent lens/camera objective model for path tracing?" Tags="&lt;raytracing&gt;&lt;lighting&gt;&lt;monte-carlo&gt;&lt;physically-based&gt;&lt;physics&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="2" />
  <row Id="247" PostTypeId="2" ParentId="245" CreationDate="2015-08-14T17:58:47.153" Score="4" Body="&lt;p&gt;One of the big ones is the use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Constructive_solid_geometry&quot; rel=&quot;nofollow&quot;&gt;constructive solid geometry&lt;/a&gt; rather than triangle meshes.  Ray-triangle intersections are faster than almost any other ray-shape intersection, but it takes huge numbers of triangles to approximate the surface of a cylinder or torus, not to mention some of the really exotic shapes such as julia fractals or generalized parametric functions that some renderers support.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another is the use of render-time &lt;a href=&quot;https://en.wikipedia.org/wiki/Photon_mapping&quot; rel=&quot;nofollow&quot;&gt;photon mapping&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Diffuse_reflection&quot; rel=&quot;nofollow&quot;&gt;diffuse interreflection&lt;/a&gt; calculations: this lets you get accurate lighting effects in a changing scene.  In realtime raytracing, these are too expensive to compute, so either light sources and major geometry elements are forced to be stationary (to permit pre-calculation), or the effects are omitted entirely.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-14T17:58:47.153" CommentCount="1" />
  <row Id="248" PostTypeId="2" ParentId="246" CreationDate="2015-08-14T21:24:23.393" Score="11" Body="&lt;p&gt;The next step up from a pinhole camera model is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Thin_lens&quot;&gt;thin lens&lt;/a&gt; model, where we model the lens as being an infinitely thin disc. This is still an idealization that pretty far from modeling a real camera, but it will give you basic depth of field effects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7QOTx.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7QOTx.jpg&quot; alt=&quot;Thin lens model&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The image above, from &lt;a href=&quot;http://www.panohelp.com/thinlensformula.html&quot;&gt;panohelp.com&lt;/a&gt;, shows the basic idea. For each point on the image, there are multiple rays arriving at that image point, via every point on the 2D lens surface. Therefore, generating an image like this using Monte Carlo will require picking, for each ray, both a 2D sample point on the image plane and an independent 2D sample point on the lens surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The user-facing parameters to set will be the lens radius (as a physical radius in scene units), which controls how shallow the focus range is (larger lens = shallower focus range), and the distance at which you want objects to be in focus.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To generate eye rays into the scene, you can calculate the position and direction of rays leaving the lens surface; in this model there's no need to explicitly simulate the image plane and the refraction through the lens.  Basically, think of the lens as centered at the camera position and oriented to face the camera direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on the image location, construct a ray from the camera position (lens center) into the scene, just as you would in the pinhole model; then find its intersection with the focal plane. That's where all the rays from that image location should converge.  Now you can offset the starting point of the ray to a randomly chosen point on the lens, and set its direction to be toward the convergence point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can generalize this a bit by allowing the focal plane to be something other than a plane, or the lens to be something other than a circular disc, and following the same process. That can produce some interesting if not-quite-physical effects.  It's also possible to go beyond this simple model with a more physically realistic simulation of a camera's lens elements&amp;mdash;but that's beyond my expertise.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-14T21:24:23.393" CommentCount="0" />
  <row Id="249" PostTypeId="2" ParentId="203" CreationDate="2015-08-14T21:40:51.403" Score="16" Body="&lt;p&gt;There are various different types of limitation to take into consideration.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Effects for which the path of a ray is dependent on its wavelength&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;These are a class of effects for which spectral rendering is required, and a number of interesting examples have already been given in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/204/231&quot;&gt;Benedikt Bitterli's answer&lt;/a&gt;. A simple example is a prism splitting white light into a spectrum, giving rainbow colours. Rays of different wavelength are refracted by different angles as they pass through the prism, resulting in the light striking the wall behind the prism being split into its constituent colours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that in real life, shining monochromatic yellow light through a prism will result in yellow light coming out, but shining a mixture of red and green light that approximates yellow will result in separate red and green light emerging. When rendering using only 3 primary colours, white light will split into only those three colours, giving rainbow effects that look discontinuous, and monochromatic light that should not split at all will split into its approximating primary colour components. The splitting of white light can be improved by using a larger number of primary colours, but this will still give discontinuities close up, and the results for monochromatic light will still be split, albeit more narrowly. For accurate results a continuous spectrum must be sampled, with wavelengths distributed according to the light source being simulated.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Surface effects that cannot be captured in a single still image&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Iridescence&quot;&gt;Iridescence&lt;/a&gt;, for example, shows a different colour to each eye so that a still image will not look the same as the original object. There are many everyday examples that you might not notice at first. Many common birds have iridescent feathers even though they appear black or grey from a distance. Close up they are surprisingly colourful. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A renderer using only 3 primary colours will not be able to produce the spreading of light based on wavelength required for this effect. A spectral renderer can simulate the spreading correctly, but the full effect still cannot be captured in a single image. Even a 2d photograph cannot capture this correctly, whereas a 3d photograph of an iridescent object will give that shimmering effect as the photographs corresponding to left and right eyes will be coloured differently. This is a limitation of 2d images rather than the RGB colour space itself. However, even in a 3d image there will be colours in the iridescent object that are not displayed correctly, due to the inability of RGB to display monochromatic colours as described below.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Colours that the human eye can detect that cannot be displayed in RGB&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;RGB was historically device dependent and therefore unreliable between platforms. There are device independent perceptually uniform improvements such as the colour space &lt;a href=&quot;http://en.wikipedia.org/wiki/Lab_color_space&quot;&gt;Lab&lt;/a&gt;, but these are still trichromatic (having 3 components). It is not immediately obvious why three components is insufficient to display all the colours that can be perceived by a trichromatic eye, but &lt;a href=&quot;http://www.babelcolor.com/download/A%20review%20of%20RGB%20color%20spaces.pdf&quot;&gt;this paper&lt;/a&gt; explains it well, and accessibly. From page 7:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For example, using a modern laser-display system with monochromatic primaries at 635 nm (red), 532 nm (green), and 447 nm (blue), lets see if we can simulate the perception of a monochromatic light at 580 nm (an orange color). Since the monochromatic orange stimulus excites the greenish and reddish cones, a contribution is required by both the green and red primaries, while no contribution is required from the blue primary. The problem is that the green primary also excites the bluish cones, making it impossible to exactly replicate the orange stimulus&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The diagram of human eye cones sensitivities (also on page 7) shows how wide the overlap is and helps to visualise this explanation. I've included a similar graph from Wikipedia here: (click on the graph for the Wikipedia location)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_sensitivity#/media/File:Cones_SMJ2_E.svg&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg&quot; alt=&quot;Graph of the sensitivities of the 3 different cones in the human eye&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, the overlap between the range of colours that can be picked up by each of the three different cones (colour sensors) of the human eye means that a monochromatic colour can be distinguished from an approximating mixture of primary colours, and therefore mixing primary colours can never accurately display all monochromatic colours.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This difference is not usually noticeable in everyday life as most of our surroundings emit or reflect light across a wide range of frequencies rather than single monochromatic colours. However, a notable exception is sodium lamps. If you live in a part of the world that uses these yellow-orange street lights, the light emitted is monochromatic and will look subtly different from a printed photograph or an image on a screen. The wavelength of sodium light happens to be the 580 nm from the example quoted above. If you don't live somewhere that has sodium street lights, you can see the same single wavelength light by sprinkling finely crushed table salt (sodium chloride) over a flame. The scintillating yellow points of light cannot be accurately captured on film or displayed on a screen. Whatever three primary colours you choose, there will always be a range of monochromatic colours that cannot be displayed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this limitation applies equally to mixing 3 primary colours of paint, using 3 photoreactive chemicals on a camera film, or taking a photograph with a digital camera with 3 different colour sensors, or a single sensor with 3 different primary colour filters. It isn't just a digital problem, and isn't just restricted to the RGB colour space. Even the improvements introduced by the Lab colour space and its variants cannot recover the missing colours.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Miscellaneous effects&lt;/h1&gt;&#xA;&#xA;&lt;h3&gt;Multiple diffuse reflections (colour bleeding)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If a brightly coloured matt surface is near a white matt surface, the white surface will show some of the colour of the other surface. This can be modeled reasonably well using purely red, green and blue components. The same combination of red, green and blue that gave the colour of the coloured surface can reflect off the white surface and show some of that colour again. However, this only works if the second surface is white. If the second surface is also coloured, then the colour bleeding will be inaccurate, in some cases drastically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine two surfaces that look a similar colour. One reflects a narrow range of wavelengths around yellow. The other reflects a wide range of wavelengths between red and green, and as a result also looks yellow. In real life, the light showing on one surface due to the other will not be symmetrical. Most of the light reaching the wide wavelength range surface from the other will be reflected again, as the narrow range of incoming wavelengths are all within the wider range. However, most of the light reaching the narrow wavelength range surface from the other will be outside the narrow range, and will not be reflected. In an RGB renderer, both surfaces will be modelled as a mixture of monochromatic red and monochromatic green, giving no difference in reflected light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is an extreme example where the difference will be instantly noticeable to the eye, but there will be at least a subtle difference in most images that include colour bleeding.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Materials that absorb one wavelength and emit another&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://computergraphics.stackexchange.com/a/207/231&quot;&gt;joojaa's answer&lt;/a&gt; describes the absorption of ultraviolet light by snow, to be re-emitted as visible light. I hadn't heard of this happening with snow before (and frustratingly I've been unable to find any evidence to support it - although it would explain why snow is &quot;whiter than white&quot;). However, there is plenty of evidence of it happening with a wide range of other materials, some of which are added to clothes washing detergents and paper, to give extra bright whites. This allows the total visible light outgoing from a surface to be more than the total visible light received by that surface, which again is not modelled well using only RGB. If you want to read more about it, the term to search for is &lt;a href=&quot;https://en.wikipedia.org/wiki/Fluorescence&quot;&gt;Fluorescence&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Eyes with more than 3 primary colours&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;There are animals that have more than 3 types of cones in their eyes, allowing them to perceive more than 3 primary colours. For example, many birds, insects and fish are &lt;a href=&quot;https://en.wikipedia.org/wiki/Tetrachromacy&quot;&gt;tetrachromats&lt;/a&gt;, perceiving four primary colours. Some are even &lt;a href=&quot;https://en.wikipedia.org/wiki/Pentachromacy&quot;&gt;pentachromats&lt;/a&gt;, perceiving five. The range of colours that such creatures can see dwarfs the range displayable using only RGB. Far beyond them is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mantis_shrimp#Eyes&quot;&gt;mantis shrimp&lt;/a&gt;, which is a dodecachromat, seeing colours based on 12 different cones. None of these animals would be satisfied by an RGB display.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But more seriously, even for images intended for human eyes, there are believed to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Tetrachromacy#Human_tetrachromats&quot;&gt;human tetrachromats&lt;/a&gt; who see in 4 primary colours, and possibly some who see as many as 5 or 6. At present, such people don't seem to be present in sufficient numbers to make displays with more than 3 primary colours commercially viable, but if in future it becomes easier to identify how many primary colours a person can see, this may become an attractive trait leading to it spreading throughout the population in future generations. So if you want your great grandchildren to appreciate your work you may need to make it compatible with a hexachromatic monitor...&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Not really relevant to this question, but related: If you want to see colours that are not available in either the real world&lt;/em&gt; &lt;strong&gt;or&lt;/strong&gt; &lt;em&gt;RGB images, have a look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Impossible_color#Chimerical_colors&quot;&gt;Chimerical Colours&lt;/a&gt;...&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-14T21:40:51.403" CommentCount="0" />
  <row Id="250" PostTypeId="1" AcceptedAnswerId="252" CreationDate="2015-08-14T22:11:19.027" Score="9" ViewCount="143" Body="&lt;p&gt;I've seen in several places that making Perlin noise loop seamlessly requires calculating it twice in slightly different ways, and summing the two results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://webstaff.itn.liu.se/~stegu/TNM022-2005/perlinnoiselinks/perlin-noise-math-faq.html#loop&quot; rel=&quot;nofollow&quot;&gt;This Perlin noise math FAQ&lt;/a&gt; gives a formula:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$F_{loop}(x, y, z) = \frac{ (t - z) \cdot F(x, y, z) + z \cdot F(x, y, z - t) }{ t}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;to make a noise function $F$ loop in the $z$ direction. It also mentions that extending this, to loop in 2 dimensions would take 4 evaluations of $F$ and to loop in 3 dimensions would take 8 evaluations of $F$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that this gives a seamless join between tiles that is not only continuous but continuously differentiable, but I intuitively expect that to be the case if the noise function is simply evaluated once with grid points reduced modulo the required tile size. If the noise function is only ever based on the immediately surrounding grid points (4 for 2D noise, 8 for 3D noise) then surely just using the leftmost grid points when the point to calculate gets past the right hand edge of the tile will give the same quality of noise as between any other grid points?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I've seen this multiple calculation approach in several places I assume it must have some advantage, but I'm struggling to see the disadvantage with simply wrapping the grid points back to the start when they get too big. What am I missing?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="127" LastEditDate="2015-11-03T14:51:01.303" LastActivityDate="2015-11-03T14:51:01.303" Title="Why is it twice as expensive to make a noise function that can be tiled?" Tags="&lt;algorithm&gt;&lt;noise&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="251" PostTypeId="2" ParentId="57" CreationDate="2015-08-15T00:14:29.170" Score="3" Body="&lt;p&gt;A good starting point is the classic paper &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=192227&quot; rel=&quot;nofollow&quot;&gt;Using particles to sample and control implicit surfaces&lt;/a&gt;, published in SIGGRAPH 1994.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple particle simulation described in the paper &lt;a href=&quot;http://dx.doi.org/10.1016/0097-8493(96)00005-2&quot; rel=&quot;nofollow&quot;&gt;Sampling implicit objects with physically-based particle systems&#xA;&lt;/a&gt; (&lt;em&gt;Computers &amp;amp; Graphics&lt;/em&gt;, 1996) for curves works for surfaces as well; see &lt;a href=&quot;http://www.visgraf.impa.br/Projects/dtexture/&quot; rel=&quot;nofollow&quot;&gt;Dynamic Texture for Implicit Surfaces&lt;/a&gt; for examples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a more recent example, see &lt;a href=&quot;http://dx.doi.org/10.1016/j.cag.2010.09.017&quot; rel=&quot;nofollow&quot;&gt;Shape and tone depiction for implicit surfaces&lt;/a&gt; (&lt;em&gt;Computers &amp;amp; Graphics&lt;/em&gt;, 2011). &lt;/p&gt;&#xA;" OwnerUserId="192" LastEditorUserId="192" LastEditDate="2015-08-18T18:26:56.627" LastActivityDate="2015-08-18T18:26:56.627" CommentCount="0" />
  <row Id="252" PostTypeId="2" ParentId="250" CreationDate="2015-08-15T05:55:52.820" Score="16" Body="&lt;p&gt;It's unfortunate that people commonly recommend this.  Blending between two (or four, etc.) translated copies of a noise function in that way is a pretty bad idea. Not only is it expensive, it doesn't even produce correct-looking results!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/DgnQE.png&quot; alt=&quot;Perlin noise&quot;&gt; &lt;img src=&quot;http://i.stack.imgur.com/949SB.png&quot; alt=&quot;Blended Perlin noise&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the left is some Perlin noise. On the right is two instances of Perlin noise, stacked and blended left-to-right.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The difference is kind of subtle, but you can see that the second image has lower contrast in a vertical column running down the middle.  That's where it's a 50% blend between two different instances of the noise function. Such a blend doesn't look like the original noise function: it just looks like a muddy mess.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OK, so it's not quite &lt;em&gt;that&lt;/em&gt; bad just looking at the raw noise...but if you then do any nonlinear transformations on the image, the nonuniform contrast can cause issues. For instance, here are those images thresholded at 60%. (Think of generating islands in an ocean, for instance.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/elyF1.png&quot; alt=&quot;Thresholded Perlin noise&quot;&gt; &lt;img src=&quot;http://i.stack.imgur.com/Micts.png&quot; alt=&quot;Thresholded blended Perlin noise&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now you can plainly see how the image on the right has fewer, smaller white areas in the middle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like you mentioned, for grid-based noise like Perlin, a better way is to tile the pseudorandom gradients at the grid points. That's easy and cheap to do, and then you can apply the interpolation algorithm to the gradients as usual (much like bilinear interpolation of a tiling texture). This produces tiling noise without any weird artifacts, because it works &lt;em&gt;with&lt;/em&gt; the underlying noise algorithm rather than over the top of it. You can use a similar strategy with Worley noise (cellular noise) by tiling the random feature points it uses as a basis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With multiple octaves of noise it's not always so easy, though. If the relative scale between the octaves (aka &quot;lacunarity&quot;) isn't an integer or simple rational number, then you may not be able to find a convenient tiling point where all the octaves' grids match up. You could tile each octave independently, but the overall noise would still not be tilable in that case.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-15T05:55:52.820" CommentCount="1" />
  <row Id="253" PostTypeId="2" ParentId="243" CreationDate="2015-08-15T10:35:37.133" Score="5" Body="&lt;p&gt;From the &lt;a href=&quot;http://docs.gimp.org/en/plug-in-gauss.html&quot;&gt;GIMP docs&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;h3&gt;IIR&lt;/h3&gt;&#xA;  &#xA;  &lt;p&gt;IIR stands for “infinite impulse response”. This blur works best for large radius values and for images which are not computer generated.&lt;/p&gt;&#xA;  &#xA;  &lt;h3&gt;RLE&lt;/h3&gt;&#xA;  &#xA;  &lt;p&gt;RLE stands for “run-length encoding”. RLE Gaussian Blur is best used on computer-generated images or those with large areas of constant intensity.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and also&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;They both produce the same results, but each one can be faster in some cases. &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So the short answer is that IIR is optimised for photographs, and RLE is optimised for images with areas of flat colour.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The claim seems to be that the only difference is the time taken to calculate the blur, and that the end results are identical. I haven't tested this, so if you need to rely on that claim you should test both and check for any differences in the results.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-15T10:35:37.133" CommentCount="0" />
  <row Id="254" PostTypeId="5" CreationDate="2015-08-15T13:07:08.230" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2015-08-15T13:07:08.230" LastActivityDate="2015-08-15T13:07:08.230" CommentCount="0" />
  <row Id="255" PostTypeId="4" CreationDate="2015-08-15T13:07:08.230" Score="0" Body="For questions about creating or working with noise functions (in 1D, 2D, 3D or more)." OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-16T05:02:48.720" LastActivityDate="2015-08-16T05:02:48.720" CommentCount="0" />
  <row Id="256" PostTypeId="1" AcceptedAnswerId="258" CreationDate="2015-08-15T18:33:42.943" Score="4" ViewCount="107" Body="&lt;p&gt;To make Gaussian blurring a 2d image faster, I know that you can do one axis and then the other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering though, if I did two Gaussian blurs of size $N$, would that be the same mathematically as doing one Gaussian blur of size $2N$?&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="16" LastEditDate="2015-11-03T15:21:02.520" LastActivityDate="2015-11-03T15:21:02.520" Title="Is doing multiple Gaussian blurs the same as doing one larger blur?" Tags="&lt;real-time&gt;&lt;post-processing&gt;&lt;gaussian-blur&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="257" PostTypeId="1" AcceptedAnswerId="263" CreationDate="2015-08-15T18:38:26.450" Score="4" ViewCount="60" Body="&lt;p&gt;In image editing software, you often have the ability to do a directional blur where you can choose and angle and a magnitude of a blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does this actually work in practice with Gaussian blurring?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could see possibly doing something like making the blur go across the axis specified by the angle and use some sort of texture sampling to find the appropriate sample (bilinear or bicubic sampling), but that seems like it probably isn't correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does directional blurring usually work?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would say this is in fact motion blur, but I'm asking specifically about the blurring portion, not the whole set up for motion blur within a renderer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance before and after:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://docs.gimp.org/en_GB/images/filters/examples/taj_orig.jpg&quot; alt=&quot;enter image description here&quot;&gt;&#xA;&lt;img src=&quot;http://docs.gimp.org/en_GB/images/filters/examples/blur-taj-mblur-linear.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="100" LastEditDate="2015-08-16T07:40:27.187" LastActivityDate="2015-08-16T12:54:36.177" Title="How does directional Gaussian blurring work?" Tags="&lt;real-time&gt;&lt;post-processing&gt;&lt;gaussian-blur&gt;&lt;blur&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="258" PostTypeId="2" ParentId="256" CreationDate="2015-08-15T19:40:53.283" Score="6" Body="&lt;p&gt;Yes, applying two Gaussian blurs is equivalent to doing one Gaussian blur, but with a slightly different size calculation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Applying a Gaussian blur to an image means doing a &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolution&quot; rel=&quot;nofollow&quot;&gt;convolution&lt;/a&gt; of the Gaussian with the image. Convolution is associative: Applying two Gaussian blurs to an image is equivalent to convolving the two Gaussians with each other, and convolving the resulting function with the image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As it happens, the convolution of two Gaussians with each other is another Gaussian, whose variance is the sum of variances of the two original Gaussians. A Gaussian of N pixel width has variance $N^2$; applying it twice is equivalent to a Gaussian with variance $2\cdot N^2$, which corresponds to a width of $\sqrt{2}\cdot N$ pixels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the same vein, applying a Gaussian of width $2\cdot N$ is equivalent to applying a Gaussian of width $N$ four times.&lt;/p&gt;&#xA;" OwnerUserId="79" LastEditorUserId="137" LastEditDate="2015-11-03T15:17:56.897" LastActivityDate="2015-11-03T15:17:56.897" CommentCount="6" />
  <row Id="259" PostTypeId="1" AcceptedAnswerId="260" CreationDate="2015-08-15T21:28:31.733" Score="15" ViewCount="296" Body="&lt;p&gt;I have set up some FPS-measuring code in WebGL (based on &lt;a href=&quot;http://stackoverflow.com/a/16447895/1633117&quot;&gt;this SO answer&lt;/a&gt;) and have discovered some oddities with the performance of my fragment shader. The code just renders a single quad (or rather two triangles) over a 1024x1024 canvas, so all the magic happens in the fragment shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider this simple shader (GLSL; the vertex shader is just a pass-through):&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;// some definitions&#xA;&#xA;void main() {&#xA;    float seed = uSeed;&#xA;    float x = vPos.x;&#xA;    float y = vPos.y;&#xA;&#xA;    float value = 1.0;&#xA;&#xA;    // Nothing to see here...&#xA;&#xA;    gl_FragColor = vec4(value, value, value, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So this just renders a white canvas. It averages around 30 fps on my machine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now let's ramp up the number crunching and compute each fragment based on a few octaves of position-dependent noise:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;void main() {&#xA;    float seed = uSeed;&#xA;    float x = vPos.x;&#xA;    float y = vPos.y;&#xA;&#xA;    float value = 1.0;&#xA;&#xA;      float noise;&#xA;      for ( int j=0; j&amp;lt;10; ++j)&#xA;      {&#xA;        noise = 0.0;&#xA;        for ( int i=4; i&amp;gt;0; i-- )&#xA;        {&#xA;            float oct = pow(2.0,float(i));&#xA;            noise += snoise(vec2(mod(seed,13.0)+x*oct,mod(seed*seed,11.0)+y*oct))/oct*4.0;&#xA;        }&#xA;      }&#xA;&#xA;      value = noise/2.0+0.5;&#xA;&#xA;    gl_FragColor = vec4(value, value, value, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;If you want to run the above code, I've been using &lt;a href=&quot;https://github.com/ashima/webgl-noise&quot;&gt;this implementation of &lt;code&gt;snoise&lt;/code&gt;&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This brings down the fps to something like 7. That makes sense.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the weird part... let's compute only one of every 16 fragments as noise and leave the others white, by wrapping the noise computation in the following conditional:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;if (int(mod(x*512.0,4.0)) == 0 &amp;amp;&amp;amp; int(mod(y*512.0,4.0)) == 0)) {&#xA;    // same noise computation&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You'd expect this to be much faster, but it's still only 7 fps.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For one more test, let's instead filter the pixels with the following conditional:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cplusplus prettyprint-override&quot;&gt;&lt;code&gt;if (x &amp;gt; 0.5 &amp;amp;&amp;amp; y &amp;gt; 0.5) {&#xA;    // same noise computation&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This gives the exact same number of noise pixels as before, but now we're back up to almost 30 fps.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What is going on here?&lt;/strong&gt; Shouldn't the two ways to filter out a 16th of the pixels give the exact same number of cycles? And why is the slower one as slow as rendering &lt;em&gt;all&lt;/em&gt; pixels as noise?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bonus question: &lt;strong&gt;What can I do about this?&lt;/strong&gt; Is there any way to work around the horrible performance if I actually &lt;em&gt;do&lt;/em&gt; want to speckle my canvas with only a few expensive fragments?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Just to be sure, I have confirmed that the actual modulo computation does not affect the frame rate at all, by rendering every 16th pixel black instead of white.)&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T14:51:28.630" LastActivityDate="2015-08-16T14:51:28.630" Title="Why is this conditional in my fragment shader so slow?" Tags="&lt;performance&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="260" PostTypeId="2" ParentId="259" CreationDate="2015-08-15T21:51:02.300" Score="17" Body="&lt;p&gt;Pixels get grouped into little squares (how big depends on the hardware) and computed together in a single &lt;a href=&quot;https://en.wikipedia.org/wiki/SIMD&quot;&gt;SIMD&lt;/a&gt; pipeline. (struct of arrays type of SIMD)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This pipeline (which has several different names depending on the vendor: warps, wavefronts) will execute operations for each pixel/fragment in lockstep. This means that if 1 pixel needs a computation done then all pixels will compute it and the ones that don't need the result will throw it away.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If all fragments follow the same path through a shader then the other branches won't get executed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that your first method of computing every 16th pixel will be worst case branching. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to still down size your image then just render to a smaller texture and then upscale it. &lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-08-15T21:51:02.300" CommentCount="1" />
  <row Id="261" PostTypeId="1" AcceptedAnswerId="262" CreationDate="2015-08-16T03:50:54.883" Score="5" ViewCount="164" Body="&lt;p&gt;When realtime renderers (such as 3d games) have motion blur, how is it usually implemented?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think I've heard something about needing to have motion vectors for each pixel but I'm not sure about that and not sure how it would be used.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-16T07:33:01.567" Title="How is motion blur implemented in modern realtime renderers?" Tags="&lt;real-time&gt;&lt;post-processing&gt;&lt;blur&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" />
  <row Id="262" PostTypeId="2" ParentId="261" CreationDate="2015-08-16T07:33:01.567" Score="5" Body="&lt;p&gt;There are several techniques used. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple, but limited, &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch27.html&quot;&gt; post-process approach&lt;/a&gt; that is not really used any more consists in reconstructing the world space position of a pixel using both the view projection matrix from current and previous frame. Using these two values you can compute the velocity at a pixel and blur accordingly, sampling along the velocity vector with how many samples you want ( the more sample the blurrier, but the more expensive as well). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This method unfortunately takes into account just the camera movement and therefore is not accurate enough if you have a dynamic scene with fast moving objects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you mentioned, other more modern techniques use a velocity buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This velocity buffer can be created during the base pass in the deferred pipeline transforming each vertex using the world-view-projection matrix of the previous frame and then computing in the pixel shader the velocity vector using the screen space position of both frames.&lt;br&gt;&#xA;With this velocity buffer you can, in a post-process step, sample across the per-pixel velocity direction like in the approach I described above.  The motion blur that you will get is obviously not camera-only; however you get an overhead in terms of memory (you need to keep an extra buffer) and time (you need to render additional info per pixel). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One technique based on velocity buffer that I particularly like is &lt;a href=&quot;http://graphics.cs.williams.edu/papers/MotionBlurI3D12/McGuire12Blur.pdf&quot;&gt;McGuire et al.&lt;/a&gt; that IIRC, is used with some variations in Unreal Engine 4 and many other games. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/pXpdE.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pXpdE.png&quot; alt=&quot; From McGuire et al. 2012 &quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;From [McGuire et al. 2012]&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They divide the scene in tiles, assuming that each of these have a dominant velocity. Then, starting from the full res velocity buffer, they locate the velocity vector with higher magnitude for each tile, producing a downsampled version of the velocity buffer where 1 pixel = 1 tile.&lt;br&gt;&#xA;Using this buffer, in another pass, they can generate a buffer that stores for each pixel/tile the dominant velocity among the neighbourhood pixels.&#xA;Note that the tile size is based on the max blur radius. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using these buffers they then proceed with the gathering step sampling in the neighbourhood dominant velocity direction as this is the most plausible direction of the blur for each sample (as I said, the tile size is the max-blur radius). The weighting used for the blur takes into account the full res velocity and depth informations. Note that the paper propose three different weighting for three different scenarios (blurry over sharp, sharp over blurry and blurry over blurry). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another important advantage of the method is that you can early-out if the dominant velocity in the neighbourhood is smaller than a certain threshold. &lt;/p&gt;&#xA;" OwnerUserId="100" LastActivityDate="2015-08-16T07:33:01.567" CommentCount="0" />
  <row Id="263" PostTypeId="2" ParentId="257" CreationDate="2015-08-16T10:57:32.917" Score="4" Body="&lt;p&gt;If I understand your question, you are asking how to actually perform said directional blur in code? A Gaussian blur is typically done by sampling your image in all directions around your current point (or if in 2 passes, one vertical and one horizontal which equates to the same thing), with a specific set of weights for the falloff.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a directional blur you just need to change the shape of your kernel to &quot;not be round&quot;, for lack of better words.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say you wanted to smear the image to the left only, you could just accumulate samples (with whatever weights you want) with &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;currentPixelUV.xy + float2( -1.f * someOffsetScale, 0.f) * invScreenRes.xy * currentIteration;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Bilinear texture filtering will be required for this to work when your resulting &lt;em&gt;uv&lt;/em&gt;s are not hitting pixel centers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The magnitude of the blur is controlled by how far you sample from your original pixel.&lt;/p&gt;&#xA;" OwnerUserId="406" LastEditorUserId="406" LastEditDate="2015-08-16T12:54:36.177" LastActivityDate="2015-08-16T12:54:36.177" CommentCount="2" />
  <row Id="264" PostTypeId="5" CreationDate="2015-08-16T12:10:39.143" Score="0" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Shader&quot; rel=&quot;nofollow&quot;&gt;Shaders&lt;/a&gt; are programs which are written to customise a programmable GPU pipeline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most common shader languages are &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenGL_Shading_Language&quot; rel=&quot;nofollow&quot;&gt;GLSL&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/High-Level_Shading_Language&quot; rel=&quot;nofollow&quot;&gt;HLSL&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T12:48:12.710" LastActivityDate="2015-08-16T12:48:12.710" CommentCount="0" />
  <row Id="265" PostTypeId="4" CreationDate="2015-08-16T12:10:39.143" Score="0" Body="For all questions related to shaders, i.e. the programmable part of the GPU pipeline. For language-specific shader questions, see also the [glsl] and [hlsl] tags." OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T12:48:36.057" LastActivityDate="2015-08-16T12:48:36.057" CommentCount="0" />
  <row Id="266" PostTypeId="1" AcceptedAnswerId="275" CreationDate="2015-08-16T13:49:05.607" Score="5" ViewCount="315" Body="&lt;p&gt;When using textures in GLSL, it is best to calculate the final texture coordinates in the vertex shader and hand them over to the fragment shader using &lt;code&gt;varying&lt;/code&gt;s. Example with a simple flip in the y coordinate:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Vertex shader&#xA;attribute vec2 texture;&#xA;varying highp vec2 texCoord;&#xA;// ...&#xA;void main() {&#xA;    texCoord = vec2(texture.x, 1.0-texture.y);&#xA;    // ...&#xA;}&#xA;&#xA;// Fragment shader&#xA;varying highp vec2 textureCoordinates;&#xA;uniform sampler2D tex;&#xA;// ...&#xA;void main() {&#xA;    highp vec4 texColor = texture2D(tex, texCoord);&#xA;    // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the flip in the y coordinate, or an even simpler operation like adding &lt;code&gt;vec2(0.5)&lt;/code&gt; to the texture coordinate is performed in the fragment shader the texture access is much slower. Why?&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;As a note, e.g. blending two textures, using a weighted sum of them, is much cheaper in terms of time and also needs to be done for each pixel, so the computation of the texture coordinate itself does not seem to be that costly.&lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-08-17T00:31:23.877" Title="Why is accessing textures much slower when calculating the texture coordinate in the fragment shader?" Tags="&lt;performance&gt;&lt;glsl&gt;&lt;fragment-shader&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="2" />
  <row Id="267" PostTypeId="5" CreationDate="2015-08-16T14:31:07.893" Score="0" Body="&lt;p&gt;Use this tag for questions about computations subject to real-time constraints. The minimum framerate for a real-time illusion is usually considered to be around 24 frames per second, at which the human will perceive a sequence of similar frames as fluid motion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more information, &lt;a href=&quot;https://en.wikipedia.org/wiki/Real-time_computer_graphics&quot; rel=&quot;nofollow&quot;&gt;see Wikipedia&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See also &lt;a href=&quot;/questions/tagged/interactive&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;interactive&amp;#39;&quot; rel=&quot;tag&quot;&gt;interactive&lt;/a&gt; for less strict constraints and &lt;a href=&quot;/questions/tagged/offline&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;offline&amp;#39;&quot; rel=&quot;tag&quot;&gt;offline&lt;/a&gt; for computations which will take significantly longer.&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T16:50:42.327" LastActivityDate="2015-08-16T16:50:42.327" CommentCount="0" />
  <row Id="268" PostTypeId="4" CreationDate="2015-08-16T14:31:07.893" Score="0" Body="For questions about computations subject to real-time constraints. The minimum framerate for a real-time illusion is usually considered to be around 24 frames per second. See also [interactive] for less strict constraints and [offline] for computations which will take significantly longer." OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T16:50:55.587" LastActivityDate="2015-08-16T16:50:55.587" CommentCount="0" />
  <row Id="269" PostTypeId="5" CreationDate="2015-08-16T14:37:22.753" Score="0" Body="&lt;p&gt;Use this tag for questions about computations with no or loose time constraints, which may take minutes, hours or days to complete.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Offline computations are commonly used as pre-computations for real-time applications or rendering of films. They usually allow for vastly superior and more realistic effects to be incorporated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For computations with time constraints, see &lt;a href=&quot;/questions/tagged/real-time&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;real-time&amp;#39;&quot; rel=&quot;tag&quot;&gt;real-time&lt;/a&gt; and &lt;a href=&quot;/questions/tagged/interactive&quot; class=&quot;post-tag&quot; title=&quot;show questions tagged &amp;#39;interactive&amp;#39;&quot; rel=&quot;tag&quot;&gt;interactive&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T16:51:11.287" LastActivityDate="2015-08-16T16:51:11.287" CommentCount="0" />
  <row Id="270" PostTypeId="4" CreationDate="2015-08-16T14:37:22.753" Score="0" Body="For questions about computations with no or loose time constraints, which may take minutes, hours or days to complete. For computations with time constraints, see [real-time] and [interactive]." OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-08-16T16:51:16.567" LastActivityDate="2015-08-16T16:51:16.567" CommentCount="0" />
  <row Id="271" PostTypeId="1" AcceptedAnswerId="274" CreationDate="2015-08-16T16:26:37.870" Score="4" ViewCount="195" Body="&lt;p&gt;Especially when rendering particle effects, the same object needs to be rendered several times with slightly modified properties. But these changes are often limited to properties like pose or textures, and are not in the geometry of the object itself. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A different pose (translation, rotation) is merely a matrix multiplication with the vertices, and a different texture with an applied texture atlas are merely different texture coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nevertheless each of the particles needs to be drawn and I currently do that by calling &lt;code&gt;glDrawElements&lt;/code&gt; for every particle, while setting uniforms appropriately. Until now this is sufficient for rendering the scene in real-time, but for more complex scenes or simply for more particles emitted by the particle system this could easily lead to dropping frame rates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, is there a way to reduce the number of draw calls for very similar objects? &lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-08-16T18:37:46.270" Title="How to reduce the number of draw calls when rendering one object multiple times?" Tags="&lt;opengl&gt;&lt;performance&gt;&lt;real-time&gt;&lt;mobile&gt;&lt;opengl-es&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="272" PostTypeId="2" ParentId="271" CreationDate="2015-08-16T16:26:37.870" Score="3" Body="&lt;p&gt;In OpenGL ES there is Instancing which provides allows for rendering one object multiple times. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using Instancing, you can use uniform arrays to provide different information, e.g. a transformation matrix, for each of the particles. In the shader you can then use &lt;code&gt;gl_InstanceID&lt;/code&gt; to distinguish between the individual particles and pick the appropriate index from the uniform array.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;attribute vec4 position;&#xA;uniform mat4 transformations[16];&#xA;// other attributes, uniforms, etc&#xA;void main() {&#xA;    // ...&#xA;    gl_Position = ... * transformations[gl_InstanceID] * position;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And you need to draw your objects using &lt;code&gt;glDrawElementsInstanced&lt;/code&gt; which takes the number of objects as an additional parameter (compared to &lt;code&gt;glDrawElements&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;Note: In OpenGL ES 2, you need to use &lt;code&gt;gl_InstanceIDEXT&lt;/code&gt; and &lt;code&gt;glDrawElementsInstancedEXT&lt;/code&gt; as it is an &lt;a href=&quot;https://www.khronos.org/registry/gles/extensions/EXT/draw_instanced.txt&quot; rel=&quot;nofollow&quot;&gt;extension&lt;/a&gt; there. Also you have to enable it in the shader using&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#extension GL_EXT_draw_instanced : enable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-08-16T16:26:37.870" CommentCount="0" />
  <row Id="273" PostTypeId="2" ParentId="175" CreationDate="2015-08-16T18:15:33.757" Score="2" Body="&lt;p&gt;I realized that littlecms, &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/175/how-is-an-icc-profile-embedded-or-built-into-an-image#comment247_175&quot;&gt;suggested by &lt;em&gt;joojaa&lt;/em&gt; in a comment above&lt;/a&gt; is very complete in dealing with the ICC profiles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did a few tests in their color translator program with different destination color spaces and the differences are great as you can see:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Original&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xe6IM.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xe6IM.jpg&quot; alt=&quot;Original&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;sRGB-like&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/NbIee.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/NbIee.jpg&quot; alt=&quot;sRGB&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;adobeRGB1998 (it seems it's the original color space)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/P7aGm.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/P7aGm.jpg&quot; alt=&quot;adobeRGB&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;appleRGB&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iAmh5.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iAmh5.jpg&quot; alt=&quot;apple&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the most desirable results in my case are within the ProPhotoRGB color space:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/gp5GS.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gp5GS.jpg&quot; alt=&quot;prophoto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;linearProPhotoRGB&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/MIJk4.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MIJk4.jpg&quot; alt=&quot;linearProPhoto&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of the above were following an automatic &lt;em&gt;rendering intent&lt;/em&gt; but other &lt;em&gt;intents&lt;/em&gt; bring slight variations (samples applied together with linearProPhotoRGB destination color space):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Absolute Colorimetric&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ik2TL.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ik2TL.jpg&quot; alt=&quot;ICC absolute colorimetric&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Legacy Unadapted Absolute Colorimetric&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vUR7g.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vUR7g.jpg&quot; alt=&quot;Legacy&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also realized that GIMP was actually restoring the original profile (not the opposite as I thought, i.e. instead of applying an embedded ICC profile, it was taking it off) and with this littlecms, now it does seem much simpler to &quot;translate&quot; color spaces than I thought.&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-08-16T18:15:33.757" CommentCount="0" />
  <row Id="274" PostTypeId="2" ParentId="271" CreationDate="2015-08-16T18:37:46.270" Score="7" Body="&lt;p&gt;There are many, many ways to draw things in OpenGL, so this is naturally confusing sometimes. The first method you describe, setting the shader parameters and issuing one draw call per object is usually the most inefficient, due to the high API overhead. The second one, using instanced drawing is a much smarter approach for objects with the same parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When it comes to particles, specifically, there are two other methods which I'm aware of and have tested: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;The first one, more traditional and easier to implement, is to generate a unique quadrilateral for each particle in the application code. Then use one of the &lt;a href=&quot;https://www.opengl.org/wiki/Buffer_Object_Streaming&quot;&gt;several optimized buffer streaming&lt;/a&gt; paths of OpenGL to upload this data and issue a single draw call. This is the most straightforward method and provides good results. It will involve very few API calls if you can map the vertex/index buffers (&lt;code&gt;glMapBuffer/MapBufferRange&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Another method is to move the whole thing to a shader program, using &lt;a href=&quot;https://www.opengl.org/wiki/Transform_Feedback&quot;&gt;Transform Feedback&lt;/a&gt;. This method is a little more complicated to get up and running, but you can find a lot of references on the subject, &lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial28/tutorial28.html&quot;&gt;such as this tutorial&lt;/a&gt;. This one should be the optimal path, since it moves the whole simulation to the GPU.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Those are some of the optimized ways of rendering particle effects. But OpenGL provides several other rendering paths that are better for different cases, one of such is &lt;a href=&quot;https://www.opengl.org/wiki/Vertex_Rendering#Indirect_rendering&quot;&gt;indirect draw&lt;/a&gt;, which isn't available on ES at the moment, but is probably one of the fastest drawing paths available on modern PC OpenGL. Transform Feedback also requires a geometry shader, so it is not available for current OpenGL-ES.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more on the optimized rendering paths of OpenGL, I recommend watching this very good presentation by NVidia: &lt;a href=&quot;http://gdcvault.com/play/1020791/&quot;&gt;Approaching Zero Driver Overhead in OpenGL&lt;/a&gt;. At the end of the talk, they show a very interesting benchmark of several methods and how they compare with the Direct3D equivalents.&lt;/p&gt;&#xA;" OwnerUserId="54" LastActivityDate="2015-08-16T18:37:46.270" CommentCount="0" />
  <row Id="275" PostTypeId="2" ParentId="266" CreationDate="2015-08-17T00:31:23.877" Score="9" Body="&lt;p&gt;What you're talking about is commonly called &quot;dependent texture reads&quot; in the mobile development community. It's an implementation detail of certain hardware, and therefore it really depends on the GPU as to whether or not it has any performance implications. Typically it's something you see brought up for PowerVR GPU's in Apple hardware, since it was explicitly mentioned in both &lt;a href=&quot;http://cdn.imgtec.com/sdk-documentation/PowerVR.Performance%20Recommendations.pdf&quot;&gt;Imagination&lt;/a&gt; and &lt;a href=&quot;https://developer.apple.com/library/ios/documentation/3DDrawing/Conceptual/OpenGLES_ProgrammingGuide/BestPracticesforShaders/BestPracticesforShaders.html&quot;&gt;Apple&lt;/a&gt; documentation. If I recall correctly, the issue basically came from hardware in the GPU that would start to pre-fetch textures before the fragment shader even began running, so that it could do a better job of hiding the latency. The docs I linked mention that it's no longer an issue on Series6 hardware, so at least on newer Apple hardware it's not something that you have to worry about. I'm honestly not sure about other mobile GPU's, since that's not my area of expertise. You should try and consult their documentation to find out for sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you decide to do some Google searches on this issue, be aware that you'll probably find some older material that's talking about dependent texture fetches on older desktop hardware. Basic in the early days of pixel/fragment shaders, the term &quot;dependent texture fetch&quot; referred to using a UV address that relied on a previous texture fetch. The classic example was bump-mapped environment map rendering, where you wanted to use a reflection vector based on the normal map in order to sample the environment map. On this older hardware there was some major performance implications, and I think it wasn't even supported on some very old GPU's. With modern GPU's the hardware and the shader ISA is much more generalized, and so the performance situation is much more complicated. &lt;/p&gt;&#xA;" OwnerUserId="207" LastActivityDate="2015-08-17T00:31:23.877" CommentCount="1" />
  <row Id="276" PostTypeId="2" ParentId="246" CreationDate="2015-08-17T06:28:22.107" Score="4" Body="&lt;p&gt;See Kolb, et al., &lt;a href=&quot;http://www.cs.virginia.edu/~gfx/Courses/2003/ImageSynthesis/papers/Cameras/Realistic%20Camera%20Model.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;A Realistic Camera Model for Computer Graphics&lt;/em&gt;&lt;/a&gt;, SIGGRAPH 95.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, do bear in mind that camera models which mimic real-world cameras aren't necessarily what you want for the rendering phase. In a visual effects/post-production scenario, the more blur/vignetting/distortion that the camera model introduces, the worse it is for the compositor/colour timer. It's often better to do it as a post-pass.&lt;/p&gt;&#xA;" OwnerUserId="159" LastEditorUserId="159" LastEditDate="2015-08-18T02:02:18.880" LastActivityDate="2015-08-18T02:02:18.880" CommentCount="0" />
  <row Id="277" PostTypeId="1" AcceptedAnswerId="289" CreationDate="2015-08-17T14:11:11.453" Score="8" ViewCount="173" Body="&lt;p&gt;Applying multiple Gaussian blurs can result in an effect that is equivalent to a stronger Gaussian blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance this question says that: &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/256/is-doing-multiple-gaussian-blurs-the-same-as-doing-one-larger-blur&quot;&gt;Is doing multiple Gaussian blurs the same as doing one larger blur?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Wikipedia also says it, but says that it will always be just as many calculations or more to do it in multiple blurs versus doing it in a single blur.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Applying multiple, successive gaussian blurs to an image has the same effect as applying a single, larger gaussian blur, whose radius is the square root of the sum of the squares of the blur radii that were actually applied. For example, applying successive gaussian blurs with radii of 6 and 8 gives the same results as applying a single gaussian blur of radius 10, since \sqrt{6^2 + 8^2} = 10. Because of this relationship, processing time cannot be saved by simulating a gaussian blur with successive, smaller blurs — the time required will be at least as great as performing the single large blur.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_blur#Mechanics&quot;&gt;https://en.wikipedia.org/wiki/Gaussian_blur#Mechanics&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I've heard and read about people doing multiple blurs in realtime graphics to achieve a stronger blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What benefit is there if it isn't a reduction in computation?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-20T05:35:41.600" Title="Why do multiple Gaussian Blurs?" Tags="&lt;real-time&gt;&lt;gaussian-blur&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="278" PostTypeId="1" CreationDate="2015-08-17T16:09:06.873" Score="8" ViewCount="89" Body="&lt;p&gt;Intuitively, dust settles onto surfaces at a higher rate in areas where the air flow is slower. This means instead of a surface gathering an even layer of dust, there will be more in corners - corners of a room/shelf, corners formed by the placement of objects on a surface, concavities in a surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can get an increase in realism simply by making the thickness/density of dust reduce with distance from an object, and combine that effect for several objects including walls. This naturally gives the expected ordering of thicknesses - the edges of a floor have more dust than the centre, the corners where the edges meet have more dust then the centre of the edges. However, the increase in realism from getting the order correct still leaves the problem of getting the ratio right. There is more dust in the places you expect to have more dust, but not necessarily the right amount more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an established method of approximating a realistic ratio of thickness between different points on a surface? I don't need this to be completely physically accurate (that would need to take into account objects that move through the environment during the long period taken for dust to accumulate). I'm just looking for average behaviour that will look believable to the human eye.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In searching online I've mostly found atmospheric models for suspended dust, rather than a way of modelling dust deposition on a surface.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;My attempts - linear and exponential distributions&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Here is some code in Python 3 using pillow (the PIL fork) that demonstrates a couple of distributions I have experimented with:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from PIL import Image&#xA;from math import exp&#xA;&#xA;&#xA;def linear(distance, scale):&#xA;    return max(0, 64 - distance * scale)&#xA;&#xA;&#xA;def exponential(distance, scale):&#xA;    return 64 * exp(-distance * scale)&#xA;&#xA;&#xA;def exponential_squared(distance, scale):&#xA;    return 64 * exp(-distance * distance * scale)&#xA;&#xA;&#xA;def gamma_corrected(value):&#xA;    corrected_value = int((value/255)**(1/2.2)*255)&#xA;    return corrected_value&#xA;&#xA;&#xA;def produce_image(image_size=(1024,1024),&#xA;                  glasses=(((100,300),90),((300,300),110)),&#xA;                  distribution=exponential,&#xA;                  scale=0.1,&#xA;                  background_level=0,&#xA;                  gamma=2.2,&#xA;                  filename='dusttest.png'&#xA;                  ):&#xA;    width, height = image_size&#xA;    pixels = []&#xA;    for y in range(height):&#xA;        for x in range(width):&#xA;            red, green, blue = pixel_value(x, y, image_size, glasses,&#xA;                                           distribution, scale,&#xA;                                           background_level&#xA;                                           )&#xA;            pixels.append((red, green, blue))&#xA;&#xA;    image = Image.new('RGB', image_size, color=None)&#xA;    image.putdata(pixels)&#xA;    image.save(filename)&#xA;&#xA;&#xA;def pixel_value(x, y, image_size, glasses, distribution, scale,&#xA;                background_level&#xA;                ):&#xA;    width, height = image_size&#xA;    value = background_level&#xA;    value += distribution(x, scale)&#xA;    value += distribution(width-x, scale)&#xA;    value += distribution(y, scale)&#xA;    for glass in glasses:&#xA;        coords, radius = glass&#xA;        a, b = coords&#xA;        distance = ((x-a) ** 2 + (y-b) ** 2) ** 0.5 - radius&#xA;        if distance &amp;lt; 0:&#xA;            value = 0&#xA;            break&#xA;        value += distribution(distance, scale)&#xA;    value = 255 - gamma_corrected(value)&#xA;    return ((value, value, value))&#xA;&#xA;&#xA;if __name__ == '__main__':&#xA;    for scale in [0.1, 0.2, 0.4, 0.8]:&#xA;        produce_image(distribution=linear,&#xA;                      scale=scale,&#xA;                      background_level=20,&#xA;                      filename='linear-' + str(scale) + '-dusttest.png'&#xA;                      )&#xA;    for scale in [0.1, 0.05, 0.03, 0.01]:&#xA;        produce_image(distribution=exponential,&#xA;                      scale=scale,&#xA;                      background_level=0,&#xA;                      filename='exponential-' + str(scale) + '-dusttest.png'&#xA;                      )&#xA;    for scale in [0.01, 0.001, 0.0001, 0.00001]:&#xA;        produce_image(distribution=exponential_squared,&#xA;                      scale=scale,&#xA;                      background_level=0,&#xA;                      filename='exponential-squared-' + str(scale) + '-dusttest.png'&#xA;                      )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Imagine looking down from above on a white shelf that has had drinking glasses placed on it. Some time later the glasses are removed to leave dust free circular regions and a distribution of dust over the rest of the shelf. The dust is affected by the positions of the glasses and the back and side walls. The front of the shelf (bottom of the image) is open, with no wall to increase dust.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Output&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;(Click for larger images)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Linear reduction in dust density plus constant background level of dust:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4FtgD.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4FtgDm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/cnyaS.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cnyaSm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/RXWlg.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RXWlgm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/IgPu0.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IgPu0m.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Exponential reduction in dust density (zero background level):&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4Td7b.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4Td7bm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/KUdEn.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KUdEnm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/hbgk3.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/hbgk3m.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/X29hi.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/X29him.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I expected the exponential version to be closer to reality, and I prefer the result visually. However, I still don't know if this is close enough.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Following &lt;a href=&quot;http://computergraphics.stackexchange.com/users/56/alan-wolfe&quot;&gt;Alan Wolfe&lt;/a&gt;'s suggestion of the normal distribution, I've also added images using &lt;code&gt;exp(-distance ** 2)&lt;/code&gt; at a variety of scales.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/n1oOr.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/n1oOrm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/2SJD9.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2SJD9m.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/LVfCg.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LVfCgm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/M0N8o.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/M0N8om.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I quite like this too, but I still couldn't guess which of this and exponential (&lt;code&gt;exp(-distance)&lt;/code&gt;) is best.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for feedback in two ways:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Do any of these distributions look right/natural to you? I want input from a wider range of human eyes, ideally with specific problems/inconsistencies.&lt;/li&gt;&#xA;&lt;li&gt;Is there a physical interpretation that would justify using one of these distributions, or that would suggest a better one?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-17T18:21:36.853" LastActivityDate="2015-08-18T18:22:01.660" Title="Is there an established method of approximating dust distribution on surfaces?" Tags="&lt;distribution&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="279" PostTypeId="2" ParentId="278" CreationDate="2015-08-18T05:30:15.183" Score="2" Body="&lt;p&gt;See the paper &lt;a href=&quot;http://dx.doi.org/10.1145/344779.344809&quot; rel=&quot;nofollow&quot;&gt;Computer Modelling of Fallen Snow&lt;/a&gt; published in SIGGRAPH 2000:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In this paper, we present a new model of snow accumulation and stability for computer graphics. Our contribution is divided into two major components, each essential for modelling the appearance of a thick layer of snowfall on the ground. Our accumulation model determines how much snow a particular surface receives, allowing for such phenomena as flake flutter, flake dusting and wind-blown snow. We compute snow accumulation by shooting particles upwards towards the sky, giving each source surface independent control over its own sampling density, accuracy and computation time. Importance ordering minimises sampling effort while maximising visual information, generating smoothly improving global results that can be interrupted at any point. Once snow lands on the ground, our stability model moves material away from physically unstable areas in a series of small, simultaneous avalanches. We use a simple local stability test that handles very steep surfaces, obstacles, edges, and wind transit. Our stability algorithm also handles other materials, such as flour, sand, and flowing water.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Its &lt;a href=&quot;http://www.cs.ubc.ca/nest/imager/contributions/fearing/snow/snow.html&quot; rel=&quot;nofollow&quot;&gt;project page&lt;/a&gt; contains explanations and example images. A PDF is &lt;a href=&quot;https://graphics.stanford.edu/courses/cs448-01-spring/papers/fearing.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An older paper is &lt;a href=&quot;http://www.cse.cuhk.edu.hk/~ttwong/papers/dust/dust.html&quot; rel=&quot;nofollow&quot;&gt;Simulating Dust Accumulation&lt;/a&gt;, published in &lt;em&gt;IEEE Computer Graphics &amp;amp; Applications&lt;/em&gt; in 1995:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;This paper describes a dust modeling technique. An empirical method is used to visually simulate the effect of dust accumulation on object surfaces. The dust amount is first predicted based on the properties of the surfaces: the surface inclination and stickiness. This predicted amount is then adjusted according to some external factors: surface exposure to wind and scraping off by other objects. The calculated dust amount is finally perturbed by a noise function on rendering to give a fuzzy visual effect.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="192" LastEditorUserId="192" LastEditDate="2015-08-18T18:22:01.660" LastActivityDate="2015-08-18T18:22:01.660" CommentCount="0" />
  <row Id="280" PostTypeId="1" AcceptedAnswerId="281" CreationDate="2015-08-18T08:24:37.133" Score="11" ViewCount="197" Body="&lt;p&gt;In general, branching in shaders is not a good idea. But now I have a shader with a condition that is constant with respect to the entire draw call. So the branch that is executed is always the same for one draw call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is such kind of branching still more costly than having multiple shaders without these branches and switch between them?&lt;/p&gt;&#xA;" OwnerUserId="386" LastEditorUserId="127" LastEditDate="2015-08-20T23:30:50.320" LastActivityDate="2016-01-21T10:12:02.363" Title="Is a constant condition more costly than switching shaders?" Tags="&lt;performance&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="10" FavoriteCount="1" />
  <row Id="281" PostTypeId="2" ParentId="280" CreationDate="2015-08-18T09:16:17.933" Score="11" Body="&lt;p&gt;On modern hardware if all invocations in a group follow the same path then the unused path doesn't get evaluated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;in pseudo code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(cond){&#xA;   res = ...&#xA;}else{&#xA;   res = ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;becomes&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(anyInvocationARB(cond)){&#xA;    res1 = ...&#xA;}&#xA;if(anyInvocationARB(!cond)){&#xA;    res2 = ...&#xA;}&#xA;res = cond?res1:res2;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;anyInvocationARB&lt;/code&gt; will be true if any invocation of the shader will have true as &lt;code&gt;cond&lt;/code&gt; (from the opengl extension &lt;a href=&quot;https://www.opengl.org/registry/specs/ARB/shader_group_vote.txt&quot; rel=&quot;nofollow&quot;&gt;ARB_shader_group_vote&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If cond is derivable from uniforms alone then the driver may optimize and evaluate the condition before starting the render and replace the if with a goto to the correct branch. OpenGL has a feature called uniform subroutines that makes it explicit.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2016-01-21T10:12:02.363" LastActivityDate="2016-01-21T10:12:02.363" CommentCount="1" />
  <row Id="283" PostTypeId="1" CreationDate="2015-08-18T17:52:08.047" Score="12" ViewCount="213" Body="&lt;p&gt;&lt;em&gt;I'm interested in how this applies to higher numbers of dimensions too, but for this question I will focus solely on 2D grids.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I know that Perlin noise is not isotropic (direction invariant), and that the underlying square grid shows up enough to be able to identify its orientation. Simplex noise is an improvement on this but its underlying equilateral triangle grid is still not completely obscured.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My intuition is that any attempt to make noise of a particular frequency on a grid will result in a lower frequency in directions not aligned to the grid. So while attempts can be made to disguise this, the noise cannot in principle be isotropic unless it is generated without reference to a grid, allowing the average frequency to be the same in all directions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, with a square grid without noise, with square side length $n$, the frequency of vertices horizontally or vertically is $\frac1n$, whereas the frequency of vertices at 45 degrees (through opposite corners of the squares) is $\frac1{\sqrt{2}n}$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Uzc5Y.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Uzc5Y.png&quot; alt=&quot;Square grid showing length of edge and diagonal&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a random distribution that could be applied to offset the vertex positions that would result in the frequency becoming identical in all directions? My suspicion is that there is no such distribution, but I don't have a way of proving either way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, is there a way of making perfect grid based noise of a given frequency, or should I be focused on other approaches (non-grid based noise or ways of disguising artifacts)?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="137" LastEditDate="2015-11-03T14:48:52.220" LastActivityDate="2015-11-03T14:48:52.220" Title="Is all grid based noise inevitably anisotropic?" Tags="&lt;noise&gt;&lt;grid&gt;" AnswerCount="1" CommentCount="11" FavoriteCount="1" />
  <row Id="284" PostTypeId="1" AcceptedAnswerId="290" CreationDate="2015-08-19T11:08:31.647" Score="5" ViewCount="70" Body="&lt;p&gt;Suppose we already have UV coordinates assigned for mesh vertices,&#xA;how is texture baking implemented?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I guess it will be something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for each coordinate (u, v) in parameter space:&#xA;    (x, y, z) = inverse(u, v) # Get the geometric space coordinate.&#xA;    f = faces(x, y, z) # Get the corresponding face. We may need face's normal for rendering.&#xA;    pixels[u, v] = render(x, y, z, f)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the &lt;code&gt;inverse&lt;/code&gt; function? Is it a projective transformation, or a bilinear transformation?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And how to get the corresponding face of one coordinate (u, v) efficiently?&lt;/p&gt;&#xA;" OwnerUserId="120" LastActivityDate="2015-08-20T05:53:23.373" Title="How is texture baking implemented?" Tags="&lt;rendering&gt;&lt;uv-mapping&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="285" PostTypeId="2" ParentId="284" CreationDate="2015-08-19T14:31:40.733" Score="1" Body="&lt;p&gt;The inverse coordinate is not entirely trivial in all cases. But basically, for a basic triangle mesh with nothing exotic and has non overlapping UV's, you need something like this:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;For the UV coordinate you need to find the corresponding  UV triangle. (step can be eliminated see below)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Then calculate the barycentric triangle coordinate. Most raytracers have simple implementations for this.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;From bary coordinate  to 3d its just&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;b1*p1_3d + b2*p2_3d+ b3*p3_3d&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where b are barycentric cords from step 2. and p's triangle point vectors&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h2&gt;How to accelerate the triangle finding.&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;You can do a a sweeping search in O(log(n)) time. You can also use a a bsp tree to do much the same. This would produce a  O(m*log(n)) algorithm where m is number of samples and n triangles. Sounds good.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Wait&lt;/strong&gt;, we can do better! What if instead of baking each pixel in order we bake each triangle in order. Then you dont have to go find the triangle. Finding the points in a triangle should be trivial. So you get something more akin to a theoretical O(m) algorithm. Which should be both conceptually easier and at least theoretically faster as you eliminate step 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also in the second case you can forego barycentric calculation if you so wish and use an alternate formulation if you wish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5FXdd.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5FXdd.png&quot; alt=&quot;Sweeping one polygon&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Sweeping one polygon at a time is easier and needs not search for polygon in question.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-19T19:29:59.697" LastActivityDate="2015-08-19T19:29:59.697" CommentCount="0" />
  <row Id="286" PostTypeId="1" AcceptedAnswerId="303" CreationDate="2015-08-19T14:36:39.547" Score="6" ViewCount="120" Body="&lt;p&gt;I was looking into ways to implement multiple lights shading and I've noticed &lt;a href=&quot;http://docs.unity3d.com/Manual/RenderTech-ForwardRendering.html&quot;&gt;Unity uses multiple passes to achieve it&lt;/a&gt;. It performs a base pass to apply the brightest directional pixel light + 4 vertex lights + SHs and an additional pass for each pixel light, up to a limit defined by the user on the editor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering, since I previously know how many pixels lights I need to shade, why not doing it in a single pass like vertex lights are done? What are the problems of doing so?&lt;/p&gt;&#xA;" OwnerUserId="250" LastEditorUserId="54" LastEditDate="2015-08-19T17:39:10.050" LastActivityDate="2015-08-21T10:07:42.677" Title="Multiple Lights Shading - One Pass vs Multiple Passes" Tags="&lt;lighting&gt;&lt;shader&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="287" PostTypeId="1" AcceptedAnswerId="348" CreationDate="2015-08-19T22:52:54.560" Score="11" ViewCount="129" Body="&lt;p&gt;I understand how a 1D Fourier transform separates a signal into its component frequencies, but I'm having difficulty understanding how a 2D Fourier transform affects a 2D image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/283/is-all-grid-based-noise-inevitably-anisotropic&quot; title=&quot;Is all grid based noise inevitably anisotropic?&quot;&gt;another question&lt;/a&gt;, &lt;a href=&quot;http://computergraphics.stackexchange.com/users/196/john-calsbeek&quot;&gt;John Calsbeek&lt;/a&gt; linked to an &lt;a href=&quot;http://www.cs.utah.edu/~aek/research/noise.pdf&quot;&gt;interesting paper about measuring the quality of noise functions&lt;/a&gt;. This showed various noise functions and the Fourier transform of each.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this a discrete transform of the pixel data, or a continuous transform of the continuous interpolating function which is used to generate the noise at arbitrary points?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the annular shape analagous to taking 1D Fourier transforms of the line through the centre of the image at every possible angle? Or is the transform for each possible angle also measured across the whole 2D space rather than only along a line through the centre? I'm trying to get an intuitive feel for what changes in the input image correspond to what changes in the Fourier transform.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-26T15:25:57.493" LastActivityDate="2015-11-03T19:18:26.903" Title="How does a 2D Fourier Transform of an image work?" Tags="&lt;noise&gt;&lt;fourier-transform&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="289" PostTypeId="2" ParentId="277" CreationDate="2015-08-20T05:35:41.600" Score="8" Body="&lt;p&gt;There are two cases I can think of where multiple blurs would be performed in succession on a single image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, when performing a large-radius blur, it may reduce the total computation if you first downsample the image (which is a blur) and then perform a smaller-radius blur on the downsampled image.  For example, downsampling an image by 4x and then performing a 10px-wide Gaussian blur on the result would approximate performing a 40px-wide Gaussian blur on the original&amp;mdash;but is likely to be significantly faster due to improved locality in sampling and fewer samples taken overall.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ugYom.png&quot; alt=&quot;Box approximation of a wide Gaussian&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The initial downsampling filter is often simply a box (as shown above), but it can also itself be something more sophisticated, such as a triangle or bicubic filter, in order to improve the approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/xdbOo.png&quot; alt=&quot;Mitchell-Netravali approximation of a wide Gaussian&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a Mitchell-Netravali (cubic) downsample followed by a Gaussian. Interestingly, it turns out that using a Gaussian for the initial downsampling doesn't make such a great approximation if your goal is to use it to produce a larger Gaussian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An initial downsampling step is also frequently used when implementing visual effects like depth of field and motion blur, for similar reasons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A second reason to perform multiple Gaussian blurs is to approximate a non-separable filter by blending between various Gaussians of different radii.  This is commonly used in bloom, for example.  The standard bloom effect works by first thresholding to extract bright objects from the image, then creating several blurred copies of the bright objects (usually using the downsample-then-blur technique just discussed), and finally weighting and summing them together.  This allows artists a greater level of control over the final shape and appearance of the bloom.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZzS3i.png&quot; alt=&quot;Sum of three Gaussians creates a &amp;quot;heavy-tailed&amp;quot; function&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here, for example, is a weighted sum of three Gaussians (red line) which produces a shape that's more narrowly peaked and heavier-tailed than a single Gaussian (blue line).  This is a popular sort of configuration to use for bloom, as the combination of a narrow, bright center with a wide, diffuse halo is visually appealing.  But since this kind of filter shape isn't separable, it's cheaper to make it out of a mixture of Gaussians than to try to filter with it directly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another variation on this idea is the concept of a diffusion profile used with subsurface scattering for skin rendering. Different blur radii may be used for the red, green, and blue channels to approximate the way different wavelengths of light scatter differently, as in &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch14.html&quot;&gt;GPU Gems 3 skin shading chapter&lt;/a&gt; by Eugene d'Eon and Dave Luebke.  In fact, that paper uses a mixture of seven different Gaussians, with different R, G, and B weights for each, to approximate the complicated non-separable, wavelength-dependent scattering response of human skin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch14.html&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YYAga.jpg&quot; alt=&quot;Diffusion profiles from GPU Gems 3 article on skin shading&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-20T05:35:41.600" CommentCount="0" />
  <row Id="290" PostTypeId="2" ParentId="284" CreationDate="2015-08-20T05:53:23.373" Score="5" Body="&lt;p&gt;Texture baking can be accomplished by simply rendering the mesh in texture space.  In other words, you set up a render target matching the size of your texture, and draw the mesh with a vertex shader that sets the output position to the vertex's UV coordinate (appropriately remapped from [0, 1] UV space to [&amp;minus;1, 1] post-projective space).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The vertex shader can still calculate the world-space position, normals, and whatever else it needs and send those down to the pixel shader.  Then the pixel shader can do its thing without even being aware that it's rendering into texture space instead of the usual screen space.  Lighting and shading calculations will work as usual as long as the pixel shader inputs are hooked up correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You don't have to do any complicated inversion operation; you just take advantage of the GPU rasterization hardware, which is happy to draw your triangles at any coordinates you choose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Naturally, this requires that your mesh already has a non-overlapping UV mapping that fits within the unit square, but texture baking doesn't really make sense without that.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-20T05:53:23.373" CommentCount="0" />
  <row Id="291" PostTypeId="2" ParentId="57" CreationDate="2015-08-20T07:48:41.177" Score="2" Body="&lt;p&gt;The following naïve approach will probably not yield as nicely distributed points as the &lt;a href=&quot;http://computergraphics.stackexchange.com/a/251/394&quot;&gt;ones given by Lhf&lt;/a&gt;, but it should be much easier to implement and computationally faster:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For two points $x$ and $y$ let $d(x,y)$ denote the average distance you want points with the average curvature of $x$ and $y$ to have, e.g., some constant multiplied with the inverse of the average curvature of $x$ and $y$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now build up your collection of points $A$ successively:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Select a random point $x$ and add two points, such that all three points form a equilateral triangle with edge length $d(x,x)$.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Add all points to $A$ and mark them as adjacent.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Repeatedly do the following until there are no adjacencies in $A$ anymore:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Select two adjacent points $x$ and $y$ from $A$. Mark them as non-adjacent.&lt;/li&gt;&#xA;&lt;li&gt;Consider a point $z$ that has a distance of $d(x,y)$ from both these points. Of the two possible such points select the one pointing outwards of $A$ (this needs some work, but should be straightforward).&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Check whether $z$ is closer than $d(x,y)$ to any point from $A$ that is still adjacent to another point.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;if yes, discard it.&lt;/li&gt;&#xA;&lt;li&gt;if no, mark $x$ and $z$ as well as $y$ and $z$ as adjacent and add $z$ to $A$.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;At the end, $A$ should be a collection of points matching your criteria. You sort-of just created a triangulation but it may be pathological and thus you should probably triangulate the points afresh.&lt;/p&gt;&#xA;" OwnerUserId="394" LastEditorUserId="137" LastEditDate="2015-11-03T17:35:30.317" LastActivityDate="2015-11-03T17:35:30.317" CommentCount="0" />
  <row Id="292" PostTypeId="1" CreationDate="2015-08-20T10:52:25.733" Score="5" ViewCount="76" Body="&lt;p&gt;I don't know any shader languages. I've heard of GLSL and HLSL, and I'm interested in learning one or both.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there significant differences between them that would make one or other better in certain situations? Is it useful to know both or would either cover most needs?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't want vague answers indicating personal preference. I'm looking for specific measurable differences so that I can decide for myself which will suit me best. I don't have a specific task in mind - I'm hoping to discover whether there is one or other that I can learn and then apply to any future tasks, rather than having to learn a new language for each new task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there are other shader languages which I have not mentioned I would be interested to hear the comparison for those too, provided they are not dependent on any particular GPU manufacturer. I want my code to be portable across different graphics cards.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-20T18:34:36.977" Title="What factors affect which shader language to learn?" Tags="&lt;gpu&gt;&lt;glsl&gt;&lt;shader&gt;&lt;hlsl&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="293" PostTypeId="2" ParentId="292" CreationDate="2015-08-20T11:02:54.187" Score="5" Body="&lt;p&gt;The shader language is bound to the APIs/engines that support it (glsl to openGL &amp;amp; WebGL and hlsl to D3D). There are tools to translate from one to the other but they aren't perfect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means the main reason to pick one over the other is which platform you will be working on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However by the end of the year a new binary shader representation will come out called Spir-V designed for Vulkan and an OpenGL extension for it is expected to follow shortly. This will allow you to write shaders in many languages as long as a translation tool to Spir-V exists (several are in the works including a Python and Haskell tool).&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-08-20T11:02:54.187" CommentCount="1" />
  <row Id="295" PostTypeId="1" CreationDate="2015-08-20T11:42:45.213" Score="9" ViewCount="237" Body="&lt;p&gt;Are the differences between these two APIs minor implementation details that mean once I have learned one I can use it for everything? Or are there reasons for learning one rather than the other if I want to be able to use it in general without having to relearn another API in future? Is one or other more general?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In particular I would like to be able to write for any graphics card, so code is not restricted to only running on a particular manuafacturer's cards or a specific model. I'd also like to be able to write code that still works in the absence of a graphics card (albeit slower).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a difference in how portable code will be across different platforms (operating systems/architectures)? I'm interested in the availability of other libraries that work with these, and whether one or the other leads to fewer licensing restrictions in its wider environment. Anything measurable that would make a difference to whether one of these could be the only one I learn without restricting me.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2016-05-30T17:48:43.137" Title="Does it matter whether I learn OpenGL or Direct3D?" Tags="&lt;opengl&gt;&lt;api&gt;&lt;direct3d&gt;" AnswerCount="4" CommentCount="1" FavoriteCount="2" />
  <row Id="296" PostTypeId="2" ParentId="295" CreationDate="2015-08-20T13:41:44.590" Score="9" Body="&lt;p&gt;I don't think it matters much, which API you want to use when leaning to &quot;program graphics&quot;. The more important things to learn are the typical concepts you use when working with 3D scenes. For example you want to learn how to write a simple (and later more sophisticated) Scene Graph. These concepts are much more important than the specific API method names you have to call. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compare it when learning how to write programs. If you are quite good at writing programs in e.g. Java, you won't have that much trouble learning C++, Objective-C, Swift, or some other object-oriented language. Because it's the concepts and the &lt;em&gt;thinking&lt;/em&gt; which you learn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The choice between OpenGL, Direct3D and Metal is primarily the choice, which operating system you target. Direct3D is primarily available on Windows, Metal on OS X and iOS, and OpenGL is supported on most systems including OS X, iOS, Windows and Linux.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The graphics card probably has little to no influence on this decision as I don't know a card that supports only one or two. If you have no dedicated graphics card, then rendering in real time will be a problem soon. Although a Intel HD Graphics and the AMD companion can already do quite much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, choose your platform.&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: As of now, I did neither use Direct3D nor Metal.&lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-08-20T13:41:44.590" CommentCount="0" />
  <row Id="297" PostTypeId="1" AcceptedAnswerId="414" CreationDate="2015-08-20T14:20:02.697" Score="7" ViewCount="133" Body="&lt;p&gt;When I implement Beer's law (color absorption over distance through an object), it never looks very good for some reason.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When i have the color behind the object, i calculate the adjusted color like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;const vec3 c_absorb = vec3(0.2,1.8,1.8);&#xA;vec3 absorb = exp(-c_absorb * (distanceInObject));&#xA;behindColor *= absorb;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That will give me something that looks like this (note a little bit of refraction applied):&#xA;&lt;a href=&quot;http://i.stack.imgur.com/XQbjV.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/XQbjV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here it is without refraction:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/gOPZn.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gOPZn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this is implemented as a shadertoy here: &lt;a href=&quot;https://www.shadertoy.com/view/4lXSDf&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/4lXSDf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That fulfills the description of what Beer's law does, but it doesn't look very good, not when compared to shots like this:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/nchoX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/nchoX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specular highlights aside, I'm trying to figure out the difference.  Could it just be that my geometry is too simple to really show it off very well? Or am i implementing it incorrectly?&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-09-01T16:25:55.340" LastActivityDate="2015-09-03T01:44:56.440" Title="Is this the correct way to implement Beer's Law?" Tags="&lt;3d&gt;&lt;refraction&gt;&lt;transparency&gt;" AnswerCount="1" CommentCount="8" FavoriteCount="1" />
  <row Id="298" PostTypeId="2" ParentId="295" CreationDate="2015-08-20T14:31:24.183" Score="7" Body="&lt;p&gt;We are currently in a transition of API paradigms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The old school method of binding buffers, uniforms, attribute, layout and programs as (implicit) global state and dispatching draws with that state is common across D3D11 and OpenGL. However it has a large amount of overhead (in verifying state and not knowing what the program wants to do until the last minute). They are also not thread safe (for the most part).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is why new apis have come up (or are coming soon) D3D12, vulkan and Metal being the most prominent. These APIs give more control to the program and lets it declare in advance what it want to do using command buffers. They also let you manage your own resources (vertex data, textures, ...) much more directly. Using them with multiple is much more straight forward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The choice between old and new is based on how well you can manage the video memory you allocate and build the command buffer. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;If you want something that &quot;just works&quot; even on older hardware then the old school APIs are better; they also are better known and you will get more help on them online.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;On the other hand if you can handle the asynchronous nature of dispatching commands and don't mind having to track all the buffers you allocate. Then the new APIs may be something for you. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-08-20T14:31:24.183" CommentCount="1" />
  <row Id="299" PostTypeId="2" ParentId="292" CreationDate="2015-08-20T15:55:56.167" Score="3" Body="&lt;p&gt;The HLSL shader language is used in DirectX family APIs and GLSL for the OpenGL family APIs. This means that the choice of graphics API will limit the shader language as well, so if cross platform is a concern then GLSL will be the choice. However, this comes with some caveats as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One difference is that in OpenGL the shading language is compiled on driver level, this means that the binary representation is not stable and can change with driver updates. In HLSL it is compiled to a hardware independent representation that can be used on multiple GPUs. This should really be a problem for writing shaders, but it sometimes means that certain things work different with other drivers and especially operating systems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another thing to take note is that OpenGL features extensions, these can be enabled inside GLSL to utilize extra functionality. With newer OpenGL versions some of these extensions might be added to the core profile. Vendors then have to implement these extensions to be compliant, which sometimes results in things working different. The HLSL shading language syntax only changes with new introduction of DirectX APIs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some syntactic differences are that HLSL can use #include, in GLSL this is not default. When searching for GLSL tutorials you might find different syntax, because it has changed quite a lot as of OpenGL 3 and 4. GLSL shaders have a void main() as shader entry point. In HLSL this entry point can be named and have input (vertex data) and output (pixel shader input).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Learning either of these will introduce you to some common ground which will make it easy to learn to other one. HLSL is more standardized, but it also only works on a single operating system. There are also other attempts of shader languages that compile to both OpenGL and DirectX, one example is &lt;a href=&quot;https://developer.nvidia.com/cg-toolkit&quot; rel=&quot;nofollow&quot;&gt;Cg&lt;/a&gt; which is deprecated by now. Many game engines (Unity, Unreal Engine) also have their own flavors, but their syntax is like HLSL or GLSL.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As already mentioned Vulkan will introduce an open intermediate language representation called Spir-V. This means that if a language can be compiled to this representation it can be used to write shaders.&lt;/p&gt;&#xA;" OwnerUserId="64" LastEditorUserId="64" LastEditDate="2015-08-20T18:34:36.977" LastActivityDate="2015-08-20T18:34:36.977" CommentCount="4" />
  <row Id="300" PostTypeId="1" AcceptedAnswerId="304" CreationDate="2015-08-20T17:35:29.830" Score="4" ViewCount="32" Body="&lt;p&gt;Imagine you have a 3D level format consisting of an arbitrary number of convex polytope brushes (or we can simplify and consider just oriented-bounding boxes) and several point entities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to find an algorithm to determine&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If the level is water-tight. That is, the brushes are adjacent to each other and fit together to form a hull without gaps or holes.&lt;/li&gt;&#xA;&lt;li&gt;If the level is disjoint, identify the set of brushes in each section.&lt;/li&gt;&#xA;&lt;li&gt;If all entities are inside the water-tight level, and in which section (from (2)) they are in.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;(1) isn't strictly necessary, but it seems necessary to ensure that (2) and (3) are well defined.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All engines I know that use this kind of level format (Doom, Quake, Source) seem to use a BSP algorithm for these tasks. However, they also use the resulting BSP tree as an acceleration structure for rendering, which I'm not interested in using.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is anyone aware of any other possible alternative algorithms, or is BSP the only choice?&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-20T18:02:34.843" LastActivityDate="2015-08-20T20:45:56.550" Title="Determining level connectivity" Tags="&lt;space-partitioning&gt;&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="301" PostTypeId="1" AcceptedAnswerId="302" CreationDate="2015-08-20T18:41:21.510" Score="6" ViewCount="78" Body="&lt;p&gt;Why do we not calibrate all devices to sRGB in the factory? The assumption is that every monitor is sRGB anyway. So why not calibrate them to that? That would make people see same color as intended on a wide range of devices.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-08-21T06:22:23.667" Title="Why not calibrate all display devices to sRGB?" Tags="&lt;color-management&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="302" PostTypeId="2" ParentId="301" CreationDate="2015-08-20T19:05:38.003" Score="6" Body="&lt;p&gt;Simple answer: because it costs. :) Some higher-end monitor models &lt;em&gt;are&lt;/em&gt; indeed calibrated at the factory (and come with a calibration report sheet) but it would not be cost-effective to do so for all models in the product line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Color reproduction varies even between batches of the exact same panel model, so it isn't possible to just use a set calibration for a given monitor model, it need to be done on a unit-by-unit basis. So to save costs, they instead determine the color reproduction curves of the &quot;average&quot; panel and use that for an ok but inconsistent result.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-20T19:42:59.837" LastActivityDate="2015-08-20T19:42:59.837" CommentCount="1" />
  <row Id="303" PostTypeId="2" ParentId="286" CreationDate="2015-08-20T20:34:15.230" Score="2" Body="&lt;p&gt;It's completely possible to do all pixel lights in the fragment shader (or, say, do 4 at a time) by looping over an array. However, this comes at a great performance cost: You're going to be calculating lighting for every light on every fragment of the scene, even if that geometry isn't actually affected by the light's influence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suspect that is reason why Unity doesn't do so: it probably culls the objects and submits only geometry that might be influenced by the respective light on each pass, meaning that shading for a light is only evaluated in the fragments where it may have an effect. This is a trade off between processing less fragments but duplicating the geometry cost.&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-20T20:34:15.230" CommentCount="1" />
  <row Id="304" PostTypeId="2" ParentId="300" CreationDate="2015-08-20T20:45:56.550" Score="2" Body="&lt;p&gt;BSP is a classic for computational geometry tasks involving polygons. However, polygonal meshes are subject to a dismaying list of pathologies such as degenerate polygons, inconsistent winding order, T-junctions, coincident vertices, cracks or holes, self-intersecting geometry, loss of coplanarity or collinearity due to roundoff error, and other such things. Making computational geometry robust on arbitrary polygon meshes is Quite Difficult™.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It worked in Doom/Quake/Half-Life days because the base level geometry (made of brushes and constructed in a level editor, as opposed to a 3D modeling tool), was relatively simple, mostly axis-aligned, and all vertices were snapped to grids. More recently, the trend seems to be for level design seems to be done by creating mesh pieces in 3D modeling tools, then sticking together a bunch of instances of them to form a level. This kind of construction is a &lt;em&gt;lot&lt;/em&gt; less friendly to BSP-type algorithms&amp;mdash;especially as the geometric complexity rises.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An alternative approach for connectivity questions is voxelization. In short, you voxelize all your level geometry at some reasonably small resolution, like 10&amp;ndash;30 cm per voxel or so. The voxelization wipes away any geometric errors smaller than the voxel size, and it gets rid of all those annoying pathologies mentioned earlier. You can use an octree or brickmap etc. to avoid voxelizing empty space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have the voxel representation of the level, you can then do all sorts of connectivity queries on it very easily because of the regular structure of the voxels. For instance, determining if a level is watertight or finding connected-components at the voxel level is basically a flood-fill. These kinds of algorithms can also be done hierarchically using the octree etc. to avoid the expense of a global flood-fill over a huge number of voxels. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more reading, there was a 2011 SIGGRAPH talk &lt;a href=&quot;http://advances.realtimerendering.com/s2011/Chen,%20Silvennoinen,%20Tatarchuk%20-%20Polygon%20Soup%20Worlds%20(Siggraph%202011%20Advances%20in%20Real-Time%20Rendering%20Course).pptx&quot; rel=&quot;nofollow&quot;&gt;Making Game Worlds From Polygon Soup&lt;/a&gt;, from some engineers at Bungie who used voxelization techniques on Halo Reach levels to build nav meshes, precompute visibility cells and portals, and so on.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-20T20:45:56.550" CommentCount="1" />
  <row Id="305" PostTypeId="1" AcceptedAnswerId="307" CreationDate="2015-08-20T20:49:17.700" Score="8" ViewCount="139" Body="&lt;p&gt;Rendering the scene usually involves more than one shader program which, in my case, all use the same attributes and share at least some of the uniforms. To have them working properly, I currently play safe, meaning I rebind the attributes and get the appropriate uniform locations every time I switch between shader programs. So basically multiple times in every frame, which is probably not the best approach.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, is it (in general) necessary to rebind attributes and uniforms after switching shader programs? And why?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, is there a way to do this once at start of the program and never have to touch them again (except for setting the uniform values)?&lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-08-20T22:56:54.543" Title="Do I need to rebind uniforms or attributes when changing shader programs?" Tags="&lt;opengl&gt;&lt;glsl&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="306" PostTypeId="1" AcceptedAnswerId="316" CreationDate="2015-08-20T21:04:22.240" Score="28" ViewCount="4079" Body="&lt;p&gt;Signed Distance Fields (SDFs) was presented as a fast solution to achieve resolution independent font rendering by Valve &lt;a href=&quot;http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf&quot;&gt;in this paper&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already have the Valve solution working but I'd like to preserve sharpness around corners. Valve states that their method can achieve sharp corners by using a second texture channels ANDed with the base one, but lacks to explain how this second channel would be generated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact there's a lot of implementation details left out of this paper. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to know if any of you could point me out a direction to get SDFs font rendering with sharp corners.&lt;/p&gt;&#xA;" OwnerUserId="250" LastActivityDate="2016-04-25T07:59:45.870" Title="Sharp Corners with Signed Distance Fields Fonts" Tags="&lt;texture&gt;&lt;signed-distance-field&gt;&lt;font-rendering&gt;" AnswerCount="3" CommentCount="8" FavoriteCount="20" />
  <row Id="307" PostTypeId="2" ParentId="305" CreationDate="2015-08-20T21:17:33.460" Score="8" Body="&lt;p&gt;You don't need to rebind the attributes, so long as you ensure that their location stays the same in both shaders. (Usually using the &lt;code&gt;layout(location = X)&lt;/code&gt; syntax in GLSL, but can also be done with &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glBindAttribLocation.xhtml&quot;&gt;&lt;code&gt;glBindAttribLocation&lt;/code&gt;&lt;/a&gt; if former is not available.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Uniforms, however, are part of the Shader Object state, and so will need to be set at least once for every shader. One way to minimize the cost of this is to use a &lt;a href=&quot;https://www.opengl.org/wiki/Uniform_Buffer_Object&quot;&gt;Uniform Buffer Object&lt;/a&gt;, which will contain all your uniform values and can then be bound to a shader with a single call.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-20T22:56:54.543" LastActivityDate="2015-08-20T22:56:54.543" CommentCount="0" />
  <row Id="308" PostTypeId="2" ParentId="301" CreationDate="2015-08-21T06:16:49.357" Score="0" Body="&lt;p&gt;There are 2 major factor to this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While it would certainly be possible to calibrate or profile each screen this would add cost to the process. Most users simply aren't that sensitive to this issue so it makes no economic sense to do so. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Its also worth noting that color reproduction is a function of several factors. Such as panel use age, operating temperature etc., but also ambient surroundings. So while this could be done it would be less than stellarly useful for the user in the long run, unless they operate in standardized lighting conditions and never use the monitor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Certainly there is value in factory calibration. It simply means that the monitor can be calibrated to sRGB. And in some usecases the drift caused by age is acceptable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second problem is that is not technically desireable for the panel manufacturers. Forcing something like this on the manufacturers would be a bit like telling people that the the only graphics available graphics api from now on is opengl 2.0 no innovation allowed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Business dictates that the manufacturers need to be able to innovate. Calibrating everything to sRGB would slow down development of panels that are significantly bettter in gamut than sRGB to evolve. Second not all display panels can reach sRGB. Developping ultra cheap panels also has a market function. No need to restrict innovation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Calibrating to sRGB would ultimately not solve all color problems. Convoluted profile to profile conversions would still &#xA;be needed. Display panels are precicion instruments and need to be handled like that if accuracy is drsired. If not then no big deal.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-21T06:22:23.667" LastActivityDate="2015-08-21T06:22:23.667" CommentCount="0" />
  <row Id="309" PostTypeId="1" CreationDate="2015-08-21T07:04:56.937" Score="8" ViewCount="216" Body="&lt;p&gt;I'm trying to figure out what the best way is to generate an OpenGL texture using a compute shader. So far, I've read that pixel buffer objects are good for non-blocking CPU -&gt; GPU transfers, and that compute shaders are capable of reading and writing buffers regardless of how they're bound. Ideally, I'd like to avoid as many copies as possible. In other words, I'd like to allocate a buffer on the GPU, write compressed texture data to it, and then use that buffer as a texture object in a shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, my code looks something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLuint buffer;&#xA;glGenBuffers(1, &amp;amp;buffer);&#xA;glBindBuffer(GL_SHADER_STORAGE_BUFFER, buffer);&#xA;glBufferStorage(GL_SHADER_STORAGE_BUFFER, tex_size_in_bytes, 0, 0);&#xA;glBindBuffer(GL_SHADER_STORAGE_BUFFER, 0);&#xA;&#xA;// Bind buffer to resource in compute shader&#xA;// execute compute shader&#xA;&#xA;glBindBuffer(GL_PIXEL_UNPACK_BUFFER, buffer);&#xA;glCompressedTexImage2D(GL_TEXTURE_2D, 0, fmt, w, h, 0, tex_size_in_bytes, 0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is this correct? I read somewhere about guaranteeing synchronization, too. What do I need to add to make sure that my compute shader completes execution prior to copying from the buffer object?&lt;/p&gt;&#xA;" OwnerUserId="197" LastEditorUserId="48" LastEditDate="2015-08-21T18:33:30.647" LastActivityDate="2015-09-19T01:13:41.147" Title="Writing to a compressed texture using a compute shader, with no extra copies" Tags="&lt;opengl&gt;&lt;texture&gt;&lt;compression&gt;&lt;compute-shader&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="310" PostTypeId="2" ParentId="286" CreationDate="2015-08-21T10:07:42.677" Score="1" Body="&lt;p&gt;If you would do all lights in a single pass then you would need to loop over all lights in the shader. You would also need to get all information to the GPU at the same time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GPUs are limited in the number of uniforms you can pass into an invocation. This limitation has lessened with UBOs and SSBOs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However you are still limited with how many textures you can bind for shadow mapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lights often only affect a small portion of the screen so the engine may only render geometry that is close to the light or adjust the viewport to only render the area the light would affect.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-08-21T10:07:42.677" CommentCount="0" />
  <row Id="311" PostTypeId="1" CreationDate="2015-08-21T13:02:45.887" Score="7" ViewCount="57" Body="&lt;p&gt;Although simulation models like &lt;a href=&quot;https://en.wikipedia.org/wiki/Boids&quot;&gt;Boids&lt;/a&gt; give good results for bird flocks or fish shoals on a small scale, simulating every single member in real time becomes unrealistic for huge numbers. Is there a way I can model a flock in the distance where only the density of birds is visible? I'd like to have that flowing, changing density gradient with a much smaller number of variables to process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried using a much smaller population and displaying each boid as a blurred area with Gaussian density so that as they overlap the density rises and falls through their interaction. This is reasonably cheap but it never leads to sharp changes in density, either spatially or temporally, which makes it look too uniform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any other way of getting away with a much smaller number of individuals? Or is the only way to get realistic results to prerender?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-22T08:02:39.787" Title="Is there a way to achieve the look of a distant flock or shoal without full simulation?" Tags="&lt;real-time&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="312" PostTypeId="1" AcceptedAnswerId="313" CreationDate="2015-08-21T14:51:37.063" Score="16" ViewCount="169" Body="&lt;p&gt;In most text books that I have seen, this is how the rendering equation is written:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$L_0( \omega_0)= L_e(\omega_0)+\int_{\Omega}{f(\omega_i, \omega_0)L_i(\omega_i)\,\mathrm{d}\omega_i}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\Omega$ is defined to be a hemisphere (and all those functions depend on more variables, omitted here for simplicity's sake).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now suppose the surface being rendered is some kind of glass, or some transparent plastic. Why would it make sense to only integrate over a hemisphere? I would imagine that there can be incoming light from any direction, and thus the integration domain should be the entire sphere. How is the light coming from behind the glass accounted for?&lt;/p&gt;&#xA;" OwnerUserId="433" LastEditorUserId="137" LastEditDate="2015-11-03T17:35:49.767" LastActivityDate="2015-11-03T17:35:49.767" Title="Why integrate over a hemisphere (and not a sphere) to solve the rendering equation?" Tags="&lt;transparency&gt;&lt;theory&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="313" PostTypeId="2" ParentId="312" CreationDate="2015-08-21T15:11:24.357" Score="17" Body="&lt;p&gt;The form of the rendering equation that uses only the BRDF ($f$ in your example, often called $f_r$) and integrates over one hemisphere does not account for transmission.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When adding in transmission, it's fairly common to add a second integral over the opposite hemisphere, using a different BTDF function (bidirectional &lt;em&gt;transmission&lt;/em&gt; distribution function). This is equivalent to an integral over the full sphere of directions with a BSDF function, but since that function would usually have to be defined as a piecewise function, writing it as two integrals can be more straightforward.&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="137" LastEditDate="2015-11-03T17:19:11.853" LastActivityDate="2015-11-03T17:19:11.853" CommentCount="2" />
  <row Id="314" PostTypeId="2" ParentId="311" CreationDate="2015-08-21T15:41:40.173" Score="4" Body="&lt;p&gt;One way that might work is to have boids but render each boid as a group of birds.  This way the simulation is still simple but the rendering makes it seem complex.  You might even allow a little drift per bird in the boid to make it look so obviously rigid.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-21T15:52:48.360" LastActivityDate="2015-08-21T15:52:48.360" CommentCount="0" />
  <row Id="315" PostTypeId="1" CreationDate="2015-08-21T20:00:04.647" Score="5" ViewCount="96" Body="&lt;p&gt;I wrote a 2D fluid-solver in OpenGL (&lt;a href=&quot;https://github.com/nilspin/SDL_OpenGL_Project&quot;&gt;code here&lt;/a&gt;) some time back. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;While it runs flawlessly on my onboard Intel GPU, &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/RPBpF.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RPBpF.png&quot; alt=&quot;when running on Intel GPU&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the simulation very quickly &quot;blows-up&quot; when same code is run using nvidia card :  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Xs5Sm.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Xs5Sm.png&quot; alt=&quot;mere half a second into the program&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the second picture, the fluid is being &quot;added&quot; to the system and &quot;diffusing away&quot; too, but unlike the 1st picture there is no advection.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know what might possibly cause this. Could this be because different vendors might be interpreting the standard differently?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS : The &quot;red&quot; and &quot;green&quot; colors represent magnitude of vector-field in &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; directions respectively.&lt;/p&gt;&#xA;" OwnerUserId="437" LastActivityDate="2015-08-21T20:00:04.647" Title="Code runs differently on different hardware" Tags="&lt;opengl&gt;&lt;fluid-sim&gt;" AnswerCount="0" CommentCount="8" FavoriteCount="1" />
  <row Id="316" PostTypeId="2" ParentId="306" CreationDate="2015-08-21T21:26:46.043" Score="5" Body="&lt;p&gt;Adam Simmons has done some interesting work in this area. I don't know specifically how he's achieving it, but his SDF-based vector rendering is the sharpest I've seen in practice outside of Valve. &lt;a href=&quot;http://twitter.com/adamjsimmons/status/611677036545863680&quot;&gt;http://twitter.com/adamjsimmons/status/611677036545863680&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="104" LastActivityDate="2015-08-21T21:26:46.043" CommentCount="1" />
  <row Id="317" PostTypeId="2" ParentId="295" CreationDate="2015-08-22T04:54:36.683" Score="3" Body="&lt;p&gt;I do not personally think it matters much. Just pick one that suits your project. I have used both D3D and OpenGL in the past. It is the concepts that matter. Whichever you grab, you need to understand (for example):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What textures are and how they are used by GPU.&lt;/li&gt;&#xA;&lt;li&gt;Basic concepts of Graphics Development (Vertices, Primitives, Fragments, etc.)&lt;/li&gt;&#xA;&lt;li&gt;How does the camera system work (MVP matrices in OpenGL for example).&lt;/li&gt;&#xA;&lt;li&gt;What shaders are and how to use them properly.&lt;/li&gt;&#xA;&lt;li&gt;How different a GPU works versus the CPU on your machine.&lt;/li&gt;&#xA;&lt;li&gt;What are the differences between GPU's memory model and CPU's memory model.&lt;/li&gt;&#xA;&lt;li&gt;What should be done on CPU and what should be offloaded to GPU for processing.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;As others mentioned here, we are in the middle of a transition to a new set of APIs (Vulkan, Metal, etc.) so at this point if you are completely new to Graphics Development, probably focusing on GLSL is a good idea since Vulkan is going to take advantage of it as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding portability, D3D is Windows only (There are rumors that the &lt;a href=&quot;http://winehq.com/&quot; rel=&quot;nofollow&quot;&gt;Wine&lt;/a&gt; project is trying to get D3D working under Linux but nothing feasible so far). OpenGL is cross-platform. If you want to go mobile, then OpenGL ES is an available option. Of course both OpenGL and OpenGL ES have different versions which are not supported by all platforms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the end of the day, all APIs access the same hardware on your machine, it is just a matter of &quot;how&quot; they access it.&lt;/p&gt;&#xA;" OwnerUserId="307" LastEditorUserId="307" LastEditDate="2015-08-22T05:00:20.590" LastActivityDate="2015-08-22T05:00:20.590" CommentCount="1" />
  <row Id="318" PostTypeId="1" AcceptedAnswerId="353" CreationDate="2015-08-22T07:21:29.817" Score="5" ViewCount="266" Body="&lt;p&gt;It is a well known &quot;standard&quot; to use bilateral upscaling when it comes to comes to combine a low resolution target and an higher res one. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have personally noticed that using the basic algorithm (with weights based on depth differences between high and low res depth values) is far from perfect in situations where high res and low res are blended, say for example an high res object inside a low res effect.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Jagged edges are still noticeable&lt;/li&gt;&#xA;&lt;li&gt;&quot;haloes&quot; around edges start to appear when the objects/effects in low res occupy a smaller area on the target. By this I mean holes around the silouhette. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am well aware that there is no way to eliminate completely these problems, but is there a way to improve bilateral upsampling to partially improve on one of the two points above? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And as side question, is there an equivalently good algorithm or variation that is cheaper?&lt;/p&gt;&#xA;" OwnerUserId="100" LastActivityDate="2015-08-26T22:00:26.977" Title="How to improve on bilateral upsampling in real time scenarios" Tags="&lt;real-time&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="319" PostTypeId="2" ParentId="311" CreationDate="2015-08-22T08:02:39.787" Score="4" Body="&lt;p&gt;I'm not sure if this counts, but you can use fluid dynamics to render large crowds (of birds, people, etc). With SPH (smoothed particle hydrodynamics) to model the &quot;fluid&quot;, you aren't really describing the motion of each bird per-se, since you can sample the crowd of birds with a representative set, and then draw birds around your samples. Take a look at this paper that compares the different methods:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://gamma.cs.unc.edu/lookahead/golas-2013-hybridcrowd.pdf&quot; rel=&quot;nofollow&quot;&gt;http://gamma.cs.unc.edu/lookahead/golas-2013-hybridcrowd.pdf&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-08-22T08:02:39.787" CommentCount="2" />
  <row Id="320" PostTypeId="1" AcceptedAnswerId="1477" CreationDate="2015-08-22T08:13:25.317" Score="9" ViewCount="83" Body="&lt;p&gt;When an image is encoded using JPEG, the RGB pixels are first encoded into YUV, and then the UV channels are downsampled. Before actually doing the DCT and encoding the coefficients, JPEG &lt;a href=&quot;http://www.w3.org/Graphics/JPEG/jfif3.pdf&quot;&gt;doesn't gamma correct&lt;/a&gt; the Y channel. Is this correct? Shouldn't we determine the DCT coefficients that will most affect our viewed result?&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-09-11T08:22:01.553" Title="Should JPEG encoding take into account gamma?" Tags="&lt;compression&gt;&lt;gamma&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="321" PostTypeId="2" ParentId="320" CreationDate="2015-08-23T04:25:08.513" Score="6" Body="&lt;p&gt;The short answer is &quot;no&quot;, for reasons covered in &lt;a href=&quot;http://www.alvyray.com/memos/CG/Microsoft/9_gamma.pdf&quot;&gt;Alvy Ray Smith's memo, &lt;em&gt;Gamma Correction&lt;/em&gt;&lt;/a&gt;. Gamma is not about nonlinearity in human perception, it's about nonlinearity in display devices (and, I suppose, acquisition devices too).&lt;/p&gt;&#xA;" OwnerUserId="159" LastActivityDate="2015-08-23T04:25:08.513" CommentCount="0" />
  <row Id="323" PostTypeId="1" CreationDate="2015-08-23T13:42:45.053" Score="9" ViewCount="110" Body="&lt;p&gt;I'd like to simulate the magnification of very distant objects by the lensing effect of a less distant galaxy. Will I need to model large numbers of point masses or can I get away with just a single average point mass?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can see how to raytrace using hyperbolae for the rays influenced by a single point mass, but I wouldn't know where to begin with multiple point masses. So before I attempt to build this raytracer I'd like to know whether I'm going to be able to avoid multiple masses, and still have believable results.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-09-01T16:29:40.593" Title="Can I raytrace gravitational lensing using only a single point source of gravity?" Tags="&lt;raytracing&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="324" PostTypeId="1" AcceptedAnswerId="412" CreationDate="2015-08-23T19:38:23.123" Score="4" ViewCount="132" Body="&lt;p&gt;I've seen a number of 2D Poisson disc sampling algorithms online that use a grid to accelerate checking for existing points within the minimum radius [![r][r image]][r link] of a candidate point. For example:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://devmag.org.za/2009/05/03/poisson-disk-sampling/&quot; rel=&quot;nofollow&quot;&gt;Dev.Mag&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://bl.ocks.org/mbostock/dbb02448b0f93e4c82c3&quot; rel=&quot;nofollow&quot;&gt;mbostock&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;They use a grid of squares of side $\frac{r}{\sqrt2}$, which is the same side length that I intuitively came up with when implementing this myself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can see the reason - that is the largest square that cannot contain more than 1 point (assuming the minimum is not attainable - the distance between two points must be &lt;em&gt;strictly&lt;/em&gt; greater than $r$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, having thought about it further, I adjusted the grid size to $\frac{r}2$ instead. This finer grid means 4 additional squares need to be checked (the 4 corner squares are now within the radius), but the total area covered by the required squares is less, so that on average fewer points will need to go through the Euclidean distance check. The difference can be visualized using the same style as the diagram in the first linked article.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/cE5KI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cE5KI.png&quot; alt=&quot;Grid with side length r over root 2&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/r3PJ9.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/r3PJ9.png&quot; alt=&quot;Grid with side length r over 2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a candidate new point, existing points must be checked in all squares that are within a radius $r$ of the corners of the candidate's square. Here the two grid sizes are shown side by side, to scale, for the same radius $r$. This shows clearly that a significantly smaller area is being checked. Each square is exactly half the area of the previous approach, and even if the 4 outer corner squares are excluded in the previous approach (left image), this still gives an area $2 \cdot \frac{21}{25} = 1.68$ times larger than in the new approach.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My main question is this: &lt;strong&gt;Is this approach still correct, and does it give identical results?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm also interested to know whether there is any reason to favor the $\frac{r}{\sqrt2}$ approach. Using $\frac{r}{2}$ seems more efficient in time, which seems worth the cost in space efficiency. Is there anything I'm missing?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;em&gt;Images produced with this &lt;a href=&quot;https://jsfiddle.net/wkneysgj/&quot; rel=&quot;nofollow&quot;&gt;jsfiddle&lt;/a&gt; (in case I need to edit them later).&lt;/em&gt;&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="137" LastEditDate="2015-11-03T17:35:05.190" LastActivityDate="2015-11-03T17:35:05.190" Title="Is this smaller grid for Poisson disc sampling still correct?" Tags="&lt;grid&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="325" PostTypeId="1" CreationDate="2015-08-24T16:52:15.143" Score="6" ViewCount="65" Body="&lt;p&gt;I'd like to know if there's any performance impact based on how I struct my shader uniforms. For instance, is passing 4 floats worse than passing a single vec4?&lt;/p&gt;&#xA;" OwnerUserId="250" LastActivityDate="2015-08-25T05:19:19.273" Title="Shader uniform performance" Tags="&lt;performance&gt;&lt;glsl&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="326" PostTypeId="1" AcceptedAnswerId="425" CreationDate="2015-08-24T17:59:01.563" Score="4" ViewCount="83" Body="&lt;p&gt;To render an image for use with red &amp;amp; blue 3d glasses, the usual way to do it is to render from one point of view, convert it to a single intensity (greyscale) value per pixel, and then put that into the red color channel.  Render from a slightly different point of view, convert that to greyscale again and put that into the blue channel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can be prohibitive when rendering a single time is already very costly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any methods by which you could take a single render, and corresponding depth buffer, and come up with something suitable for both red and blue channels?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-09-03T15:09:52.093" Title="Is it possible to render red / blue 3d from one image and a depth buffer?" Tags="&lt;rendering&gt;&lt;3d&gt;&lt;stereo-rendering&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="327" PostTypeId="1" AcceptedAnswerId="328" CreationDate="2015-08-25T00:01:43.080" Score="5" ViewCount="104" Body="&lt;p&gt;When applying multiple textures to a mesh, like for bump-mapping, I usually bind the textures to the first few fixed texture units, e.g.: diffuse = unit 0, bump = unit 1, specular = unit 2, then keep reusing those to each different mesh with different textures. But I've always wondered why &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glActiveTexture.xhtml&quot;&gt;&lt;code&gt;glActiveTexture&lt;/code&gt;&lt;/a&gt; supports so many texture units (in the previous link, it says at least 80).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So it occurred to me that one possible way of managing textures is to bind distinct textures to each available unit and leave them enabled, just updating the uniform sampler index. That should improve rendering perf by reducing the number of textures switches. If you have less textures than the max texture units, you never have to unbind a texture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this standard practice on real-time OpenGL applications (I believe this also applies to D3D)? And are there any non obvious performance implications of taking this approach? Memory overhead perhaps?&lt;/p&gt;&#xA;" OwnerUserId="54" LastActivityDate="2015-08-25T04:34:12.847" Title="Is it good practice to use all the available texture units?" Tags="&lt;opengl&gt;&lt;texture&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="328" PostTypeId="2" ParentId="327" CreationDate="2015-08-25T00:33:25.640" Score="7" Body="&lt;p&gt;Modern hardware doesn't really have the concept of texture binding points as exposed by OpenGL. Rather, the shader unit uses a descriptor (which is just some kind of fat pointer) which can potentially address any texture as long as it's resident in video memory. This is what makes things like &lt;a href=&quot;https://www.opengl.org/registry/specs/ARB/bindless_texture.txt&quot;&gt;bindless textures&lt;/a&gt; possible. So the large amount of &quot;texture units&quot; available in current implementations is simply trying to handwave this now-irrelevant part of the API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, the recommended way of implementing what you're trying to do (avoiding re-binding of textures) is to use &lt;a href=&quot;https://www.opengl.org/wiki/Array_Texture&quot;&gt;texture arrays&lt;/a&gt;, which allow you to dynamically index into a set of textures in the shader, as long as they all have the same format and size. This presentation contains more details about these and other techniques for reducing driver overhead when rendering with modern OpenGL: &lt;a href=&quot;http://www.slideshare.net/CassEveritt/approaching-zero-driver-overhead&quot;&gt;Approaching Zero Driver Overhead&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-08-25T04:34:12.847" LastActivityDate="2015-08-25T04:34:12.847" CommentCount="1" />
  <row Id="329" PostTypeId="1" AcceptedAnswerId="331" CreationDate="2015-08-25T04:14:15.283" Score="2" ViewCount="46" Body="&lt;p&gt;I am implementing a trackball, i.e. a camera orbiting about a fixed point, in my case the origin. When I do left click with the mouse and start moving it, I compute delta values for the angles of a sphere centered at the origin given the displacement in x and y directions of the mouse. My implementation is as follows&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void SimplePerspectiveCamera::&#xA;mouseMove(const double&amp;amp; mouseX, const double&amp;amp; mouseY)&#xA;{&#xA;    if(m_setPan)&#xA;    {&#xA;        float sensitivity = 0.0075f;&#xA;        float newX = (mouseX - m_prevX) * sensitivity;&#xA;        float newY = (mouseY - m_prevY) * sensitivity;&#xA;        // TODO: Understand if it is necessary to reset the up vector to (0,1,0)&#xA;        m_up = Vector(0.0f, 1.0f, 0.0f);&#xA;        setLookAt(m_lookAt + Vector(-newX, newY, 0.0f));&#xA;        setPosition(m_position + Vector(-newX, newY, 0.0f));&#xA;        m_prevX = mouseX;&#xA;        m_prevY = mouseY;&#xA;    }&#xA;    if(m_setRotate)&#xA;    {&#xA;        float sensitivity = 0.005f;&#xA;        float newPhi = (mouseX - m_prevX) * sensitivity;&#xA;        float newTheta = (mouseY - m_prevY) * sensitivity;&#xA;        // Transform to spherical coordinates to use mouse move as deltas for&#xA;        // the angles.&#xA;        float r = m_position.norm();&#xA;        float theta = std::acos(m_position.y / r);&#xA;        float phi = std::atan(m_position.x / m_position.z);&#xA;&#xA;        theta += newTheta;&#xA;        phi += newPhi;&#xA;&#xA;        m_up = Vector(0.0f, 1.0f, 0.0f);&#xA;        Vector newPosition(r*std::sin(theta)*std::sin(phi), r*std::cos(theta),&#xA;                r*std::sin(theta)*std::cos(phi));&#xA;        setPosition(newPosition);&#xA;        m_prevX = mouseX;&#xA;        m_prevY = mouseY;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I was expecting is to be able to rotate the camera around the cube in the scene so that I can see all around it. My code almost achieves this, but I don't understand why after rotating a certain amount the camera jumps to the exact opposite side of the cube.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can check what weird swap I am talking about &lt;a href=&quot;https://vimeo.com/137210181&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I missing so that the rotation around the object goes all the way around it?&lt;/p&gt;&#xA;" OwnerUserId="116" LastEditorUserId="116" LastEditDate="2015-08-25T04:46:15.257" LastActivityDate="2015-08-25T05:04:30.273" Title="Why is the color in the cube being weirdly swapped?" Tags="&lt;transformations&gt;&lt;trackball&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="330" PostTypeId="2" ParentId="326" CreationDate="2015-08-25T04:59:24.840" Score="4" Body="&lt;p&gt;Yes, this is essentially the same problem that occours in paralax mapping. What you basically have is a colored height field. That needs to be rendered from a second view.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are several ways in which you can approach this. You could cheat and just shift pixels by their depth and ignore occlusion. Or you could just render a textured height map as if every pixel would be opaque. Or you could use any of the available paralax mapping methods to eliminate occlusion. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How well this works depends on the. A scene multiple transparent surfaces wouldnt work very well without some deep map.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-25T05:06:26.757" LastActivityDate="2015-08-25T05:06:26.757" CommentCount="2" />
  <row Id="331" PostTypeId="2" ParentId="329" CreationDate="2015-08-25T05:04:30.273" Score="5" Body="&lt;p&gt;It is likely that your specific problem arises from a singularity in &lt;code&gt;atan&lt;/code&gt; when &lt;code&gt;m_position.z&lt;/code&gt; goes to 0. Rather than try to debug that, I'll point out that you don't need to transform your camera position into an angle to then modify it: rather, just create a rotation from your &lt;code&gt;newPhi&lt;/code&gt; and &lt;code&gt;newTheta&lt;/code&gt; angles, in the form of a rotation matrix or quaternion, and then apply it to your vector, directly rotating it without doing the perilous round-trip to an angle.&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-25T05:04:30.273" CommentCount="2" />
  <row Id="332" PostTypeId="2" ParentId="325" CreationDate="2015-08-25T05:19:19.273" Score="2" Body="&lt;p&gt;In any modern desktop hardware, there shouldn't be: uniform buffers will just be memory blobs read by the shader. (See also: &lt;a href=&quot;https://www.opengl.org/wiki/Uniform_Buffer_Object&quot; rel=&quot;nofollow&quot;&gt;Uniform Buffer Objects&lt;/a&gt;)&lt;/p&gt;&#xA;" OwnerUserId="327" LastActivityDate="2015-08-25T05:19:19.273" CommentCount="0" />
  <row Id="333" PostTypeId="1" AcceptedAnswerId="334" CreationDate="2015-08-25T05:32:18.040" Score="2" ViewCount="47" Body="&lt;p&gt;Im developing a deep texture processing engine. Right now I have a software renderer to generate the raster samples. Now, for bigger renders it would be nice to do this on hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a standard mechanism on how to capture the data of each shader evaluation in OpenGL? I could do some sort of depth peeling but it seems to me as a bit too brute forcelike approach. I don't mind the time cost just as long as it does not take ages to implement the infrastructure needed.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="127" LastEditDate="2015-08-25T06:12:31.683" LastActivityDate="2015-08-25T06:53:55.193" Title="Is there a way for me to record every shading sample for a deep texture" Tags="&lt;opengl&gt;&lt;rendering&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="334" PostTypeId="2" ParentId="333" CreationDate="2015-08-25T06:53:55.193" Score="4" Body="&lt;p&gt;If you really want to obtain every fragment (shading sample) within the camera frustum, regardless of occlusion, one way to do it would be to disable depth testing, and then use a large &lt;a href=&quot;https://www.opengl.org/wiki/Shader_Storage_Buffer_Object&quot; rel=&quot;nofollow&quot;&gt;SSBO&lt;/a&gt; together with an &lt;a href=&quot;https://www.opengl.org/wiki/Atomic_Counter&quot; rel=&quot;nofollow&quot;&gt;atomic counter&lt;/a&gt; to append data into the buffer from the fragment shader.  Each fragment could then store a record of its screen position, depth, color, and any other desired information into the buffer to be read out later.  The buffer has to be large enough to store every fragment rendered, as it will throw away any extra data once it's full.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using one huge SSBO for the whole screen likely isn't ideal, as there will be huge contention on its counter with every fragment trying to increment it.  A better way might be to slice up the screen in tiles with an SSBO+counter per tile.  There is a limit of 16 bound SSBOs on all current GPUs, and on AMD GPUs, there is a further limit of 8 atomic counters (source: Sascha Willem's &lt;a href=&quot;http://delphigl.de/glcapsviewer/gl_listreports&quot; rel=&quot;nofollow&quot;&gt;OpenGL Hardware Database&lt;/a&gt;).  Those limits will determine how many tiles you can create. This will reduce contention and also ensure that even if you run out of memory in one tile, you'll still have valid data in the others.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-25T06:53:55.193" CommentCount="0" />
  <row Id="335" PostTypeId="1" AcceptedAnswerId="343" CreationDate="2015-08-25T10:58:31.680" Score="8" ViewCount="306" Body="&lt;p&gt;I'm trying to understand NURBS curves (surfaces later!) but I have some trouble understanding the very basics of its inner workings. Could someone please explain a few things to me? As I come from Bezier curves, a comparison between these two would be especially useful.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-uniform_rational_B-spline#Technical_specifications&quot;&gt;&quot;rational basis function&quot;&lt;/a&gt; looks a tiny bit like Bernstein polynomial of (rational) Bezier curve. Does the parameter &lt;code&gt;u&lt;/code&gt; also go from 0 to 1?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How to &quot;add detail&quot; to a curve? I mean, with Beziers if I needed to describe a more complicated shape I would simply &quot;stitch&quot; several Beziers together. Or less often, increase the degree. I understand I can increase the degree of NURBS as well and put several NURBS curves side-by-side, but is this how it should be done?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The Wikipedia article, at least to me, seems not very clear about this &quot;knot vector&quot;. What is it anyway?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="141" LastEditorUserId="38" LastEditDate="2015-08-25T15:31:50.937" LastActivityDate="2015-08-27T04:39:05.643" Title="Non-uniform rational B-spline (NURBS) Basics" Tags="&lt;nurbs&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="336" PostTypeId="1" AcceptedAnswerId="347" CreationDate="2015-08-25T13:58:27.733" Score="4" ViewCount="162" Body="&lt;p&gt;I'm not sure how practical this might be but is it possible to use Fast Fourier Transform to rotate a raster image?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be honest, I never really understood FFT, but I saw it being used for JPEG, for example. What I try to say is let's pretend I can grab some FFT library, what do I do next?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I read that there is this &lt;a href=&quot;https://en.wikipedia.org/wiki/Gibbs_phenomenon&quot; rel=&quot;nofollow&quot;&gt;Gibbs phenomenon&lt;/a&gt; which causes ringing artifacts. Would it be a problem here as well?&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-11-03T19:15:29.400" Title="Image rotation using FFT" Tags="&lt;transformations&gt;&lt;fourier-transform&gt;&lt;raster-image&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="337" PostTypeId="1" AcceptedAnswerId="344" CreationDate="2015-08-25T14:04:58.757" Score="7" ViewCount="383" Body="&lt;p&gt;Radiosity is basically what allows this:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/j9CFM.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/j9CFM.jpg&quot; alt=&quot;Direct Illumination VS Radiosity&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a tutorial of &lt;a href=&quot;http://www.graphics.cornell.edu/online/tutorial/radiosity/&quot;&gt;Cornell University about Radiosity&lt;/a&gt; it is mentioned that:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;A ray-traced version of the image shows only the light reaching the viewer by direct reflection -- hence misses the color effects.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However in &lt;a href=&quot;https://en.wikipedia.org/wiki/Radiosity_%28computer_graphics%29&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;...&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)#Confusion_about_terminology&quot;&gt;radiosity method&lt;/a&gt; in the current computer graphics context derives from (and is fundamentally the same as) the radiosity method in heat transfer.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And if &lt;a href=&quot;https://en.wikipedia.org/wiki/Ray_tracing_%28graphics%29&quot;&gt;ray tracing&lt;/a&gt; is capable of:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;simulating a wide variety of optical effects, such as reflection (&lt;a href=&quot;https://en.wikipedia.org/wiki/Diffuse_reflection&quot;&gt;diffuse reflection&lt;/a&gt;) and &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_scattering&quot;&gt;&lt;strong&gt;scattering&lt;/strong&gt; (i.e. the deflection of a ray from a straight path, for example by irregularities in the propagation medium, particles, or in the interface between two media)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Has that tutorial not considered these effects or are there radiosity methods that can be used in ray tracing in order to enable them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If not, couldn't these optical effects simulate radiosity entirely or is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)#Overview_of_the_radiosity_algorithm&quot;&gt;radiosity algorithm&lt;/a&gt; more efficient in solving the diffuse reflection problem?&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-08-25T22:00:23.613" Title="Radiosity VS Ray tracing" Tags="&lt;raytracing&gt;&lt;reflection&gt;&lt;scattering&gt;&lt;radiosity&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="338" PostTypeId="1" CreationDate="2015-08-25T14:05:06.773" Score="5" ViewCount="75" Body="&lt;p&gt;One of the benefits of NURBS curve over, say Bezier curve, is the ability to create offset curves exactly. How to proceed with such computation? Do I just translate the control points?&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2015-08-25T14:17:11.867" LastActivityDate="2015-09-23T20:49:14.113" Title="NURBS curve offset" Tags="&lt;nurbs&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="339" PostTypeId="1" AcceptedAnswerId="341" CreationDate="2015-08-25T14:07:20.570" Score="6" ViewCount="57" Body="&lt;p&gt;How to draw a NURBS curve?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compared with Bezier curve, I just evaluate the Bernstein polynomial, multiply it with control point positions and that's it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at the &quot;General form of a NURBS curve&quot; paragraph of &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-uniform_rational_B-spline#Technical_specifications&quot;&gt;NURBS Wikipedia page&lt;/a&gt; I have a hard time seeing a polynomial in it. Maybe this &quot;basis function&quot; is a polynomial in the end? Is there an efficient way of constructing the basis function and evaluating it?&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-08-25T17:05:45.507" Title="NURBS curve drawing" Tags="&lt;nurbs&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="340" PostTypeId="1" AcceptedAnswerId="352" CreationDate="2015-08-25T14:08:56.157" Score="7" ViewCount="187" Body="&lt;p&gt;Splitting Bezier curve into two parts at some parameter &lt;code&gt;t&lt;/code&gt; is easy thanks to &lt;a href=&quot;https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm&quot;&gt;De Casteljau's algorithm&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a similar algorithm for NURBS curves? How to split a NURBS curve?&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2015-08-25T14:18:49.773" LastActivityDate="2015-08-27T01:43:13.840" Title="Splitting of NURBS curves" Tags="&lt;nurbs&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="341" PostTypeId="2" ParentId="339" CreationDate="2015-08-25T14:24:03.697" Score="6" Body="&lt;p&gt;A NURBS differs from a Bezier curve in two ways...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) It's rational which means that it's one curve divided by another. (rational comes from &quot;ratio&quot;, not anything to do with it's temperament!)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Being rational is not an issue though for being simple to evaluate.  There are even such things as rational Bezier curves, where again, you just divide one Bezier curve by another to get the final result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows you to make shapes with Bezier curves that you wouldn't be able to otherwise, for instance, rational Bezier curves can EXACTLY represent conic sections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's an HTML5 demo where you can play with a rational Bezier curve:&#xA;&lt;a href=&quot;http://demofox.org/bezquadrational.html&quot; rel=&quot;nofollow&quot;&gt;http://demofox.org/bezquadrational.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, here is a shadertoy which uses a rational Bezier curve to calculate sin!&#xA;&lt;a href=&quot;https://www.shadertoy.com/view/XtfSRH&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/XtfSRH&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) The second difference is the one that makes it more difficult to evaluate, at least for me.  The second difference is that a NURBS, like a B-spline, can have any number of control points, but has a fixed degree.  How this works is that any given point on the curve is only influenced by a specific number of control points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could have for example a quadratic curve with 8 control points,  where each point on the curve was only defined by 3 of the control points at a time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where the influence of the control points begin and end is called the knot vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So basically, if you can evaluate a rational curve, and you can evaluate a b-spline, you will then know how to evaluate a NURBS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Explaining how to evaluate a b-spline is too long for an answer I think, and there is lots of info on the web, and in text books, so i'll link to a couple things that might help you!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Read up on De Boor's algorithm, its the b-spline equivalent of De Casteljeau's algorithm:&#xA;&lt;a href=&quot;https://en.m.wikipedia.org/wiki/De_Boor%27s_algorithm&quot; rel=&quot;nofollow&quot;&gt;https://en.m.wikipedia.org/wiki/De_Boor%27s_algorithm&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a shadertoy rendering a b-spline in a pixel shader:&#xA;&lt;a href=&quot;https://www.shadertoy.com/view/MtfSRN&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/MtfSRN&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is info on how to evaluate a b-spline:&#xA;&lt;a href=&quot;http://web.mit.edu/hyperbook/Patrikalakis-Maekawa-Cho/node18.html&quot; rel=&quot;nofollow&quot;&gt;http://web.mit.edu/hyperbook/Patrikalakis-Maekawa-Cho/node18.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, here is some code (GLSL) that shows how to evaluate an 8 control point cubic B-spline:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float N_i_1 (in float t, in float i)&#xA;{&#xA;    // return 1 if i &amp;lt; t &amp;lt; i+1, else return 0&#xA;    return step(i, t) * step(t,i+1.0);&#xA;}&#xA;&#xA;float N_i_2 (in float t, in float i)&#xA;{&#xA;    return&#xA;        N_i_1(t, i)       * (t - i) +&#xA;        N_i_1(t, i + 1.0) * (i + 2.0 - t);&#xA;}&#xA;&#xA;float N_i_3 (in float t, in float i)&#xA;{&#xA;    return&#xA;        N_i_2(t, i)       * (t - i) / 2.0 +&#xA;        N_i_2(t, i + 1.0) * (i + 3.0 - t) / 2.0;&#xA;}&#xA;&#xA;float N_i_4 (in float t, in float i)&#xA;{&#xA;    return&#xA;        N_i_3(t, i)       * (t - i) / 3.0 +&#xA;        N_i_3(t, i + 1.0) * (i + 4.0 - t) / 3.0;&#xA;}&#xA;&#xA;float SplineValue(in float t)&#xA;{&#xA;    return&#xA;        P0 * N_i_4(t, 0.0) +&#xA;        P1 * N_i_4(t, 1.0) +&#xA;        P2 * N_i_4(t, 2.0) +&#xA;        P3 * N_i_4(t, 3.0) +&#xA;        P4 * N_i_4(t, 4.0) +&#xA;        P5 * N_i_4(t, 5.0) +&#xA;        P6 * N_i_4(t, 6.0) +&#xA;        P7 * N_i_4(t, 7.0);   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I guess your question boils down to:  Is there a way to do De Boor's algorithm as an equivalent equation, the same way Bernstein polynomials are an equation form of De Casteljeau's algorithm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure, but since there are &quot;branches&quot; (see N_i_1 in the glsl code), it seems like it'd be difficult.  Maybe someone else will have a more direct answer to that part of it though.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-08-25T17:05:45.507" LastActivityDate="2015-08-25T17:05:45.507" CommentCount="2" />
  <row Id="342" PostTypeId="1" CreationDate="2015-08-25T14:55:49.330" Score="3" ViewCount="89" Body="&lt;p&gt;I thought of this after my &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/337/radiosity-vs-ray-tracing&quot;&gt;radiosity vs ray tracing question&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A radiosity algorithm seems to require &lt;a href=&quot;https://en.wikipedia.org/wiki/Radiosity_%28computer_graphics%29&quot; rel=&quot;nofollow&quot;&gt;increased calculations because it's viewpoint independent&lt;/a&gt; and therefore, &lt;a href=&quot;https://en.wikipedia.org/wiki/False_radiosity&quot; rel=&quot;nofollow&quot;&gt;false radiosity&lt;/a&gt; was born (at least it's the way it was explained). It is mentioned that in order to simulate radiosity, changes were done directly in texture maps (in PS for example) or omni-type lights are placed where radiosity effects occur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if false radiosity is still used nowadays (I'm assuming this may happen more often in games development). If so, are these techniques currently used or are there others methods to achieve the same purpose? Any examples would be a plus.&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-10-13T06:34:33.383" Title="Are there specific techniques for generating False Radiosity in surfaces?" Tags="&lt;radiosity&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="343" PostTypeId="2" ParentId="335" CreationDate="2015-08-25T16:12:23.277" Score="6" Body="&lt;p&gt;B-Splines and Beziers are parallel inventions of more or less the same thing. Where Beziers try to start from the idea of fitting tangents. B-Splines start with the idea of basis functions. NURB Splines (or the rational part in fact) are just generalizations of B-Splines so you can describe accurate conic sections*, as they are of special interest in engineering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First let us start with a simple NURB Spline terminology. The rationale of these curves is a bit different than for Beziers. First there is the concept of a span. A span would roughly be equivalent to a whole Bezier spline except in nurbs you can have any number of spans. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mHFPu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: One cubic NURBS span. This is a bit atypical in formulation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each span is formed by the curve degree + 1  control points**. Each curve can consist of any number of points. Each consecutive span reuses the points form previous span by dropping one point and taking one point more in the list. So making more complex curves is as easy as just appending more points to the curve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: The images curves are a bit atypically parametrized, ill explain what this means in the next section. When i take the concept of knots up. This is just a easier way to explain how the curves glue together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/PzKKi.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: 2 cubic spans after each other, each span uses 4 points. together they form one curve. They share most points with each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By now we have probably answered the 2 question about adding complexity. But I would like to add that this scheme ensures better continuity than a bezier curve. Additionally you can make the point array that forms the hull cyclic. Forming a closed curve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/rolEZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: A closed cubic NURBS surface has as many spans as it has points. Each color is one span.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Parametrization&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Up until this point one could just say that the stringing together the spans is a trick just as &quot;sewing&quot; the Bezier curves. But there is a difference. The curve is parametrisized along its length. So the curves are not separate they do not interpolate form 0 to 1 on each span like Beziers do. Instead the underlying curve has a cusomizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary increasing value in the sequence. So you can parametrize the entire curves u range to 0 - 1 or 0 to 12. The parametrization also does not have to be uniform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This paremetrization changes how the curve is shaped. Why would this be useful? Well you can adjust the tension along the curve for one. Or you could encode the length of the curve into to the U parameter. One peculiar use is to make the NURBS curve act like a Bezier curve either fully or just partially (bezier like in the ends but not in the middle for example).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/w7sHM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: Same points different knot sequences. The green NURBS curve corresponds to a Bezier curve that has a parameter range of 0-2 instead of 0-1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ok so what are the knots?&lt;/strong&gt; They are simply the ranges of the basis functions. Since the cubic b-spline with 4 points has 4 interpolating functions it needs 8 knots. Only areas where 3 functions overlap and sum up to 1.0 can a line be drawn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GnJeM.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 5&lt;/strong&gt;: 2 different basis functions, a bezier like and a uniform segment parametrisation, spread to 0-1 range. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And now we have mostly described the answer to question 1. The range is not defined you can stretch the basis functions as you see fit. And finally the knot vector simply produces the parameter ranges for the basis functions. Theres still one more thing that governs the shape of the curve and that is the weight vector. But that another story to be told elsewhere.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;* This rational in this case means that a NURBS curve does not have to be a polynomial, as you cant describe a circle with polynomials. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;** One can define other types of points.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-27T04:39:05.643" LastActivityDate="2015-08-27T04:39:05.643" CommentCount="9" />
  <row Id="344" PostTypeId="2" ParentId="337" CreationDate="2015-08-25T16:47:01.460" Score="9" Body="&lt;p&gt;Radiosity does not account for specular reflections (i.e. it only handles diffuse reflections). Whitted's ray-tracing only considers glossy or diffuse reflection, possibly mirror-reflected. And finally, Kajiya's path-tracing is the most general one [2], handling any number of diffuse, glossy and specular reflections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I think it depends on what you means by &quot;ray-tracing&quot;: the technique developed by Whitted or any kind of &quot;tracing rays&quot;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Side-note: Heckbert [1] (or Shirley?) devised a classification of light scattering events which took place as the light traveled from the luminaire to the eye. In general it has the following form:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;L(S|D)*E&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&quot;L&quot; stands for luminaire, &quot;D&quot; for diffuse reflection, &quot;S&quot; for specular reflection or refraction, &quot;E&quot; for eye, and the symbols &quot;*&quot;, &quot;|&quot;, &quot;()&quot;, &quot;[]&quot; come from regular expressions notation and denote &quot;zero or more&quot;, &quot;or&quot;, &quot;grouping&quot;, &quot;one of&quot;, respectively. Veach [3] extended the notation in his famous dissertation by &quot;D&quot; for Lambertian, &quot;S&quot; for specular and &quot;G&quot; for glossy reflection, and &quot;T&quot; for transmission.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In particular, the following techniques are classified as:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;OpenGL shading: &lt;code&gt;EDL&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Appel's ray-casting: &lt;code&gt;E(D|G)L&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Whitted's ray-tracing: &lt;code&gt;E[S*](D|G)L&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Kajiya's path-tracing: &lt;code&gt;E[(D|G|S)+(D|G)]L&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Golar's radiosity: &lt;code&gt;ED*L&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;[1] Paul S. Heckbert. Adaptive radiosity textures for bidirectional ray tracing. SIGGRAPH Computer Graphics, Volume 24, Number 4, August 1990&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[2] The Siggraph 2001 course &quot;State of the Art in Monte Carlo Ray Tracing for Realistic Image Synthesis&quot; says the following: &quot;Distributed ray tracing and path tracing&#xA;includes multiple bounces involving non-specular scattering such as &lt;code&gt;E(D|G)*L&lt;/code&gt;. However, even these methods ignore paths of the form &lt;code&gt;E(D|G)S*L&lt;/code&gt;; that is, multiple specular bounces from the light source as in a caustic.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[3] Eric Veach. Robust Monte Carlo Methods for Light Transport Simulation. Ph.D. dissertation, Stanford University, December 1997&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2015-08-25T22:00:23.613" LastActivityDate="2015-08-25T22:00:23.613" CommentCount="7" />
  <row Id="346" PostTypeId="1" AcceptedAnswerId="349" CreationDate="2015-08-25T19:42:26.127" Score="7" ViewCount="87" Body="&lt;p&gt;As of now, when rendering my scene and while iterating through the scene graph, for each node its model matrix is calculated from the parent's model matrix and the scene node's pose. This is quite simple and already reduces the number of matrix multiplications to one multiplication per node and frame. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But all these matrix multiplications have to be done on the CPU and in every frame to be able to perform (multiple consecutive) movements of scene nodes very fast. The GPU, however, is probably much better suited to perform lots of matrix multiplications, so I'm thinking about sending multiple partial model matrices to the vertex shader instead of computing everything on the CPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sending every single part (so the pose of every node) to the GPU probably does not make much sense, as in this case all the computations are done for every vertex instead of every node, which would actually decrease performance. But maybe scene nodes with lots of children or only non-moving children (relative to its parent) could be a place to split the model matrix and shift the multiplications to the shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, when is it better to send partial model matrices to the shader and move the multiplication to the GPU? Or is it simply a bad idea to to this?&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-08-27T09:10:02.950" LastActivityDate="2015-08-28T04:03:06.527" Title="When is it better to upload partial model matrices to the vertex shader?" Tags="&lt;opengl&gt;&lt;performance&gt;&lt;transformations&gt;&lt;scene-graph&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="347" PostTypeId="2" ParentId="336" CreationDate="2015-08-25T21:30:55.597" Score="6" Body="&lt;p&gt;Fourier transforms wouldn't help you with a rotation. You'd just end up having to rotate the matrix of Fourier coefficients, instead of rotating the original image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider for example an image made of a perfect sine wave along the x-axis with wave-vector $(k, 0)$.  (The wave-vector is the spacial frequencies along the $x$ and $y$ axes).  The Fourier transform of this would be all black, with a single white pixel at position $(k, 0)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you rotate the image by an angle $\theta$, you'll get a perfect sine wave along an oblique axis.  If you draw a diagram and do a little trigonometry, you can see that the new wave-vector will be $(k \cos \theta, k \sin \theta)$.  (It helps for this part to know that the frequency $k$ is one over the period.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, the Fourier transform of the rotated result would be all black, with a single white pixel at position $(k \cos \theta, k \sin \theta)$.  But that's just what you'd get if you rotated the Fourier transform of the original wave by $\theta$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the Gibbs phenomenon, it's an issue when downsampling an image, or compressing it by frequency quantization (as in JPEG). But simply taking the Fourier transform of an image doesn't introduce any Gibbs ringing. The Fourier transform is lossless and perfectly reversible if done properly: it represents all the information in the original image.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-11-03T19:15:29.400" LastActivityDate="2015-11-03T19:15:29.400" CommentCount="0" />
  <row Id="348" PostTypeId="2" ParentId="287" CreationDate="2015-08-25T22:04:13.903" Score="6" Body="&lt;p&gt;A 2D Fourier transform is performed by first doing a 1D Fourier transform on each row of the image, then taking the result and doing a 1D Fourier transform on each column.  Or vice versa; it doesn't matter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just as a 1D Fourier transform allows you to decompose a function into a sum of (1D) sine waves at various frequencies, a 2D Fourier transform decomposes a function as a sum of 2D sine waves.  These waves can have different frequencies along the x and y axes.  They generically have the form:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ \exp \bigl(i \cdot (k_x x + k_y y) \bigr) $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $k_x$ and $k_y$ are the frequencies along the $x$ and $y$ axes. These two values form a vector called the wave-vector.  In the spatial domain, the wave is oriented along the $(k_x, k_y)$ vector with a frequency along its axis of $\sqrt{k_x^2 + k_y^2}$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just as for the 1D Fourier transform, there exist both discrete and continuous versions.  The result of a discrete 2D Fourier transform is a matrix of complex amplitudes for a set of discrete $(k_x, k_y)$ values.  This is commonly visualized (like in the paper you linked to) as an image where the pixel at coordinates $(k_x, k_y)$ represents the amplitude of that wave-vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, an annular shape in a 2D Fourier transform indicates rotational invariance of the distribution of frequencies (i.e. just as much amplitude for waves in every direction), with a narrow range of magnitudes (from the inside of the annulus to the outside).  In other words, the paper is using the Fourier transform to demonstrate that their noise is reasonably isotropic and band-limited.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-11-03T19:18:26.903" LastActivityDate="2015-11-03T19:18:26.903" CommentCount="0" />
  <row Id="349" PostTypeId="2" ParentId="346" CreationDate="2015-08-26T05:11:33.353" Score="9" Body="&lt;p&gt;Doing math with uniforms is a shader won't usually get you any performance over doing it on the CPU. A CPU isn't slower than a GPU at doing matrix math, it just isn't structured so as to do large amounts of math in parallel. But you have to actually do that large amount of math to get a win. Sending extra data to the GPU just to have the GPU multiply two matrices together will rarely net you anything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, say you have a buffer of skinning matrices. It &lt;em&gt;might&lt;/em&gt; start to matter whether you transform them all into world space on the CPU or just pass an extra model-to-world-space matrix to the GPU. But even then it depends on your ratio of vertices to bones.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-08-26T05:11:33.353" CommentCount="1" />
  <row Id="350" PostTypeId="1" AcceptedAnswerId="351" CreationDate="2015-08-26T20:05:29.413" Score="13" ViewCount="1343" Body="&lt;p&gt;Every time I think I understand the relationship between the two terms, I get more information that confuses me. I thought they were synonymous, but now I'm not sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the difference between &quot;diffuse&quot; and &quot;albedo&quot;? Are they interchangeable terms or are they used to mean different things in practice?&lt;/p&gt;&#xA;" OwnerUserId="423" LastActivityDate="2015-08-26T22:28:28.447" Title="Albedo vs Diffuse" Tags="&lt;rendering&gt;&lt;shading&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" />
  <row Id="351" PostTypeId="2" ParentId="350" CreationDate="2015-08-26T20:57:03.073" Score="12" Body="&lt;p&gt;The short answer: They are not interchangeable, but their meaning can sometimes appear to overlap in computer graphics literature, giving the potential for confusion.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Albedo&lt;/em&gt;&lt;/strong&gt; is the proportion of incident light that is reflected away from a surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Diffuse reflection&lt;/em&gt;&lt;/strong&gt; is the reflection of light in many directions, rather than in just one direction like a mirror (&lt;a href=&quot;https://en.wikipedia.org/wiki/Specular_reflection&quot;&gt;specular reflection&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of ideal diffuse reflection (&lt;a href=&quot;https://en.wikipedia.org/wiki/Lambertian_reflectance&quot;&gt;Lambertian reflectance&lt;/a&gt;), incident light is reflected in all directions independently of the angle at which it arrived. Since in computer graphics rendering literature there is sometimes a &quot;diffuse coefficient&quot; when calculating the colour of a pixel, which indicates the proportion of light reflected diffusely, there is an opportunity for confusion with the term &lt;em&gt;albedo&lt;/em&gt;, which also means the proportion of light reflected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are rendering a material which only has ideal diffuse reflectance, then the albedo will be equal to the diffuse coefficient. However, in general a surface may reflect some light diffusely and other light specularly or in other direction-dependent ways, so that the diffuse coefficient is only a fraction of the albedo.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Note that &lt;a href=&quot;https://en.wikipedia.org/wiki/Albedo&quot;&gt;albedo&lt;/a&gt; is a term from observation of planets, moons and other large scale bodies, and is an average over the surface, and often an average over time. The albedo is thus not a useful value in itself for rendering a surface, where you need the specific, current surface property at any given location on the surface. Also note that in astronomy the term albedo can refer to different parts of the spectrum in different contexts - it will not always be refering to human visible light.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Another difference, as &lt;a href=&quot;http://computergraphics.stackexchange.com/users/48/nathan-reed&quot;&gt;Nathan Reed&lt;/a&gt; points out in a comment, is that albedo is a single average value, which gives you no colour information. For basic rendering the diffuse coefficient gives proportions for red, green and blue components separately, so albedo would only allow you to render greyscale images. For more realistic images, spectral rendering requires the reflectance of a surface as a function of the whole visible spectrum - far more than a single average value.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-08-26T22:28:28.447" LastActivityDate="2015-08-26T22:28:28.447" CommentCount="5" />
  <row Id="352" PostTypeId="2" ParentId="340" CreationDate="2015-08-26T21:57:14.240" Score="5" Body="&lt;p&gt;The way that NURBS curves are typically split at an arbitrary point is by &lt;a href=&quot;http://www.cs.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/NURBS-knot-insert.html&quot;&gt;knot insertion&lt;/a&gt;. You insert knots at the split point until it is at maximum multiplicity, at which point you can just read off the two split curves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, you may not want to split at an arbitrary point. If the ultimate goal is to draw the curves or something like that, then it's worth splitting the curve at the existing knot points (that is, performing knot insertion until all knots are at maximum multiplicity) rather than inserting new ones.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This process splits the NURBS into &lt;em&gt;uniform&lt;/em&gt; rational B-splines. Once you have that, you can use &lt;a href=&quot;https://en.wikipedia.org/wiki/De_Boor%27s_algorithm&quot;&gt;de Boor's algorithm&lt;/a&gt; to split further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The number of knots in the knot vector is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;numKnots = degreeOfCurve + numControlPoints - 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Inserting a knot thus increases the number of control points by one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you travel along a NURBS curve, each knot represents a place where one control point &quot;drops out&quot; and another one &quot;enters&quot;. If a knot value is repeated, this means that more than one control point is replaced at this place.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a curve of degree greater than 1, the and last knots are repeated multiple times for one simple reason: you need to bring in more than one point to start and you need to eject more than one point to end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's think about cubic curves for the moment, just to keep things simple.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A curve with the knot vector [0,0,1,1] is a uniform B-spline curve. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A curve with the knot vector [0,0,1,1,2,2] is non-uniform, but can be thought of as two uniform B-spline curves which connect at t=1, one corresponding to [0,0,1,1] and one corresponding to [1,1,2,2]. You can do this because the multiplicity of the knots is enough to &quot;start&quot; and &quot;end&quot; a cubic curve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're faced with a curve with a knot vector like [0,0,1,2,2], you can insert a knot at 1 without changing the shape of the curve (this is the knot insertion procedure). This increases the number of control points by one; the knot insertion procedure adjusts the points around the new knot to accommodate it. But once you've done that, you have two uniform B-spline curves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Knot insertion won't create overlapping control points unless you insert too many knots at the same place, and by &quot;too many&quot;, I mean the degree of the curve. So for a non-uniform cubic curve, you'd insert knots so that every knot had multiplicity 2. That gives you a number of abutting uniform cubic curves, which you can then use de Boor's algorithm to split further.&lt;/p&gt;&#xA;" OwnerUserId="159" LastEditorUserId="159" LastEditDate="2015-08-27T01:43:13.840" LastActivityDate="2015-08-27T01:43:13.840" CommentCount="4" />
  <row Id="353" PostTypeId="2" ParentId="318" CreationDate="2015-08-26T22:00:26.977" Score="5" Body="&lt;p&gt;&lt;a href=&quot;http://developer.download.nvidia.com/assets/gamedev/files/sdk/11/OpacityMappingSDKWhitePaper.pdf&quot;&gt;Nearest-depth filtering&lt;/a&gt; is an alternative to bilaterial filtering that was specifically developed for upsampling low-resolution transparent rendering. It's a bit simpler than bilateral sampling, in that it only requires one sample from your low-resolution texture. However it can still have issues, particularly with high-frequency geometry that isn't well-represented in your low-resolution depth buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At SIGGRAPH 2013, Bungie presented a new approach for handling very-low-resolution particles using what they called a &quot;variance depth map&quot;. You can find the slides &lt;a href=&quot;http://advances.realtimerendering.com/s2013/index.html&quot;&gt;here&lt;/a&gt; in PDF and PPTX format. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One interesting approach that hasn't been used yet (at least, to my knowledge) would be to leverage MSAA. The idea is that you take your full-resolution depth buffer, and then alias it (or convert it, if the API doesn't allow you to do so) as a 4xMSAA depth buffer with half the width and height. You could then render your low-resolution transparents to a half-resolution render target with 4xMSAA, using your &quot;4xMSAA&quot; depth buffer for depth testing. Then to upscale, for each high-resolution pixel you would access the corresponding subsample from your low-resolution MSAA render target, and blend it on top. This would give you full-resolution depth testing, but half-resolution shading (with framebuffer bandwidth somewhere in between full-res and half-res due to compression). The catch here is that you need programmable MSAA sample points to do this, since you'll want to use an ordered grid sampling pattern for your low-resolution rendering instead of the typical rotated grid pattern. Newer GPU's (Maxwell from Nvidia, GCN from AMD) support doing this through vendor extensions in &lt;a href=&quot;https://developer.nvidia.com/nvapi&quot;&gt;D3D11&lt;/a&gt; and &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/opengl/specs/GL_NV_sample_locations.txt&quot;&gt;OpenGL&lt;/a&gt;, and it can be done natively on consoles.&lt;/p&gt;&#xA;" OwnerUserId="207" LastActivityDate="2015-08-26T22:00:26.977" CommentCount="1" />
  <row Id="354" PostTypeId="1" CreationDate="2015-08-26T22:13:24.200" Score="4" ViewCount="71" Body="&lt;p&gt;I know GPU prefetches textures and that's why dependent texture reads are slower, but how does it work and at what point that happens?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;EDIT: Split the content of this question into others as suggested by trichoplax&#xA;Here's a link to other questions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://computergraphics.stackexchange.com/questions/355/how-does-texture-cache-work-considering-multiple-shader-units&quot;&gt;How does Texture Cache work considering multiple shader units&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://computergraphics.stackexchange.com/questions/356/how-does-texture-cache-work-in-tile-based-rendering-gpu&quot;&gt;How does Texture Cache work in Tile Based Rendering GPU&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://computergraphics.stackexchange.com/questions/357/is-using-many-texture-maps-bad-for-caching&quot;&gt;Is using many texture maps bad for caching?&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="250" LastEditorUserId="250" LastEditDate="2015-08-26T22:53:30.343" LastActivityDate="2015-08-26T22:53:30.343" Title="How Texture Prefetch works?" Tags="&lt;opengl&gt;&lt;texture&gt;&lt;gpu&gt;&lt;optimisation&gt;" AnswerCount="0" CommentCount="3" ClosedDate="2015-08-30T18:52:08.443" />
  <row Id="355" PostTypeId="1" AcceptedAnswerId="365" CreationDate="2015-08-26T22:43:22.360" Score="5" ViewCount="214" Body="&lt;p&gt;Modern GPUs have many parallel shading units. I'd like to know how texture cache is managed in that scenario. &lt;/p&gt;&#xA;" OwnerUserId="250" LastActivityDate="2015-08-28T06:49:13.630" Title="How does Texture Cache work considering multiple shader units" Tags="&lt;texture&gt;&lt;gpu&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="356" PostTypeId="1" AcceptedAnswerId="386" CreationDate="2015-08-26T22:44:44.380" Score="6" ViewCount="214" Body="&lt;p&gt;How does cache work with tile based rendering? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any tips on how to improve cache hit ratio for it? (for instance, if tiles are processed horizontally and I have vertical segments of triangles with the same texture, does it work worse for cache than if I had triangles layout out horizontally?)&lt;/p&gt;&#xA;" OwnerUserId="250" LastActivityDate="2015-09-09T21:13:03.580" Title="How does Texture Cache work in Tile Based Rendering GPU" Tags="&lt;texture&gt;&lt;gpu&gt;&lt;shader&gt;&lt;optimisation&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="357" PostTypeId="1" AcceptedAnswerId="419" CreationDate="2015-08-26T22:47:25.997" Score="6" ViewCount="242" Body="&lt;p&gt;If I have many textures (say 5+ maps) bound to the same texture unit, does it work worse for cache than if I had only 2 or 3 textures?&lt;/p&gt;&#xA;" OwnerUserId="250" LastActivityDate="2015-10-21T07:37:20.547" Title="Is using many texture maps bad for caching?" Tags="&lt;texture&gt;&lt;shader&gt;&lt;optimisation&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="3" />
  <row Id="358" PostTypeId="1" AcceptedAnswerId="360" CreationDate="2015-08-27T11:20:20.270" Score="7" ViewCount="93" Body="&lt;p&gt;In &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/340/splitting-of-nurbs-curves&quot;&gt;Splitting of NURBS curves&lt;/a&gt; there the answer relied on &quot;maximum knot multiplicity&quot;. In order not to mix-up different topics I would like to kindly ask to answer it in another question: what is this &quot;knot multiplicity&quot; all about?&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-08-27T13:02:58.983" Title="NURBS knot multiplicity" Tags="&lt;nurbs&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="359" PostTypeId="1" CreationDate="2015-08-27T11:33:54.510" Score="8" ViewCount="98" Body="&lt;p&gt;I'm interested to know whether the demoscene has historically introduced new techniques that would have otherwise taken longer to discover, contributing to the progress of computer graphics. Has it become more or less relevant over the years?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that some of the early demos took advantage of known hardware errors and went beyond what was considered possible at the time, but what I'm interested in is whether any of the new techniques introduced were then taken up by researchers or professional programmers to become part of the mainstream accepted way of doing things.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-28T15:05:33.153" Title="Is the demoscene a source of advancements in computer graphics?" Tags="&lt;demoscene&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" ClosedDate="2015-08-30T18:51:12.580" />
  <row Id="360" PostTypeId="2" ParentId="358" CreationDate="2015-08-27T11:55:04.603" Score="5" Body="&lt;p&gt;When you have a curve you can adjust the knots so that they lie on top of each other. This is essentially a bit like having several control points on top of each other, except there's only one point. This is sometimes known as multiplicity or duplicity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you have as many knots on top of each other as you have degrees in the curve smoothness, you end up with a cusp, also known as as sharp corner. Once you have a sharp corner you can just go and delete the points on the opposite side as they no longer affect points no the other side of the corner. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6DWhc.png&quot; alt=&quot;Curve with cusp&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Curve and control cage (Top) and basis functions (bottom). In this case the curve acts as 2 adjoining bezier curves, The image is using the knot vector [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we look at a knot vector then the multiplicity looks like a seqence of n knots that have the same number. Note if you do not add ecactly as many knots on top as there is smoothness then you get a partially sharp corner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iuSp4.gif&quot; alt=&quot;Animation&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Animation with interpolation of knot form [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2] to  [0, 0, 0, 0, 0.34, 1, 1.66, 2, 2, 2, 2] and back. If you do proper point insertion then the curve does not change. Just talking about what multiplicity is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively to changing parametrization you can just add points on top of each other. This is equivalent although slight misuse of resources, but useful in surface modeling. Having many control points on top could also be useful for the uniform nature of the knot distribution.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-27T13:02:58.983" LastActivityDate="2015-08-27T13:02:58.983" CommentCount="7" />
  <row Id="361" PostTypeId="1" CreationDate="2015-08-27T12:43:44.723" Score="10" ViewCount="173" Body="&lt;p&gt;In reading about &lt;a href=&quot;https://en.wikipedia.org/wiki/Lambertian_reflectance#Use_in_computer_graphics&quot; rel=&quot;nofollow&quot;&gt;Lambertian reflectance&lt;/a&gt; on Wikipedia I found the following phrase (in bold) which doesn't sound right to me:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In computer graphics, Lambertian reflection is often used as a model for diffuse reflection. This technique causes all closed polygons (such as a triangle within a 3D mesh) to reflect light equally in all directions when rendered. In effect, a point rotated around its normal vector will not change the way it reflects light. However, the point will change the way it reflects light if it is tilted away from its initial normal vector &lt;strong&gt;since the area is illuminated by a smaller fraction of the incident radiation.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The way I picture the situation described in the paragraph, only tilting away from the light source would cause  less light to be incident in a given area. In general, tilting away from the initial normal vector could lead to either an increase &lt;strong&gt;&lt;em&gt;or&lt;/em&gt;&lt;/strong&gt; a decrease in incident light per area, as this says nothing about the location of the light source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have I misunderstood the context, or is this something that should be rewritten on Wikipedia?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="2041" LastEditDate="2015-11-23T13:16:19.920" LastActivityDate="2015-11-23T13:16:19.920" Title="Is a Lambertian reflector illuminated by a smaller fraction of the incident radiation when it's tilted?" Tags="&lt;shading&gt;" AnswerCount="3" CommentCount="3" FavoriteCount="1" />
  <row Id="362" PostTypeId="1" AcceptedAnswerId="364" CreationDate="2015-08-28T02:22:47.113" Score="11" ViewCount="140" Body="&lt;p&gt;I've been reading the following article on how to do a parallel scan in CUDA:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html&quot;&gt;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the article, there is an emphasis on making the scan &quot;work efficient&quot;. In other words, a GPU algorithm should perform no more additions than a CPU algorithm, O(n). The authors present two algorithms, one &quot;naive&quot; that does O(nlogn) additions, and one that they consider &quot;work efficient&quot;. However, the work efficient algorithm does twice as many loop iterations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my understanding, GPUs are simply giant SIMD processors and should operate in lock-step. Doing twice as many loops in the &quot;work efficient&quot; algorithm seems to imply that many threads will be idle and decrease performance in the long run. What am I  missing?&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-08-28T05:27:35.067" Title="Why is work-efficiency desired in GPU programming?" Tags="&lt;gpu&gt;&lt;compute-shader&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="363" PostTypeId="2" ParentId="346" CreationDate="2015-08-28T04:03:06.527" Score="5" Body="&lt;p&gt;Rarely, if ever. You half-answered it in your own question: a vertex shader runs once per-vertex, a fragment shader once per-fragment. If you're not doing something that's unique to that vertex or fragment, then you're doing literally the exact same thing every time you invoke a shader. That doesn't sound more efficient to me.&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-08-28T04:03:06.527" CommentCount="0" />
  <row Id="364" PostTypeId="2" ParentId="362" CreationDate="2015-08-28T05:27:35.067" Score="15" Body="&lt;p&gt;First of all, re: &quot;GPUs are simply giant SIMD processors and should operate in lock-step&quot;, it's a bit more complicated than that. The &lt;em&gt;entire&lt;/em&gt; GPU does not run in lockstep. Shader threads are organized into groups of 32 called &quot;warps&quot; (on NVIDIA; on AMD they're groups of 64 called &quot;wavefronts&quot;, but same concept).  Within a warp, all the threads do run in lockstep as a SIMD array. However, different warps are not in lockstep with each other. In addition, some warps may be actively running while others may be suspended, much like CPU threads.  Warps can be suspended either because they're waiting for something (such as memory transactions to return or barriers to clear), or because there isn't a slot available for them (since the GPU can only actively run a certain number of warps at a time).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, back to your question. I can see two ways that the &quot;work-efficient&quot; algorithm from that paper looks like it would be more efficient than the &quot;naive&quot; algorithm.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The work-efficient version requires half as many threads to begin with. In the naive algorithm, they have one thread per array element; but in the work-efficient version, each thread operates on two adjacent elements of the array and so they need only half as many threads as array elements. Fewer threads means fewer warps, and so a larger fraction of the warps can be actively running.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Although the work-efficient version requires more steps, this is offset by the fact that the number of active threads decreases faster, and the total number of active threads over all the iterations is considerably smaller. If a warp has no active threads during an iteration, that warp will just skip to the following barrier and get suspended, allowing other warps to run. So, having fewer active warps can often pay off in execution time. (Implicit in this is that GPU code needs to be designed in such a way that active threads are packed together into as few warps as possible&amp;mdash;you don't want them to be sparsely scattered, as even one active thread will force the whole warp to stay active.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the number of active threads in the naive algorithm.  Looking at Figure 2 in the article, you can see that all the threads are active &lt;em&gt;except&lt;/em&gt; for the first 2&lt;sup&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sup&gt; on the &lt;em&gt;k&lt;/em&gt;​th iteration.  So with &lt;em&gt;N&lt;/em&gt; threads, the number of active threads goes like &lt;em&gt;N&lt;/em&gt; ​− 2&lt;sup&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sup&gt;. For example, with &lt;em&gt;N&lt;/em&gt; = 1024, the number of active threads per iteration is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1023, 1022, 1020, 1016, 1008, 992, 960, 896, 768, 512&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I convert this to number of active warps (by dividing by 32 and rounding up), I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;32, 32, 32, 32, 32, 31, 30, 28, 24, 16&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;for a sum of 289.  On the other hand, the work-efficient algorithm starts with half as many threads, then it halves the number of active ones on each iteration until it gets down to 1, then starts doubling until it gets back up to half the array size again:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; 512, 256, 128, 64, 32, 16, 8, 4, 2, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Converting this to active warps:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;16, 8, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 8, 16&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The sum is 71, which is only a quarter as many. So you can see that over the course of the entire operation, the number of active warps is much smaller with the work-efficient algorithm. (In fact, for a lengthy run in the middle there are only a handful of active warps, which means most of the chip is not occupied. If there are additional compute tasks running, e.g. from other CUDA streams, they could expand to fill that unoccupied space.)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;All that being said, it's unfortunate that the GPU Gems article does not clearly explain any of this, instead focusing on big-O &quot;number of additions&quot; analysis that, while not entirely irrelevant, misses a lot of the details about why this algorithm is faster.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-28T05:27:35.067" CommentCount="1" />
  <row Id="365" PostTypeId="2" ParentId="355" CreationDate="2015-08-28T06:49:13.630" Score="4" Body="&lt;p&gt;At the top level, a GPU is subdivided into a number of shader cores.  A small GPU in a notebook or tablet may have only a few cores while a high-end desktop GPU may have dozens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition to the shader cores there are also texture units. They may be grouped together with one texture unit per shader core, or one texture unit shared among two or three shader cores, depending on the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The whole chip shares a single L2 cache, but the different units will have individual L1 caches.  Texture units have texture caches, and shader units have caches for instructions and constants/uniforms, and maybe a separate cache for buffer data depending on whether buffer loads are a separate path from texture loads or not (varies by GPU architecture).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Texture units operate independently and asynchronously from shader cores. When a shader performs a texture read, it sends a request to the texture unit across a little bus between them; the shader can then continue executing if possible, or it may get suspended and allow other shader threads to run while it waits for the texture read to finish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The texture unit batches up a bunch of requests and performs the addressing math on them&amp;mdash;selecting mip levels and anisotropy, converting UVs to texel coordinates, applying clamp/wrap modes, etc. Once it knows which texels it needs, it reads them through the cache hierarchy, the same way that memory reads work on a CPU (look in L1 first, if not there then L2, then DRAM). If many pending texture requests all want the same or nearby texels (as they often do), then you get a lot of efficiency here, as you can satisfy many pending requests with only a few memory transactions.  All these operations are pipelined, so while the texture unit is waiting for memory on one batch it can be doing the addressing math for another batch of requests, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the data comes back, the texture unit will decode compressed formats, do sRGB conversion and filtering as necessary, then return the results back to the shader core.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-28T06:49:13.630" CommentCount="2" />
  <row Id="366" PostTypeId="2" ParentId="359" CreationDate="2015-08-28T15:05:33.153" Score="7" Body="&lt;p&gt;The demo scene has been doing Ray marching for a long time, but only recently have main stream AAA games started using it.  The main use I've seen it for is for screenspace reflections, raymarching against the zbuffer (killzone, call of duty).  Also, shadertoy was made by inigo quillez, who is from the demo scene.  People are now using shadertoy to prototype and share graphics technique research.  So I'd say the answer is yes, but I'd like to hear more examples if other people have any (:&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-08-28T15:05:33.153" CommentCount="4" />
  <row Id="367" PostTypeId="1" AcceptedAnswerId="368" CreationDate="2015-08-28T18:47:48.083" Score="12" ViewCount="475" Body="&lt;p&gt;I'd like to use OpenCL to accelerate rendering of raytraced images, but I notice that the &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenCL#OpenCL_C_language&quot;&gt;Wikipedia page&lt;/a&gt; claims that recursion is forbidden in Open CL. Is this true? As I make extensive use of recursion when raytracing, this will require a considerable amount of redesign in order to benefit from the speed up. What is the underlying restriction that prevents recursion? Is there any way around it?&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-29T02:17:36.830" Title="Why is recursion forbidden in OpenCL?" Tags="&lt;raytracing&gt;&lt;opencl&gt;&lt;gpgpu&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="368" PostTypeId="2" ParentId="367" CreationDate="2015-08-29T02:17:36.830" Score="18" Body="&lt;p&gt;It's essentially because not all GPUs can support function calls&amp;mdash;and even if they can, function calls may be quite slow or have limitations such as a very small stack depth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Shader code and GPU compute code may appear to have function calls all over the place, but under normal circumstances they're all 100% inlined by the compiler. The machine code executed by the GPU contains branches and loops, but no function calls. However, recursive function calls cannot be inlined for obvious reasons. (Unless some of the arguments are compile-time constants, in such a way that the compiler can fold them and inline the entire tree of calls.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to implement true function calls, you need a stack.  Most of the time, shader code doesn't use a stack at all&amp;mdash;GPUs have large register files and shaders can keep all their data in registers the whole time.  It's difficult to make a stack work because (a) you would need a lot of stack space to provide for all the many warps that can be in flight at a time, and (b) the GPU memory system is optimized for batching together a lot of memory transactions to achieve high throughput, but this comes at the expense of latency, so my guess is stack operations like saving/restoring local variables would be awfully slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Historically, hardware-level function calls haven't been too useful on the GPU, as it has made more sense to inline everything in the compiler.  So GPU architects haven't focused on making them fast.  Probably some different tradeoffs could be made, if there is a demand for efficient hardware-level calls in the future, but (as with everything in engineering) it will incur a cost somewhere else.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as raytracing is concerned, the way people usually handle this sort of thing is by creating queues of rays that are in the process of being traced.  Instead of recursing, you add a ray to a queue, and at the high level somewhere you have a loop that keeps processing until all the queues are empty.  It does require significant reorganization of your rendering code if you're starting from a classic recursive raytracer, though.  For more info, a good paper to read on this is &lt;a href=&quot;https://research.nvidia.com/publication/megakernels-considered-harmful-wavefront-path-tracing-gpus&quot;&gt;Wavefront Path Tracing&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-08-29T02:17:36.830" CommentCount="2" />
  <row Id="369" PostTypeId="1" CreationDate="2015-08-29T07:00:30.047" Score="7" ViewCount="51" Body="&lt;p&gt;My postscript interpreter currently implements the &lt;a href=&quot;https://github.com/luser-dr00g/xpost/blob/master/data/clip.ps&quot; rel=&quot;nofollow&quot;&gt;Hodgeman-Sutherland clipping algorithm&lt;/a&gt; but this is limited to simpler shapes and doesn't have a provision for utilizing various winding-number rules. So it doesn't help me to implement even-odd filling or handle complex self-intersecting shapes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Weiler%E2%80%93Atherton_clipping_algorithm&quot; rel=&quot;nofollow&quot;&gt;Weiler-Atherton algorithm&lt;/a&gt; promises to offer all these features, but every time I sit down with it, I get hung up on implementing the basic constructs. Specifically it requires making bi-directional associations between nodes which can then be traversed in either direction. But, how do you do that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I implement my path lists in Postscript (or any dynamic object-based language) as an array of points, the points themselves implemented as an array of coordinates, do I make a list of index pairs and do a linear search for traversing? Or can I use an associative array to make a mapping, and is it sufficient to add both (node1-&gt;node2) and (node2-&gt;node1)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or what if the paths are in a linked-list in a C like language with pointers to the next vertex and a NULL pointer designating the end, do you just add more links (pointers) to the nodes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You don't need to address all my questions here, it should be sufficient to describe the strategy for building this data structure for any 1 language (not necessarily Postscript or C), and I should be able to apply it to my specific case which is usually to prototype in Postscript and translate to C if necessary for speed.&lt;/p&gt;&#xA;" OwnerUserId="482" LastEditorUserId="482" LastEditDate="2015-08-30T20:53:57.460" LastActivityDate="2015-08-30T20:53:57.460" Title="How to describe the bi-directional links for the Weiler-Atherton algorithm?" Tags="&lt;polygon&gt;&lt;clipping&gt;&lt;data-structure&gt;" AnswerCount="0" CommentCount="4" FavoriteCount="1" />
  <row Id="370" PostTypeId="1" AcceptedAnswerId="371" CreationDate="2015-08-29T15:15:24.327" Score="7" ViewCount="643" Body="&lt;p&gt;Looking at &lt;a href=&quot;http://www.oxidegames.com/star-swarm/&quot;&gt;Star Swarm&lt;/a&gt;, a demo for the Nitrous engine, I found this little line:&#xA;&quot;Nitrous uses Object Space Lighting, the same techniques used in film, including real-time film-quality motion blur.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried looking around for anything on &quot;object space lighting&quot; or &quot;object space rendering&quot; but couldn't come up with anything. When I hear &quot;object space&quot; I think of doing it per-object but I was hoping to find a more detailed description of the method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know anything about &quot;object space lighting&quot; and if so could you go into some technical details(how its done, pros, cons, etc)?&lt;/p&gt;&#xA;" OwnerUserId="376" LastActivityDate="2015-08-29T19:29:17.383" Title="What is &quot;Object Space Lighting&quot;?" Tags="&lt;rendering&gt;&lt;real-time&gt;&lt;lighting&gt;&lt;space&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="371" PostTypeId="2" ParentId="370" CreationDate="2015-08-29T19:29:17.383" Score="6" Body="&lt;p&gt;According to the Star Swarm developers this helps them with LOD and enables greater shading scaling. Based on this I guess its simply texture space lighting.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Because we do what we’re calling object space lighting, we calculate the projected size of each of those objects on screen, and based on that we shade it in a priority manner based on how large they are. We can scale the shading quality at a different frequency than we scale the geometry level or something else.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://venturebeat.com/2014/01/21/a-deep-dive-into-the-making-of-the-eye-popping-star-swarm-demo-interview/view-all/&quot;&gt; A deep dive into the making of the eye-popping Star Swarm demo (interview)&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="93" LastActivityDate="2015-08-29T19:29:17.383" CommentCount="0" />
  <row Id="372" PostTypeId="1" AcceptedAnswerId="383" CreationDate="2015-08-30T15:46:52.053" Score="6" ViewCount="215" Body="&lt;p&gt;I've been playing around with shadow mapping in OpenGL using depth textures. The depth texture is fine and I can map it onto the scene but I have some strange artefacts on the back of the object:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/V4oJ8fq.png&quot; alt=&quot;Sphere casting a shadow, with artefacts on rear&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is what is causing this and how can I fix it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fragment shader I'm using is fairly straightforward (I stripped out the colour for simplicity in case you're wondering why there's no blue here):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;in vec4 vShadowCoord;&#xA;&#xA;uniform sampler2DShadow shadowMap;&#xA;&#xA;out vec4 fragColor;&#xA;&#xA;void main()&#xA;{&#xA;    float bias = 0.005;&#xA;    float visibility = 1.0;&#xA;    if (texture(shadowMap, vec3(vShadowCoord.xy, vShadowCoord.z / vShadowCoord.w)) &amp;lt; (vShadowCoord.z - bias) / vShadowCoord.w)&#xA;        visibility = 0.25;&#xA;&#xA;    fragColor = vec4(visibility);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Edit: As requested, minimum working example screenshot that uses only the above code (no colour).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/K69JKyS.png&quot; alt=&quot;Same sphere, minus colour&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="193" LastEditorUserId="193" LastEditDate="2015-08-31T13:33:46.303" LastActivityDate="2015-08-31T22:01:39.200" Title="Shadow Mapping artefacts" Tags="&lt;opengl&gt;&lt;glsl&gt;&lt;shadow-mapping&gt;&lt;shadow&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="374" PostTypeId="1" CreationDate="2015-08-30T21:59:24.907" Score="11" ViewCount="255" Body="&lt;p&gt;I tried &lt;a href=&quot;http://math.stackexchange.com/questions/348806/intersect-a-line-with-a-bicubic-bezier-surface-patch&quot;&gt;this question on math.SE&lt;/a&gt; and surprisingly, the answer was &quot;the equations are too nasty, just feed the function it to a numerical root-finder&quot;. But if you consider yourself &quot;a graphics guy&quot; like me, and have played extensively with Bezier curves for design work, I got to believe that better can be done. There is a published algorithm by Kajiya that I don't have the background to understand (Sylvester Matrices), but the related advice on math.SE was that the result is a degree-18 polynomial in t, and you still need to solve that numerically. &lt;a href=&quot;http://math.stackexchange.com/questions/1083240/can-a-bicubic-bezier-surface-be-represented-as-a-bernstein-polynomial&quot;&gt;I had another idea with similar result&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, is it a total pipe dream to hope to solve the Ray/Bezier-surface intersection algebraically, thus making it possible to code explicitly and have super-fast super-smoothness?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Barring that, what's the fastest method for performing this calculation? Can you &quot;find the wiggles&quot; to get a tight bound (and target) for recursive subdivision? If you have to use a numerical root-finder (sigh), what properties does it need and is there a best choice for speed?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My original thought was about preparing for a specific surface, similar to Laplace expansion as described in the answer to my &lt;a href=&quot;http://math.stackexchange.com/questions/297588/for-bsp-generation-how-to-intersect-or-locate-a-triangle-with-a-plane-defined-b&quot;&gt;other math question about triangles&lt;/a&gt;. But I'd be interested in general methods, too. I'm just thinking of a fixed set of shapes, like &lt;a href=&quot;http://stackoverflow.com/questions/14807341/how-to-do-a-space-partitioning-of-the-utah-teapot&quot;&gt;the Utah teapot&lt;/a&gt;. But I'd be very interested in ways of optimizing for temporal coherence across animated frames.&lt;/p&gt;&#xA;" OwnerUserId="482" LastEditorUserId="482" LastEditDate="2015-08-30T22:32:51.197" LastActivityDate="2015-09-03T16:41:57.353" Title="How to raytrace Bezier surfaces?" Tags="&lt;raytracing&gt;&lt;bezier-curve&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="375" PostTypeId="1" AcceptedAnswerId="379" CreationDate="2015-08-31T01:30:55.070" Score="9" ViewCount="479" Body="&lt;p&gt;Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Shading#Ambient_lighting&quot; rel=&quot;nofollow&quot;&gt;says&lt;/a&gt;: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;An ambient light source represents a fixed-intensity and fixed-color light source that affects all objects in the scene equally.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;By saying &quot;affects all objects equally&quot; they mean all objects get the same amount of light? So if you have three houses in your scene, you would need to calculate a specific position for the ambient lighting source, in order that every object gets the same amount of light? If not, you would not have an ambient light source? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the difference between &quot;normal&quot; lighting from the sun and ambient lighting?&lt;/p&gt;&#xA;" OwnerUserId="480" LastEditorUserId="127" LastEditDate="2015-10-18T14:48:44.737" LastActivityDate="2015-10-18T14:48:44.737" Title="What is ambient lighting?" Tags="&lt;lighting&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="2" />
  <row Id="376" PostTypeId="1" AcceptedAnswerId="2173" CreationDate="2015-08-31T01:48:27.043" Score="10" ViewCount="196" Body="&lt;p&gt;I do not understand the &lt;a href=&quot;https://en.wikipedia.org/wiki/Screen_space_ambient_occlusion#Implementation&quot;&gt;explanation&lt;/a&gt; from wikipedia.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For every pixel on the screen, the pixel shader samples the depth values around the current pixel and tries to compute the amount of occlusion from each of the sampled points.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;How can depth values of surrounding pixels tell you something about occlusion? &lt;strong&gt;Occlusion&lt;/strong&gt;, as I understand, happens when an object A stands in front of another object B, so you cannot see the object B. But why would you now look at the depth pixels of &lt;strong&gt;surrounding&lt;/strong&gt; pixels? I mean you can see those pixels, so there is no occlusion. Maybe I understood occlusion wrong.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And what I also did not understand is the term kernel in some other tutorials. What is a kernel and why would you use it for ssao?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could someone make a detailed explanation of the algorithm, with regard to my questions?&lt;/p&gt;&#xA;" OwnerUserId="480" LastEditorUserId="127" LastEditDate="2015-08-31T07:00:54.113" LastActivityDate="2016-03-12T00:10:18.600" Title="How is screen space ambient occlusion implemented?" Tags="&lt;algorithm&gt;&lt;lighting&gt;&lt;shading&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="377" PostTypeId="1" AcceptedAnswerId="382" CreationDate="2015-08-31T01:58:54.870" Score="4" ViewCount="902" Body="&lt;p&gt;How do they work and what are the differences between them? In what scenario should you use which one?&lt;/p&gt;&#xA;" OwnerUserId="480" LastActivityDate="2015-08-31T08:46:16.797" Title="Shading: Phong vs Gouraud vs Flat" Tags="&lt;algorithm&gt;&lt;shading&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="378" PostTypeId="2" ParentId="374" CreationDate="2015-08-31T02:12:03.533" Score="11" Body="&lt;p&gt;First off, here's the Kajiya method I think you're thinking of: Kajiya, &lt;em&gt;Ray Tracing Parametric Patches&lt;/em&gt;, SIGGRAPH 82. &lt;a href=&quot;http://core.ac.uk/download/pdf/4891798.pdf&quot;&gt;The tech report version&lt;/a&gt; might be more informative.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I hope you get from that is that it's not impossible and it's not conceptually difficult if you don't mind getting your hands dirty with some algebraic geometry and complex numbers. However, doing it directly is absurdly expensive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Real&quot; ray tracers tend to do some combination of two things:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Placing a bounding hierarchy (e.g. AABBs) on the patch to get a good &quot;initial value&quot; for a numeric root finder. If you do this well, you can avoid the &quot;wrinkle&quot; problem.&lt;/li&gt;&#xA;&lt;li&gt;Tesselating the patch into &lt;a href=&quot;http://ddg.cs.columbia.edu&quot;&gt;DDG shells&lt;/a&gt; and ray tracing them like polygon meshes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;That last point sounds like it kills the &quot;super-smoothness&quot; requirement, but it isn't nearly as bad as that if you're using &lt;a href=&quot;https://graphics.stanford.edu/papers/trd/&quot;&gt;ray differentials&lt;/a&gt;. Matching the tessellation level to the &quot;size&quot; of the ray bounds the error nicely. Besides, you probably need differentials for texture coordinates anyway, so you might as well use it to control the accuracy of the intersection test too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Exploiting temporal coherence isn't a bad idea, but exactly how you'd do that depends a lot on your scene graph representation. You might want to look at ray coherence. Ask your favourite search engine about &lt;a href=&quot;http://cseweb.ucsd.edu/~ravir/whitted.pdf&quot;&gt;ray packet tracing&lt;/a&gt; and &lt;a href=&quot;https://graphics.stanford.edu/~boulos/papers/reorder_rt08.pdf&quot;&gt;ray reordering&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="159" LastActivityDate="2015-08-31T02:12:03.533" CommentCount="0" />
  <row Id="379" PostTypeId="2" ParentId="375" CreationDate="2015-08-31T04:28:42.567" Score="12" Body="&lt;p&gt;In this context, &lt;code&gt;Ambient lighting&lt;/code&gt; refers to a very crude approximation of indirect lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Direct lighting from a direct source is relatively simple to evaluate and model, even in real-time. But the light that is not absorbed will bounce all over the place and cause indirect lighting. This is why for example a lamp will a lampshade will light a whole room and not only the narrow area underneath.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But modeling indirect lighting is difficult and costly. So an approximation is to consider that lighting to be &lt;strong&gt;constant and independent from position&lt;/strong&gt;: that's ambient lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of outdoor scene, ambient lighting would represent the blue light coming from the sky dome, as opposed to the orange direct light coming from the Sun.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-08-31T04:28:42.567" CommentCount="3" />
  <row Id="380" PostTypeId="2" ParentId="375" CreationDate="2015-08-31T04:42:14.127" Score="10" Body="&lt;p&gt;Traditional rendering solutions do not do account for secondary light bounces (called indirect light). Even with strategically placed fill lights you still have areas where none of the direct light hits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ambient light tries to solve this problem by shining by a constant amount in all directions. In practice this means that light position or surface normal has no meaning, one just adds some of the shaders color multiplied by ambient light color to the shading result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ambient light has a tendency to look artificial when overused. But the opposite problem is that surfaces look like they are on outer space. Ambient light also makes the difference between dark materials and light materials more apparent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eQWU1.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eQWU1.gif&quot; alt=&quot;Ambient light&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1:&lt;/strong&gt; Image without ambient light (left) looks like it was shot in space. Image with ambient (right) looks more natural, although possibly a bit flat if overused.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The real problem is that ambient light does not really exist. Even if youd argue that its useful model, it certainly is not uniform. Its just a quick fix. Therefore all kinds of tricks, like ambient occlusion, have been proposed to enhance the quality of ambient light.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-08-31T11:31:51.543" LastActivityDate="2015-08-31T11:31:51.543" CommentCount="0" />
  <row Id="382" PostTypeId="2" ParentId="377" CreationDate="2015-08-31T08:46:16.797" Score="8" Body="&lt;p&gt;Flat shading is the simplest shading model.  Each rendered polygon has a single normal vector; shading for the entire polygon is constant across the surface of the polygon.  With a small polygon count, this gives curved surfaces a faceted look.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Phong shading is the most sophisticated of the three methods you list.  Each rendered polygon has one normal vector per vertex; shading is performed by interpolating the vectors across the surface and computing the color for each point of interest.  Interpolating the normal vectors gives a reasonable approximation to a smoothly-curved surface while using a limited number of polygons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gourard shading is in between the two: like Phong shading, each polygon has one normal vector per vertex, but instead of interpolating the vectors, the color of each vertex is computed and then interpolated across the surface of the polygon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On modern hardware, you should either use flat shading (if speed is everything) or Phong shading (if quality is important).  Or you can use a programmable-pipeline shader and avoid the whole question.&lt;/p&gt;&#xA;" OwnerUserId="158" LastActivityDate="2015-08-31T08:46:16.797" CommentCount="3" />
  <row Id="383" PostTypeId="2" ParentId="372" CreationDate="2015-08-31T19:56:41.090" Score="10" Body="&lt;p&gt;This issue looks like standard shadow map acne artifacts. Additionally your's lighting equation is incomplete or wrong. Light shouldn't influence faces with normals facing away from it. This also means that with a proper equation the &quot;dark&quot; side of the sphere shouldn't have any acne artifacts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are three sources of acne artifacts:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;First acne source is shadow map precision. Make sure that near and far planes of shadow casting light are as tight as possible. All objects before the near plane can be pancaked, as their exact depth isn't important.&lt;/li&gt;&#xA;&lt;li&gt;Second acne source is shadow map resolution. For a directional light you should be doing cascaded shadow maps with at least 3 1024x1012 cascades for ~100-200m of shadow distance. It's hard to cover similar shadow distance with one shadow map with uniform projection.&lt;/li&gt;&#xA;&lt;li&gt;Third acne source is wide shadow map filter like PCF, as using a single depth depth comparison value across a wide kernel is insufficient. There are many methods to fix it, but none of them is robust.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To sum up, a tight frustum with a few cascades and some bias tweaking is enough to get the general case working (directional light). Start tweaking by disabling shadow filtering and tweak for filtering only when basic shadow maps are robust enough. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally apart from the constant depth bias (which you currently use) you should also add slope depth bias and max slope depth bias. Both can be be implemented either as render state or as shader code during shadowmap rendering. Slope depth bias is simply a magic bias value scaled by dot( normal, lightDir ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are some additional interesting methods and most of them are implemented in the excellent demo: &lt;a href=&quot;https://mynameismjp.wordpress.com/2013/09/10/shadow-maps/&quot;&gt;Matt Pettineo - &quot;A sampling of shadow techniques&quot;&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Flip culling during shadow map rendering (trades acne for Peter panning).&lt;/li&gt;&#xA;&lt;li&gt;Normal offset shadow mapping does wonders for bias issues, but requires to have vertex normals around during shading.&lt;/li&gt;&#xA;&lt;li&gt;Variance based methods (ESM, VSM, EVSM) completely remove bias issues, but have other drawbacks (light leaking and/or performance issues).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="93" LastEditorUserId="93" LastEditDate="2015-08-31T22:01:39.200" LastActivityDate="2015-08-31T22:01:39.200" CommentCount="0" />
  <row Id="385" PostTypeId="2" ParentId="376" CreationDate="2015-08-31T21:43:48.277" Score="4" Body="&lt;p&gt;As Alan and trichoplax mention in the comments, the effect that ambient occlusion simulates is not the occlusion of a surface from the camera but the surface’s occlusion from its surroundings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Think of it this way: say you have even illumination coming from every direction, so that the total incoming light at any point sums up to a value of 1. If you place a flat plane in that environment and look at one side of it, that side is going to receive 50% of that illumination, or 0.5, because the other half is blocked by the plane itself. In other words, any point on the plane’s surface can only “see” the light coming from half of the environment, so it’s half as brightly illuminated. If you fold that plane towards your viewpoint (a “valley” fold), then you decrease the incoming illumination to that side of the plane further, to some value below 0.5, because, again, each point on the plane “sees” a bit less of the light coming from the surroundings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Screen-space ambient occlusion works more or less by looking for these “folds”—areas where the depth changes sharply, as defined by comparing the depths of neighboring pixels—and darkening them to simulate the decreased illumination from the points’ environment.&lt;/p&gt;&#xA;" OwnerUserId="506" LastEditorUserId="506" LastEditDate="2015-08-31T23:07:44.043" LastActivityDate="2015-08-31T23:07:44.043" CommentCount="0" />
  <row Id="386" PostTypeId="2" ParentId="356" CreationDate="2015-08-31T22:03:38.707" Score="16" Body="&lt;p&gt;Whether it's a tile based GPU or not doesn't really affect the texture cache architecture. The memory layout of texture will look like some flavor of &lt;a href=&quot;https://en.wikipedia.org/wiki/Z-order_curve&quot; rel=&quot;nofollow&quot;&gt;Morton order&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_curve&quot; rel=&quot;nofollow&quot;&gt;Hilbert curve&lt;/a&gt; in all GPUs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a result, it's more efficient to render triangles that are close to equilateral triangles because GPU memory system fetches cache lines of texels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So obviously on tile borders, it may happen that you have to fetch texels twice. This has a small cost as tile borders are only &quot;few&quot; pixels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Arguably desktop GPUs behave identically to tile based GPUs as experiments such as the following demonstrate:&#xA;&lt;a href=&quot;http://www.g-truc.net/post-0597.html&quot; rel=&quot;nofollow&quot;&gt;http://www.g-truc.net/post-0597.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The size of the tiles differ but both architecture actually process fragments into a hierarchy of tiles of different sizes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When coding for tile based GPUs my recommendation is to always have in mind:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Don't switch framebuffer objects unless you &lt;em&gt;really&lt;/em&gt; need to.&lt;/li&gt;&#xA;&lt;li&gt;When binding a new framebuffer object, if you don't need to save the content of the current framebuffer, discard it. If don't want to load content of the new framebuffer, then you should clear the framebuffer.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="511" LastEditorUserId="511" LastEditDate="2015-09-09T21:13:03.580" LastActivityDate="2015-09-09T21:13:03.580" CommentCount="7" />
  <row Id="387" PostTypeId="1" AcceptedAnswerId="390" CreationDate="2015-08-31T22:59:35.523" Score="5" ViewCount="128" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_occlusion&quot;&gt;Ambient occlusion&lt;/a&gt; &quot;is a shading and rendering technique used to calculate &lt;strong&gt;how exposed&lt;/strong&gt; each point in a scene is &lt;strong&gt;to ambient lighting&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ambient lighting is regarding to &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/375/what-is-ambient-lighting/&quot;&gt;What is ambient lighting?&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;... lighting to be constant and independent from position: that's ambient&#xA;  lighting.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If ambient lighting has no fixed position, how can points then be more or less &lt;strong&gt;exposed&lt;/strong&gt; to it? Ambient lighting is &quot;&lt;a href=&quot;http://computergraphics.stackexchange.com/a/380/480&quot;&gt;shining by a constant amount in all directions.&lt;/a&gt;&quot; So if the light is coming from &lt;strong&gt;all directions&lt;/strong&gt;, every point should be exposed by &lt;strong&gt;the same factor&lt;/strong&gt; to the ambient lighting.&lt;/p&gt;&#xA;" OwnerUserId="480" LastEditorUserId="480" LastEditDate="2015-08-31T23:45:58.230" LastActivityDate="2015-09-01T11:26:19.687" Title="What is ambient occlusion?" Tags="&lt;shading&gt;&lt;ambient-occlusion&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="388" PostTypeId="1" AcceptedAnswerId="389" CreationDate="2015-08-31T23:13:35.870" Score="5" ViewCount="109" Body="&lt;p&gt;&lt;a href=&quot;http://whatis.techtarget.com/definition/occlusion&quot; rel=&quot;nofollow&quot;&gt;Occlusion&lt;/a&gt; &quot;is the effect of one object in a 3-D space blocking another object from view.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With this definiton in mind I cannot understand sentences like:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The interior of a tube is typically &lt;strong&gt;more occluded&lt;/strong&gt; (and hence darker)&#xA;  than the exposed outer surfaces, and the deeper you go inside the&#xA;  tube, &lt;strong&gt;the more occluded&lt;/strong&gt; (and darker) the lighting becomes.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Occlusion is for me a question of &lt;strong&gt;yes or no&lt;/strong&gt;. Yes an object/point is in front of another, or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can some points be &lt;strong&gt;more&lt;/strong&gt; or &lt;strong&gt;less&lt;/strong&gt; occluded then others? I think I need another definiton for occlusion. &lt;/p&gt;&#xA;" OwnerUserId="480" LastEditorUserId="480" LastEditDate="2015-08-31T23:32:18.633" LastActivityDate="2015-08-31T23:39:22.343" Title="What is occlusion?" Tags="&lt;occlusion&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="389" PostTypeId="2" ParentId="388" CreationDate="2015-08-31T23:39:22.343" Score="5" Body="&lt;p&gt;This quote sounds like it may relate to &lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_occlusion&quot;&gt;ambient occlusion&lt;/a&gt;, which is the shielding of a surface from the light approaching from all angles. Direct lighting requires that the light source have a straight line route to the surface being lit, whereas ambient lighting is the result of light approaching along many different paths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine the whole of a cloudy sky as a light source. Each point on a surface is lit according to the proportion of the sky that can directly light it. The floor next to a wall might only be able to be lit by half the sky, so will be half as bright. In this situation you could say that half of the sky is occluded, leaving only half to light the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly with a tube. At the entrance of the tube, 50% of the sky may be able to light it. Further inside the tube only the small patch of sky visible through the end of the tube can light it, and that patch becomes smaller the further you go into the tube. The lighting in the tube therefore diminishes the further you go in.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-31T23:39:22.343" CommentCount="0" />
  <row Id="390" PostTypeId="2" ParentId="387" CreationDate="2015-08-31T23:55:57.460" Score="4" Body="&lt;p&gt;Your point is correct when applied to &lt;a href=&quot;https://en.wikipedia.org/wiki/Shading#Ambient_lighting&quot; rel=&quot;nofollow&quot;&gt;ambient lighting&lt;/a&gt; as an approximation technique. This crude approach does indeed give the same lighting level to all surfaces regardless of their surroundings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_occlusion&quot; rel=&quot;nofollow&quot;&gt;ambient occlusion&lt;/a&gt; is a different (less crude) approach, that models how the light levels in a real scene vary depending on how much reflected light can reach a surface. The confusion is due to two very different approaches having similar names.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ambient occlusion is a much better approximation of what in real life is described as ambient light - light that arrives from the background rather than directly from a light source.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If ambient lighting has no fixed position, how can points then be more or less exposed to it?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;When used in the context of ambient occlusion, the term &quot;ambient lighting&quot; still means background light approximated as being the same in all directions, but the surface is lit based on how many of those directions are not occluded by objects in the scene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This gives subtly varying light levels that give much more realistic images than with a single level of ambient lighting. This gives the effects seen in real life such as the corners of a room being slightly darker than elsewhere.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2015-08-31T23:55:57.460" CommentCount="6" />
  <row Id="391" PostTypeId="1" CreationDate="2015-09-01T00:39:04.890" Score="9" ViewCount="401" Body="&lt;p&gt;What are Affine Tranformations? Do they apply just to points or to other shapes as well? What does it mean that they can be &quot;composed&quot;?&lt;/p&gt;&#xA;" OwnerUserId="482" LastActivityDate="2015-11-03T13:55:53.667" Title="What are Affine Transformations?" Tags="&lt;transformations&gt;&lt;affine-transformations&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="392" PostTypeId="2" ParentId="391" CreationDate="2015-09-01T00:39:04.890" Score="17" Body="&lt;p&gt;An Affine Transform is a Linear Transform + a Translation Vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}x'&amp;amp; y'\end{bmatrix}&#xA;=&#xA;\begin{bmatrix}x&amp;amp; y\end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}a&amp;amp; b \\ c&amp;amp;d\end{bmatrix}&#xA;+&#xA;\begin{bmatrix}e&amp;amp; f\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can be applied to individual points or to lines or even Bezier curves. For lines, it preserves the property that parallel lines remain parallel. For Bezier curves, it preserves the convex-hull property of the control points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Multiplied-out, it produces 2 equations for yielding a &quot;transformed&quot; coordinate pair $(x', y')$ from the original pair $(x, y)$ and a list of constants $(a, b, c, d, e, f)$.&#xA;$$&#xA;    x' =  a\cdot x + c\cdot y + e \\&#xA;    y' =  b\cdot x + d\cdot y + f&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Conveniently, the Linear transform and the Translation vector can be put together into a 3D matrix which can operate over 2D homogeneous coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}x'&amp;amp; y'&amp;amp;1\end{bmatrix}&#xA;=&#xA;\begin{bmatrix}x&amp;amp; y&amp;amp;1\end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}a&amp;amp; b &amp;amp;0\\ c&amp;amp;d&amp;amp;0 \\ e&amp;amp;f&amp;amp;1\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which yields the same 2 equations above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Very conveniently&lt;/em&gt;, the matrices themselves can be multiplied together to produce a third matrix (of constants) which performs the same transformation as the original 2 would perform in sequence. Put simply, the matrix multiplications are associative.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{matrix}&#xA;\begin{bmatrix}x''&amp;amp; y''&amp;amp;1\end{bmatrix}&#xA;&amp;amp; = &amp;amp;&#xA;\left(&#xA;\begin{bmatrix}x&amp;amp; y&amp;amp;1\end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}a&amp;amp; b &amp;amp;0\\ c&amp;amp;d&amp;amp;0 \\ e&amp;amp;f&amp;amp;1\end{bmatrix}\right)&#xA;\cdot&#xA;\begin{bmatrix}g&amp;amp; h &amp;amp;0\\ i&amp;amp;j&amp;amp;0 \\ k&amp;amp;m&amp;amp;1\end{bmatrix}&#xA;\\&#xA;&amp;amp; = &amp;amp;&#xA;\begin{bmatrix}a \cdot x + c \cdot y+e &amp;amp; b \cdot x + d \cdot y+f &amp;amp;1 \end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}g&amp;amp; h &amp;amp;0\\ i&amp;amp;j&amp;amp;0 \\ k&amp;amp;m&amp;amp;1\end{bmatrix}&#xA;\\&#xA;&amp;amp;=&amp;amp;&#xA;\begin{bmatrix}g(a \cdot x + c \cdot y+e) + i( b \cdot x + d \cdot y+f) + k &#xA;\\&#xA;h(a \cdot x + c \cdot y+e) + j( b \cdot x + d \cdot y+f) + m&#xA;\\1 \end{bmatrix}^T&#xA;\\&#xA;&amp;amp;=&amp;amp;&#xA;\begin{bmatrix}x&amp;amp; y&amp;amp;1\end{bmatrix} \cdot&#xA;\left(&#xA;\begin{bmatrix}a&amp;amp; b &amp;amp;0\\ c&amp;amp;d&amp;amp;0 \\ e&amp;amp;f&amp;amp;1\end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix}g&amp;amp; h &amp;amp;0\\ i&amp;amp;j&amp;amp;0 \\ k&amp;amp;m&amp;amp;1\end{bmatrix}&#xA;\right)&#xA;\\&#xA;&amp;amp;=&amp;amp;&#xA;\begin{bmatrix}x&amp;amp; y&amp;amp;1\end{bmatrix} \cdot&#xA;\begin{bmatrix}ag+bi&amp;amp; ah+bj &amp;amp;0\\ cg+di&amp;amp;ch+dj&amp;amp;0 \\ eg+fi+k&amp;amp;eh+fj+m&amp;amp;1\end{bmatrix}&#xA;\end{matrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively you can consider a few basic transform types and compose any more complex transform by combining these (multiplying them together).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Identity transform&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/icgx0.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/icgx0.png&quot; alt=&quot;identity transform&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    1 &amp;amp; 0 &amp;amp; 0 \\&#xA;    0 &amp;amp; 1 &amp;amp; 0 \\&#xA;    0 &amp;amp; 0 &amp;amp; 1&#xA;\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Scaling&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/YVdBw.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YVdBw.png&quot; alt=&quot;scaling&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    S_x &amp;amp; 0 &amp;amp; 0 \\&#xA;    0  &amp;amp; S_y &amp;amp; 0 \\&#xA;    0 &amp;amp; 0 &amp;amp; 1 \\&#xA;\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Translation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kH8uS.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kH8uS.png&quot; alt=&quot;translation&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    1 &amp;amp; 0 &amp;amp; 0 \\&#xA;    0 &amp;amp; 1 &amp;amp; 0 \\&#xA;    T_x &amp;amp; T_y &amp;amp; 1&#xA;\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Skew x by y&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/gyZCZ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gyZCZ.png&quot; alt=&quot;skew x by y&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    1  &amp;amp; Q_x &amp;amp; 0 \\&#xA;    0 &amp;amp; 1 &amp;amp; 0 \\&#xA;    0 &amp;amp; 0 &amp;amp; 1&#xA;\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Skew y by x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7zEWw.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7zEWw.png&quot; alt=&quot;skew y by x&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    1  &amp;amp; Q_x &amp;amp; 0 \\&#xA;    Q_y &amp;amp; 1 &amp;amp; 0 \\&#xA;    0 &amp;amp; 0 &amp;amp; 1&#xA;\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rotation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Ltr0w.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ltr0w.png&quot; alt=&quot;rotation&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix}&#xA;    \cos\theta &amp;amp; -sin\theta &amp;amp; 0\\&#xA;    \sin\theta &amp;amp; \cos\theta &amp;amp; 0\\&#xA;    0  &amp;amp;  0 &amp;amp;  1&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[Note I've shown the form of Matrix here which accepts a row vector on the &lt;em&gt;left&lt;/em&gt;. The transpose of these matrices will work with a column vector on the right.]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A matrix composed purely from scaling, rotation, and translation can be &lt;a href=&quot;http://stackoverflow.com/a/11942636/733077&quot;&gt;decomposed back into these three components&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="482" LastEditorUserId="16" LastEditDate="2015-11-03T13:55:53.667" LastActivityDate="2015-11-03T13:55:53.667" CommentCount="3" />
  <row Id="393" PostTypeId="1" AcceptedAnswerId="395" CreationDate="2015-09-01T07:14:36.680" Score="6" ViewCount="172" Body="&lt;p&gt;Last time I see a lot of papers which operates on this words, i.e. &lt;em&gt;stochastic AA&lt;/em&gt;, &lt;em&gt;temporal coherence&lt;/em&gt; etc. What does that means? Can you provide examples with explanations in plain English?&lt;/p&gt;&#xA;" OwnerUserId="386" LastActivityDate="2015-09-01T07:55:02.207" Title="What is the meaning of words 'stochastic', 'temporal' in computer graphics?" Tags="&lt;terminology&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="394" PostTypeId="2" ParentId="393" CreationDate="2015-09-01T07:48:57.517" Score="5" Body="&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Stochastic = Random. Stochastic AA, Anti-aliasing technique based on randomized sample distribution (as opposed to fixed sample pattern). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also see: &lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot;&gt;Monte Carlo Method&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Temporal = depends on time. Temporal coherence, how consistent X is over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Typically you hear of temporal aliasing, temporal artefacts etc.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-09-01T07:48:57.517" CommentCount="0" />
  <row Id="395" PostTypeId="2" ParentId="393" CreationDate="2015-09-01T07:55:02.207" Score="5" Body="&lt;ul&gt;&#xA;&lt;li&gt;A stochastic process is one where you gather your samples randomly. So stochastic AA is antialiasing where you gather multiple samples for the same pixel with small random changes.&lt;/li&gt;&#xA;&lt;li&gt;Temporal refers to a process over time. Temporal coherence is for instance mentioned when you have no flickering artifacts, or aliasing that you see during movement. It means that frames are coherent in a certain time window.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="343" LastActivityDate="2015-09-01T07:55:02.207" CommentCount="0" />
  <row Id="396" PostTypeId="2" ParentId="387" CreationDate="2015-09-01T11:26:19.687" Score="5" Body="&lt;p&gt;Ambient light does not really exist, if we do not talk about cosmology. What we call ambient is just light form many secondary reflections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/b1FF5.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/b1FF5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: No ambient light (left) and ambient light (right). Both look artificial.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can approximate ambient light by a constant factor. But this looks slightly washed out as ambient light is not constant over the scene. To make the ambient look better we can use a trick. If we make a hemispherical probe for object coverage (occlusion), we get an estimate of how much the pixel lies inside a cavity. The less you live in cavity the more likely the ambient light is going to hit here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/g57I5.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/g57I5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: By modulating the ambient light with local geometric occlusion we get a much nicer and realistic contribution of ambient light. Final color and contribution also tweaked slightly (right). Just plain ambient (left).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NOTE: Ambient occlusion component is also used by other phenomena so it can also simulate dirt packing etc. In any case your NOT supposed to multiply ambient occlusion by everything. Quite many tutorials make this mistake. You should add the ambient occlusion to existing image, that is if you only estimate ambient light with it.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-09-01T11:26:19.687" CommentCount="1" />
  <row Id="397" PostTypeId="1" CreationDate="2015-09-01T13:07:44.900" Score="6" ViewCount="131" Body="&lt;p&gt;Video noise is random and not correlated in time. I assume, the best method is using &lt;strong&gt;wavelet denoise&lt;/strong&gt; in this case, right? If yes, how is wavelet denoise implemented for video? Is there something to consider in order to apply the calculation to the whole image sequence?&lt;/p&gt;&#xA;" OwnerUserId="18" LastActivityDate="2015-09-15T23:13:13.823" Title="How to denoise video?" Tags="&lt;image&gt;&lt;denoise&gt;&lt;video&gt;&lt;image-processing&gt;" AnswerCount="3" CommentCount="7" FavoriteCount="2" />
  <row Id="398" PostTypeId="2" ParentId="209" CreationDate="2015-09-01T16:16:10.817" Score="8" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Is there a similarly fast way to draw antialiased lines?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No, because by definition an anti-aliased line touches more pixels.  Such algorithms will be slower.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;In a software rasterizer, the ubiquitous way to draw anti-aliased lines is &lt;a href=&quot;https://en.wikipedia.org/wiki/Xiaolin_Wu&amp;#39;s_line_algorithm&quot;&gt;Xiaolin Wu's line algorithm&lt;/a&gt;.  It's not hard to implement, and anyway there's unusually high-quality pseudocode at that link.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a hardware raster pipe, the line primitive is expanded to a screen-space quad by the default (or user-provided) geometry shader, and then drawn as two triangles, which can then be anti-aliased in the usual ways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a raytracer, there are a variety of options.  It's worth thinking about how you actually want to draw a 1D object.  Maybe as a cylinder (woo shadows!).  Note that this introduces issues of perspective/foreshortening which may (or may not) be what you want.  There isn't a clear generalization.  Then, obviously, whatever you do, you just supersample it.&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-01T16:16:10.817" CommentCount="0" />
  <row Id="399" PostTypeId="2" ParentId="323" CreationDate="2015-09-01T16:29:40.593" Score="8" Body="&lt;p&gt;As you have probably observed, in general, you cannot solve in closed-form the equations for ray trajectories around multiple distorting objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The standard approach for such things is just to discretize it.  This looks like Eulerian integration.  Just step your ray a little bit toward the object, calculate gravity from all sources and bend it, then step it more, und so weiter.  This is called &lt;em&gt;raymarching&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since you're interested in problems on an astrophysical scale, you can assume the ray is linear far enough away from the object, with minimal error.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here's a scene I rendered a while ago using a homebrew photonmapper (no QMC) using this raymarching technique.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I present: relativistic photon mapping: black hole in a Cornell box!&#xA;&lt;a href=&quot;http://i.stack.imgur.com/0rxaX.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0rxaX.png&quot; alt=&quot;black hole in a Cornell box&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-01T16:29:40.593" CommentCount="0" />
  <row Id="400" PostTypeId="1" AcceptedAnswerId="402" CreationDate="2015-09-01T18:18:11.607" Score="10" ViewCount="202" Body="&lt;p&gt;I have a couple of compute shaders that need to be executed in a certain order and whose outputs depend on previous inputs. Ideally, I'll never need to copy a buffer client-side and do all of my work on the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider I have two compute shaders compiled and linked as &lt;code&gt;program_one&lt;/code&gt; and &lt;code&gt;program_two&lt;/code&gt;. Suppose I also have a &lt;code&gt;GL_SHADER_STORAGE_BUFFER&lt;/code&gt; that contains the data that is written to by &lt;code&gt;program_one&lt;/code&gt; and read by &lt;code&gt;program_two&lt;/code&gt;. Can I simply do the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glUseProgram(program_one);&#xA;glBindBuffer(GL_SHADER_STORAGE_BUFFER, buffer);&#xA;glBindBufferBase(GL_SHADER_STORAGE_BUFFER, index, buffer);&#xA;glDispatchCompute(16, 16, 1);&#xA;&#xA;glUseProgram(program_two);&#xA;glBindBuffer(GL_SHADER_STORAGE_BUFFER, buffer);&#xA;glBindBufferBase(GL_SHADER_STORAGE_BUFFER, index, buffer);&#xA;glDispatchCompute(16, 16, 1);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it guaranteed that all invocations of the first compute shader will finish before any invocations of the second (to avoid data races between reading and writing &lt;code&gt;buffer&lt;/code&gt;)? If not, how do I synchronize them?&lt;/p&gt;&#xA;" OwnerUserId="197" LastActivityDate="2015-09-01T20:07:25.123" Title="Synchronizing successive OpenGL Compute Shader invocations" Tags="&lt;opengl&gt;&lt;compute-shader&gt;&lt;gpgpu&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="401" PostTypeId="2" ParentId="397" CreationDate="2015-09-01T19:15:45.790" Score="3" Body="&lt;p&gt;Elaboration on temporal solve:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't have much concrete info for you, but I'm drawing from the idea of &quot;temporal anti aliasing&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, if a camera was stationary, you could average pixel values over the last N frames, possibly using harmonic mean or something else like that to help filter out spikes.  The result would be a cleaner, less noisy, more correct image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But not all cameras (or objects!) are stationary, so what then?  Well, if you have some way of identifying where a pixel this frame matches a pixel on the previous N frames, you could average them in the same way.  If a current pixel has no matching previous pixel (due to something previously occluding becoming visible) you just show the raw current value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Games use this for antialiasing by simulating super sampling over time, but they have the benefit of per pixel motion vectors as well as the current and previous camera matrix, so it's a lot harder in your situation!&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-09-01T20:03:34.453" LastActivityDate="2015-09-01T20:03:34.453" CommentCount="0" />
  <row Id="402" PostTypeId="2" ParentId="400" CreationDate="2015-09-01T20:07:25.123" Score="10" Body="&lt;p&gt;No it is not guaranteed, since the OpenGL specification allows that two Compute Shader run concurrently or even in different order.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You need to call &lt;code&gt;glMemoryBarrier(GL_SHADER_STORAGE_BARRIER_BIT)&lt;/code&gt; before the second &lt;code&gt;glDispatchCompute&lt;/code&gt; to ensure visibility of the writes from &lt;code&gt;program_one&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.opengl.org/wiki/Memory_Model&quot;&gt;From the OpenGL.org wiki article on the memory model&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;[...] invocations between stages may be executed in any order. &lt;strong&gt;This includes invocations launched by different rendering commands.&lt;/strong&gt; While it is perhaps unlikely that two vertex shaders from different rendering operations could be running at the same time, it is also possible, so OpenGL provides &lt;strong&gt;no guarantees&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.opengl.org/wiki/Shader_Storage_Buffer_Object&quot;&gt;From the Opengl.org wiki article on Shader Storage Buffer&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;SSBOs reads and writes use incoherent memory accesses, so they need the appropriate barriers, just as Image Load Store operations.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="528" LastActivityDate="2015-09-01T20:07:25.123" CommentCount="0" />
  <row Id="403" PostTypeId="2" ParentId="214" CreationDate="2015-09-02T01:54:45.520" Score="4" Body="&lt;p&gt;It sounds like you're trying to do adaptive subdivision on the surface of a hemisphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This has indeed been tried before, although perhaps not in the exact way you're thinking.  Usually for hemispheric integration, people use some well-distributed, pre-defined sampling density over the hemisphere and forget about adaptive subdivision.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;But you may be missing the point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The whole idea of doing such things is to get a better estimate of the integral.  But it's not the integral over the hemisphere you care about.  You're integrating over the pixel to get AA, over the aperture to get depth-of-field, over the lights to get soft-shadowing, over the hemisphere to get GI, over time to get motion blur, over wavelength to get dispersion, and so on.  You're integrating over all that.  This is an absurdly high-dimensional space, and decades of development have shown that you need to be exploring it not more carefully, but &lt;em&gt;faster&lt;/em&gt;.  That is, you get better results from breadth, not depth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All very abstract.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The point is this: you don't want to sample over the hemisphere with lots of samples.  You want to sample over your hemisphere with &lt;em&gt;one&lt;/em&gt; sample and then &lt;em&gt;move on&lt;/em&gt;.  This is called path tracing.  It differs from ray tracing in that the maximum width of the ray tree is 1 (or maybe 2).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The key insight is that if your random samples are well-distributed with respect to all dimensions, other rays will &quot;fill-in&quot; the rest of your missing tree.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For an intuitive example: say you're doing 4x AA.  So you shoot four rays through a pixel.  Probably, they hit in very similar points (creating similar reflectance hemispheres).  Then, they each shoot exactly one recursive ray.  But, since the four hemispheres are in almost the same place, the recursive rays (kindof) count for &lt;em&gt;all&lt;/em&gt; the hemispheres.  So each of the four paths gets some of the benefit of the other three.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's sortof handwavy math, but since each path is actually independent of the others, and the color you get is an unbiased estimate of the entire scene's illumination, averaging them together is unbiased.  It just converges much faster in practice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Obviously, if a cheap estimate of the hemisphere's integral is available, you can just use that--this is preintegration (cone tracing, etc.), importance sampling (next event estimation, light sampling), or better paths (BDPT, photon mapping)--but for a simple approach, it's basically always better to use path tracing instead of stochastic ray tracing.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;Also &lt;code&gt;matt&lt;/code&gt; -&gt; &lt;code&gt;matte&lt;/code&gt; (or &lt;code&gt;mat&lt;/code&gt;, in a pinch).  It's a French loanword, not a person's name, and I don't care what the American dictionaries say.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-02T01:54:45.520" CommentCount="1" />
  <row Id="404" PostTypeId="2" ParentId="357" CreationDate="2015-09-02T01:59:37.613" Score="5" Body="&lt;p&gt;The answer depends on what you mean.  Modern hardware (e.g. with bindless textures) really doesn't care too much how many textures are &quot;bound&quot;.  The real question is &lt;em&gt;how many you use&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Textures generally store data in a cache-friendly way (a Morton curve, I believe).  If you use more textures, you'll get more cache misses, since now the textures compete with each other for space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This really just comes down to the well-known, old shader programming heuristic: texture taps are slow; don't use too many.&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-02T01:59:37.613" CommentCount="0" />
  <row Id="405" PostTypeId="1" CreationDate="2015-09-02T03:23:51.497" Score="13" ViewCount="263" Body="&lt;p&gt;In my answer about &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/391/what-are-affine-transformations/392#392&quot;&gt;Affine Transformations&lt;/a&gt; I made some little illustrations to help the explanation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ltr0w.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you make an upload-able PNG quickly which expresses a geometrical or mathematical idea?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;This question was contrived to support my postscript answer but is intended to solicit other solutions that may benefit the SE network more generally. But for credibility, an answer must link to a post where this technique has in fact been used, as well as a sufficient &quot;howto&quot; portion if it involves more than something obvious like &quot;Select export from menu&quot;.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;There are two meta questions &lt;a href=&quot;http://meta.computergraphics.stackexchange.com/questions/147/simple-2d-illustrations-question-main-or-meta&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://meta.computergraphics.stackexchange.com/questions/150/2d-illustrations-question-help-crafting-the-question&quot;&gt;here&lt;/a&gt; concerning how to scope this question in light of the precedences it may set for our site.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="482" LastEditorUserId="482" LastEditDate="2015-09-06T16:07:07.420" LastActivityDate="2015-09-16T08:30:31.280" Title="How to produce simple 2D illustrations to accompany geometry answers?" Tags="&lt;untagged&gt;" AnswerCount="5" CommentCount="8" FavoriteCount="5" />
  <row Id="406" PostTypeId="2" ParentId="405" CreationDate="2015-09-02T03:23:51.497" Score="3" Body="&lt;h1&gt;PostScript+ps2eps+mogrify&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;For the &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/391/what-are-affine-transformations/392#392&quot;&gt;Affine Transformations answer&lt;/a&gt;, I wrote a very small postscript program and then saved several copies under different names and edited each one to do each different task: scale, translate, rotate. For example, here's the basic one, &lt;code&gt;ident.ps&lt;/code&gt; which does nothing to change the coordinates between the red drawing and the black drawing. I made the red lines a little larger and always draw it first, so the two show up nicely superposed and visible (IMO). I previewed with ghostscript with the vi (ex) command &lt;code&gt;:!gs %&lt;/code&gt;. (&lt;a href=&quot;https://groups.google.com/d/topic/comp.lang.postscript/m2QqV4QFFaM/discussion&quot; rel=&quot;nofollow&quot;&gt;all source files&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;%!&#xA;/axis {&#xA;    currentpoint 4 { &#xA;        2 copy moveto&#xA;            50 0 lineto&#xA;            90 rotate&#xA;    } repeat pop pop &#xA;} def &#xA;/shape {&#xA;    20 0 moveto&#xA;    30 20 lineto&#xA;    40 0 lineto&#xA;    closepath&#xA;} def &#xA;&#xA;100 100 translate&#xA;&#xA;gsave&#xA;% do transform here&#xA;currentlinewidth 1.5 mul setlinewidth&#xA;1 0 0 setrgbcolor&#xA;0 0 moveto axis shape stroke&#xA;grestore&#xA;&#xA;0 0 moveto axis shape stroke&#xA;&#xA;showpage&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then used the &lt;code&gt;ps2eps&lt;/code&gt; command from &lt;code&gt;psutils&lt;/code&gt; to calculate bounding-boxes automatically&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ps2eps -f *.ps&#xA;Input files: ident.ps rotate.ps scale.ps skewx.ps skewy.ps translate.ps&#xA;Processing: ident.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 50 50 150 150&#xA;Creating output file ident.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;Processing: rotate.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 50 50 150 150&#xA;Creating output file rotate.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;Processing: scale.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 25 25 175 175&#xA;Creating output file scale.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;Processing: skewx.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 50 49 150 151&#xA;Creating output file skewx.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;Processing: skewy.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 49 50 151 150&#xA;Creating output file skewy.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;Processing: translate.ps&#xA;Calculating Bounding Box...ready. %%BoundingBox: 50 50 165 165&#xA;Creating output file translate.eps ... ** Warning **: Weird heading line -- %! -- ready.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And finally used imagemagick's &lt;code&gt;convert&lt;/code&gt; command to produce cropped png images. (I didn't think of a quick way to automate this part. &lt;em&gt;Edit:&lt;/em&gt; as commented, this can be done with &lt;code&gt;mogrify -format png *.eps&lt;/code&gt;. Thanks, joojaa!)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ convert ident.eps ident.png&#xA;&#xA;josh@cadabra ~/affine&#xA;$ convert scale.eps scale.png&#xA;&#xA;josh@cadabra ~/affine&#xA;$ convert rotate.eps rotate.png&#xA;&#xA;josh@cadabra ~/affine&#xA;$ convert skewx.eps skewx.png&#xA;&#xA;josh@cadabra ~/affine&#xA;$ convert translate.eps translate.png&#xA;&#xA;josh@cadabra ~/affine&#xA;$ convert skewy.eps skewy.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which produces PNGs, ready to upload.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ls -l *.png&#xA;-rw-r--r-- 1 josh None  587 Sep  1 21:16 ident.png&#xA;-rw-r--r-- 1 josh None 1431 Sep  1 21:21 rotate.png&#xA;-rw-r--r-- 1 josh None  798 Sep  1 21:19 scale.png&#xA;-rw-r--r-- 1 josh None 1133 Sep  1 21:22 skewx.png&#xA;-rw-r--r-- 1 josh None 1197 Sep  1 21:23 skewy.png&#xA;-rw-r--r-- 1 josh None  756 Sep  1 21:23 translate.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/gyZCZ.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="482" LastEditorUserId="482" LastEditDate="2015-09-02T21:26:53.980" LastActivityDate="2015-09-02T21:26:53.980" CommentCount="2" />
  <row Id="407" PostTypeId="1" AcceptedAnswerId="422" CreationDate="2015-09-02T11:40:47.137" Score="6" ViewCount="230" Body="&lt;p&gt;I am currently working on some simple pixel shader in HLSL. I send to shader texture and i want to make it more colorful (something like in the picture below). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/AvcUu.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/AvcUu.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the picture 1 there is original texture. Picture 2 shows an effect that i want to achieve. Is there some mathematical formula to do that? My input is rgba value of each pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;I'll try to write more concrete. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's say i want to make that garden texture more red. I suppose that what i need to do is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;OutputR = InputR * X,&#xA;&#xA;OuputG = InputG * Y,&#xA;&#xA;OutputB = InpputB * Z&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But how do I find X, Y and Z?&lt;/p&gt;&#xA;" OwnerUserId="205" LastEditorUserId="205" LastEditDate="2015-09-02T11:51:23.007" LastActivityDate="2015-09-07T07:59:28.727" Title="Changing image so it would look like through colorful glasses" Tags="&lt;texture&gt;&lt;algorithm&gt;&lt;pixel-shader&gt;&lt;image-processing&gt;" AnswerCount="3" CommentCount="6" FavoriteCount="1" />
  <row Id="408" PostTypeId="2" ParentId="407" CreationDate="2015-09-02T11:58:46.027" Score="2" Body="&lt;p&gt;Choose the color RGB of your colorful glasses and choose how transparent they are by choosing an alpha value A. Then &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpha_compositing&quot; rel=&quot;nofollow&quot;&gt;alpha composite&lt;/a&gt; the glasses on top of the input image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;OutputR = R * A + InputR * (1-A)&#xA;OutputG = G * A + InputG * (1-A)&#xA;OutputB = B * A + InputB * (1-A)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="192" LastEditorUserId="192" LastEditDate="2015-09-02T12:01:17.487" LastActivityDate="2015-09-02T12:01:17.487" CommentCount="3" />
  <row Id="409" PostTypeId="2" ParentId="374" CreationDate="2015-09-02T12:40:38.147" Score="4" Body="&lt;p&gt;Another option, which I used a couple of decades ago (yikes!), is to use  &lt;a href=&quot;https://www.cs.drexel.edu/~david/Classes/CS586/Papers/p171-toth.pdf&quot; rel=&quot;nofollow&quot;&gt;Toth's scheme from 1985&lt;/a&gt; that employed interval arithmetic to narrow down the search space. IIRC, eventually it will resort to Newton-Rhapson but, again IIRC, I think it rarely required more than one or two steps to get to a good solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though I haven't looked at it (well, apart from a quick glance) &lt;a href=&quot;http://mentallandscape.com/Papers_graphicsinterface90b.pdf&quot; rel=&quot;nofollow&quot;&gt;Mitchell&lt;/a&gt; has published some more recent work on ray tracing with interval maths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;(I should add that, if you are only doing Bezier surfaces, then the interval method might be a bit &quot;overkill&quot; since you can use tricks like blossoming to get bounds and derivatives. If, however, you combine Bezier curves with other functions, e.g. rotation around an axis, then its generality is more useful.)&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-09-03T16:41:57.353" LastActivityDate="2015-09-03T16:41:57.353" CommentCount="0" />
  <row Id="410" PostTypeId="2" ParentId="407" CreationDate="2015-09-02T13:47:43.743" Score="11" Body="&lt;p&gt;(XYZ) can be the RGB colour you want to tint your scene by. For the above scene it can be a red colour (1.0, 0.0, 0.0) or something similar with a strong red component. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bear in mind that since you are applying the colour in a multiplicative way it will act as a filter suppressing original colour components. So if your scene is mostly green but you apply (i.e. multiply it with) a red filter (1,0,0) the resulting scene will be very dark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A trick you could potentially do in such cases is to desaturate the original scene and then multiply it by the tint colour. That way you will keep the overall image intensity and achieve the colour tint you require.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, here is the image multiplied by (1, 0, 0), (1, 0.2, 0.2), and (1, 0.5, 0.5) from left to right:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/N4YCV.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/N4YCV.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://i.stack.imgur.com/95LBW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/95LBW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://i.stack.imgur.com/mkIgR.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/mkIgR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="270" LastEditorUserId="48" LastEditDate="2015-09-03T05:29:26.583" LastActivityDate="2015-09-03T05:29:26.583" CommentCount="4" />
  <row Id="411" PostTypeId="1" CreationDate="2015-09-02T15:02:06.727" Score="2" ViewCount="111" Body="&lt;p&gt;I've inherited a system which animates a humanoid avatar by loading various models and manually calculating transformations for each one. I'm in the process of porting part of this system to Three.js, and rather than port the manual calculations across I'd prefer to pre-compose an articulated model and let the engine handle the transformations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any software which would let me import the existing models, define the bone hierarchy and the model which corresponds to each bone, and export a single articulated model in T-pose?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For what it's worth, I have access to both Windows and Linux, and if really necessary I can probably obtain access to OS X. The original models are in FBX format, but I have managed to export them to Three.js files, so I could import from either format.&lt;/p&gt;&#xA;" OwnerUserId="540" LastEditorUserId="540" LastEditDate="2015-09-02T15:11:55.147" LastActivityDate="2015-09-08T03:58:26.653" Title="Software to compose separate models into an articulated model?" Tags="&lt;transformations&gt;&lt;3d&gt;&lt;animation&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="412" PostTypeId="2" ParentId="324" CreationDate="2015-09-02T21:44:15.287" Score="5" Body="&lt;p&gt;You approach will work, and in general any square of size &amp;lt; &lt;a href=&quot;http://i.stack.imgur.com/sYEU8.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sYEU8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; will work fine because the invariant &quot;at most 1 point per square&quot; is valid. Extrapolating from your idea, it means that one should get the best reduction in area checks for infinitesimally small squares. But we don't want to do that, right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are two reasons why &lt;a href=&quot;http://i.stack.imgur.com/SnBBv.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SnBBv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; is usually better than &lt;a href=&quot;http://i.stack.imgur.com/qH0GY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qH0GY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;How much state do you want to manage? Each square can either be &quot;occupied&quot; or &quot;unoccupied&quot; and contains a pointer to its sample point (if any).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For very large domains with small r, storage costs can be hard to manage. The problem is much worse if you're working with many dimensions. See &lt;a href=&quot;http://staffwww.dcs.shef.ac.uk/people/S.Maddock/research/gamito/papers/poisson.pdf&quot; rel=&quot;nofollow&quot;&gt;Gamito et al.'s paper&lt;/a&gt;, Figure 12. Even in 4D you can be severely storage limited.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In your proposed grid size, each candidate points checks 24 squares, as opposed to 20 in the usual method. There is a per-square cost (e.g. checking whether square is occupied and fetching sample point if it is) which increases the overall cost in your method.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Basically, your method works, but it is usually not more efficient.&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, the state of the art in unbiased Poisson-disk sampling techniques actually successively reduces the grid size as the uncovered area reduces. In addition to &lt;a href=&quot;http://staffwww.dcs.shef.ac.uk/people/S.Maddock/research/gamito/papers/poisson.pdf&quot; rel=&quot;nofollow&quot;&gt;Gamito et al.'s paper&lt;/a&gt;, also see &lt;a href=&quot;http://www.sandia.gov/~samitch/papers/eurographics_mps-final-with-appendix.pdf&quot; rel=&quot;nofollow&quot;&gt;Ebeida et al.'s paper&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S. Sorry, I don't know how to do math formatting in Stack Exchange.&lt;/p&gt;&#xA;" OwnerUserId="14" LastEditorUserId="14" LastEditDate="2015-09-08T18:22:51.277" LastActivityDate="2015-09-08T18:22:51.277" CommentCount="6" />
  <row Id="413" PostTypeId="2" ParentId="405" CreationDate="2015-09-02T21:57:49.120" Score="3" Body="&lt;p&gt;I just use Powerpoint to make such diagrams and it works great as soon as you're comfortable with its basic primitives. It supports all simple operations like rotation, translation, etc. and recent revisions also have shear and 3D transforms. Examples:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Hzmcd.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Hzmcd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ESDaY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ESDaY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1487/14&quot;&gt;example of a Powerpoint diagram&lt;/a&gt; I made for an answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more complicated diagrams, especially in 3D, I sometimes use Blender with shadeless materials. I can elaborate and show examples if it is relevant to your question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looks like &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/324/is-this-smaller-grid-for-poisson-disc-sampling-still-correct&quot;&gt;jsfiddle&lt;/a&gt; is also popular around here.&lt;/p&gt;&#xA;" OwnerUserId="14" LastEditorUserId="14" LastEditDate="2015-09-13T17:20:53.280" LastActivityDate="2015-09-13T17:20:53.280" CommentCount="3" />
  <row Id="414" PostTypeId="2" ParentId="297" CreationDate="2015-09-03T01:44:56.440" Score="5" Body="&lt;p&gt;Your image definitely does not look correct, and it appears that you are not correctly computing the internal path of light rays as they travel through your mesh. From the looks of it, I would say that you are computing the distance between the point where the view ray first enters the cube and where it first hits the interior wall, and using that as your absorption distance. This essentially assumes that light will always exit the glass the first time it hits a wall, which is a poor assumption.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In reality, When light enters glass from air, it often does not immediately exit the glass. This is because when the light strikes the glass/air interface, a phenomenon known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_internal_reflection&quot;&gt;total internal reflection&lt;/a&gt; (TIR) can occur. TIR occurs when light travels from a medium with a higher index of refraction (IOR) to one with a lower IOR, which is precisely what happens in the case of light hitting the interior wall of a glass object. This image from Wikipedia is a good visual demonstration of what it looks like when it occurs:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4lSRl.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4lSRl.jpg&quot; alt=&quot;total internal reflection&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In basic terms, what it means is that if the light hits at a shallow angle, the light will completely reflect off of the interior of the medium. To account for this, you need to evaluate the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fresnel_equations&quot;&gt;Fresnel equations&lt;/a&gt; every time that your light ray hits a glass/air interface (AKA the interior surface of your mesh). The Fresnel equations will tell you the ratio of reflected light to the amount of refracted light, while will be 1 in the case of TIR. You can then compute the appropriate reflected and refracted light directions, and continue to trace the light's path either through the medium or outside of it. If you assume a simple convex mesh with a uniform scattering coefficient, then the the distance to use for Beer's law will be the sum of all internal path lengths before exiting the medium. Here is what a cube looks like with your scattering coefficients and IOR of 1.526 (soda lime glass), rendered using my own path tracer that accounts for both internal and external reflections and refractions:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7MOow.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7MOow.png&quot; alt=&quot;path-traced glass cube&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ultimately the internal reflections and refractions are a major part of what makes glass look like glass. Simple approximations really don't cut it, as you've already found out. It gets even worse if you add multiple meshes and/or non-convex meshes, as you not not only have to account for internal reflections but must also account for rays that leave the medium and enter it at a different point.&lt;/p&gt;&#xA;" OwnerUserId="207" LastActivityDate="2015-09-03T01:44:56.440" CommentCount="2" />
  <row Id="416" PostTypeId="1" AcceptedAnswerId="1453" CreationDate="2015-09-03T08:16:51.817" Score="3" ViewCount="89" Body="&lt;p&gt;I'm currently researching my options on effeciently exchanging data for a webgl application. I understand the gltf format is still being &lt;a href=&quot;https://github.com/KhronosGroup/glTF/blob/master/specification/README.md&quot; rel=&quot;nofollow&quot;&gt;drafted&lt;/a&gt; but I need some example data to understand whether this format is really useful for my application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently using JSON data and I need to figure out whether there are any benefits to using data formats which are closer to the hardware. As far as I understand gltf also uses some JSON structures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any gltf example datasets available already?&lt;/p&gt;&#xA;" OwnerUserId="361" LastEditorUserId="361" LastEditDate="2016-04-19T09:09:34.377" LastActivityDate="2016-04-19T09:09:34.377" Title="Are there any gltf example datasets available?" Tags="&lt;webgl&gt;&lt;vertex-buffer-object&gt;&lt;data-structure&gt;&lt;gltf&gt;&lt;buffers&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="417" PostTypeId="2" ParentId="416" CreationDate="2015-09-03T08:26:17.907" Score="2" Body="&lt;p&gt;Ok sometimes you have to ask a question to figure out the answer already.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example datasets are available in the same repository as linked in the question already. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/KhronosGroup/glTF/blob/e9cf7b0de2d1daed58317f5e6c8b9a2b7d543375/model/SuperMurdoch/SuperMurdoch.gltf&quot; rel=&quot;nofollow&quot;&gt;supermurdoc.gltf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/KhronosGroup/glTF/blob/e9cf7b0de2d1daed58317f5e6c8b9a2b7d543375/model/duck/duck.gltf&quot; rel=&quot;nofollow&quot;&gt;duck.gltf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/KhronosGroup/glTF/tree/master/model&quot; rel=&quot;nofollow&quot;&gt;...&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="361" LastActivityDate="2015-09-03T08:26:17.907" CommentCount="0" />
  <row Id="418" PostTypeId="2" ParentId="405" CreationDate="2015-09-03T09:18:23.187" Score="10" Body="&lt;p&gt;Personally my choice of poison is Adobe Illustrator. Other apps such as inkscape, Corel Draw, Xara Designer will also do. In a pinch even PowerPoint will work. Though I do not personally recommend PowerPoint due to severe issues with its data model, even tough is possibly worlds most deployed drawing app.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It can also be a good idea to have a graphing application at hand. I personally use Mathematica for this although many other apps would work too. Or if you want you can use Wolfram alpha that's a free online version of mathematica.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Disclaimer: Because examples are mine and shameless self endorsement take with a pinch of salt.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Illustrator&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The first thing you should do in any vector drawing app is to &lt;em&gt;enable the snapping grid&lt;/em&gt;. This reduces the tediousness of accurately placing lines. One of the big reasons i do not like to use Inkscape is the inferior way it implements this snapping. PowerPoint actually comes with grid snapping on by default. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not really satisfied by Illustrator as it does not fill all my needs. But no software so far does (Maya used to fill this niche but then autodesk happened).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second reason i choose Illustrator is that it understands postscript natively. So if i need to program something i can do so quickly and existing documents update as my code matures Image 3 is done this way.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/37998/how-to-create-a-vector-curve-programmatically/38010#38010&quot;&gt;Resource&lt;/a&gt; showing snap, postscript and java script access to illustrator and links forward.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here are  some examples of drawings that I have done across the network with Illustrator, mostly by power of grid snap:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/dFHFY.png&quot; alt=&quot;Circle march&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: One of many images i drew in illustrator on this site. Note: I am a quite experienced Illustrator (sometimes commercial) and this is extremely simple by my standards.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Circle march image in is fairly simple to do. Just draw a random shape by clicking around. After this disable snap and let the smart guides do your work for you. Draw lines to closest point on the line, make circles to the line and (i deleted the guide lines after i was done). Rinse and repeat. It took me about 3-5 minutes to draw the image once i got my workspace started.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Wg2jD.png&quot; alt=&quot;Continuation wave&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: GD.SE &lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/45929/what-is-this-design-pattern-called-continuation-wave/45936#45936&quot;&gt;Continuation wave&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While this image seems quite more elaborate, its not much more work than the first image. It is more technical so in addition to grid snap I and now using the shift key to constrain lines to 45 degree angles. Hatch fills are easily just assigned to regions with the shape builder tool of illustrator. (example of &lt;a href=&quot;http://graphicdesign.stackexchange.com/a/57267/18306&quot;&gt;shape builder in use&lt;/a&gt;, screen capture)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/fSG6X1l.png&quot; alt=&quot;tetra image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: Sometimes its easier to start from scratch in postscript. Image added to post on &lt;a href=&quot;http://engineering.stackexchange.com/questions/2949/minimum-number-of-tetra-elements-required-to-represent-a-cube&quot;&gt;ENG.SE&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally a image that really is easier to do by typing the values in postscript. It is relatively easy to do in postscript. Ive still done the typography in illustrator. While drawing this by hand is not a big deal i prefer not to, but i will do it if i must do judgement call by eye.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Mathematica&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Mathematica is wonderful if you want to explore things a bit more. The only drawback to Mathematica is that it is NOT cheap. But it can do nearly anything quickly. Ive used Mathematica for some of my pictures on of &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/358/nurbs-knot-multiplicity&quot;&gt;nurbs basis&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of Mathematicas core competencies s that its easy to develop even fairly complex pictures without much hassle. It can also easily animate your simple picture &lt;em&gt;within you editor&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Graphics[{&#xA;      orig = {&#xA;        Table[GeometricTransformation[&#xA;          Line[{{0, 0}, {50, 0}}],&#xA;          RotationTransform[ang]],&#xA;         {ang, 0, 3 Pi/2, Pi/2}&#xA;         ],&#xA;        Line[{{20, 0}, {30, 20}, {40, 0}}]&#xA;        }, Red,&#xA;      GeometricTransformation[&#xA;       orig,&#xA;       RotationTransform[Pi/6]&#xA;       ]&#xA;      }&#xA;     ]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Results in same picture as others have. Wrapping it inside a table &lt;a href=&quot;http://stackoverflow.com/questions/6109786/how-to-generate-animated-gif-of-a-manipulate-8-0-1&quot;&gt;as per&lt;/a&gt; results in:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/5lcbX.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: Animation is simple to do inside the code editor using Mathematica.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-03T11:27:23.207" LastActivityDate="2015-09-03T11:27:23.207" CommentCount="0" />
  <row Id="419" PostTypeId="2" ParentId="357" CreationDate="2015-09-03T09:20:31.290" Score="10" Body="&lt;p&gt;Just adding to &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/357/is-using-many-texture-maps-bad-for-caching/404#404&quot;&gt;imallett's answer&lt;/a&gt;, it is true that increasing the number of accesses to different texture data in a shader will increase pressure on the GPU cache(s), but there are several other factors that can significantly influence the effect. It's also possibly complicated by the fact that, like CPU caches, there may be several layers of cache in a GPU, ie. Texture Unit &amp;lt;= L0 &amp;lt;= L1 &amp;lt;= ..Memory&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Avoid aliasing -&gt; Use MIP maps&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;If you have a scene that has minification of texture data, be it due to perspective or simple scaling and you aren't using MIP mapping, then you will get aliasing. This is not just going to produce visual artefacts; it is very likely to be a performance problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As soon as you get aliasing, the address accesses to the texture will become incoherent which will not only end up thrashing the caches but introduce a lot of DRAM &quot;page breaks&quot; (more correctly, row breaks) which can be costly. MIP mapping helps reduce the incoherency.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Texture compression&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Perhaps a bit of an obvious option, but if you can use texture compression (e.g. DXTn|ETC*|PVRTC*|etc) targeting from 8bpp down to, say, 2bpp, you can greatly increase the effectiveness of the memory bandwidth/cache by factors of 4x through to 16x.&#xA;Now I can't speak for all GPUs, but &lt;em&gt;some&lt;/em&gt; texture compression schemes (e.g. those listed above) are so simple to decode in hardware, that the data &lt;em&gt;could&lt;/em&gt; stay compressed throughout the entire cache hierarchy and only be decompressed in the texture unit, thus effectively multiplying the size of those caches. &lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Data size&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Obviously, some data, e.g. render targets used as texture data in subsequent renders, can't employ texture compression. Whenever you can, use the smallest pixel format that will do the job, i.e, if 32/16bpp (A)RGB will do, don't use 4x32 float formats!&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Sparse access&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;This is somewhat related to the aliasing example above, but we've seen cases where large render targets are created, but then only very sparsely sampled. Cache lines, be it in CPUs or GPUs, are quite long so if you are only using one pixel in each cache line, you will be wasting transfers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, WRT compressed textures, these achieve compression by effectively sharing data between a local region of texels. If you don't have coherent access then, apart from the memory footprint reduction, the compression probably won't help.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Dependent Texture Reads&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Not so much of a cache issue (well, unless the computed accesses are quite incoherent), but texture accesses that aren't directly defined by the UV coordinates supplied with vertices &lt;em&gt;might&lt;/em&gt; be slower than those that are directly defined.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;&lt;strong&gt;Tiled/Morton VS Strided textures&lt;/strong&gt;&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Although I suspect most textures these days will be stored in either a tiled or Morton-like (aka Twiddled/Swizzled) order (or even a combination of both), some textures &lt;em&gt;might&lt;/em&gt; still be in scan-line order, which means that rotation of the texture is likely to lead to a significant number of cache misses/page breaks. Unfortunately, I don't really know how to spot if a particular format is arranged in such a way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(For background reading, try Blinn's &lt;a href=&quot;http://penguin.ewu.edu/class/cscd470/Winter_13/JimBlinnsCorner/TruthAboutTextureMapping.pdf&quot; rel=&quot;nofollow&quot;&gt;The Truth About Texture Mapping&lt;/a&gt;. FWIW, taking that a few steps further led to the use of Twiddled-order (i.e. Morton order) textures in &lt;a href=&quot;http://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&amp;amp;date=19960814&amp;amp;DB=EPODOC&amp;amp;locale=en_ep&amp;amp;CC=GB&amp;amp;NR=2297886A&amp;amp;KC=A&amp;amp;ND=4&quot; rel=&quot;nofollow&quot;&gt;at least some early PC hardware&lt;/a&gt;). &lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-10-21T07:37:20.547" LastActivityDate="2015-10-21T07:37:20.547" CommentCount="0" />
  <row Id="420" PostTypeId="2" ParentId="151" CreationDate="2015-09-03T09:38:08.500" Score="-2" Body="&lt;p&gt;In case you want to take a look into a ready solution.I have &lt;a href=&quot;https://github.com/sasmaster/TrackballControls&quot; rel=&quot;nofollow&quot;&gt;a port&lt;/a&gt; of THREE.JS TrackBall controlls in C++ and C#&lt;/p&gt;&#xA;" OwnerUserId="213" LastActivityDate="2015-09-03T09:38:08.500" CommentCount="4" />
  <row Id="421" PostTypeId="1" AcceptedAnswerId="427" CreationDate="2015-09-03T13:15:11.750" Score="6" ViewCount="140" Body="&lt;p&gt;In his classic paper &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.6170&quot;&gt;Ray Tracing with Cones&lt;/a&gt;, John Amanatides describes a variation on classical ray tracing. By extending the concept of a &lt;em&gt;ray&lt;/em&gt; by an &lt;em&gt;aperture angle&lt;/em&gt;, making it a cone, aliasing effects (including those originating from too few Monte Carlo samples) can be reduced.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;During cone-triangle-intersection a scalar coverage value is calulated. This value represents the fraction of the cone that is covered by the triangle. If it is less than one it means that the triangle doesn't fully cover the cone. Further tests are required. Without the usage of more advanced techniques however we only know how much of the cone is covered, but not which parts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Amanatides states:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Since at present only the fractional coverage value is used in mixing&#xA;  the contributions from the various objects, overlapping surfaces will&#xA;  be calculated correctly but abutting surfaces will not.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This does not make sense to me. From my point of view it is the other way around. Let's take an example: we have two &lt;em&gt;abutting&lt;/em&gt; triangles, a green and a blue one, each of which covers exactly 50% of our cone. They are at the same distance from the viewer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/n5JTo.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/n5JTo.png&quot; alt=&quot;Green and blue triangle&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green triangle is tested first. It has a coverage value of 0.5, so the blue triangle is tested next. With the blue one's coverage value of 0.5 our cone is fully covered, so we're done and end up with a 50:50 green-blue mixture. Great!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now imagine that we kill the blue triangle and add a red one some distance behind the green one - &lt;em&gt;overlapping&lt;/em&gt;. Greeny gives us a coverage value of 0.5 again. Since we don't have the blue one to test anymore we look further down the cone and soon find the red one. This too returns some coverage value greater than 0, which it shouldn't because it is &lt;em&gt;behind&lt;/em&gt; the green one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, from this I conclude that abutting triangles work fine, while overlapping triangles would need some more magic like coverage masks to be correct. This is the opposite of what Amanatides says. Did I misunderstand something or is this a slip in the paper?&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-09-05T10:03:50.143" Title="Ray Tracing with Cones: coverage, overlapping and abutting triangles" Tags="&lt;raytracing&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="422" PostTypeId="2" ParentId="407" CreationDate="2015-09-03T13:53:39.800" Score="4" Body="&lt;p&gt;Extending Kostas Anagnostou's answer, a commonly used formula for desaturation is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float value = 0.3 * InputR + 0.59 * InputG + 0.11 * InputB;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This accomodates the fact that different color hues are perceived with a different intensity by a human observer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further following the example, you would then define some tint color that is multiplied with the desaturated value. Also you could mix the tinted image with your original to get a less drastic effect. In this case, we're mixing 80% of the desaturated image tinted with (1.0, 0.2, 0.2) with 20% of the original:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3 tint = float3(1.0, 0.2, 0.2);&#xA;float tintMix = 0.8;&#xA;OutputR = tintMix * value * tint.r + (1.0 - tintMix) * InputR;&#xA;OutputG = tintMix * value * tint.g + (1.0 - tintMix) * InputG;&#xA;OutputB = tintMix * value * tint.b + (1.0 - tintMix) * InputB;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-09-07T07:59:28.727" LastActivityDate="2015-09-07T07:59:28.727" CommentCount="0" />
  <row Id="423" PostTypeId="2" ParentId="185" CreationDate="2015-09-03T14:12:49.450" Score="3" Body="&lt;p&gt;The hemispherical intensity function, i.e. the hemispherical function of incident light multiplied by the BRDF, correlates to the number of samples required per solid angle. Take the sample distribution of any method and compare it to that hemispherical function. The more similar they are, the better the method is in that particular case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that since this intensity function is typically &lt;em&gt;unknown&lt;/em&gt;, all of those methods use heuristics. If the assumptions of the heuristics are met, the distribution is better (= closer to the desired function) than a random distribution. If not, it's worse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, importance sampling uses the BRDF to distribute samples, which is simple but only uses a part of the intensity function. A very strong light source illuminating a diffuse surface at shallow angle will get few samples, although its influence might still be huge. Metropolis Light Transport generates new samples from previous one's with high intensity, which is good for few strong light sources, but doesn't help if light arrives evenly from all directions.&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-09-03T14:12:49.450" CommentCount="0" />
  <row Id="424" PostTypeId="1" CreationDate="2015-09-03T15:00:16.987" Score="14" ViewCount="565" Body="&lt;p&gt;In font rendering, it is common to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Subpixel_rendering&quot;&gt;subpixel rendering&lt;/a&gt;.  The basic idea here is to break the pixel into its RGB components and then compute a value for each separately.  Since each component is smaller than the entire pixel, higher quality antialiasing is possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is an obviously analogous way to do the same for a ray tracer.  You do reconstruction filtering on each subchannel separately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was surprised that, however, I couldn't really find reference to ray tracers doing this.  Especially if you're already doing spectral rendering, this seems like an obvious thing to do.  There's &lt;a href=&quot;http://wscg.zcu.cz/wscg2012/full/E03-full.pdf&quot;&gt;this paper&lt;/a&gt; from a journal I've never heard of which seems to be related.  But overall, subpixel rendering just doesn't seem to be a common thing to do.  My question: &lt;strong&gt;why not&lt;/strong&gt;?&lt;/p&gt;&#xA;" OwnerUserId="523" LastEditorUserId="506" LastEditDate="2015-09-05T19:34:22.307" LastActivityDate="2015-09-09T01:43:39.973" Title="Subpixel Rendering for a Ray Tracer" Tags="&lt;raytracing&gt;&lt;antialiasing&gt;&lt;subpixel-rendering&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="425" PostTypeId="2" ParentId="326" CreationDate="2015-09-03T15:09:52.093" Score="2" Body="&lt;p&gt;I've done some VR research; this comes up a lot since rendering the scene multiple times (especially at predicted VR resolutions) is expensive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic problem is that two views provide more information than only one.  In particular, you have two slices of the light field instead of one.  It's related to depth-of-field: screen-space methods fundamentally are incorrect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There has been some work in this area, most related to reprojection techniques that try to do some kind of geometry-aware holefilling in the final image.  This sortof works.  As far as I know, the best approach so far is to render the scene directly for the dominant eye, and then reproject it to the other one.&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-03T15:09:52.093" CommentCount="0" />
  <row Id="427" PostTypeId="2" ParentId="421" CreationDate="2015-09-03T16:11:42.543" Score="3" Body="&lt;p&gt;I did implement a ray tracer based on Amantides' work but, as that was  years ago, my memory of the paper is a little rusty.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, ignoring this particular case, in general when it comes to working with fractional coverage e.g. Alpha compositing, (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpha_compositing#Description&quot; rel=&quot;nofollow&quot;&gt;&quot;A over B&quot;&lt;/a&gt;) my understanding is that the usual assumption is that the items being composited are uncorrelated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus if A with X% coverage is on top of B with Y% coverage and C in the background, then it's assumed that one will see&lt;br&gt;&#xA;    X%*A + (100-X%)*Y% * B + (100-X%)(100-Y%)*C&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does that make sense? Obviously this will give &quot;leaks&quot; in the case where A and B are strongly correlated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I &lt;em&gt;think&lt;/em&gt; I might have put a small bit mask on the rays to avoid these problems, but it was a very long time ago.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-09-05T10:03:50.143" LastActivityDate="2015-09-05T10:03:50.143" CommentCount="2" />
  <row Id="428" PostTypeId="2" ParentId="424" CreationDate="2015-09-03T17:28:50.833" Score="12" Body="&lt;h2&gt;This is perfectly possible&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Although the difference may not especially noticeable, I would expect sampling taking into account the exact &lt;a href=&quot;https://en.wikipedia.org/wiki/Pixel_geometry&quot;&gt;pixel geometry&lt;/a&gt; to give a slightly more accurate image. You just need to offset your pixel centres per colour component according to the (average) location of the subpixel(s) of that colour. Note that not all pixel layouts have a one to one correspondence between pixels and sub pixels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, &lt;a href=&quot;https://en.wikipedia.org/wiki/PenTile_matrix_family#PenTile_RGBG&quot;&gt;penTile RGBG&lt;/a&gt; has twice as many green subpixels as red and blue, as this image from Wikipedia shows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/PenTile_matrix_family#/media/File:Nexus_one_screen_microscope.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oB5Rn.png&quot; alt=&quot;Close up image of RGBG pixel geometry&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not aware of any technical reason that would prevent this being used to produce arbitrary full colour images. In fact a colourful scene will have less noticeable colour artefacts than black on white text, which makes the colour differences hardest to camouflage.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Fonts are rendered on demand&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The relevant difference between rendering a raytraced scene and rendering fonts is that fonts tend to be rendered on demand, and can take into account the screen being used. Contrasting with this, a raytraced scene is often prerendered and then displayed on many different types of screen (with different pixel geometry). For example, displaying your raytraced image on a webpage will prevent tailoring it to a specific monitor type.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you were designing a realtime raytracing program and you had access to check the pixel geometry of the monitor, then you could raytrace to the specific sub pixel layout. However, offline raytracing that produces a still image can only be tailored to a single type of pixel geometry, which will then make the image look &lt;em&gt;worse&lt;/em&gt; on any other pixel geometry. You could work around this by rendering a set of different images and choosing the appropriate one when it is later displayed on a particular type of monitor.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;There is unlikely to be a long term benefit&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;So there is no reason you couldn't develop sub pixel rendering for a raytracer, but it means taking into account an aspect of the monitor that is not always known. Another thing to bear in mind is that you will be developing this software for a shrinking market. Sub pixel rendering is useful for screens that have relatively low resolution. As more and more screens (even mobile screens) are approaching such high resolution that the human eye cannot detect the difference made by sub pixel rendering, your work is likely to be more of theoretical interest than practical use.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-09-03T17:43:37.623" LastActivityDate="2015-09-03T17:43:37.623" CommentCount="7" />
  <row Id="429" PostTypeId="2" ParentId="405" CreationDate="2015-09-03T19:04:15.140" Score="11" Body="&lt;p&gt;I use a variety of tools depending on the particular needs of the moment. I have a bias toward free tools, as I don't have access to things like Illustrator or Mathematica.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For general vector art purposes I use the free &lt;a href=&quot;https://inkscape.org/en/&quot;&gt;Inkscape&lt;/a&gt;. It operates on .svg files and can also render them out as high-quality .pngs or other bitmap formats. For example, here's a diagram I made in Inkscape &lt;a href=&quot;https://gamedev.stackexchange.com/questions/22103/confusion-with-the-texture-coordinate-system-in-xna&quot;&gt;to explain UV mapping&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/t99e7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For 2D function graphs, I often use the online graphing calculator &lt;a href=&quot;https://www.desmos.com/&quot;&gt;Desmos&lt;/a&gt;, which has the handy capability of letting you share graphs online so others can view and modify them. You can also create sliders that can be varied to play with curve parameters in real time. For example, here is a &lt;a href=&quot;https://www.desmos.com/calculator/id7k8wn7ud&quot;&gt;configurable sharpening function&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more complex graphing tasks that involve a bit of programming, I use &lt;a href=&quot;http://maxima.sourceforge.net/&quot;&gt;Maxima&lt;/a&gt;, a free computer algebra system that also has graphing features. I used Maxima to create the graphs in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/289/48&quot;&gt;this answer about Gaussian blurs&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/ugYom.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This was a bit too complex for Desmos, since it involved programmatically creating a piecewise approximation of a Gaussian.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For really odd graphing tasks that aren't handled well by anything else, I've used Python with the &lt;a href=&quot;https://pypi.python.org/pypi/svgwrite/&quot;&gt;svgwrite&lt;/a&gt; package to programmatically generate diagrams into .svg files. These can then be touched up in Inkscape if desired and rendered to bitmaps using Inkscape or ImageMagick. I used that workflow to create the images in &lt;a href=&quot;http://www.reedbeta.com/blog/2015/07/03/depth-precision-visualized/&quot;&gt;this blog post on depth precision&lt;/a&gt;, like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/MVLUh.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case I wanted to not only graph a function, but place tick marks programmatically and nonuniformly along both the axes and the graph, to illustrate discrete depth buffer values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And finally, if nothing else suffices, I'll use Python with &lt;a href=&quot;http://www.numpy.org/&quot;&gt;numpy&lt;/a&gt; and &lt;a href=&quot;https://python-pillow.github.io/&quot;&gt;pillow&lt;/a&gt; to render images pixel-by-pixel. That's how I created the Perlin noise images in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/252/48&quot;&gt;this answer&lt;/a&gt; (with the help of the &lt;a href=&quot;https://pypi.python.org/pypi/noise/&quot;&gt;noise&lt;/a&gt; package too, in this case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/Micts.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-09-03T20:43:56.083" LastActivityDate="2015-09-03T20:43:56.083" CommentCount="1" />
  <row Id="430" PostTypeId="2" ParentId="424" CreationDate="2015-09-03T21:46:36.893" Score="7" Body="&lt;p&gt;Sure, you &lt;a href=&quot;http://new.ics.ele.tue.nl/~dehaan/pdf/90_jsid2003.pdf&quot;&gt;can use subpixel rendering for arbitrary images.&lt;/a&gt;  However, subpixel rendering is really a generic 2D image processing technique &amp;mdash; it has nothing to do with ray tracing specifically.  You could just as well use it with any other 3D rendering method, or even with a simple 2D drawing, photograph or even video.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, I'd say that &quot;subpixel rendering for ray tracing&quot; is really conflating two distinct problem domains that are best treated separately.  About the only relevant connection is that, if you're ray tracing the scene in real time and know that the resulting image is going to be drawn on the screen using subpixel rendering, you can use this information to optimize the pixel density (and aspect ratio) of the intermediate image (e.g. using 3x horizontal pixel density for a typical RGB LCD screen).&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;A potential source of confusion may be that, on current computer system, subpixel rendering is commonly used only for text, and is typically integrated into the font rendering code.  The main reasons for this are arguably historical, but it's also where the biggest payoffs (in terms of visual improvement and readability) typically are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, due to the way text tends to consist of simple and repetitive vector shapes, integrating the subpixel rendering into the font renderer offers some extra optimization opportunities over just rendering the text into a high-resolution buffer and then postprocessing it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, I fully expect that eventually, as technology matures, we'll move to a system where subpixel rendering is simply done transparently by the GPU, or possibly by the screen itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(This will most likely require applications that want to make full use of this feature to deal with physical pixels that are smaller, and not necessarily the same shape as, the &quot;logical pixels&quot;.  But then again, we're already moving in that direction with high-DPI screens.)&lt;/p&gt;&#xA;" OwnerUserId="525" LastActivityDate="2015-09-03T21:46:36.893" CommentCount="1" />
  <row Id="431" PostTypeId="1" AcceptedAnswerId="1431" CreationDate="2015-09-04T00:22:06.640" Score="4" ViewCount="63" Body="&lt;p&gt;Subpixel rendering is used most commonly to anti alias fonts. It works by leveraging the physical layout of the color components of a display to give geometry details to an image that are smaller than a pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, this font has a height of 3 pixels but can easily be read:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/cq392.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cq392.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;from: &lt;a href=&quot;http://www.sitepoint.com/two-teeny-tiny-fonts/&quot; rel=&quot;nofollow&quot;&gt;http://www.sitepoint.com/two-teeny-tiny-fonts/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a monochromatic case, and i could see this possibly working in a greyscale case, but it seems like it always has to be used in situations where you have high contrast.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to use sub pixel rendering in a full color situation, such as those you'd find while doing a 3d render of a scene?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think that it must not be, since it won't be high contrast enough, but does anyone have examples to the contrary?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-09-04T05:59:27.147" Title="Is colorized subpixel rendering possible?" Tags="&lt;color&gt;&lt;subpixel-rendering&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="432" PostTypeId="2" ParentId="361" CreationDate="2015-09-04T00:53:07.067" Score="3" Body="&lt;p&gt;You are correct, it's badly worded. Illumination falls off with the cosine of the angle between the surface normal and the inverse light direction, so the wording implies the light is shining down the original surface normal, and so any tilting away would be tilting away from the lighting direction.&lt;/p&gt;&#xA;" OwnerUserId="554" LastActivityDate="2015-09-04T00:53:07.067" CommentCount="3" />
  <row Id="1431" PostTypeId="2" ParentId="431" CreationDate="2015-09-04T05:13:15.667" Score="3" Body="&lt;p&gt;Here's an example of a downsampling filter that takes pixel geometry into account: &lt;a href=&quot;http://journals.cambridge.org/article_S2048770312000030&quot; rel=&quot;nofollow&quot;&gt;Increasing image resolution on portable displays by subpixel rendering&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Image (a) below is an image downsampled using pixel-based downsampling. Image (b) is downsampled using direct subpixel-based downsampling, which (as far as I can tell) effectively downsamples the R, G, and B planes of the image independently. Image (c) is downsampled using diagonal direct subpixel-based downsampling, which uses a diagonal pattern to improve apparent resolution in both horizontal and vertical directions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kZlsj.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Z4bmt.jpg&quot; alt=&quot;Figure 11 from the paper&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see that the last two images look sharper, but also have the characteristic color fringing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the downsampling pattern used for the last image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/cWk6g.jpg&quot; alt=&quot;DDSD downsampling pattern&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2015-09-04T05:59:27.147" LastActivityDate="2015-09-04T05:59:27.147" CommentCount="3" />
  <row Id="1432" PostTypeId="1" CreationDate="2015-09-04T06:14:31.387" Score="6" ViewCount="716" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Anisotropic_filtering&quot; rel=&quot;nofollow&quot;&gt;Anisotropic filtering&lt;/a&gt; &quot;retains the sharpness of a texture normally lost by MIP map texture's attempts to avoid aliasing&quot;. The Wikipedia article gives hints about how it can be implemented (&quot;probe the texture (...) for any orientation of anisotropy&quot;), but it does't read very clear to me.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the concrete computations performed by (modern) GPUs to choose the correct MIP level when using anisotropic filtering?&lt;/p&gt;&#xA;" OwnerUserId="110" LastEditorUserId="127" LastEditDate="2015-09-04T08:45:57.997" LastActivityDate="2015-09-09T18:33:28.380" Title="How is anisotropic filtering typically implemented in modern GPUs?" Tags="&lt;texture&gt;&lt;gpu&gt;&lt;algorithm&gt;&lt;implementation&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="1433" PostTypeId="2" ParentId="361" CreationDate="2015-09-04T07:40:47.200" Score="5" Body="&lt;p&gt;I see some problems in the quote you posted.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;In effect, a point rotated around its normal vector will not change&#xA;  the way it reflects light.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This is true, because a Lambertian reflector will &lt;strong&gt;never&lt;/strong&gt; change the &lt;em&gt;way&lt;/em&gt; it reflects light. The underlying principle stays the same. Also, Lambertian surfaces are isotropic, so the &lt;em&gt;amount&lt;/em&gt; of reflected light won't change either (which is probably what this sentence is aiming for).&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;However, the point will change the way it reflects light if it is tilted away from its initial normal vector since the area is illuminated by a smaller fraction of the incident radiation.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Again not true, because the principle doesn't change. The amount &lt;em&gt;may&lt;/em&gt; change, except for the special case that the cosine is &amp;lt;= 0 before and after the tilting. The amount does not necessarily &lt;em&gt;grow&lt;/em&gt;, except if we define that the cosine equals 1 before, i.e. that the normal points directly towards the light source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This whole paragraph should probably be rewritten to be less ambiguous. Including isotropy could make it more complete.&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-09-04T07:40:47.200" CommentCount="0" />
  <row Id="1434" PostTypeId="1" AcceptedAnswerId="1440" CreationDate="2015-09-04T08:18:09.100" Score="7" ViewCount="165" Body="&lt;p&gt;In the style of &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/361/is-this-a-mistake-in-the-wikipedia-article-for-lambertian-reflectance&quot;&gt;trichoplax' question&lt;/a&gt;, I want to talk about yet another Wikipedia article: &lt;a href=&quot;https://en.wikipedia.org/wiki/Radiosity_%28computer_graphics%29&quot; rel=&quot;nofollow&quot;&gt;Radiosity (computer graphics)&lt;/a&gt;. The article states:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The technique works with diffuse surfaces only. This way, form factors can be precalculated and are independent of the viewer. The lighting needs to be updated only if some light source changed. If, on the other hand, the technique would support specular reflection, the form factors would depend on the viewer. Form factors and lighting would need to be updated constantly when the camera moves.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does the limitation to diffuse surfaces &lt;em&gt;increase&lt;/em&gt; calculations? Diffuse surfaces need to take light from all directions into account, which is more complex than taking light only from a smaller specular lobe. Is this what this sentence means? Is it just me or should this be rephrased?&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="2041" LastEditDate="2015-11-23T13:17:09.853" LastActivityDate="2015-11-23T13:17:09.853" Title="How does the view-independence of radiosity increase the calculations involved?" Tags="&lt;radiosity&gt;" AnswerCount="3" CommentCount="4" FavoriteCount="2" />
  <row Id="1436" PostTypeId="2" ParentId="1432" CreationDate="2015-09-04T09:18:52.440" Score="12" Body="&lt;p&gt;The texture filtering hardware takes several samples of the various mipmap levels (the maximum amount of samples is indicated by the anisotropic filtering level, though the exact amount of samples taken in a given filtering operation will depend on the proportion between the derivatives on the fragment.) If you project the cone viewing a surface at an oblique angle onto the texture space, it will result in approximately an oval shaped projection, which is more elongated for more oblique angles. Extra samples are taken along the axis of this oval (from the correct mip levels, to take advantage of the pre-filtering they offer) and combined to give a sharper texture sample.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another technique know as &lt;a href=&quot;https://en.wikipedia.org/wiki/Mipmap&quot; rel=&quot;nofollow&quot;&gt;rip-mapping&lt;/a&gt; (mentioned in the Wikipedia article on Mipmapping), which is &lt;em&gt;not&lt;/em&gt; commonly found in contemporary GPUs, uses prefiltering of textures. In contrast to mips, the texture is not scaled down uniformly but using various height-width-ratios (up to a ratio dependent on your chosen anisotropic filtering level). The variant - or maybe two variants if using trilinear filtering - of the texture is then chosen based on the angle of the surface to minimize distortion. Pixel values are fetched using default filtering techniques (bilinear or trilinear). Rip-maps are not used in any hardware that I know of due to their prohibitive size: while mipmaps use additional 33% storage, ripmaps use 300%. This can be verified by noting that texture usage requirements don't increase when using AF, rather, only bandwidth does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For futher reading, you might want to take a look at the specification for the &lt;a href=&quot;https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt&quot; rel=&quot;nofollow&quot;&gt;EXT_texture_filter_anisotropic&lt;/a&gt; OpenGL extension. It details the formulas used to calculate samples and how to combine them when using anisotropic filtering.&lt;/p&gt;&#xA;" OwnerUserId="327" LastEditorUserId="327" LastEditDate="2015-09-09T18:33:28.380" LastActivityDate="2015-09-09T18:33:28.380" CommentCount="4" />
  <row Id="1437" PostTypeId="2" ParentId="1432" CreationDate="2015-09-04T21:38:10.823" Score="6" Body="&lt;p&gt;The API requirements can be found in any of the specs or extensions. Here is one: &lt;a href=&quot;https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt&quot;&gt;https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All GPU vendors likely deviate from the spec because AF-quality used to be a part of many benchmarks. And current implementations will continue to keep on evolving as new workloads stress the existing approximations. Unfortunately, to know exactly what either does, you will need to be a part of one of the companies. But you can gauge the spectrum of possibilities from the following papers, listed in increasing order of quality and implementation cost:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.researchgate.net/publication/3208533_Texram_a_smart_memory_for_texturing&quot;&gt;TexRam&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-99-1.pdf&quot;&gt;FELINE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4056910&amp;amp;tag=1&quot;&gt;EWA&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Quoting from the spec:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Anisotropic texture filtering substantially changes Section 3.8.5.&#xA; Previously a single scale factor P was determined based on the&#xA; pixel's projection into texture space.  Now two scale factors,&#xA; Px and Py, are computed.&#xA;&#xA;   Px = sqrt(dudx^2 + dvdx^2)&#xA;   Py = sqrt(dudy^2 + dvdy^2)&#xA;&#xA;   Pmax = max(Px,Py)&#xA;   Pmin = min(Px,Py)&#xA;&#xA;   N = min(ceil(Pmax/Pmin),maxAniso)&#xA;   Lamda' = log2(Pmax/N)&#xA;&#xA; where maxAniso is the smaller of the texture's value of&#xA; TEXTURE_MAX_ANISOTROPY_EXT or the implementation-defined value of&#xA; MAX_TEXTURE_MAX_ANISOTROPY_EXT.&#xA;&#xA; It is acceptable for implementation to round 'N' up to the nearest&#xA; supported sampling rate.  For example an implementation may only&#xA; support power-of-two sampling rates.&#xA;&#xA; It is also acceptable for an implementation to approximate the ideal&#xA; functions Px and Py with functions Fx and Fy subject to the following&#xA; conditions:&#xA;&#xA;   1.  Fx is continuous and monotonically increasing in |du/dx| and |dv/dx|.&#xA;       Fy is continuous and monotonically increasing in |du/dy| and |dv/dy|.&#xA;&#xA;   2.  max(|du/dx|,|dv/dx|} &amp;lt;= Fx &amp;lt;= |du/dx| + |dv/dx|.&#xA;       max(|du/dy|,|dv/dy|} &amp;lt;= Fy &amp;lt;= |du/dy| + |dv/dy|.&#xA;&#xA; Instead of a single sample, Tau, at (u,v,Lamda), 'N' locations in the mipmap&#xA; at LOD Lamda, are sampled within the texture footprint of the pixel.&#xA;&#xA; Instead of a single sample, Tau, at (u,v,lambda), 'N' locations in&#xA; the mipmap at LOD Lamda are sampled within the texture footprint of&#xA; the pixel.  This sum TauAniso is defined using the single sample Tau.&#xA; When the texture's value of TEXTURE_MAX_ANISOTROPHY_EXT is greater&#xA; than 1.0, use TauAniso instead of Tau to determine the fragment's&#xA; texture value.&#xA;&#xA;                i=N&#xA;                ---&#xA; TauAniso = 1/N \ Tau(u(x - 1/2 + i/(N+1), y), v(x - 1/2 + i/(N+1), y)),  Px &amp;gt; Py&#xA;                /&#xA;                ---&#xA;                i=1&#xA;&#xA;                i=N&#xA;                ---&#xA; TauAniso = 1/N \ Tau(u(x, y - 1/2 + i/(N+1)), v(x, y - 1/2 + i/(N+1))),  Py &amp;gt;= Px&#xA;                /&#xA;                ---&#xA;                i=1&#xA;&#xA;&#xA; It is acceptable to approximate the u and v functions with equally spaced&#xA; samples in texture space at LOD Lamda:&#xA;&#xA;                i=N&#xA;                ---&#xA; TauAniso = 1/N \ Tau(u(x,y)+dudx(i/(N+1)-1/2), v(x,y)+dvdx(i/(N+1)-1/2)), Px &amp;gt; Py&#xA;                /&#xA;                ---&#xA;                i=1&#xA;&#xA;                i=N&#xA;                ---&#xA; TauAniso = 1/N \ Tau(u(x,y)+dudy(i/(N+1)-1/2), v(x,y)+dvdy(i/(N+1)-1/2)), Py &amp;gt;= Px&#xA;                /&#xA;                ---&#xA;                i=1 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-04T21:38:10.823" CommentCount="0" />
  <row Id="1438" PostTypeId="1" AcceptedAnswerId="1451" CreationDate="2015-09-05T01:37:17.573" Score="10" ViewCount="693" Body="&lt;p&gt;How do modern games do geometry level-of-detail for object meshes like characters, terrain, and foliage? There are two parts to my question:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What does the asset pipeline look like? Do artists make a high-poly model which is later decimated? If so, what decimation algorithms are most popular? Are LOD meshes sometimes done by hand?&lt;/li&gt;&#xA;&lt;li&gt;How do engines transition between different object LODs at run time? Are there any smooth or progressive transitions?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The answer might be &quot;different studios use different techniques.&quot; If so, please identify some of the most common practices. It would also be great if you could point me to whitepapers/slides that cover specific examples.&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-06T17:57:59.280" Title="What is the state of art in geometric LOD in games?" Tags="&lt;geometry&gt;" AnswerCount="3" CommentCount="5" FavoriteCount="4" />
  <row Id="1439" PostTypeId="1" CreationDate="2015-09-05T03:54:59.473" Score="6" ViewCount="93" Body="&lt;p&gt;Do modern GPUs support anisotropic filtering for 3D textures? If yes, how can one use it? The OpenGL spec doesn't seem to be very precise on this. From &lt;a href=&quot;https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt&quot;&gt;this link&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Should anything particular be said about anisotropic 3D texture filtering?&#xA;&#xA;  Not sure.  Does the implementation example shown in the spec for&#xA;  2D anisotropic texture filtering readily extend to 3D anisotropic&#xA;  texture filtering?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-07T01:22:17.117" Title="Can we use anisotropic filtering on 3D textures?" Tags="&lt;texture&gt;&lt;3dtexture&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1440" PostTypeId="2" ParentId="1434" CreationDate="2015-09-05T12:12:49.197" Score="8" Body="&lt;p&gt;While it may not entirely clear from the formulation of the wikipedia article, the author raises an important issue: In contrast to many other approaches, Radiosity needs to perform its calculations &lt;strong&gt;for all existing patches, not only the visible ones&lt;/strong&gt;.&#xA;It is not the limitation to diffuse surfaces that increases the involved computations, but the fact that radiance is computed for all surfaces in the entire scene, not only the visible ones.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is in strong contrast to other global illumination techniques like path-tracing, where radiance is computed only for visible samples. While the view-paths may still reach every point of the scene, there might be parts of a scene that are never reached by any view-rays/paths. Therefore, there are no computations at all. Comparing with local GI the &quot;problem of viewpoint independence&quot; its even more apparent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, as the Wikipedia article suggests, this may also be seen as a very useful property, since the computations do not need to be performed for each different viewpoint. This is not the case for most other techniques.&lt;/p&gt;&#xA;" OwnerUserId="528" LastActivityDate="2015-09-05T12:12:49.197" CommentCount="1" />
  <row Id="1441" PostTypeId="1" AcceptedAnswerId="1444" CreationDate="2015-09-05T14:53:31.797" Score="10" ViewCount="179" Body="&lt;p&gt;I have heard that recent GPUs all support non-power-of-2 textures and all features just work. However, I don't understand how mip-mapping would work in such a scenario. Can someone explain?&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2016-05-02T22:40:12.990" Title="How does mip-mapping work with non-power-of-2 textures?" Tags="&lt;texture&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1442" PostTypeId="2" ParentId="1441" CreationDate="2015-09-05T22:42:13.970" Score="4" Body="&lt;p&gt;One way to think of it is that graphics cards often implement non-power-of-2 textures simply by padding them until they are a power of 2 in each direction. This makes most things &quot;just work&quot;: tiling and hardware filtering, for example. The only thing that needs to change is the conversion from texture coordinates to image coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If implemented like that, it's obvious how to do mipmapping: nothing changes. Even if you have a GPU that supports non-power-of-2 textures without padding, the mipmap levels would end up with &quot;padding&quot;. e.g. a 3x3 texture would have a 2x2 texture as lod 1.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-09-05T22:42:13.970" CommentCount="3" />
  <row Id="1444" PostTypeId="2" ParentId="1441" CreationDate="2015-09-06T05:18:09.663" Score="11" Body="&lt;p&gt;The rule is that to compute the next mipmap size, you divide by two and round down to the nearest integer (unless it rounds down to 0, in which case, it's 1 instead). For example, a 57x43 image would have mipmaps like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;level 0: 57x43&#xA;level 1: 28x21&#xA;level 2: 14x10&#xA;level 3: 7x5&#xA;level 4: 3x2&#xA;level 5: 1x1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;UV mapping, LOD selection, and filtering work just the same way as for power-of-two texture sizes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generating good quality mips for a non-power-of-two texture is a little trickier, as you can't simply average a 2x2 box of pixels to downsample in all cases. However, a 2x2 box filter wasn't that great to begin with, so using a better downsampling filter such as Mitchell-Netravali is recommended regardless of the texture size.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-05-02T22:40:12.990" LastActivityDate="2016-05-02T22:40:12.990" CommentCount="1" />
  <row Id="1445" PostTypeId="1" AcceptedAnswerId="1448" CreationDate="2015-09-06T05:41:42.837" Score="6" ViewCount="99" Body="&lt;p&gt;i've recently started to read about spherical harmonics, i have a question about this  spherical harmonics basis function equation which is mentioned in this StupidSH article :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sBx4G.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sBx4G.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;what are those lm() and Re() used for ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the article can be found here : &lt;a href=&quot;http://www.ppsloan.org/publications/StupidSH36.pdf&quot;&gt;http://www.ppsloan.org/publications/StupidSH36.pdf&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1571" LastEditorUserId="1571" LastEditDate="2015-09-06T06:11:09.357" LastActivityDate="2015-11-03T20:25:54.380" Title="Legendre Polynomial equation in Spherical Harmonics" Tags="&lt;lighting&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="1446" PostTypeId="2" ParentId="1438" CreationDate="2015-09-06T06:44:25.300" Score="3" Body="&lt;p&gt;LOD (Level of Detail) means managing objects in different display scales, which could be devided by two parts. However, you may use one of them and that would be enough for most cases.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Show/hide layers ( group of objects of same type) depending on magnitude (display scale).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Algebraic Geomery based method, called Generalization (which is an algorithm to simplify polygons). look at the following picture&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/salks3K.jpg&quot; alt=&quot;generalization&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The most famous and efficient method to generalize (simplify) a polygon mesh is known as &lt;a href=&quot;https://books.google.com/books?isbn=0080477267&quot; rel=&quot;nofollow&quot;&gt;Descartes-Euler polyhedron theorem&lt;/a&gt; (Equation 4.5 sorry if I am refering to a book, that was best I can do) and is used by most of the spatial databases for example PostGIS modules in PostgreSQL. It simply removes smaller sides of a polygon and makes a very rounded one.(above picture)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To implement LOD in a game you need to save and manage the scale of your map (scene) during th zoom in/out operations. The scale changes from zero to infinity and you have to divide this into a particular number of ranges for example something like this:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;1/zero = infinity to 1/50&lt;/li&gt;&#xA;&lt;li&gt;1/50 to 1/100&lt;/li&gt;&#xA;&lt;li&gt;1/100 to 1/1000&lt;/li&gt;&#xA;&lt;li&gt;1/1000 to 1/infinity = 0&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Then you need to define which types of your objects (layers) should be visible or invisible in each of the above ranges. For example a small type of object like a hydrant valve should not be visible when the user is in the fourth range because it will be very small at that scale and can not be discriminated so it doesn't matter if you skip drawing it on the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So when a user uses zoom in and zoom out to change the magnification he moves through the above limits from one to the other range, and your game uses these display scales to manage level of details by showing or hiding objects on the scene. This makes a discrete solution that objects suddenly fade during your zoom out operation, however having the display scales and magnification ranges defined carefully, the user woudn't feel anything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The above group of 4 ranges are just an example and you need to find the best for your own case by trial and error. There is no rule for that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sometimes the games use their own LOD methods, Subway Surfer for instant, shows a small, without texture rectangle to show a building at far, and by getting close suddenly it gets texture,  gamer feels it. You didnt talk about your projection system which is very important also didn't talk about what kind of game you are creating.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However suppose you are implementing a full 3D game with openGl and you wish to filter some mesh before transfering them to graphic hardware, I am sure this will help you to reduce binding/unbinding operations with buffer objects and vertex arrays (VBO,VAO) while dealing with OpenGl.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Using only a layer managment or just implement Euler's Generalization&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In most cases it is not necessary to implement a generalization algorithm, and filtering objects just works and gets you to the efficiency(refresh rate) you need, however it totally depends, case by case. Although it is an easy algorithm that just removes the small sides of a polygon, you need to define a threshold which is the product of magnitude and a constant number, so bigger sides get filtered in a much farther point of view.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Forgeting the layer manamgment and only implementing Eulers generalization algorithm, provides a very neat and continuous method in which you just check each sides and lines against a pre-defined threshold and show them only in case they are big enough to be discriminated on the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S : magnification is a float number &gt; 0 which is equal to 1/scale and the scale is usually &amp;lt; 1 since a 1:1 scale means you have real world lengths in your game.&lt;/p&gt;&#xA;" OwnerUserId="537" LastEditorUserId="537" LastEditDate="2015-09-06T13:03:40.150" LastActivityDate="2015-09-06T13:03:40.150" CommentCount="4" />
  <row Id="1447" PostTypeId="2" ParentId="1445" CreationDate="2015-09-06T07:34:59.560" Score="-2" Body="&lt;p&gt;These are boundary conditions that imposed over Laplace equation to solve it and get an orthogonal result set. (Sturm-Liouville theory)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think you need to learn more with the basics, start with spherical integration and Stokes theorim.&lt;/p&gt;&#xA;" OwnerUserId="537" LastActivityDate="2015-09-06T07:34:59.560" CommentCount="7" />
  <row Id="1448" PostTypeId="2" ParentId="1445" CreationDate="2015-09-06T08:03:12.167" Score="4" Body="&lt;p&gt;The notation Re() and Im() refer to the real and imaginary parts of a complex number.  Mathematicians and physicists are accustomed to using spherical harmonics (and Fourier transforms too) that are complex-valued, due to the factor $e^{im\phi}$.  You would then also have complex coefficients, in general, in the spherical harmonic expansion of a (real or complex) function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler&amp;#39;s_formula&quot; rel=&quot;nofollow&quot;&gt;Euler's formula&lt;/a&gt;, $e^{im\phi} = \cos(m\phi) + i\sin(m\phi)$.  So that factor encodes cosine and sine waves  (that oscillate $m$ times as you move around the equator of the sphere) in its real and imaginary parts, respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When we know we're going to be working strictly with real-valued functions, it may be more convenient to use real-valued variants of the spherical harmonics, where $e^{im\phi}$ is replaced by either $\cos(m\phi)$ or $\sin(m\phi)$, and using real coefficients instead of complex ones.  We trade a single complex coefficient for two real coefficients, so we haven't lost any information or flexibility; it's essentially a change of basis.  This formulation just explicitly ensures that everything always comes out real.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-11-03T20:25:54.380" LastActivityDate="2015-11-03T20:25:54.380" CommentCount="1" />
  <row Id="1449" PostTypeId="2" ParentId="1438" CreationDate="2015-09-06T13:21:32.750" Score="3" Body="&lt;p&gt;Iman already prepared a complete answer but I wish to add something to it&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LOD can be done in two different way&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continous which is called CLOD and is a polygon mesh optimization&lt;/li&gt;&#xA;&lt;li&gt;Discrete which almost every other algorithm than polygon mesh optimization, considered to be in this group.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For example mipmaping is a good,fast but heavy one that belongs to secound group at the above &lt;/p&gt;&#xA;&#xA;&lt;p&gt;here you may find a good explanation to mipmaping and implementation code with OpenGl.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://people.cs.clemson.edu/~dhouse/courses/405/notes/OpenGL-mipmaps.pdf&quot; rel=&quot;nofollow&quot;&gt;http://people.cs.clemson.edu/~dhouse/courses/405/notes/OpenGL-mipmaps.pdf&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1575" LastEditorUserId="537" LastEditDate="2015-09-06T15:59:40.140" LastActivityDate="2015-09-06T15:59:40.140" CommentCount="0" />
  <row Id="1450" PostTypeId="1" AcceptedAnswerId="1452" CreationDate="2015-09-06T17:16:09.613" Score="5" ViewCount="126" Body="&lt;p&gt;I'm making a voxel engine in OpenGL and wondering how many 3D textures I can have at once. They are fairly large (256x256x256 in GL_R32UI format). I want it to be able to run on any graphics card supporting OpenGL 3.3, if possible. I'm accessing them all from the same fragment shader, by the way. So how many can I have? Will 8 work? Thanks!&lt;/p&gt;&#xA;" OwnerUserId="1578" LastActivityDate="2015-09-06T20:52:27.517" Title="How many 3D textures does OpenGL support" Tags="&lt;opengl&gt;&lt;3dtexture&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="1451" PostTypeId="2" ParentId="1438" CreationDate="2015-09-06T17:57:59.280" Score="14" Body="&lt;p&gt;For the geometry LOD most games simply switch between a number of predefined LOD meshes. For example &quot;Infamous: Second Son&quot; uses 3 LOD meshes (&lt;a href=&quot;http://suckerpunch.playstation.com/images/stories/GDC14_infamous_second_son_engine_postmortem.pdf&quot;&gt;Adrian Bentley - &quot;inFAMOUS: Second Son engine postmortem&quot;, GDC 2014&lt;/a&gt;) and &quot;Killzone: Shadow Fall&quot; uses 7 LOD meshes per character (&lt;a href=&quot;http://www.guerrilla-games.com/presentations/Valient_Killzone_Shadow_Fall_Demo_Postmortem.pdf&quot;&gt;Michal Valient - &quot;Killzone: Shadow fall demo postmortem&quot;, Devstation2013&lt;/a&gt;). Most of them are generated, but more important ones (like main character) can be hand made. Meshes are often generated using a popular Simplygon middleware, but sometimes they are simply generated by graphics artists in their favorite 3D package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Games with a large draw distance additionally use imposters for foliage, trees and high buildings (&lt;a href=&quot;http://suckerpunch.playstation.com/images/stories/GDC14_infamous_second_son_engine_postmortem.pdf&quot;&gt;Adrian Bentley - &quot;inFAMOUS: Second Son engine postmortem&quot;, GDC 2014&lt;/a&gt;). They also employ hierarchical LODs, which replace a set of objects with one. For example in &quot;Just Cause 2&quot; trees are first rendered individually as normal LOD meshes, then individually as imposters and finally as a single merged forest mesh (&lt;a href=&quot;http://www.humus.name/Articles/PopulatingAMassiveGameWorld.pptx&quot;&gt;Emil Persson, Joel de Vahl - &quot;Populating a Massive Game World&quot;, Siggraph2013&lt;/a&gt;) and in &quot;Sunset Overdrive&quot; distant parts of the world are replaced by single automatically offline generated mesh (&lt;a href=&quot;http://s3.crashworks.org/gdc15/ElanRuskin_SunsetOverdrive_Streaming_notes.pdf&quot;&gt;Elan Ruskin - &quot;Streaming Sunset Overdrive's Open World&quot;, GDC2015&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another component of a LOD system is simplification of materials and shaders. For example &quot;Killzone: Shadow Fall&quot; disables tangent space and normal mapping for the distant LODs (&lt;a href=&quot;http://www.guerrilla-games.com/presentations/Valient_Killzone_Shadow_Fall_Demo_Postmortem.pdf&quot;&gt;Michal Valient - &quot;Killzone: Shadow fall demo postmortem&quot;, Devstation2013&lt;/a&gt;). This usually is implemented by disabling globally a set of shader features per LOD, but for engines with shader graphs, where artists can create custom shaders, this needs to be done manually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the LOD transitions some games simply switch meshes and some use dithering for smooth LOD transitions - at the LOD switch two meshes are rendered: first gradually fades out and second fades in (&lt;a href=&quot;http://simonschreibt.de/gat/assassins-creed-3-lod-blending/&quot;&gt;Simon schreibt Blog - &quot;Assassins Creed 3 – LoD Blending&quot;&lt;/a&gt;). Classic CPU progressive mesh techniques aren't used as they require a costly mesh update and upload to GPU. Hardware Tessellation is used in a few titles, but only for the refinement of a most detailed LOD, as it's slow and in general case it can't replace predefined geometry LODs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Terrain LODs are handled separately in order to exploit it's specific properties. Terrain geometry LOD is usually implemented using clipmaps (&lt;a href=&quot;http://gdcvault.com/play/1020197/Landscape-Creation-and-Rendering-in&quot;&gt;Marcin Gollent - &quot;Landscape creation and rendering in REDengine 3&quot;&lt;/a&gt;). Terrain material LODs either are handled similarly to mesh LODs or using some kind of a virtual texture &lt;a href=&quot;http://twvideo01.ubm-us.net/o1/vault/gdc2015/presentations/Chen_Ka_AdaptiveVirtualTexture.pdf&quot;&gt;Ka Chen - &quot;Adaptive Virtual Texture Rendering In Far Cry 4&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally if you are interested to see real game LOD pipelines then just browse through documentation of any of the modern games engines: &lt;a href=&quot;https://docs.unrealengine.com/latest/INT/Engine/Content/Types/StaticMeshes/HowTo/LODs/index.html&quot;&gt;Unreal Engine 4 - &quot;Creating and Using LODs&quot;&lt;/a&gt;, &lt;a href=&quot;http://docs.cryengine.com/pages/viewpage.action?pageId=15012200&quot;&gt;CryEgnine - Static LOD&lt;/a&gt; and &lt;a href=&quot;http://docs.unity3d.com/Manual/LevelOfDetail.html&quot;&gt;Unity - LOD&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="93" LastActivityDate="2015-09-06T17:57:59.280" CommentCount="2" />
  <row Id="1452" PostTypeId="2" ParentId="1450" CreationDate="2015-09-06T20:31:32.580" Score="5" Body="&lt;p&gt;As gllampert pointed out in the comments the value is hardware dependent. You can retrieve it with &lt;a href=&quot;https://www.opengl.org/sdk/docs/man3/xhtml/glGet.xml&quot;&gt;glGet&lt;/a&gt;, using &lt;code&gt;GL_MAX_COMBINED_TEXTURE_IMAGE_UNIT&lt;/code&gt;. You can find how different hardware performs &lt;a href=&quot;http://delphigl.de/glcapsviewer/gl_stats_caps_single.php?listreportsbycap=GL_MAX_COMBINED_TEXTURE_IMAGE_UNITS&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in OpenGL 3 there is a lower bound of &lt;strong&gt;at least 48 simultaneously used textures&lt;/strong&gt;, no matter which type. [&lt;a href=&quot;https://www.opengl.org/sdk/docs/man3/xhtml/glActiveTexture.xml&quot;&gt;source&lt;/a&gt;]&lt;/p&gt;&#xA;" OwnerUserId="528" LastEditorUserId="528" LastEditDate="2015-09-06T20:52:27.517" LastActivityDate="2015-09-06T20:52:27.517" CommentCount="2" />
  <row Id="1453" PostTypeId="2" ParentId="416" CreationDate="2015-09-06T21:08:56.360" Score="3" Body="&lt;p&gt;Also check out the glTF models used in Cesium:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Apps/SampleData/models&quot; rel=&quot;nofollow&quot;&gt;https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Apps/SampleData/models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Specs/Data/Models&quot; rel=&quot;nofollow&quot;&gt;https://github.com/AnalyticalGraphicsInc/cesium/tree/master/Specs/Data/Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Patrick&lt;/p&gt;&#xA;" OwnerUserId="1580" LastActivityDate="2015-09-06T21:08:56.360" CommentCount="1" />
  <row Id="1454" PostTypeId="2" ParentId="1439" CreationDate="2015-09-07T01:22:17.117" Score="4" Body="&lt;p&gt;I am not aware of hardware support for 3D anisotropic filtering.  I could be wrong about its existence though.  I believe it has been tried.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The motivation for 2D anisotropic filtering is to prefilter the texture function over the screen area it falls under in a more accurate way than doing a box filter in texture space (usually, anisotropic filtering does &lt;em&gt;several&lt;/em&gt; box filters in texture space).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This definition is key.  2D anisotropic filtering tries to:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Integrate over the part of a flat surface that you can see&lt;br/&gt;(within a single pixel on the screen).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What does 3D anisotropic filtering mean?&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Integrate over the part of  a . . . &lt;em&gt;volume?&lt;/em&gt; . . .  that you can . . . &lt;em&gt;see?&lt;/em&gt;&lt;br/&gt;(within a single . . . &lt;em&gt;voxel?&lt;/em&gt; . . . on the screen)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The best analogue I can think of would be doing some sort of 3D texture lookup anisotropically along the viewing ray.  This isn't quite a direct analogy.  Anyway, the problem with this is that when you're doing volume rendering, you actually care about how this integration is done a lot more than in the 2D case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you want absorption?  Emission?  (You need to do some kind of exponential to integrate.)  Do you want scattering?  (You need to do some kind of recursion or approximating blur to integrate).  And this doesn't even get into what kind of data the 3D texture stores.  Does this texture mean &lt;em&gt;opacity&lt;/em&gt;, or does it mean &lt;em&gt;density&lt;/em&gt;?  Is it &lt;em&gt;diffuse&lt;/em&gt; or is it &lt;em&gt;emission&lt;/em&gt;?  All of these need to be integrated differently.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;New developments in volume rendering are happening all the time, and in any case there are so many variously suboptimal ways you could try to generalize 2D anisotropic filtering, the doubt expressed by the specification makes sense to me:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Does the implementation . . . for 2D anisotropic texture filtering readily extend to 3D anisotropic texture filtering?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No.&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-07T01:22:17.117" CommentCount="2" />
  <row Id="1455" PostTypeId="1" CreationDate="2015-09-07T11:53:53.267" Score="4" ViewCount="315" Body="&lt;p&gt;I know that there are 3 techniques to draw 3D objects:&#xA;(1) Wireframe Modeling and rendering&#xA;(2) Additive Modeling&#xA;(3) Subtractive Modeling&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I correct?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What formula or algorithm can I use to draw a 3D Sphere?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using a low-level library named &lt;a href=&quot;http://winbgim.codecutter.org/&quot; rel=&quot;nofollow&quot;&gt;WinBGIm from colorado university&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, how to draw this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/35GPL.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/35GPL.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And, this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1zy5r.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1zy5r.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="464" LastEditorUserId="464" LastEditDate="2015-12-22T00:24:07.023" LastActivityDate="2015-12-22T00:24:07.023" Title="What formula or algorithm can I use to draw a 3D Sphere without using OpenGL?" Tags="&lt;3d&gt;&lt;geometry&gt;" AnswerCount="1" CommentCount="10" FavoriteCount="1" ClosedDate="2015-09-11T12:10:57.173" />
  <row Id="1456" PostTypeId="2" ParentId="1455" CreationDate="2015-09-07T23:16:56.550" Score="7" Body="&lt;p&gt;Scratchapixel has a nice tutorial on writing a basic rasterizer &lt;a href=&quot;http://scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation&quot;&gt;here&lt;/a&gt;. Also, you could use the projection algorithm &lt;a href=&quot;http://www.dreamincode.net/forums/topic/239174-3d-perspective-projection/&quot;&gt;here&lt;/a&gt; to get the position of the vertices in screen space, then use Bresenham's algorithm or DDA to draw lines in between. If you want to fill them too you can use scanline (you can find it on Wikipedia).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For ellipsoids, you can either just turn them into a triangle mesh, or the approach &lt;a href=&quot;http://comp.graphics.algorithms.narkive.com/0JsT26Rp/efficiently-rasterize-ellipsoid&quot;&gt;here&lt;/a&gt; might work, although I haven't tried it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Z-Buffer algorithm is very straightforward, just calculate the distance from the point to the camera and store that somewhere for each pixel, then make sure that that's less than whatever was already there.&lt;/p&gt;&#xA;" OwnerUserId="1578" LastActivityDate="2015-09-07T23:16:56.550" CommentCount="1" />
  <row Id="1458" PostTypeId="2" ParentId="411" CreationDate="2015-09-08T03:52:28.850" Score="1" Body="&lt;p&gt;FBX is a pretty common interchange format for 3D animation software. The format is originally made for a software called filmbox which handled motion capture data (since renamed to Motion Builder). Currently the format is owned by Autodesk who make and maintain a sdk for the format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Bone structures and mesh bindings are pretty basic functions within 3D modellimg and animation applications. Possible applications are (in alphabetic order):&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;3DS Max&lt;/li&gt;&#xA;&lt;li&gt;Blender&lt;/li&gt;&#xA;&lt;li&gt;Cinema 4D&lt;/li&gt;&#xA;&lt;li&gt;Lightwave3D&lt;/li&gt;&#xA;&lt;li&gt;Maya&lt;/li&gt;&#xA;&lt;li&gt;Modo&lt;/li&gt;&#xA;&lt;li&gt;Motion Builder (this is the software the format is built for)&lt;/li&gt;&#xA;&lt;li&gt;SoftImage3D&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What app to choose depends on your exact needs*. For example do you need to retarget a existing animation to the new bone structure?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many of the game engines also suppprt FBX and possibly limited tweaking within their editor. Atleast Unreal and Unity support fbx. And if you do not try to do anything really invasive you might be able to use these tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* using these apps is outside the scope of this forum.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-08T03:58:26.653" LastActivityDate="2015-09-08T03:58:26.653" CommentCount="1" />
  <row Id="1459" PostTypeId="1" CreationDate="2015-09-08T11:56:20.203" Score="5" ViewCount="265" Body="&lt;p&gt;What is the practical difference between 3D Graphics Engine (OGRE), 3D Game Engine (Quake), 3D Software Rendering Engine, 3D Graphics API (OpenGL/DirectX)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is OpenGL/DirectX a 3D Graphics API or a 3D Software Rendering Engine?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I want to start learning 3D computer graphics and geometric modeling, which one do I need?&lt;/p&gt;&#xA;" OwnerUserId="464" LastEditorUserId="528" LastEditDate="2015-09-16T13:05:43.533" LastActivityDate="2015-09-16T13:05:43.533" Title="What is the practical difference between 3D Graphics Engine, 3D Game Engine, 3D ........?" Tags="&lt;terminology&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" />
  <row Id="1460" PostTypeId="2" ParentId="1459" CreationDate="2015-09-08T12:35:15.227" Score="12" Body="&lt;p&gt;At the base, &lt;strong&gt;rendering APIs&lt;/strong&gt; like OpenGL or Direct3D provide a unified interface to communicate with graphics adapters at a low level. Differences in hardware are hidden behind the abstraction which simplifies development a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The low-level APIs provide basic functionality, which gives you total control but leaves you without a lot of desired features. &lt;strong&gt;3D Rendering Frameworks&lt;/strong&gt; like OGRE or Irrlicht introduce the more understandable concept of 3D objects that can be moved in space (without having to worry about transformation matrices most of the time), introduce light sources and algorithms for shadow casting (e.g. shadow mapping), loading of common mesh formats, and the list goes on. I call this a &lt;em&gt;framework&lt;/em&gt;, because you still need to develop the application yourself using actual code, while an &lt;em&gt;engine&lt;/em&gt; provides you with an application to develop your software in. This is just my personal distinction and both terms are used differently in a variety of contexts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a &lt;strong&gt;3D Authoring / Rendering Software&lt;/strong&gt; like Blender or Autodesk Maya (offline) or 3DExcite DeltaGen (real-time) you can load, display or create meshes using the built-in functionality. These applications target the creation of 3D data, images or movies, and usually you can't create interactive applications easily with it. Be aware that some 3D Authoring have modules specifically targeting this, e.g. Blender and the integrated Blender Game Engine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &lt;strong&gt;3D Game Engine&lt;/strong&gt; finally provides you with all the tools to make it easy for you to develop a game. You will usually be provided with a level editor, a material editor and maybe a visible scripting language (like Unreal's Blueprint or earlier Kismet) to implement dynamic behavior and interactivity. You don't have to worry about rendering algorithms, because they are already implemented for you. Your engine of choice may for example feature &lt;a href=&quot;http://people.mpi-inf.mpg.de/~ritschel/Papers/SSDO.pdf&quot;&gt;SSAO&lt;/a&gt; and &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;cascaded shadow maps&lt;/a&gt; for dynamic objects and a solution to bake global illumination for static geometry. Also, a game engine includes way more than only rendering, e.g. artificial intelligence, sound, loading of assets, exporting your content into a ready-to-install package etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, each software package generally puts another layer of abstraction on, ideally making your life simpler. Each has its own purpose and allows you to do different things. Note that said terms are very broad and people have different opinions and sometimes even debates about where one category ends and the next one begins. My advice is to not lose sleep over it.&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-09-10T09:25:22.543" LastActivityDate="2015-09-10T09:25:22.543" CommentCount="3" />
  <row Id="1461" PostTypeId="1" AcceptedAnswerId="1464" CreationDate="2015-09-08T23:27:42.983" Score="13" ViewCount="444" Body="&lt;p&gt;I have heard from many sources that having T-junctions in 3D meshes is a bad idea because it could result in cracks during rendering. Can someone explain why that happens, and what one can do to avoid them?&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-28T10:31:28.150" Title="Why do T-junctions in meshes result in cracks?" Tags="&lt;mesh&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="1462" PostTypeId="2" ParentId="1461" CreationDate="2015-09-09T01:44:12.587" Score="5" Body="&lt;p&gt;When modeling parametric surfaces with a mesh in the parameter domain, T-junctions will most probably appear as &lt;em&gt;discontinuities&lt;/em&gt; in the surface. These will show up as gaps in the rendering. See below.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More generally, T-junctions in triangle meshes will probably result in discontinuities of interpolated attributes, such as color and normals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5U2ei.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5U2ei.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/d2248.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/d2248.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="192" LastEditorUserId="192" LastEditDate="2015-09-09T01:52:32.200" LastActivityDate="2015-09-09T01:52:32.200" CommentCount="1" />
  <row Id="1463" PostTypeId="2" ParentId="1461" CreationDate="2015-09-09T08:04:11.530" Score="3" Body="&lt;p&gt;Floating-point rounding error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After you transform the T junction and the point in the T can get rounded away from the edge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then it can happen that a fragment that gets sampled for a pixel lies in the gap between the 2 surfaces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can be fixed by not having a T-junction in the first place.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-09-09T08:04:11.530" CommentCount="0" />
  <row Id="1464" PostTypeId="2" ParentId="1461" CreationDate="2015-09-09T08:38:29.530" Score="15" Body="&lt;p&gt;lhf's answer is good from the perspective of tessellation, but these can occur with simpler triangle mesh use cases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Take this trivial example of three, &lt;em&gt;screen-space&lt;/em&gt; triangles, ABC, ADE and DBE...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sFxmG.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sFxmG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although point E was, mathematically, intended to be exactly on the line segment AB, the pipeline won't be using fully precise values, such as rational numbers (e.g. &lt;a href=&quot;https://gmplib.org/&quot; rel=&quot;nofollow&quot;&gt;https://gmplib.org/&lt;/a&gt;). Instead, it will likely be using floats, and so some approximation/error will be introduced. The result is probably going to be something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2QYZw.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2QYZw.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that &lt;em&gt;all&lt;/em&gt; of the vertices may have inaccuracies. Although the example above shows a crack, the T-junction may instead result in overlap along the edge causing pixels to be drawn twice. This might not seem as bad, but it can cause problems with transparency or stencil operations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You &lt;em&gt;might&lt;/em&gt; then think that with floating-point the error introduced will be insignificant, but in a renderer, the screen-space vertex (X,Y) values &lt;em&gt;are nearly always represented by fixed-point numbers&lt;/em&gt; and so the displacement from the ideal location will usually be much greater. Further, as the rendering hardware &quot;interpolates&quot; the line segment pixel-by-pixel with its own internal precision, there is even more chance it will diverge from the rounded location of E.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the T-junction is &quot;removed&quot; by, say, also dividing triangle ABC into two, i.e. AEC and EBC, the problem will go away as the shifts introduced by the errors will all be consistent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, you might ask why do renderers (especially HW) use fixed-point maths for the vertex XY coordinates? Why don't they use floating-point in order to reduce the problem? Although some did (e.g. Sega's Dreamcast) it can lead to another problem where the triangle set-up maths becomes catastrophically inaccurate, particularly for long-thin triangles, and they change size in unpleasant ways.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-09-28T10:31:28.150" LastActivityDate="2015-09-28T10:31:28.150" CommentCount="2" />
  <row Id="1465" PostTypeId="2" ParentId="1461" CreationDate="2015-09-09T09:04:21.967" Score="2" Body="&lt;p&gt;The simple way to avoid this is to ensure that all your vertices are welded&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You issue is that you have cuts along edges with a vertex, but you do not have a corresponding vertex on the adjacent edge to weld/connect it to, if you think of it like a button on a shirt, you've sown on a button to the edge, but haven't given it a hole so the fabric is open.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the picture below, the red dots represent correctly welded vertices, the blue dots all need an additional vertex to be cut in to the adjacent edge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Xy7f2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Xy7f2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generally speaking it is good practice to keep your modelling in quad's &amp;amp; tri's, this helps to alleviate this issue as you should always have a corresponding vertex to weld too. It also helps to keep to quads if you plan to use any subdivision methods on the mesh.&lt;/p&gt;&#xA;" OwnerUserId="118" LastActivityDate="2015-09-09T09:04:21.967" CommentCount="0" />
  <row Id="1466" PostTypeId="1" AcceptedAnswerId="1468" CreationDate="2015-09-09T09:08:28.230" Score="7" ViewCount="362" Body="&lt;p&gt;I've got a lot of confusion and I need to clarify some terminology and put together the knowledge.&lt;br&gt;&#xA;If I say that an engine is a ray tracer (so it uses the ray tracing algorithm to render the scene), is it automatically a physically based engine? I mean, ray tracing is by its definition physically based or there could be some ray tracers that are not physically based? (and, symmetrically, are there some non-ray tracers that are physically based?)&lt;br&gt;&#xA;Also, is it true that &quot;physically based&quot; means also that &quot;resolves the light transport equation&quot; ?&lt;/p&gt;&#xA;" OwnerUserId="1605" OwnerDisplayName="Marina Cooper" LastActivityDate="2015-09-10T06:14:22.217" Title="Physically based rendering and ray tracing" Tags="&lt;rendering&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1467" PostTypeId="1" CreationDate="2015-09-09T14:23:46.073" Score="3" ViewCount="118" Body="&lt;p&gt;I am looking for a good introduction to Lane-Riesenfeld algorithms, which are a family of subdivision methods for generating uniform B-splines. Any suggestions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note.&lt;/em&gt; This is a copy of &lt;a href=&quot;http://math.stackexchange.com/questions/1428101/an-introduction-to-lane-riesenfeld-algorithms&quot;&gt;the question I asked on Mathematics StackExchange&lt;/a&gt;. One of the users there suggested this site might have more answers to offer.&lt;/p&gt;&#xA;" OwnerUserId="1606" LastEditorUserId="1606" LastEditDate="2015-09-09T15:59:58.710" LastActivityDate="2015-09-09T15:59:58.710" Title="An introduction to Lane-Riesenfeld algorithms" Tags="&lt;algorithm&gt;&lt;geometry&gt;&lt;subdivision&gt;&lt;computational-geometry&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="1468" PostTypeId="2" ParentId="1466" CreationDate="2015-09-09T16:46:00.053" Score="11" Body="&lt;p&gt;&quot;Physically based&quot; is not a very well defined term, so it's difficult to answer this question exactly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general, &quot;physically based&quot; refers to the fact that the algorithm in question is derived from physically based principles. It's not physically correct (because we can't afford that) and some approximations usually have to be made, but it's well known what those approximations are and ideally there is some intuition as to what kind of error it introduces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is in contrast to ad hoc models, which are usually made by an artist who observes an effect in real life and tries to write a shader or similar that somehow mimics the look of it. Usually ad hoc models are simpler and cheaper and tend to be the first solutions to turn up for a given problem, but they don't offer any particular insight into what's actually happening. It's also practically impossible to say how accurately such a model is able to reproduce the effect that it attempts to simulate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the context of rendering, a &quot;physically based renderer&quot; would therefore be simply something that renders an image using physically based principles, which is a very vague classification. A ray tracer is not inherently physically based, and most early ray tracers in fact used ad hoc models for lighting and similar. From my personal experience, &quot;physically based rendering&quot; used to usually refer to solving the rendering equation. However, it seems that in recent years, many game engines have claimed this term as well to mean &quot;we do energy conservation&quot; or &quot;we no longer use phong&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So really, there's no hard classification of what &quot;physically based rendering&quot; means, and using ray tracing by itself does not make a renderer physically based. In offline rendering, this term is still mostly used to refer to renderers that solve the rendering equation, whereas in real-time rendering, it more likely refers to the use of microfacet models or similar. But it's possible that the meaning of this term will change over the years.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-09-09T16:46:00.053" CommentCount="1" />
  <row Id="1469" PostTypeId="2" ParentId="1466" CreationDate="2015-09-10T05:55:21.797" Score="2" Body="&lt;p&gt;No, simply:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Physically based rendering does not necessitate raytracing. One can use other means.*&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Raytracing can be used to do other effects than physically based rendering. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Raytracing is often easiest to implement and think out. Therefore its widely deployed for physically based rendering. But for same reason many nonrealistic renders use raytracing tricks to get what they need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* Personally ive been toying with unstructured FEM for rendering images&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-10T06:14:22.217" LastActivityDate="2015-09-10T06:14:22.217" CommentCount="0" />
  <row Id="1470" PostTypeId="1" CreationDate="2015-09-10T08:28:52.850" Score="8" ViewCount="163" Body="&lt;p&gt;When I was reading theory behind physical based rendering I noticed that definition of BSDF and radiance has some problems. For example BSDF of purely specular surfaces is zero almost everywhere and infinite in one point or radiance of directional light is zero for almost all directions except for one where it is again infinite.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This causes problems in rendering equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$L_0(x, \omega_0)= \int_{S^2}{\rho(x, \omega_i,\omega_0)L(x, \omega_i)\,\mathrm{d}\sigma_\perp(\omega_i)}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For purely specular surface this integral has to be zero, from strict mathematical point of view. This is because the BSDF is zero (solid angle)-almost everywhere. You can argue that BSDF is infinite in one point and you have to take this point into account. But how do you know what is the reflectance of the surface at that point? From the infinity you can really tell. Furthermore from mathematical standpoint of view you cannot integrate infinite valued function, even if you could than the only sensible answers would be zero or infinity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know these are subtle problems and in practice can be solved with few &lt;code&gt;if&lt;/code&gt;s but I would like to have theory without holes. I believe that if you embrace these problems in theory than it helps you with placing those &lt;code&gt;if&lt;/code&gt;s in the right place.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;How to deal with this problem? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Not sure entirely but as I can remember Erich Veach &lt;a href=&quot;https://graphics.stanford.edu/papers/veach_thesis/&quot; rel=&quot;nofollow&quot;&gt;in his thesis&lt;/a&gt; address this problem only slightly and tries to get away with it by saying that BSDF and radiance are &lt;a href=&quot;https://en.wikipedia.org/wiki/Distribution_(mathematics)&quot; rel=&quot;nofollow&quot;&gt;distributions&lt;/a&gt;. This is problematic, you cannot multiply two distributions, which is needed in rendering equation. For example when a light from  directional light hits specular surface than you need to multiply two Dirac delta functions together.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is: Is there any work which reformulates rendering equation, BSDF and radiance in such a way that it does not suffer from mentioned problems?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the state of the art theory behind raytracing? Is is still Erich Veach's thesis?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I'm only aware of &lt;a href=&quot;http://cybertron.cg.tu-berlin.de/lessig/projects/dissertation/index.html&quot; rel=&quot;nofollow&quot;&gt;work&lt;/a&gt; of Christian Lessig, but his work is still beyond my mathematical reach.)&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I already have &lt;a href=&quot;http://tomsmathjourneys.blogspot.cz/2015/09/better-code-with-radiance-and-bsdf-as.html&quot; rel=&quot;nofollow&quot;&gt;proposal&lt;/a&gt; how to deal with this problem. I define BSDF and radiance as measure. The basic idea works fine, but the whole theory of light transport needs to be redone to find out if it really works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main purpose of this question is to find out if someone else already did it, so I can read it and then focus my energy somewhere else.&lt;/p&gt;&#xA;" OwnerUserId="1613" LastEditorUserId="16" LastEditDate="2015-11-03T15:19:40.177" LastActivityDate="2015-11-03T15:19:40.177" Title="Problem with definition of BSDF and radiance" Tags="&lt;raytracing&gt;&lt;radiosity&gt;&lt;theory&gt;" AnswerCount="0" CommentCount="7" FavoriteCount="3" />
  <row Id="1471" PostTypeId="2" ParentId="185" CreationDate="2015-09-10T11:11:54.013" Score="7" Body="&lt;p&gt;There is one important distinction to make. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Markov Chain Monte Carlo (such as Metropolis Light Transport) methods fully acknowledge the fact that they produce lots of highly correlated, it is actually the backbone of the algorithm. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;On other hand there are algorithms as Bidirectional Path Tracing, Many Light Method, Photon Mapping where the crucial role plays Multiple Importance Sampling and its balance heuristics. Optimality of balance heuristic is proven only for samples that are independent. Many modern algorithms have correlated samples and for some of them this causes troubles and for some it doesn't.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem with correlated samples was acknowledged in the paper &lt;a href=&quot;https://scholar.google.com/citations?view_op=view_citation&amp;amp;hl=en&amp;amp;user=Glq3dWkAAAAJ&amp;amp;citation_for_view=Glq3dWkAAAAJ:WF5omc3nYNoC&quot;&gt;Probabilistic Connections for Bidirectional Path Tracing&lt;/a&gt;. Where they have altered balance heuristics to take into account the correlation. Have a look at figure 17 in the paper to see the result.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I would like to point out that correlation is &quot;always&quot; bad. If you can afford to make brand new sample than do it. But most of the time you can't afford it so you hope that the error due to the correlation is small.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit to explain the &quot;always&quot;&lt;/strong&gt;: I mean this in the context of MC integration&#xA;&lt;a href=&quot;http://i.stack.imgur.com/st1XC.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/st1XC.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where you measure the error with variance of the estimator&#xA;&lt;a href=&quot;http://i.stack.imgur.com/wEs1y.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wEs1y.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the samples are independent that the covariance term is zero. Correlated samples make always this term nonzero thus increasing variance of the final estimator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is at first sight somewhat contradictory what we encounter with stratified sampling because stratification lowers the error. But you can't prove that stratified sampling converges to the desired result just from the probabilistic point of view, because in the core of stratified sampling there is no probability involved.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;And the deal with stratified sampling is that it is basically not a Monte Carlo method. Stratified sampling comes from standard quadrature rules for numerical integration which works great for integrating smooth function in low dimensions. This is why it is used for handling direct illumination which is low dimensional problem, but its smoothness is disputable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So stratified sampling is yet different kind of correlation than for example correlation in Many Light methods.&lt;/p&gt;&#xA;" OwnerUserId="1613" LastEditorUserId="1613" LastEditDate="2015-09-10T12:27:27.860" LastActivityDate="2015-09-10T12:27:27.860" CommentCount="3" />
  <row Id="1472" PostTypeId="1" AcceptedAnswerId="1476" CreationDate="2015-09-10T13:20:30.943" Score="4" ViewCount="163" Body="&lt;p&gt;I have an issue with rendering my textures in Silverlight. When I look at it from above everything looks fine:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zjAeu.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zjAeu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But if I only change angle of watching it it looks terrible:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4b2AU.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4b2AU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using very simple pixel shader (hlsl):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float4 main(VsOutput vertex) : COLOR&#xA;{&#xA;        float4 texColor = tex2D(textureSampler, texCoord);&#xA;&#xA;        return texColor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I don' t use that pixel shader and load texture via BasicEffect like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;BasicEffect.texture = myTexture&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Everything looks fine. Why my pixel shader affects texture so bad?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I expect that it might be somehow connected with mipmapping, but I am not sure - my texture is pretty small so I guess it shouldn' t have such a big effect. Anyway I need some advices :).&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;" OwnerUserId="205" LastEditorUserId="205" LastEditDate="2015-09-10T13:26:03.863" LastActivityDate="2015-09-11T08:28:29.850" Title="Texture glitch when using simple pixel shader" Tags="&lt;texture&gt;&lt;rendering&gt;&lt;pixel-shader&gt;&lt;hlsl&gt;&lt;artifacts&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1473" PostTypeId="1" CreationDate="2015-09-10T16:45:30.430" Score="10" ViewCount="269" Body="&lt;p&gt;DirectX 12 exposes command queues for either graphics (called &quot;Direct&quot;), compute or copy tasks. In terms of provided functionality, each one is a super-set of the following one. The &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn899124%28v=vs.85%29.aspx&quot;&gt;specification&lt;/a&gt; states that command queues can be executed concurrently by the device.  However, the API does not limit the number of command queues in any way (at least I am not aware any limitation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apparently, different vendors handle this very different:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Intel states in a &lt;a href=&quot;https://software.intel.com/sites/default/files/managed/4a/38/Efficient-Rendering-with-DirectX-12-on-Intel-Graphics.pdf&quot;&gt;recent presentation&lt;/a&gt; (slide 23) that currently their GPUs are not able to handle Graphics &amp;amp; Compute in parallel and that the copy engine has a weak throughput. They advise against the use of multiple graphics/compute queues.&lt;/li&gt;&#xA;&lt;li&gt;AMD started &lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Asynchronous-Shaders-White-Paper-FINAL.pdf&quot;&gt;long time ago&lt;/a&gt; to advertise the use of queues / &quot;asynchronous shaders&quot; starting with Mantle and the current gen consoles. There are also some developers (&lt;a href=&quot;http://fumufumu.q-games.com/archives/TheTechnologyOfTomorrowsChildrenFinal.pdf&quot;&gt;example&lt;/a&gt;) that confirm significant performance gains by executing compute and graphics tasks in parallel.&lt;/li&gt;&#xA;&lt;li&gt;There has been recently &lt;a href=&quot;http://www.guru3d.com/news-story/nvidia-wanted-oxide-dev-dx12-benchmark-to-disable-certain-settings.html&quot;&gt;some fuss&lt;/a&gt; about Nvidia not supporting asynchronous shader in the hardware: Using separate Graphics and Compute queue at once seems to make things slower which indicates driver emulation. Parallel copy operations, on the other hand, have been supported by CUDA for a very long time, which makes it clear that the DMA engine can work independently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Is there any way to decide at runtime if it is meaningful to commit CommandLists to multiple CommandQueues instead of a single one?&lt;/strong&gt; (given that former case does not involve much engineering-overhead)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While I can easily see how it is useful to perform memory operations parallel to compute/graphics operations, it strikes me as unnecessarily complicated to run multiple compute and graphics processes in parallel (unless there is no major perf. benefit).&#xA;It is also not clear to me, how this can lead to significantly better performance anyways; except for pathological cases where many small sequential tasks are not able to generate enough GPU load.&lt;/p&gt;&#xA;" OwnerUserId="528" LastActivityDate="2016-05-04T22:38:30.980" Title="How many Direct/Compute/Copy Queues are meaningful?" Tags="&lt;directx12&gt;&lt;api&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="1476" PostTypeId="2" ParentId="1472" CreationDate="2015-09-11T06:22:54.503" Score="5" Body="&lt;p&gt;So I followed Alan Wolfe suggestion (in comment to my question) And turned out he was right. I was using &lt;code&gt;SamplerState.LinearWrap&lt;/code&gt; and that was the issue. When I changed this to &lt;code&gt;AnisotropicWrap&lt;/code&gt; it looked much better. Below are some examples of different sampling types and how they affect texture:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;graphicsDevice.SamplerStates[0] = SamplerState.PointWrap;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/p7YQk.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/p7YQk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;graphicsDevice.SamplerStates[0] = SamplerState.LinearWrap;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/pldvb.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pldvb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;graphicsDevice.SamplerStates[0] = SamplerState.AnisotropicWrap;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xNkBM.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xNkBM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="205" LastEditorUserId="385" LastEditDate="2015-09-11T08:28:29.850" LastActivityDate="2015-09-11T08:28:29.850" CommentCount="1" />
  <row Id="1477" PostTypeId="2" ParentId="320" CreationDate="2015-09-11T08:22:01.553" Score="2" Body="&lt;p&gt;According to Wikipedia (insert standard disclaimer RE accuracy):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;JPEG does not define which color encoding is to be used for images. JFIF&#xA;  defines the color model to be used: either Y for greyscale, or YCbCr as&#xA;  defined by CCIR 601.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;YCbCr is a non-linear format. As I mentioned earlier, &quot;Video Demystified&quot; states:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;YCbCr is the color space originally defined by BT.601, and now used for&#xA;  all digital component video formats. .... The technically correct notation&#xA;  is Y'Cb'Cr' since all three components are derived from R'G'B'.&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and Charles' Poynton's &lt;a href=&quot;http://poynton.com/notes/colour_and_gamma/ColorFAQ.html&quot; rel=&quot;nofollow&quot;&gt;Colour FAQ&lt;/a&gt; states&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Video systems approximate the lightness response of vision using R'G'B'&#xA;  signals that are each subject to a 0.45 power function.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;With compression we are after the best visual representation per bit and, as the eye is non-linear in approximately this way, it makes sense to use non-linear representations.&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2015-09-11T08:22:01.553" CommentCount="2" />
  <row Id="1478" PostTypeId="1" CreationDate="2015-09-11T13:43:28.783" Score="1" ViewCount="145" Body="&lt;p&gt;There's a thing called &lt;a href=&quot;http://enbdev.com/&quot; rel=&quot;nofollow&quot;&gt;ENB&lt;/a&gt; for PC which works by modifying render functions calls of the games and applying additional effects. I wonder if there is a similar way for console games.&lt;/p&gt;&#xA;" OwnerUserId="1622" LastActivityDate="2015-09-11T13:43:28.783" Title="Is it possible to modify a console game graphics (Xbox 360)?" Tags="&lt;rendering&gt;&lt;3d&gt;&lt;direct3d&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="1479" PostTypeId="1" CreationDate="2015-09-11T18:12:10.897" Score="8" ViewCount="274" Body="&lt;p&gt;Space-filling curves are important in many graphics applications because they help expose spatial locality. We often hear about different algorithms using Z-curves, Morton codes, Hilbert curves, etc. What are the differences between some of these different curves and how do they apply to various applications?&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-12T06:03:22.127" Title="What is the difference between various space-filling curves?" Tags="&lt;space-filling&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="1480" PostTypeId="2" ParentId="1479" CreationDate="2015-09-11T21:24:34.860" Score="7" Body="&lt;p&gt;The difference is how well a mapping preserves locality and how easy it is to encode/decode the keys. The paper &quot;Linear Clustering of Objects with Multiple Attributes&quot; by H V Jagadish says: &quot;Through algebraic analysis, and through computer simulation, we showed that under most circumstances, the Hilbert mapping performed as well as or better than the best of alternative mappings suggested in the literature&quot;. On the other hand, z-order is a bit simpler to use, for example compare the various methods listed in &lt;a href=&quot;https://graphics.stanford.edu/~seander/bithacks.html#InterleaveTableObvious&quot;&gt;Bit Twiddling Hacks&lt;/a&gt; for z-order and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hilbert_curve#Applications_and_mapping_algorithms&quot;&gt;Wikipedia&lt;/a&gt; for Hilbert-order.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the applications, I think the main advantage in using space filling curves is that they map points from higher dimensional space to space of lower dimension. For example, they make it possible to window query for points using traditional B-tree database index. Again, on the other hand, the disadvantage is that one needs to know the bounds of the input in advance as it is difficult to &quot;resize&quot; the mapping later.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: &quot;Z-curve&quot; is the same as &quot;Morton code&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PPS: Additional mappings include &lt;a href=&quot;https://en.wikipedia.org/wiki/Peano_curve&quot;&gt;Peano curve&lt;/a&gt; and for applications see also &lt;a href=&quot;https://en.wikipedia.org/wiki/Geohash&quot;&gt;Geohash&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2015-09-11T21:36:23.550" LastActivityDate="2015-09-11T21:36:23.550" CommentCount="0" />
  <row Id="1481" PostTypeId="2" ParentId="1479" CreationDate="2015-09-12T05:55:26.723" Score="7" Body="&lt;p&gt;Those space filling curves allow to keep locality in multiple dimensions when you &quot;walk&quot; linearly along the curve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I have seen, Z-Order (also known as Morton code) is the most employed because of its computational cost which is constant (and cheap) to access any point of the curve directly. (And easy to implement in hardware with 0 cycle penalty, as it corresponds to &quot;just switching&quot; address wires).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A concrete example of Z-Order curve is texture swizzling : which is basically increasing the cache-hit rate for texture read on GPUs.&#xA;(See images in the article about Z-Curve &lt;a href=&quot;https://en.wikipedia.org/wiki/Z-order_curve&quot;&gt;https://en.wikipedia.org/wiki/Z-order_curve&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the texture is simply stored linearly, you get the maximum cache hit if you render just the texture as 2D image, but if you rotated it by 90 degree on screen, you get into the worst case scenario (cache miss for every texture read).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a result, it is better to trade off a little and lower your best case scenario and have better cache hit for most of the patterns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a personal note, from what I have seen, other curves may require recursive step for their computation and result in bigger cost than Z-Curve with a minimal gain in term of locality coherence. So, I have not heard about those curve used with a practical purpose, except as a research subject in mathematic or creative/funny rendering.&lt;/p&gt;&#xA;" OwnerUserId="105" LastEditorUserId="105" LastEditDate="2015-09-12T06:03:22.127" LastActivityDate="2015-09-12T06:03:22.127" CommentCount="0" />
  <row Id="1482" PostTypeId="2" ParentId="342" CreationDate="2015-09-13T06:13:54.857" Score="1" Body="&lt;p&gt;Gobal illumination, radiosity, etc is not a mutually exclusive set with using a fill light to adjust the feeling. Sometimes the artist can do wonders with a few well placed lights.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So yes they are used, each game and situation is different. Perhaps they are just going for a unrealistic mood or the artist wants to tweak the result a bit.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-09-13T06:13:54.857" CommentCount="0" />
  <row Id="1483" PostTypeId="2" ParentId="241" CreationDate="2015-09-13T07:36:40.457" Score="2" Body="&lt;p&gt;It turns out that the chromaticity chart is harder to read than I anticipated. The CMYK slice is actually triangular of sorts its just that the chromaticity chart is not really linear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The chromaticity chart is made with the assumption that light is a spectra. The curved arc is the pure spectral color of the rainbow. Everything in between is interpolated edge values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/COa4K.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/COa4K.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Visible specturm on the edge, interpolations on surface. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now magentas are a bit peculiar. They only exist in our brains interpretation, thus they can not be measured. Magentas are sort of virtual colors that make the color circle full. This means that when you move from absence of blue towards absence of green your line kinks since the graph has no well defined direction for magenta's. This makes a 4th corner where none should be located.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Magentas are thus a bit of virtual colors that only exist in our brain. Not in physics, they are 2 spectra interleaved. There are off course many such metamers.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-13T11:52:18.847" LastActivityDate="2015-09-13T11:52:18.847" CommentCount="0" />
  <row Id="1484" PostTypeId="1" CreationDate="2015-09-13T12:04:33.700" Score="5" ViewCount="82" Body="&lt;p&gt;How do I go about defining this area without using a piecewise function?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it has something to do with Bilinear Surfaces but I'm not sure how to get started.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/MpA53.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MpA53.png&quot; alt=&quot;Graph showing an area composed of a square region adjoining a triangular region&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1630" LastEditorUserId="231" LastEditDate="2015-09-13T15:45:49.400" LastActivityDate="2015-11-03T15:12:26.733" Title="Two-dimensional bounded area defined parametrically" Tags="&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1485" PostTypeId="1" CreationDate="2015-09-13T14:07:26.800" Score="7" ViewCount="160" Body="&lt;p&gt;Assume my 3D model is a pyramid with a rectangular base and I numerate four corners of the base with 1, 2, 3, 4 and give 5 to the top point. If I project this pyramid into a 2D star-shaped object, would not all four outer points of the star be associated with the point number 5 in the 3D model? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4jnxw.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4jnxw.jpg&quot; alt=&quot;Hand drawn image of a square base pyramid and its star shaped 2D net&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1628" LastEditorUserId="231" LastEditDate="2015-09-13T15:56:23.850" LastActivityDate="2015-09-15T02:18:19.527" Title="Does UV-mapping generate for some points of the 3D model several points in the 2D projection?" Tags="&lt;uv-mapping&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="1486" PostTypeId="2" ParentId="1484" CreationDate="2015-09-13T14:24:06.633" Score="4" Body="&lt;p&gt;One solution is bilinear mapping, which works for every convex quadrilateral.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $A, B, C, D$ be the vertices of a quadrilateral $Q$ in the plane, in this order.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then $f(u,v)=(1-u)(1-v)A + u(1-v)B + uvC + (1-u)vD$ maps the square $[0,1] \times [0,1]$ to $Q$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/DKYWs.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/DKYWs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/3OIsg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3OIsg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="192" LastEditorUserId="137" LastEditDate="2015-11-03T15:12:26.733" LastActivityDate="2015-11-03T15:12:26.733" CommentCount="6" />
  <row Id="1487" PostTypeId="2" ParentId="1485" CreationDate="2015-09-13T17:18:59.183" Score="10" Body="&lt;p&gt;The way it is usually defined, every vertex in a mesh corresponds to a UV-coordinate, and there must be exactly one such coordinate per vertex, because that is how a renderer would fetch texels. So it is impossible to have multiple UV coordinates for a single vertex unless you have multiple parameterizations for your mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, meshes often get split up during parameterizations and the same vertices can end up in multiple locations, just like in your example. Most often, those vertices are simply replicated while loading/rendering so the vertex-to-uv mapping remains 1-to-1 or many-to-1. So a renderer will likely interpret your example as the following (v5 is split into 4):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/gqVkk.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gqVkk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="14" LastEditorUserId="14" LastEditDate="2015-09-13T17:26:02.537" LastActivityDate="2015-09-13T17:26:02.537" CommentCount="0" />
  <row Id="1488" PostTypeId="1" CreationDate="2015-09-13T17:41:06.043" Score="-4" ViewCount="409" Body="&lt;p&gt;i'm using unity 5 to make a small game (im a beginner) and i want to make a 3d model of a character. Im not sure what software there is to do this and if there is some, i would like it to be prefferably free, but if anyone can help or give advice i would appreciate it, just want to be able to make a 3d model, then import to unity 5. Thank you.&lt;/p&gt;&#xA;" OwnerUserId="1632" LastActivityDate="2015-09-13T23:39:40.247" Title="How to make a 3d model for unity 5" Tags="&lt;3d&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2015-09-18T11:02:08.207" />
  <row Id="1489" PostTypeId="2" ParentId="1488" CreationDate="2015-09-13T18:04:12.780" Score="1" Body="&lt;p&gt;&lt;a href=&quot;https://www.blender.org/&quot; rel=&quot;nofollow&quot;&gt;Blender&lt;/a&gt; likely meets your requirements. I have used it in the past to make 3D models and export to model files which Unity can import.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would highly recommend watching some tutorial videos before using it:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gF6qkByl-_M&quot; rel=&quot;nofollow&quot;&gt;one&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=y__uzGKmxt8&quot; rel=&quot;nofollow&quot;&gt;two&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2zd1AI198I8&quot; rel=&quot;nofollow&quot;&gt;three&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://blender.stackexchange.com/&quot;&gt;Blender StackExchange&lt;/a&gt; can help answer more questions about Blender.&lt;/p&gt;&#xA;" OwnerUserId="14" LastEditorUserId="14" LastEditDate="2015-09-13T23:39:40.247" LastActivityDate="2015-09-13T23:39:40.247" CommentCount="4" />
  <row Id="1490" PostTypeId="1" CreationDate="2015-09-14T06:51:06.910" Score="5" ViewCount="85" Body="&lt;p&gt;In all computer graphics books there are algorithms for scan converting simple primitives like lines, circles, ellipse,...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't find algorithms for more advanced curves like bezier curves, b-spline, nurbs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where can I find the references ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also another doubt: are scan converting and rastering the same thing ?&lt;/p&gt;&#xA;" OwnerUserId="1636" LastActivityDate="2015-09-14T13:18:08.537" Title="Algorithms for scan converting b-spline and nurbs" Tags="&lt;curve&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1491" PostTypeId="2" ParentId="1490" CreationDate="2015-09-14T13:10:41.883" Score="6" Body="&lt;p&gt;Ignoring Non-uniform B-splines (rational or not), I have had some experience with rasterisation of Beziers and, since there is a trivial mapping from &lt;em&gt;Uniform&lt;/em&gt; B-Splines to Beziers, those too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have used two different techniques: The first was a scan-line renderer that used Newton-Rhapson to compute the intersection of the current scanline with the curve. This requires the first derivative but, as that can be trivially derived, again as a Bezier representation, from the control points of the parent curve, it's easy to obtain. Further, the bounding box of the control points of the 1st derivative can be useful to detect if the region might have more than 1 intersection with the scan line.&#xA;The current scanline can use the previous scanline's solution as a starting point - usually that gives a highly accurate solution with one iteration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, a second approach is to simply apply binary subdivision of the curve and render it as series of straight line segments with your favourite polygonal renderer. If I recall correctly, at least with cubic Beziers, with each subdivision the error between the line segment joining the end points of the (sub)Bezier and the true curve decreases by a factor of 4. In most cases it therefore doesn't take many subdivisions before the difference between the polygonal approximation and the true curve is insignificant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, if you need info on derivatives of Beziers (or almost everything else related to them), &lt;a href=&quot;https://pomax.github.io/bezierinfo/&quot;&gt;this astounding web-page appeared&lt;/a&gt; in my twitter feed recently.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-09-14T13:18:08.537" LastActivityDate="2015-09-14T13:18:08.537" CommentCount="1" />
  <row Id="1492" PostTypeId="2" ParentId="1485" CreationDate="2015-09-15T02:18:19.527" Score="4" Body="&lt;p&gt;I just need to add a bit to ap_'s answer.&lt;br&gt;&#xA;Of course you can have multiple UV for one vertex, this is called UV layers. This is how engines have displayed lightmaps and albedo maps on the same time, since the advent of lightmaps. (quake engine in 1996 maybe ?)&lt;br&gt;&#xA;You can have more UV layers, if you want to do multitexturing like for terrains, lightmap, albedo large, albedo detail, normal map...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, the rasterizer will never chose a UV layer depending on the primitive it is rendering. Such a renderer could be done with a supplementary index buffer (primitive to UV layer mapping buffer) but no graphic cards and no graphics API supports this. This is an unnecessary violation to KISS anyway.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what one would do in this case is to separate your pyramid apex into 4 distinct vertices during design time. Anyway the first reason for this, not being UV unwrapping but mostly normal definition. If you want sharp edges, you need to break the shading, therefore you need to have different normals, they too, like UV, are usually defined 1-1 in the vertex stream.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Final suggestion, maybe you dont really want to use this unwrapping, I would rather suggest this:  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/r1OYC.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/r1OYC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This way you have seamless continuity for filtering and looks on the edges and on the apex.&lt;/p&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2015-09-15T02:18:19.527" CommentCount="0" />
  <row Id="1493" PostTypeId="1" CreationDate="2015-09-15T11:55:18.913" Score="15" ViewCount="1325" Body="&lt;p&gt;My intuition has always been that when any sphere is projected into 2D space that the result will always mathematically be an ellipse (or a circle in degenerate cases).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the past when I was actively doing my own graphics programming and brought this up with other people they were adamant that I was wrong. If I recall correctly they believed the result could be something vaguely &quot;egg shaped&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Who was correct?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Since there is already one answer submitted, I don't wish to totally change my question but I realize I left out important details due to losing familiarity with the field over the years.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I intended to ask specifically about &lt;strong&gt;perspective projection where the projection is a linear application&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other projections are of course interesting for many uses so I wouldn't want them removed at this point. But it would be great if answers could have perspective projection as their most prominent section.&lt;/p&gt;&#xA;" OwnerUserId="1647" LastEditorUserId="231" LastEditDate="2015-09-18T08:03:39.967" LastActivityDate="2016-02-02T10:36:32.350" Title="Does a sphere projected into 2D space always result in an ellipse?" Tags="&lt;projections&gt;&lt;perspective&gt;" AnswerCount="4" CommentCount="10" FavoriteCount="3" />
  <row Id="1494" PostTypeId="2" ParentId="397" CreationDate="2015-09-15T12:01:59.980" Score="1" Body="&lt;p&gt;In 2009 it seems there was a development of an hierarchical and comparison motion detection algorithm that was proposed in this paper:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://perso.ensta-paristech.fr/~manzaner/Publis/icip09.pdf&quot; rel=&quot;nofollow&quot;&gt;Motion detection: fast and robust algorithms for embedded systems&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They managed to get a decent reduction of noise, as you can see in the last image of the paper. It seems it's the &quot;morphological post-processing&quot; that removes stand-alone pixels (section &lt;strong&gt;2.4 New hierarchical algorithm&lt;/strong&gt;). Perhaps a similar technique can be applied to denoise the video.&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-09-15T12:01:59.980" CommentCount="0" />
  <row Id="1495" PostTypeId="1" CreationDate="2015-09-15T12:02:02.450" Score="4" ViewCount="120" Body="&lt;p&gt;I first read &lt;a href=&quot;http://computergraphics.stackexchange.com/a/401/157&quot;&gt;Alan's answer about video noise removal&lt;/a&gt; which lead me to this question. I found later &lt;a href=&quot;http://www.codeproject.com/Articles/10248/Motion-Detection-Algorithms&quot; rel=&quot;nofollow&quot;&gt;some algorithms ideas&lt;/a&gt; which capture motion in a similar way (frame comparison, as Alan described).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then I found this &lt;a href=&quot;http://perso.ensta-paristech.fr/~manzaner/Publis/icip09.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;motion detection: fast and robust algorithms&quot; paper&lt;/a&gt; where they propose a hierarchical and conditional motion detection algorithm... I didn't read it throughly, but they seem to have isolated well all the objects in motion, since noise was almost completely removed (last image of the paper). However they say:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;As its complexity remains low, this algorithm is well suited for very&#xA;  light embedded systems. Future work will consider other difficult&#xA;  sequences with the presence of clutter like snow, rain or moving&#xA;  trees.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What if my intention was actually use their &quot;morphological post-processing&quot; technique to remove the detection of small objects instead of &quot;stand-alone pixels&quot; (their size would be defined according to the frame resolution)? This paper was published in 2009 and I found &lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs11554-008-0096-7&quot; rel=&quot;nofollow&quot;&gt;this related paper&lt;/a&gt;, also from 2009... Are there any newer updates on this or different techniques?&lt;/p&gt;&#xA;" OwnerUserId="157" LastActivityDate="2015-09-15T12:02:02.450" Title="How to ignore rain or smaller objects movements in motion detection?" Tags="&lt;algorithm&gt;&lt;denoise&gt;&lt;motion&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" ClosedDate="2016-04-07T15:23:12.463" />
  <row Id="1496" PostTypeId="1" CreationDate="2015-09-15T12:06:48.257" Score="6" ViewCount="241" Body="&lt;p&gt;Back in the day when you often had to write your own low level rendering algorithms we all used to learn the Bresenham algorithms for lines and circles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It was almost trivially easy to extend the Bresenham circle algorithm to cover ellipses whose axes were parallel to the X and Y axes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But it always eluded me if there was some Bresenham-style way to render an arbitrary ellipse whose axes were at some arbitrary angle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does such a scanline method for rotated ellipses exist?&lt;/p&gt;&#xA;" OwnerUserId="1647" LastEditorUserId="231" LastEditDate="2015-09-26T21:37:14.117" LastActivityDate="2015-09-29T15:00:23.567" Title="Is there some kind of Bresenham algorithm or equivalent for scanline rendering a rotated ellipse?" Tags="&lt;rendering&gt;&lt;algorithm&gt;&lt;curve&gt;&lt;scanline&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1497" PostTypeId="2" ParentId="1493" CreationDate="2015-09-15T14:39:41.883" Score="8" Body="&lt;p&gt;Projection systems are used to convert a 3D shape to a planar (2D) shape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the type of projection system, different results and shapes like rectangles, pies, ellipses, circles, ... can be produced out of a sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Projection systems can be classified by the characteristics of the result they generate. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To continue, I would like to use a very touchable and common example that we have all seen before, Earth sphere and global wide maps, they are everywhere.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Suppose your sphere is the earth!&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Imagine the earth as your sphere and a planar world map that is created from the spherical shape of the earth. In most of the world maps you see the countries near to the poles are getting much bigger than they are in reality, like Iceland which is 1/14 of Africa continent in reality but the map shows them both as equal. This is because when we are omitting one dimension we loose one characteristic of our shapes.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Different projection systems and their results&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;This is a planar projection which doesn't conserve distance, angles or area. The red circles show the amount of exaggeration that is the product of this projection.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/oOdDDuk.jpg&quot; alt=&quot;First&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Equal-Area, look at Iceland and Africa in this one and compare with above.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/SE2kwxL.jpg&quot; alt=&quot;Second&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Projection systems can be classified by what they preserve.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Equal area.&lt;/li&gt;&#xA;&lt;li&gt;Equal angle which preserve the shape without distortion (conformal).&lt;/li&gt;&#xA;&lt;li&gt;Equal distance.&lt;/li&gt;&#xA;&lt;li&gt;......&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Conformal projections preserve the shapes but area will not be preserved (the first above picture) this one is the most famous projection system that is used in many applications. Your sphere is a rectangle here!&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h3&gt;So you cannot say a sphere will be projected to an ellipse always. As mentioned above a sphere can be projected to a rectangle (first shape) or can be an ellipse but with different characteristics (equal angle, distance, shape, area - see the following picture), or you may also project a sphere into a conic and then open the conic so you will have a pie.&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Each of the above projection systems can be applied with iterative or direct algorithms that can be found on the internet. I didn't talk about the formula and transformations because you didn't ask. Although I wish you to find this answer useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/o1rYfhh.jpg&quot; alt=&quot;Third&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h3&gt;In perspective projections I say yes only ellipses will be produced out of spheres&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Cutting a conic with a horizontal plane creates a circle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cutting with an oblique plane creates a bevel which would be an ellipse or a hyperbola depending on the cutting angle, and when this angle inclines to be vertical in will create a parabola (following picture).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/1FYpdUj.jpg&quot; alt=&quot;Ellipse&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe this is obvious but take a look at their equations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For simplicity I assumed all geometries are origin centered.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Equations:&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Circle: $x^2+y^2=r^2$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ellipse: $x^2/a^2+y^2/b^2=1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hyperbola: $x^2/a^2-y^2/b^2=1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Parabola: $y^2=4ax$&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Morphology :&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;An ellipse has two foci obviously. A circle as a special kind of ellipsis has two foci too but they are coincident. A hyperbola however is a y axis mirror of its equal ellipsis and it has two foci too. A parabola has one focus but actually it has two because the second one is at infinity: when the cutting plane inclines to 90 degrees (bearing angle), second focus goes to infinity.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Conclusion&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;As you see all are ellipses, however you may name them differently to describe special cases, but if you are going to implement it in a game, you need to assume an ellipse equation and it is enough. I can't tell which one of you guys are right, you or your friend, because both could be right.&lt;/p&gt;&#xA;" OwnerUserId="537" LastEditorUserId="137" LastEditDate="2015-11-03T15:17:54.333" LastActivityDate="2015-11-03T15:17:54.333" CommentCount="8" />
  <row Id="1498" PostTypeId="2" ParentId="1493" CreationDate="2015-09-15T15:22:22.677" Score="12" Body="&lt;p&gt;Assuming a perspective projection and a view point external to the sphere, then the 'boundary' formed by the view point and the circle on the sphere which forms the horizon WRT the view point, will be a cone. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Doing a perspective projection (onto a plane) is then equivalent to intersecting this cone with the plane which thus produces a conic section. FYI the four, non-degenerate, cases are shown in this image from Wikipedia &lt;a href=&quot;http://i.stack.imgur.com/RAwDQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RAwDQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;An ellipse/circle is thus a possibility, but not the only one - unbounded parabolas or hyperbolas (and I guess if the plane passes through the eye, even degenerate cases ) are possible.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-02-02T10:36:32.350" LastActivityDate="2016-02-02T10:36:32.350" CommentCount="5" />
  <row Id="1499" PostTypeId="2" ParentId="1496" CreationDate="2015-09-15T16:14:06.920" Score="8" Body="&lt;p&gt;The classic book &lt;em&gt;Computer Graphics: Principles and Practice&lt;/em&gt; (second edition) by Foley, van Dam, et al. describes such an algorithm in section 19.2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The explanation in the book seems to come from an MSc thesis, &lt;a href=&quot;http://cs.brown.edu/research/pubs/theses/masters/1989/dasilva.pdf&quot;&gt;Raster Algorithms for 2D Primitives&lt;/a&gt; by Dilip Da Silva. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;See also these papers:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1145/282918.282943&quot;&gt;Curve-drawing algorithms for raster displays&lt;/a&gt; by Van Aken and Novak (&lt;em&gt;ACM TOG&lt;/em&gt;, 1985). &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://doi.ieeecomputersociety.org/10.1109/38.506&quot;&gt;A tracking algorithm for implicitly defined curves&lt;/a&gt; by Chandler (&lt;em&gt;IEEE Computer Graphics and Applications&lt;/em&gt;, 1988).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="192" LastEditorUserId="192" LastEditDate="2015-09-15T16:31:12.100" LastActivityDate="2015-09-15T16:31:12.100" CommentCount="0" />
  <row Id="1500" PostTypeId="2" ParentId="1493" CreationDate="2015-09-15T19:36:41.750" Score="10" Body="&lt;p&gt;This is more like a long comment to @SimonF's answer that I'm trying to make somewhat self contained.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All cuts of cone are possible, hyperbola, parabola and ovals. This is easy to test by drawing images in a 3D engine by a extremely wide angle camera. Rotate the camera to say in 30 degree angle so the object is not in the middle of your focus. Then gradually move the camera closer to the sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zi6eX.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zi6eX.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1:&lt;/strong&gt; Flying very close to a sphere looking slightly sideways. Notice how we suddenly puncture the surface form inside.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So to recap when the sphere is very close so it exits the picture in wide image it can be a parabola or hyperbola. But the shape will just exit the frame to do so.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-09-15T19:36:41.750" CommentCount="2" />
  <row Id="1501" PostTypeId="2" ParentId="1493" CreationDate="2015-09-15T20:39:57.047" Score="9" Body="&lt;p&gt;SimonF's reasoning basically convinced me, but I decided to do a sanity check. I loaded up a UE4 level that happens to have some spheres, like this one:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/RQ16v.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RQ16v.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I set the camera FOV up to 160 degrees to give lots of perspective distortion, and positioned it so the sphere was near the corner of the image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/yzF3C.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/yzF3C.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I took this into Inkscape and used the ellipse tool to draw on it:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LuSyP.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LuSyP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Surprise! It's a perfect fit!&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-09-15T20:39:57.047" CommentCount="3" />
  <row Id="1502" PostTypeId="1" AcceptedAnswerId="1506" CreationDate="2015-09-15T21:08:13.860" Score="13" ViewCount="664" Body="&lt;p&gt;When rendering 3D scenes with transformations applied to the objects, normals have to be transformed with the transposed inverse of the model view matrix. So, with a normal $n$, modelViewMatrix $M$, the transformed normal $n'$ is &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$n' = (M^{-1})^{T} \cdot n $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When transforming the objects, it is clear that the normals need to be transformed accordingly. But why, mathematically, is this the corresponding transformation matrix?&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-11-03T16:02:39.780" LastActivityDate="2015-11-03T19:14:57.103" Title="Why is the transposed inverse of the model view matrix used to transform the normal vectors?" Tags="&lt;transformations&gt;&lt;geometry&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="3" />
  <row Id="1503" PostTypeId="2" ParentId="1502" CreationDate="2015-09-15T21:15:36.447" Score="3" Body="&lt;p&gt;This is simply because normals are not really vectors! They are created by cross products, which results in &lt;a href=&quot;https://en.wikipedia.org/wiki/Bivector&quot; rel=&quot;nofollow&quot;&gt;bivectors&lt;/a&gt;, not vectors. Algebra works much different for these coordinates, and geometric transformation is just one operation that behaves differently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A great resource for learning more about this is &lt;a href=&quot;http://www.terathon.com/gdc12_lengyel.pdf&quot; rel=&quot;nofollow&quot;&gt;Eric Lengyel's presentation on Grassman Algebra&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-15T21:15:36.447" CommentCount="0" />
  <row Id="1505" PostTypeId="2" ParentId="397" CreationDate="2015-09-15T23:13:13.823" Score="2" Body="&lt;p&gt;A fairly basic but effective technique is &lt;a href=&quot;https://en.wikipedia.org/wiki/Median_filter&quot; rel=&quot;nofollow&quot;&gt;median filtering&lt;/a&gt;.  For video, you can apply it (spatio)temporally by replacing the value of each pixel in each frame by the median of the values of the pixel (and its neighbors) in the current and the &lt;em&gt;N&lt;/em&gt; previous and later frames.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A nice feature of median filtering is that it preserves linear edges (and, when used temporally, edges that move at a steady rate).  It does, however, tend to erode sharp corners and narrow ridges (and, temporally, narrow fast-moving features).  When overused, spatial median filtering has a tendency to create an over-smooth &quot;plastic&quot; appearance, while excessive temporal median filtering can even make small, fast-moving objects disappear completely.  (&lt;a href=&quot;http://photo.stackexchange.com/a/20960&quot;&gt;Sometimes&lt;/a&gt; this is considered a &lt;a href=&quot;http://www.deke.com/content/dekes-techniques-022-removing-people-with-image-stacks&quot; rel=&quot;nofollow&quot;&gt;feature.&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's possible to fine-tune and improve median filtering further with advanced techniques like motion tracking and threshold detection, but those also introduce an extra layer of complexity, and thus extra opportunities for unwanted artifacts if applied carelessly.  For many purposes, a simple median filter &lt;em&gt;of moderate size and strength&lt;/em&gt; is often all you really need.&lt;/p&gt;&#xA;" OwnerUserId="525" LastActivityDate="2015-09-15T23:13:13.823" CommentCount="0" />
  <row Id="1506" PostTypeId="2" ParentId="1502" CreationDate="2015-09-15T23:32:23.543" Score="13" Body="&lt;p&gt;Here's a simple proof that the inverse transpose is required. Suppose we have a plane, defined by a plane equation $n \cdot x + d = 0$, where $n$ is the normal. Now I want to transform this plane by some matrix $M$. In other words, I want to find a new plane equation $n' \cdot Mx + d' = 0$ that is satisfied for exactly the same $x$ values that satisfy the previous plane equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To do this, it suffices to set the two plane equations equal. (This gives up the ability to rescale the plane equations arbitrarily, but that's not important to the argument.) Then we can set $d' = d$ and subtract it out. What we have left is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$n' \cdot Mx = n \cdot x$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll rewrite this with the dot products expressed in matrix notation (thinking of the vectors as 1-column matrices):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$${n'}^T Mx = n^T x$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now to satisfy this for &lt;em&gt;all&lt;/em&gt; $x$, we must have:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$${n'}^T M = n^T$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now solving for $n'$ in terms of $n$,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{aligned}{n'}^T &amp;amp;= n^T M^{-1} \\&#xA;n' &amp;amp;= (n^T M^{-1})^T\\&#xA;n' &amp;amp;= (M^{-1})^T n\end{aligned}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Presto! If points $x$ are transformed by a matrix $M$, then plane normals must transform by the inverse transpose of $M$ in order to preserve the plane equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is basically a property of the dot product. In order for the dot product to remain invariant when a transformation is applied, the two vectors being dotted have to transform in corresponding but different ways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically, this can be described by saying that the normal vector isn't an ordinary vector, but a thing called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_form&quot; rel=&quot;nofollow&quot;&gt;covector&lt;/a&gt; (aka covariant vector, dual vector, or linear form). A covector is basically defined as &quot;a thing that can be dotted with a vector to produce an invariant scalar&quot;. In order to achieve that, it has to transform using the inverse transpose of whatever matrix is operating on ordinary vectors. This holds in any number of dimensions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that in 3D specifically, a bivector is similar to a covector. They're not &lt;em&gt;quite&lt;/em&gt; the same since they have different units: a covector has units of inverse length while a bivector has units of length squared (area), so they behave differently under scaling. However, they do transform the same way with respect to their orientation, which is what matters for normals. We usually don't care about the magnitude of a normal (we always normalize them to unit length anyway), so we usually don't need to worry about the difference between a bivector and a covector.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-11-03T19:14:57.103" LastActivityDate="2015-11-03T19:14:57.103" CommentCount="5" />
  <row Id="1507" PostTypeId="1" AcceptedAnswerId="1510" CreationDate="2015-09-16T06:04:16.483" Score="13" ViewCount="1401" Body="&lt;p&gt;I notice that most photorealistic renderers have very similar material test scenes. Here are a few examples:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blender&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/3uG1m.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3uG1m.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mitsuba&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EJIHh.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EJIHh.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vray&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dlDd6.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dlDd6.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: why is this specific model / style chosen to test materials? Does it have some advantage over &lt;a href=&quot;http://i9.photobucket.com/albums/a90/thatsmessedup/teapot74hoursl24.png&quot;&gt;teapots&lt;/a&gt;, spheres, and &lt;a href=&quot;http://www.flickr.com/photos/thomasberglund/5215885180/&quot;&gt;suzannes&lt;/a&gt;? Are there other common scenes for material testing?&lt;/p&gt;&#xA;" OwnerUserId="14" LastEditorUserId="38" LastEditDate="2015-09-16T13:37:34.167" LastActivityDate="2015-09-17T10:55:21.367" Title="Why do most photorealistic renderers have similar material test scenes?" Tags="&lt;material&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="3" />
  <row Id="1508" PostTypeId="2" ParentId="15" CreationDate="2015-09-16T06:05:56.157" Score="5" Body="&lt;p&gt;More of an extended comment than an answer:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What do you mean by &quot;optimization and accuracy&quot;? Do you mean computational efficiency for some particular application, like ray tracing, physical simulation, CAD modeling, ....?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that the idea of &quot;accuracy&quot; for a subdivision scheme is not well-posed. Different schemes will have different limit surfaces, but there is no canonical way of declaring one limit surface to be &quot;more accurate&quot; than any another. One can pose some constraints on the type of limit surface one desires, but these constraints are again very application-dependent: one person might ask for $G^n$ everywhere, the next will complain because this precludes preserving sharp creases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Catmull-Clark (and Loop, for triangle meshes) remains popular because of its simplicity, which in many cases outweigh its weaknesses (no handling of sharp features; loss of regularity at extraordinary vertices). Countless alternative schemes (which may or may not be improvements over Catmull-Clark, depending on the specific application) have been proposed -- if you have a &lt;em&gt;specific&lt;/em&gt; application in mind, with specific requirements, we may be better able to help you navigate your options.&lt;/p&gt;&#xA;" OwnerUserId="1657" LastActivityDate="2015-09-16T06:05:56.157" CommentCount="1" />
  <row Id="1509" PostTypeId="2" ParentId="405" CreationDate="2015-09-16T06:48:47.123" Score="3" Body="&lt;p&gt;Not the definitive toolbox you might be looking for, but just an extra tool I use myself: for 3d graphics, Blender might come in handy. Especially if you use the Export_SVG plugin (available from &lt;a href=&quot;http://dl.dropbox.com/u/16486113/Blender/archivos/temp/SVG/export_svg.py&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/16486113/Blender/archivos/temp/SVG/export_svg.py&lt;/a&gt; and &lt;a href=&quot;http://blenderartists.org/forum/showthread.php?282824-SVG-output-script&quot; rel=&quot;nofollow&quot;&gt;http://blenderartists.org/forum/showthread.php?282824-SVG-output-script&lt;/a&gt; )&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows you to get Blenders 3d objects as vector graphics in Illustrator or Inkscape. I find it gives me a nice quick start for creating &lt;strong&gt;simple&lt;/strong&gt; 3d illustrations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Allow  me to paste an example. (In this example I use a lot of colors, but ofcourse, it can be all made black-and-red-on-white instead)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dIyo9.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dIyo9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1659" LastEditorUserId="1659" LastEditDate="2015-09-16T08:30:31.280" LastActivityDate="2015-09-16T08:30:31.280" CommentCount="2" />
  <row Id="1510" PostTypeId="2" ParentId="1507" CreationDate="2015-09-16T07:47:23.190" Score="14" Body="&lt;p&gt;While teapot, spheres and trusty Suzanne are not per se bad test scenes for materials, here are some things to consider. You can apply them to said examples to forge your own opinion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First and foremost, the viewer needs to be able to examine the behavior of the BRDF. Since it is dependent on both the direction of incident light and the direction from surface to viewer, you need to have as many combinations as possible in your test scene. A spheroid nicely covers all possible viewer angles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Notice that all three scenes feature some sharper edges or creases too. Differences in reflective behavior across a smooth surface (like a sphere) may be hard to conceive, so these edges put emphasis on this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the observed brightness and color is a combination of the material (i.e. the BRDF) and the lighting environment, it may be hard to tell one from another. The Vray test scene takes care of this. The 100% white 'core' and the 25% black floor help you get an idea of what the lighting in the scene is like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blender and Vray both feature a grid that helps you estimate the total size of the scene. This helps with evaluating grain or surface texture.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here is a side-by-side comparison of a cube, notched sphere and Suzanne for a glossy test material with a slight normal map (rendered in Blender with Cycles, lit with the &lt;a href=&quot;http://www.hdrlabs.com/sibl/archive.html&quot; rel=&quot;nofollow&quot;&gt;Dutch Skies 360 - Free 002&lt;/a&gt; HDR):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/wtFVi.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wtFVi.jpg&quot; alt=&quot;Material test scene comparing cube, spheroid and Suzanne&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The cube does a pretty good job on depicting the normal map. The sphere, as expected, is reasonably good for normals and gloss. Suzanne, arguably, makes both a bit harder to perceive than the spheroid.&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-09-17T10:55:21.367" LastActivityDate="2015-09-17T10:55:21.367" CommentCount="4" />
  <row Id="1511" PostTypeId="2" ParentId="1507" CreationDate="2015-09-16T07:58:58.323" Score="7" Body="&lt;p&gt;You can choose whatever you want offcourse. A good material sample ideally has a few properties.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;It needs to show how the light falls of as the angle between light and surface normal changes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A sphere is pretty good for this.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;It needs to show how any possible environment, maps, reflects, refraction etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A sphere is pretty good for this.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;It may need to show how nonlocal effects happen such as global lightning, incandescence, subsurface scattering work&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some cavities and variation would be nice. A sphere is abysmal for this.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The sample should be able to show how displacement looks like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple shape demonstrates this better. Complex shapes can cause displacements to self intersect. And its hard to know what is modeling and what is not.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Also it would be nice to see, different surface thicknesses, sharp corners, one way curved surfaces and doublecurvature surfaces. A neutral but nonuniform  environment would be nice too. Object should have a reasonable uv map. Apparent scale might be needed in some projects.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-16T08:06:13.643" LastActivityDate="2015-09-16T08:06:13.643" CommentCount="0" />
  <row Id="1513" PostTypeId="1" AcceptedAnswerId="1514" CreationDate="2015-09-17T12:29:48.510" Score="19" ViewCount="658" Body="&lt;p&gt;The classical way of shading surfaces in real-time computer graphics is a combination of a (Lambertian) diffuse term and a specular term, most likely Phong or Blinn-Phong.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/VOfxv.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/VOfxv.png&quot; alt=&quot;Image from Wikipedia&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now with the trend going towards physically-based rendering and thus material models in engines such as &lt;a href=&quot;http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr.pdf&quot;&gt;Frostbite&lt;/a&gt;, &lt;a href=&quot;http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_slides.pdf&quot;&gt;Unreal Engine&lt;/a&gt; or &lt;a href=&quot;http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_slides.pdf&quot;&gt;Unity 3D&lt;/a&gt; these BRDFs have changed. For example (a pretty universal one at that), the latest Unreal Engine still uses Lambertian diffuse, but in combination with the Cook-Torrance microfacet model for specular reflection (specifically using GGX/Trowbridge-Reitz and a modified Slick approximation for the Fresnel term). Furthermore, a 'Metalness' value is be used to distinguish between conductor and dielectric.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For dielectrics, diffuse is colored using the albedo of the material, while specular is always colorless. For metals, diffuse is not used and the specular term is multiplied with the albedo of the material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding real-world physical materials, does the strict separation between diffuse and specular exist and if so, where does it come from? Why is one colored while the other is not? Why do conductors behave differently?&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-09-22T09:51:46.387" Title="How physically-based is the diffuse and specular distinction?" Tags="&lt;physically-based&gt;&lt;physics&gt;&lt;brdf&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="8" />
  <row Id="1514" PostTypeId="2" ParentId="1513" CreationDate="2015-09-17T17:02:42.913" Score="19" Body="&lt;p&gt;To start, I highly suggest reading Naty Hoffman's Siggraph &lt;a href=&quot;http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf&quot;&gt;presentation&lt;/a&gt; covering the physics of rendering. That said, I will try to answer your specific questions, borrowing images from his presentation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at a single light particle hitting a point on the surface of a material, it can do 2 things: reflect, or refract. Reflected light will bounce away from the surface, similar to a mirror. Refracted light bounces around inside the material, and may exit the material some distance away from where it entered. Finally, every time the light interacts with the molecules of the material, it loses some energy. If it loses enough of its energy, we consider it to be fully absorbed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To quote Naty, &quot;Light is composed of electromagnetic waves. So the optical properties of a substance are closely linked to its electric properties.&quot; This is why we group materials as metals or non-metals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Non metals will exhibit both reflection and refraction.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/ECwfp.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ECwfp.png&quot; alt=&quot;Non-Metals&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Metallic materials only have reflection. All refracted light is absorbed.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/JyBy3.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JyBy3.png&quot; alt=&quot;Metals&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be prohibitively expensive to try to model the light particle's interaction with the molecules of the material. We instead, make some assumptions and simplifications.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/PXEQW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/PXEQW.png&quot; alt=&quot;Simplifying refraction&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the pixel size or shading area is large compared to the entry-exit distances, we can make the assumption that the distances are effectively zero.&#xA;For convenience, we split the light interactions into two different terms. We call the surface reflection term &quot;specular&quot; and the term resulting from refraction, absorption, scattering, and re-refraction we call &quot;diffuse&quot;.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/85Pmg.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/85Pmg.png&quot; alt=&quot;Splitting into diffuse and specular&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, this is a pretty large assumption. For most opaque materials, this assumption is ok and doesn't differ too much from real-life. However, for materials with any kind of transparency, the assumption fails. For example, milk, skin, soap, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A material's observed color is the light that is not absorbed. This is a combination of both the reflected light, as well as any refracted light that exits the material. For example, a pure green material will absorb all light that is not green, so the only light to reach our eyes is the green light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore an artist models the color of a material by giving us the attenuation function for the material, ie how the light will be absorbed by the material. In our simplified diffuse/specular model, this can be represented by two colors, the diffuse color, and the specular color. Back before physically-based materials were used, the artist would arbitrarily choose each of these colors. However, it should seem obvious that these two colors should be related. This is where the albedo color comes in. For example, in UE4, they calculate diffuse and specular color as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DiffuseColor = AlbedoColor - AlbedoColor * Metallic;&#xA;SpecColor = lerp(0.08 * Specular.xxx, AlbedoColor, Metallic)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where Metallic is 0 for non-metals and 1 for metals. The 'Specular' parameter controls the specularity of an object (but it's usually a constant 0.5 for 99% of materials)&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2015-09-17T21:52:08.073" LastActivityDate="2015-09-17T21:52:08.073" CommentCount="8" />
  <row Id="1515" PostTypeId="1" AcceptedAnswerId="1517" CreationDate="2015-09-18T04:29:45.047" Score="8" ViewCount="363" Body="&lt;p&gt;Most modern renderers use physically-based materials and their models are often parameterized over roughness. Since this wasn't always the case with renderers, conventional assets often don't have a notion of roughness. Instead, we see &quot;shininess&quot; or &quot;specular power&quot; as a common material parameter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that there is no exact conversion between the two, but is there a rule-of-thumb / approximate way to get roughness for a material whose specular power or shininess is known?&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-09-18T16:01:43.147" Title="What is the accepted method of converting shininess to roughness, and vice versa?" Tags="&lt;specular&gt;&lt;brdf&gt;&lt;pbr&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="2" />
  <row Id="1517" PostTypeId="2" ParentId="1515" CreationDate="2015-09-18T07:30:54.973" Score="6" Body="&lt;p&gt;As you already note, there is no clear cut interpretation/conversion for these values. I think it is even much worse: Depending on your BRDF and internal limitations (like having defined exponents ranging from 2-2048) the interpretation is &lt;em&gt;completely&lt;/em&gt; different. Like suggested in the comments, it might be the best to render a series with different values and fit a conversion curve until the value looks intuitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A few examples I was able to find some blog posts that mention something about that topic:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;At Dontnod entertainment they use a &quot;perceptual linear distribution&quot;. &#xA;Sébastien Lagarde acknowledges the problem in &lt;a href=&quot;https://seblagarde.wordpress.com/2011/08/17/feeding-a-physical-based-lighting-mode/&quot; rel=&quot;nofollow&quot;&gt;this blog post&lt;/a&gt; and writes a few notes on that.&lt;/li&gt;&#xA;&lt;li&gt;Brian Karis uses a squared roughness values in this &lt;a href=&quot;http://graphicrants.blogspot.de/2013/08/specular-brdf-reference.html&quot; rel=&quot;nofollow&quot;&gt;Microfacet BRDF overview&lt;/a&gt;. This illustrates also nicely how differently  roughness is used in different Normal Distribution Functions. Blinn-Phong power is here defined as &lt;code&gt;2/roughness^4 - 2&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Frostbite uses a squared remapping. ie. Roughness =  (1 − Smoothness)^2&#xA;Details about it and their entire material system is explained in section 3.2 of Sebastien Lagarde's &lt;a href=&quot;http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr.pdf&quot; rel=&quot;nofollow&quot;&gt;writeup&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://simonstechblog.blogspot.de/2011/12/microfacet-brdf.html&quot; rel=&quot;nofollow&quot;&gt;This blog post&lt;/a&gt; suggests to define the roughness for a Beckmann distribution from the roughness alpha as:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1NWdA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1NWdA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="528" LastEditorUserId="310" LastEditDate="2015-09-18T16:01:43.147" LastActivityDate="2015-09-18T16:01:43.147" CommentCount="1" />
  <row Id="1518" PostTypeId="2" ParentId="309" CreationDate="2015-09-19T00:23:24.103" Score="4" Body="&lt;p&gt;After looking into this for a while, I found out a couple of things:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;You cannot avoid a memcpy&lt;/strong&gt;: You cannot write directly into texture storage allocated for a compressed texture using only OpenGL API calls. This means that you cannot avoid the call to &lt;code&gt;glCompressedTexImage2D&lt;/code&gt; with a bound PBO. That being said, you may be able to use a 16-bit RGBA texture and a GLSL image type in your compute shader.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;You do need to synchronize memory&lt;/strong&gt;: In order to make sure that your compute shader finishes writing to your storage buffer, you must make sure that all reads and writes to it finish. This is done by calling &lt;code&gt;glMemoryBarrier&lt;/code&gt; with &lt;code&gt;GL_SHADER_STORAGE_BARRIER_BIT&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The full code for something that writes into a buffer to be used as a compressed texture looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLuint buffer;&#xA;glGenBuffers(1, &amp;amp;buffer);&#xA;glBindBuffer(GL_SHADER_STORAGE_BUFFER, buffer);&#xA;glBufferStorage(GL_SHADER_STORAGE_BUFFER, tex_size_in_bytes, 0, 0);&#xA;&#xA;glUseProgram(compute_shader_prog);&#xA;glBindBufferBase(GL_SHADER_STORAGE_BUFFER, compute_shader_bind_point, buffer);&#xA;glDispatchCompute(wg_x, wg_y, wg_z);&#xA;glMemoryBarrier(GL_SHADER_STORAGE_BUFFER_BIT);&#xA;&#xA;glBindBuffer(GL_PIXEL_UNPACK_BUFFER, buffer);&#xA;glCompressedTexImage2D(GL_TEXTURE_2D, 0, fmt, w, h, 0, tex_size_in_bytes, 0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="197" LastEditorUserId="197" LastEditDate="2015-09-19T01:13:41.147" LastActivityDate="2015-09-19T01:13:41.147" CommentCount="2" />
  <row Id="1519" PostTypeId="2" ParentId="1513" CreationDate="2015-09-21T04:47:42.927" Score="9" Body="&lt;p&gt;I was actually wondering about exactly this a few days ago.  Not finding any resources within the graphics community, I actually walked over to the Physics department at my university and &lt;em&gt;asked&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It turns out that there are a lot of lies we graphics people believe.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;First, when light hits a surface, the Fresnel equations apply.  The proportions of reflected/refracted light depend on them.  You probably knew this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;There's no such thing as a &quot;specular color&quot;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you might not have known is that the Fresnel equations vary based on wavelength, because the &lt;em&gt;refractive index&lt;/em&gt; varies based on wavelength.  The variation is relatively small for dielectrics (dispersion, anyone?), but can be enormous for metals (I presume this has to do with these materials' differing electric structures).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, the Fresnel reflection term varies by wavelength, and therefore &lt;em&gt;different wavelengths are reflected preferentially&lt;/em&gt;.  Seen under broad-spectrum illumination, this is what leads to specular color.  But in particular, there is &lt;em&gt;no absorption&lt;/em&gt; that magically happens at the surface (the other colors are just refracted).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;There's no such thing as &quot;diffuse reflection&quot;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As Naty Hoffman says in the talk linked in the other answer, this is really an approximation to outscattered subsurface scattering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Metals DO transmit light&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Naty Hoffman is wrong (more precisely, simplifying).  Light does &lt;em&gt;not&lt;/em&gt; get absorbed immediately by metals.  In fact, it will pass quite handily through materials several nanometers thick.  (For example, for gold, it takes 11.6633nm to attenuate 587.6nm light (yellow) by half.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Absorption, as in dielectrics, is due to the Beer-Lambert Law.  For metals, the absorption coefficient is just much larger (α=4πκ/λ, where κ is the imaginary component of the refractive index (for metals ~0.5 and up), and λ is given in &lt;em&gt;meters&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This transmission (or more accurately the &lt;a href=&quot;https://en.wikipedia.org/wiki/Subsurface_scattering&quot;&gt;SSS&lt;/a&gt; it produces) is actually responsible for a significant portion of metals' colors (although it is true that metals' appearances are dominated by their specular).&lt;/p&gt;&#xA;" OwnerUserId="523" LastEditorUserId="231" LastEditDate="2015-09-22T09:51:46.387" LastActivityDate="2015-09-22T09:51:46.387" CommentCount="7" />
  <row Id="1521" PostTypeId="1" AcceptedAnswerId="1527" CreationDate="2015-09-21T16:16:37.923" Score="4" ViewCount="92" Body="&lt;p&gt;I encountered this problem doing my project. Suppose there is a 3D mesh model, e.g. a human face, I need the 3D visibility map at each of the vertices of this model. By the &quot;3D visibility map&quot;, I mean the binary image covering the spherical surface indicating whether the point is visible from each spherical angle(different from ambient occlusion). So, it is like a mask image. For example, the visibility map for any point on a convex surface is all ones (occlusion by other objects is ignored, only self-occlusion). See the following figure for illustration &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iPtZq.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iPtZq.jpg&quot; alt=&quot;spherical visibility map in two cases&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to know is there some efficient ways solving this problem? I need to apply it to all vertices. All I have in mind is doing brute force verification for all sampling directions. What's more, is there any existing software or tool (e.g. pbrt, Unity, Blender) that is capable of this functionality? Any suggestions and links are welcome. Thanks!&lt;/p&gt;&#xA;" OwnerUserId="1692" LastEditorUserId="1692" LastEditDate="2015-09-22T01:52:57.000" LastActivityDate="2015-09-23T15:57:04.210" Title="How can I get a spherical visibility mask map for a point on a concave surface?" Tags="&lt;lighting&gt;&lt;occlusion&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1523" PostTypeId="1" AcceptedAnswerId="1545" CreationDate="2015-09-21T20:41:30.860" Score="10" ViewCount="456" Body="&lt;p&gt;That it compresses the data compared to the pixel array is obvious.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what makes it different from from normal compression (like png, jpeg)? &lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-09-21T21:22:30.087" LastActivityDate="2016-01-18T13:52:42.230" Title="How does hardware texture compression work?" Tags="&lt;texture&gt;&lt;compression&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="3" />
  <row Id="1524" PostTypeId="1" AcceptedAnswerId="1525" CreationDate="2015-09-22T09:04:58.360" Score="9" ViewCount="102" Body="&lt;p&gt;I wrote a particle based fluid simulating program. It's hard to tell if I get the right result. The visualized result seems reasonable, but some part of it looks weird. I don't know wether it's a feature of fluid. Is there some accurate method to verify if my program is right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Amending some details：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My program is a 2D particle-based simulating program. The fluid is compressible. The implementation is almost based on a classical paper:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Müller, Matthias, David Charypar, and Markus Gross. &quot;Particle-based fluid simulation for interactive applications.&quot; Proceedings of the 2003 ACM SIGGRAPH&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I solved the Navier-Stokes equation with iteration method. It only considered pressure, gravity, viscosity and surface tension.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://latex.codecogs.com/gif.latex?%5Crho&amp;space;(%5Cfrac%7B%5Cmathbf%7B%5Cpartial&amp;space;v%7D%7D%7B%5Cpartial&amp;space;t%7D&amp;space;&amp;plus;&amp;space;%5Cmathbf%7Bv%7D&amp;space;%5Ccdot&amp;space;%5Cbigtriangledown&amp;space;%5Cmathbf%7Bv%7D)&amp;space;=&amp;space;-%5Cbigtriangledown&amp;space;p&amp;space;&amp;plus;&amp;space;%5Crho%5Cmathbf%7Bg%7D&amp;space;&amp;plus;&amp;space;%5Cmu&amp;space;%5Cbigtriangledown%5E2&amp;space;%5Cmathbf%7Bv%7D&quot; title=&quot;\rho (\frac{\mathbf{\partial v}}{\partial t} + \mathbf{v} \cdot \bigtriangledown \mathbf{v}) = -\bigtriangledown p + \rho\mathbf{g} + \mu \bigtriangledown^2 \mathbf{v}&quot; /&gt;&lt;/p&gt;&#xA;" OwnerUserId="1697" LastEditorUserId="1697" LastEditDate="2015-09-22T14:29:58.353" LastActivityDate="2015-09-22T14:29:58.353" Title="How could I check the correctness of my result of fluid simulation?" Tags="&lt;physics&gt;&lt;simulation&gt;&lt;fluid-sim&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1525" PostTypeId="2" ParentId="1524" CreationDate="2015-09-22T11:30:06.133" Score="2" Body="&lt;p&gt;Compare it with someone else's software. Run some standardized test and find out if you get roughly the same answer as others. If you get the same answer, than the probability of having your code right is quite high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some tests:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Flow past cylinder. In 2d take rectangular domain, cylinder in the middle, inflow on the left, outflow on the fight and calculate the force on the cylinder. Here is &lt;a href=&quot;http://www.karlin.mff.cuni.cz/~hron/NMMO403/dfg_benchmark_results.pdf&quot; rel=&quot;nofollow&quot;&gt;benchmark&lt;/a&gt; comparing handful of codes.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Buoyancy flow. Closed box, hot plate on bottom, cold plate on top, hot fluid starts to rise due to the buoyancy force. Here is &lt;a href=&quot;http://www.karlin.mff.cuni.cz/~hron/NMMO403/mit_bench.pdf&quot; rel=&quot;nofollow&quot;&gt;benchmark&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Rising bubble, &lt;a href=&quot;http://www.karlin.mff.cuni.cz/~hron/NMMO403/1934_ftp.pdf&quot; rel=&quot;nofollow&quot;&gt;benchmark&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;But unfortunately it might be quite difficult to compare your code to scientific codes in those benchmarks. I guess you implemented something as SPH or stable fluids which are not made for accuracy but for stability. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Take for example the flow past a cylinder. I would start the test with very small Reynolds number and then measure the force on cylinder as you increase accuracy of your simulation(lower time step, increase subdivision or increase number of particles). Does the force converges to some number? If no, than you have a problem, if yes, than have a look at the benchmark paper and compare your result with others.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;This method is very similar technique to one I use for testing my raytracer. I just render test scene with someone else's renderer and compare it with my result. Do they converge to the same result? If yes than I have it right, if no, than I have it wrong.&lt;/p&gt;&#xA;" OwnerUserId="1613" LastActivityDate="2015-09-22T11:30:06.133" CommentCount="6" />
  <row Id="1526" PostTypeId="2" ParentId="159" CreationDate="2015-09-22T12:22:01.213" Score="11" Body="&lt;p&gt;For a realistic Iris, you need:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Geometry&lt;/strong&gt;: Irides are very different for different people and their appearance changes drastically depending on whether the pupil is dilated (sphicter muscle relaxed) or not. You will not get anywhere with primitives only. The whole structure is like a thick stretchy wrinkly cloth suspended in water. Wrinkles appear and disappear as it moves. Use a sculpting software and an anatomy textbook.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Material&lt;/strong&gt;: There is almost no specular on the iris since it is suspended in (basically) water which has a similar refractive index as tissue. I do not know the roughness values since I have never seen a dry Iris. With the &lt;a href=&quot;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&quot; rel=&quot;nofollow&quot; title=&quot;Microfacet Models for Refraction through Rough Surfaces (2007)&quot;&gt;GGX specular model&lt;/a&gt;, roughness 0.4 looks okay. Brown irides seem to be more rough, almost like very fine sand. A shading model that uses roughness in the diffuse component would be nice.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Textures&lt;/strong&gt;: While some are colorful, the vast majority of irides in the world are brown. You don't need too much variation inside the texture, it can be quite uniform. Make it relatively saturated and don't try to hint at geometry inside the texture by drawing dark areas for shadows or occlusion. Have a normal map. If the eye is animated, you need to blend between at least two normal maps for the dilated and undilated pupil. I don't think you need a roughness map, but especially for blue irides it might help.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Refraction and reflection&lt;/strong&gt; at the cornea: The cornea (the fluid layer coating it) acts as a lens. This distorts the image of the iris as seen by the camera. Without this, it will not look real, especially when looking at the eye from the side. The unusual thing about the eye is that, from a graphics point of view, the object is &lt;em&gt;inside&lt;/em&gt; the lens. So there's no thin lens equation going to come to your rescue. You have to refract light once, at the interface between the tear layer and air.&#xA;The other interfaces (tear layer to outside of cornea, layers and membranes inside the cornea, inside of cornea to chamber fluid) are not relevant since the indices of refraction are all very similar or the structures are extremely thin. The only other optically important structure is the lens, which is behind the iris and can be ignored.&#xA;The shape of the cornea is critical, as it will distort everything. People with cornea deformations such as a keratoconus have weird-looking eyes. Also the whole scene must reflect on the cornea/liquid. We expect to see reflections of light sources over the iris/pupil.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;strong&gt;limbus&lt;/strong&gt;: Looking frontally at the eye, there is a greyish ring around the Iris. This is the part where the cornea merges into the sclera (the &quot;white&quot; of the eye), transitioning from transparent to nontransparent. Behind the limbus, anatomically, is the iridocorneal angle, where the iris meets the rest of the eye. This area is impossible to see due to the refraction mentioned above, it is literally a place that has never seen any light. Light emitted from there experiences total internal refraction at the interface of the cornea, you can't look at it from the outside. Thus, it looks very very weird when the transition from iris to the rest of the eye is visible. You need both the refraction and the gradual transition from cornea to sclera to get this transition right.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Subsurface scattering&lt;/strong&gt;: You might possibly get away without it on the iris itself since there will never be light coming from the back, especially with a brown iris (more heavily pigmented -&gt; more attenuation during scattering). For some light directions, it will make a difference though. And you need it on the surrounding skin and sclera anyway. But it is not the most important effect.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ambient occlusion&lt;/strong&gt;: You need good &lt;a href=&quot;https://en.wikipedia.org/wiki/Screen_space_ambient_occlusion&quot; rel=&quot;nofollow&quot; title=&quot;Screen Space Ambient Occlusion&quot;&gt;SSAO&lt;/a&gt; (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Global_illumination&quot; rel=&quot;nofollow&quot; title=&quot;Global Illumination&quot;&gt;GI&lt;/a&gt;) to make the fine structure stand out. There's almost no specular, so you need other visual cues for the geometry. Choose a method that uses your normals from the normal map. &lt;a href=&quot;http://www.twistedsanity.org/rdimitrov/HBAO_SIGGRAPH08.pdf&quot; rel=&quot;nofollow&quot; title=&quot;Image-Space Horizon-Based Ambient Occlusion&quot;&gt;HBAO&lt;/a&gt; will work well. For offline rendering, the GI will be done by your renderer of choice.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lighting&lt;/strong&gt;: Using some physically based method goes without saying. Image based lighting, especially for the cornea reflection. Under certain angles, refraction of the light at the cornea may become relevant as well (only for analytic lights). Postprocessing (bloom), especially for the cornea reflex.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Backscattered light from the retina, although it can be seen through the iris in very extreme viewing conditions during some medical examinations, is completely irrelevant for normal purposes. Basically, paint the retina black. If you want to go that far, you can add a little orange-red to the retina color whenever a strong light comes from a direction very very similar to the camera and the pupil is dilated (like a flash from a camera). But it will still not be strong enough to shine through the iris, by far. Black is an okay approximation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That list looks big, but you asked for photorealism ;) It will still look good if you only do the easy half and fake the rest.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Source: I do graphics programming for medical simulations with a focus on the human eye)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Material:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://hal.inria.fr/inria-00166304/&quot; rel=&quot;nofollow&quot;&gt;Anatomically accurate modeling and rendering of the human eye&lt;/a&gt; (2007)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.disneyresearch.com/publication/high-quality-capture-of-eyes/&quot; rel=&quot;nofollow&quot;&gt;High-Quality Capture of Eyes&lt;/a&gt; (Disney Research, Siggraph Asia 2014)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1699" LastEditorUserId="231" LastEditDate="2015-09-23T16:24:11.350" LastActivityDate="2015-09-23T16:24:11.350" CommentCount="2" />
  <row Id="1527" PostTypeId="2" ParentId="1521" CreationDate="2015-09-22T17:04:29.490" Score="2" Body="&lt;p&gt;This can be done analytically at least for polygonal meshes. You can convert points into polar coordinates and project on a sphere. Edges form planes that pass through projection circles center forming circles on the sphere. These lines are all great circles, because they must pass sphere center. Great circles form linear interpolations in polar coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JZHaM.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JZHaM.png&quot; alt=&quot;Project triangle on sphere&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Projecting a geometry on a sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Converting these great circles into either Cartesian coordinates is quite well known math. Projecting them onto 2d representation by Mercator projection is well known math on account fo this being central to map making. Since overlap is not a  problem you can just slap these triangles on top of each other and merge the vector results. Or use the polar coordinates for pixel graphics and even use z buffered overlap just like a normal camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only problematic point is the one where your sphere is generated (if its on a open face edge). You can simply project the point on center into its inverse normal direction on the projection sphere. Or if for example Mercator projection is desired, you can align the sphere with surface normal. then just project them to a point on the south pole.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can even interpolate the results of individual polygons positions to get any point on the triangle. And a analytic surface coverage for the triangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I haven't implemented this, but Ive done it manually a few times* so i know its possible. Should be pretty easy to do as the maths involved aren't all that complicated only problem is solving which side of the problem to keep.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/j6qF1.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/j6qF1.png&quot; alt=&quot;Local horizon map&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Horizon map of a simple all local geometry.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Comparison with raytracing&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Both sampling triangles analytically is O(N) algorithm, if vector data is sufficient. Conditioning this data may push you over to O(N^2). Raytracing is O(N log(N)). Raytracing algorithms are easier to find of the shelf so implementing analytic polygon to sphere rendering is harder. In terms of speed analytical rendering is faster as long as theres a limited number of triangles, much like how scanline rendering is still sometimes faster than tracing despite being algorithmically more complex. This similarity is not just a coincidence it can be shown that this method can be turned into a scanline renderer.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Resources for drawing great circles in Mercator projection:&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://demonstrations.wolfram.com/GreatCirclesOnMercatorsChart/&quot; rel=&quot;nofollow&quot;&gt;Wolfram demonstration&lt;/a&gt; on subject.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://adsabs.harvard.edu/full/1932JRASC..26..161G&quot; rel=&quot;nofollow&quot;&gt;The Projection of Circles on a Mercator Map&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;* About 20-25 times. I also did it for this image (yes in 3d), its as accurate as a single span Bezier curve can represent a circle. Ive also done this in map making software a few times.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-23T15:57:04.210" LastActivityDate="2015-09-23T15:57:04.210" CommentCount="3" />
  <row Id="1528" PostTypeId="2" ParentId="1496" CreationDate="2015-09-22T22:09:32.430" Score="8" Body="&lt;p&gt;All conics (including rotated ellipses) can be described by an implicit equation of the form&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;H(x, y) = A x² + B xy + C y² + D x + E y + F = 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The basic principle of the incremental line tracing algorithms (I wouldn't call them scanline) is to follow the pixels that fulfill the equation as much as possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on the local slope of the line, you progress more in the x or y direction, following either a lateral or diagonal move. This decision is made so that you minimize the absolute value of H(x, y).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of a straight line, the slope is a constant so that lateral and diagonal moves are always in the same direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of a circular arc, the slope varies and one can distinguish 8 cases corresponding to 8 octants of the curve (there are four possible lateral moves and for each two diagonal moves).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the ellipse and other conics, you can generalize the octant decomposition. This leads to a rather tedious discussion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can avoid the octant discussion and modify the algorithm to look at all neighbors of the current pixel. This leads to a general contour tracing algorithm like &lt;a href=&quot;http://www.imageprocessingplace.com/downloads_V3/root_downloads/tutorials/contour_tracing_Abeer_George_Ghuneim/moore.html&quot; rel=&quot;nofollow&quot;&gt;Moore's neighborhood&lt;/a&gt;, where you will follow the outline of the area &lt;code&gt;H(x, y) &amp;gt;= 0&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that lines, circles and axis-aligned ellipses can be described by an implicit equation with integer coefficients. This is no more possible with general conics and you will need to resort to floating-point, or good rational approximations.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Lastly, note that incremental computation saves work when evaluating H, on the line of&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;H(x+1, y) = H(x, y) + A.(2x+1) + B.y + D = H(x, y) + (2A).x + (A + B.y + D)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;A true scanline solution is also possible. Let &lt;code&gt;y&lt;/code&gt; vary incrementally, and solve the &lt;code&gt;H&lt;/code&gt; equation for &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;H(x, y) = A x² + (B y + D) x + (C y² + E y + F) = 0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will yield two &lt;code&gt;x&lt;/code&gt; values for each &lt;code&gt;y&lt;/code&gt;. The resulting curve will be much less pleasant as it will always count two pixels per row, which will leave holes when the slope is below &lt;code&gt;1&lt;/code&gt;. You can cope by filling the stretch of pixels between the roots on the successive rows.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2015-09-29T15:00:23.567" LastActivityDate="2015-09-29T15:00:23.567" CommentCount="2" />
  <row Id="1529" PostTypeId="1" CreationDate="2015-09-23T04:09:55.423" Score="2" ViewCount="276" Body="&lt;p&gt;I recently noticed that in some locations you could switch to a 3d mapping mode in Google Street View.&lt;br&gt;&#xA;Which algorithm(s) did they use to generate the 3d models of the streets? I assume it was more than just capture, given the huge amount of data that would be necessary to produce an accurate representation of the streets.&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2015-10-07T20:34:05.963" Title="What algorithm(s) are behind Google Street View's 3d mapping mode?" Tags="&lt;algorithm&gt;&lt;3d&gt;&lt;mapping&gt;&lt;model&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="1531" PostTypeId="1" CreationDate="2015-09-23T16:46:18.353" Score="2" ViewCount="251" Body="&lt;p&gt;I'm working on an assignment and I need to draw using only GL_POINTS. I realize this is an expensive approach but it's for the homework, so no GL_TRIANGLES / GL_POLYGON / GL_LINES etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, I'm trying to understand the concept of using only points. Say I want to draw a square that's 100 x 100 pixels. Would I need four for-loops drawing 100 pixels each in straight lines to create the square? What if I want to fill the square with color?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand drawing a square using GL_POLYGON as that's fairly straight forward. We're using the GLUT library just to draw shapes with points.&lt;/p&gt;&#xA;" OwnerUserId="1705" LastActivityDate="2015-09-29T07:42:29.213" Title="GLUT OpenGL - Drawing Shapes using only GL_POINTS" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1533" PostTypeId="2" ParentId="338" CreationDate="2015-09-23T20:49:14.113" Score="1" Body="&lt;p&gt;Quickly Googling produces:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/4148831/how-to-offset-a-cubic-bezier-curve&quot;&gt;This related question&lt;/a&gt; on StackOverflow&lt;/li&gt;&#xA;&lt;li&gt;&quot;Computing offsets of NURBS curves and surfaces&quot; (paper)&lt;/li&gt;&#xA;&lt;li&gt;Discussions of 2.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;To summarize them:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Suggests that this is impossible to do exactly.&lt;/li&gt;&#xA;&lt;li&gt;Gives an algorithm to compute an offset curve approximately (though to within a tiny epsilon).  If you can't access the paper, then:&lt;/li&gt;&#xA;&lt;li&gt;Look at any of these.  &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=7&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CDwQFjAGahUKEwiihLzeg47IAhVHN4gKHfsmCaM&amp;amp;url=http%3A%2F%2Fwww.math.zju.edu.cn%2Fcagd%2FSeminar%2F2005_AutumnWinter%2F2005_Master_LH.ppt&amp;amp;usg=AFQjCNGlb556TMftSL8d4xU7kT1x7Q9G7Q&amp;amp;sig2=9rYaCjpqD-VMKu94ke5O7w&quot; rel=&quot;nofollow&quot;&gt;This presentation&lt;/a&gt; for example seems to cover it.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-09-23T20:49:14.113" CommentCount="7" />
  <row Id="1535" PostTypeId="1" AcceptedAnswerId="1548" CreationDate="2015-09-24T10:41:59.760" Score="10" ViewCount="125" Body="&lt;p&gt;&lt;a href=&quot;http://www.ppsloan.org/publications/StupidSH36.pdf&quot;&gt;Spherical Harmonics&lt;/a&gt; (SH) are a way to represent low-frequency spherical functions with only a handful of coefficients. They have some nice mathematical properties, e.g. a convolution with a kernel function &lt;em&gt;h(x)&lt;/em&gt; (that has circular symmetry) can be calculated as&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/oh3mt.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oh3mt.gif&quot; alt=&quot;(h * f)^m_l = \sqrt{\frac{4\pi}{2l+1}} h^0_l f^m_l&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of a &lt;a href=&quot;https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/&quot;&gt;convolution with a cosine lobe&lt;/a&gt; for rank 3 SH this results in a simple scaling of the bands with the factors&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tIf94.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tIf94.gif&quot; alt=&quot;[\pi, \frac{2\pi}{3}, \frac{\pi}{4}]&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In many cases, e.g. incident light for a given point on an opaque surface, full spherical information is not needed, since half of the sphere is zero / undefined / unused anyway. Thus, &lt;a href=&quot;http://www.cs.ucf.edu/~ceh/Publications/Papers/Rendering/EGSR04GautronEtAl.pdf&quot;&gt;Hemispherical Harmonics&lt;/a&gt; (HSH) were born.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does convolution with an arbitrary kernel (with circular symmetry) work for HSH? Can the convolution from SH be extended or is there any paper that goes into details on this?&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-09-28T22:41:47.203" Title="Convolution of Hemispherical Harmonics" Tags="&lt;untagged&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="3" />
  <row Id="1536" PostTypeId="1" CreationDate="2015-09-25T00:13:21.510" Score="9" ViewCount="930" Body="&lt;p&gt;Why are Homogeneous Coordinates used in Computer Graphics?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the problem if Homogeneous Coordinates were not used in matrix transformations?&lt;/p&gt;&#xA;" OwnerUserId="464" LastActivityDate="2015-11-03T15:15:14.507" Title="Why are Homogeneous Coordinates used in Computer Graphics?" Tags="&lt;transformations&gt;" AnswerCount="5" CommentCount="1" FavoriteCount="0" />
  <row Id="1537" PostTypeId="2" ParentId="1536" CreationDate="2015-09-25T01:49:44.247" Score="7" Body="&lt;p&gt;They simplify and unify the mathematics used in graphics:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;They allow you to represent translations with matrices.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;They allow you to represent the division by depth in perspective projections.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The first one is related to affine geometry.&#xA;The second one is related to projective geometry.&lt;/p&gt;&#xA;" OwnerUserId="192" LastActivityDate="2015-09-25T01:49:44.247" CommentCount="4" />
  <row Id="1538" PostTypeId="1" CreationDate="2015-09-25T03:16:41.270" Score="4" ViewCount="94" Body="&lt;p&gt;For normalization purposes, how to &quot;extract&quot; a BRDF function from an ad-hoc piece of pixel shading code ?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we have a shader function like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float4 MyShadingFunction(float3 incomingLightDirection, float incomingRadiance)&#xA;{&#xA;    float4 lightContributionToPixelColor;&#xA;&#xA;    // ...&#xA;    // several complicated formulas (to which we have access)&#xA;    // ...&#xA;&#xA;    return lightContributionToPixelColor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the procedure to translate the &quot;complicated formulas&quot; into a BRDF function f that can then be mathematically integrated ?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;For example to use f in an expression such as:&lt;/em&gt;&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/InSPM.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/InSPM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2015-09-25T03:16:41.270" Title="How to extract a BRDF from a shader" Tags="&lt;shader&gt;&lt;brdf&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2015-09-30T07:48:52.413" />
  <row Id="1539" PostTypeId="2" ParentId="1536" CreationDate="2015-09-25T04:08:40.117" Score="1" Body="&lt;p&gt;Calculations in affine coordinates often require divisions, which are expensive as compared to additions or multiplications. One usually does not need to divide when using projective coordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using projective coordinates (and more generally, projective geometry) tends to eliminate special cases too, making everything simpler and more uniform.&lt;/p&gt;&#xA;" OwnerUserId="1713" LastActivityDate="2015-09-25T04:08:40.117" CommentCount="5" />
  <row Id="1540" PostTypeId="2" ParentId="1536" CreationDate="2015-09-25T07:03:11.327" Score="1" Body="&lt;p&gt;As a personal taste I have always abstained (when possible) from using homogeneous coordinates and preferred the plain Cartesian formulation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Main reason is the fact that homogeneous coordinates uses 4 trivial entries in the transformation matrices (0, 0, 0, 1), involving useless storage and computation (also the overhead of general-purpose matrix computation routines which are &quot;by default&quot; used in this case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The downside is that you need more care when writing the equations and lose support of matrix theory, but so far I have survived.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2015-09-25T07:09:26.657" LastActivityDate="2015-09-25T07:09:26.657" CommentCount="2" />
  <row Id="1541" PostTypeId="2" ParentId="1536" CreationDate="2015-09-25T08:28:40.463" Score="3" Body="&lt;p&gt;It's in the name: Homogeneous coordinates are well ... homogeneous.&#xA;Being homogeneous means a uniform representation of rotation, translation, scaling and other transformations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A uniform representation allows for optimizations. 3D graphics hardware can be specialized to perform matrix multiplications on 4x4 matrices. It can even be specialized to recognize and save on multiplications by 0 or 1, because those are often used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Not using homogeneous coordinates may make it hard to use strongly optimized hardware to its fullest. Whatever program recognizes that optimized instructions of the hardware can be used (typically a compiler but things are more complicated sometimes) for homogeneous coordinates will have a hard time with optimizing for other representations. It will choose less optimized instructions and thus not use the potential of the hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As there were calls for examples:&#xA;Sony's PS4 can perform massive matrix multiplications. It's so good at it that it was sold out for some time, because clusters of them were used instead of more expensive super-computers. Sony subsequently demanded that their hardware may not be used for military purposes. Yes, super-computers are military equipment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It has become quite usual for researchers to use graphic cards to calculate their matrix multiplications even if no graphic is involved. Simply because they are magnitudes better in it than general purpose CPUs.&#xA;For comparison modern multi-core CPUs have on the order of 16 pipelines (x0.5 or x2 doesn't matter so much) while GPUs have on the order of 1024 pipelines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's not so much the cores than the pipelines that allow for actual parallel processing. Cores work on threads. Threads have to be programmed explicitly. Pipelines work on instruction level. The chip can parallelize instructions more or less on its own.&lt;/p&gt;&#xA;" OwnerUserId="1714" LastActivityDate="2015-09-25T08:28:40.463" CommentCount="1" />
  <row Id="1542" PostTypeId="1" AcceptedAnswerId="1543" CreationDate="2015-09-26T04:38:14.037" Score="4" ViewCount="63" Body="&lt;p&gt;Forgive me in advance for my ignorance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the PHP function imagecolorat, I have a solution that presently delivers a set of the most dominant hex color codes available inside of an image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a known library that will allow me to then take the hex color code, RGB code (or any other standard color code) and simplify them down, or rather &quot;match&quot; them to a limited set of &quot;common colors&quot;, such as red, brown, green, yellow, grey, blue, etc?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, #944D70, which is a shade of purple, would be matched to the color 'purple'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Alternatively:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a particular pattern or range to RGB colors that would allow me to write such a library myself?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, 0,0,0 thru 20,20,20 might all be considered a &quot;black&quot; color. However, it seems as if organizing such a library would be very exhausting if done manually.&lt;/p&gt;&#xA;" OwnerUserId="1723" LastActivityDate="2015-09-26T14:34:19.023" Title="Convert RGB, Hex or any other color format to &quot;Standard Color&quot; Programmatically" Tags="&lt;conversion&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1543" PostTypeId="2" ParentId="1542" CreationDate="2015-09-26T09:19:28.477" Score="6" Body="&lt;p&gt;Problem is that people do not really agree on a 1:1 mapping of color names. But yes such tools exists, this is a simple nearest neighborhood test. You could use any spatial query module to do this. Here is a simple (naive) python example: &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;from __future__ import print_function&#xA;import webcolors&#xA;from scipy.spatial import KDTree&#xA;&#xA;# lets populate some names into spatial name database&#xA;hexnames = webcolors.css3_hex_to_names&#xA;names = []&#xA;positions = []&#xA;&#xA;for hex, name in hexnames.iteritems():&#xA;    names.append(name)&#xA;    positions.append(webcolors.hex_to_rgb(hex))&#xA;&#xA;spacedb = KDTree(positions)&#xA;&#xA;# query nearest point&#xA;querycolor = (10,88, 200)&#xA;dist, index = spacedb.query(querycolor)&#xA;&#xA;print('The color %r is closest to %s.'%(querycolor, names[index]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which results in: &lt;code&gt;The color (10, 88, 200) is closest to royalblue.&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using here the 140 css3 defined colors. Bigger and more comprehensive lists can be found &lt;a href=&quot;https://en.wikipedia.org/wiki/Lists_of_colors&quot; rel=&quot;nofollow&quot;&gt;online&lt;/a&gt;, if you want a smallerlist that would also work. Theres all kinds of possible things you could do, you could for example linearize the sRGB space before calculating distances to name. You could ask for 3 closest names and give double names if the names are halfway.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-09-26T14:34:19.023" LastActivityDate="2015-09-26T14:34:19.023" CommentCount="6" />
  <row Id="1545" PostTypeId="2" ParentId="1523" CreationDate="2015-09-27T02:32:20.067" Score="21" Body="&lt;p&gt;As Simon's comment alluded to, one major difference between hardware texture compression and other commonly used image compression is that the former does not use entropy coding. Entropy coding is the use of shorter bit-strings to represent commonly-occurring or repeating patterns in the source data&amp;mdash;as seen in container formats like ZIP, many common image formats such as GIF, JPEG, and PNG, and also in many common audio and video formats.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Entropy coding is good at compressing all sorts of data, but it intrinsically produces a variable compression ratio. Some areas of the image may have little detail (or the detail is predicted well by the coding model you're using) and require very few bits, but other areas may have complex detail that requires more bits to encode. This makes it difficult to implement random access, as there is no straightforward way to calculate where in the compressed data you can find the pixel at given (&lt;em&gt;x&lt;/em&gt;,&amp;nbsp;&lt;em&gt;y&lt;/em&gt;) coordinates. Also, most entropy coding schemes are stateful, so it's not possible to simply start decoding at an arbitrary place in the stream; you have to start from the beginning to build up the correct state. However, random access is necessary for texture sampling, since a shader may sample from any location in a texture at any time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, rather than entropy coding, hardware compression uses fixed-ratio, block-based schemes.  For example, in &lt;a href=&quot;http://www.reedbeta.com/blog/2012/02/12/understanding-bcn-texture-compression-formats/&quot;&gt;DXT / BCn compression&lt;/a&gt;, the texture is sliced up into 4&amp;times;4 pixel blocks, each of which is encoded in either 64 or 128 bits (depending on which format is picked); in &lt;a href=&quot;https://en.wikipedia.org/wiki/Adaptive_Scalable_Texture_Compression&quot;&gt;ASTC&lt;/a&gt;, different formats use block sizes from 4&amp;times;4 up to 12&amp;times;12, and all blocks are encoded in 128 bits. The details of how the bits represent the image data vary between formats (and may even vary from one block to the next within the same image), but because the ratio is fixed, it's easy for hardware to calculate where in memory to find the block containing a given (&lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt;) pixel, and each block is self-contained, so it can be decoded independently of any other blocks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another consideration in hardware texture compression is that the decoding should be efficiently implementable in hardware. This means that heavy math operations and complex dataflow are strongly disfavored. The BCn formats, for instance, can be decoded by doing a handful of 8-bit integer math operations per block to populate a small lookup table, then just looking up the appropriate table entry per pixel.  This requires very little area on-chip, which is important because you probably want to decode several blocks in parallel, and thus need several copies of the decode hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast, DCT-based formats like JPEG require a nontrivial amount of math per pixel, not to mention a complex dataflow that swaps and broadcasts various intermediate values across pixels within a block. (Look at &lt;a href=&quot;https://fgiesen.wordpress.com/2013/11/04/bink-2-2-integer-dct-design-part-1/&quot;&gt;this article&lt;/a&gt; for some of the gory details of DCT decoding.) This would be a lot grosser for hardware implementation, which I'm guessing is why AFAICT, no GPU hardware has ever implemented DCT-based or wavelet-based texture compression.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-09-27T02:32:20.067" CommentCount="3" />
  <row Id="1546" PostTypeId="1" CreationDate="2015-09-27T05:14:24.863" Score="9" ViewCount="194" Body="&lt;p&gt;Suppose we have a 64-bit word-addressable computer and we want to program it to output a 5x7 character stored as a binary image bitmap (such as the one below) to a memory-mapped display.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LfHfE.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LfHfE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://i.stack.imgur.com/jaPjM.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jaPjM.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since we have 5 x 7 = 35 pixels per character, we could store a character using 35 bits in a single word. With the least significant bit starting on the left side of the word and with each pixel in the image being represented by the &lt;em&gt;n&lt;/em&gt;th bit as shown above, the number &quot;3&quot; above would be stored in memory as: 01110100010000100110000011000101110, followed by 29 unused bits set to 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this how characters were/are stored in old/modern computers? Or do they use a single byte/word per pixel instead?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If they are stored in this manner, what would the routine in assembly/machine-code (using nothing more than elementary instructions such as bitwise, arithmetic and data transport operations from the computer's Instruction Set Architecture) used to convert this data into an image on the display look like? Would it be something like:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Store the x and y display coordinate for the current pixel to be updated in a certain register.&lt;/li&gt;&#xA;&lt;li&gt;Store the two chosen RGB values (in this case 0,255,0 for green and 0,0,0 for black) in two other separate registers.&lt;/li&gt;&#xA;&lt;li&gt;Have two further registers act as counters initialized to 5 and 7 to keep track of the current row and column of the image being rendered.&lt;/li&gt;&#xA;&lt;li&gt;Test if the column register is not 0. If it isn't, test if the LSB of the bitmap is set to 1, then AND the respective RGB value register with the x and y coordinate register depending on the result, then MOV that result to the display output register.&lt;/li&gt;&#xA;&lt;li&gt;Decrement the row counter register by 1, test to see if it is 0. If it is, then set it back to 5 and increment the y coordinate by 1 and decrement the column counter by 1.&lt;/li&gt;&#xA;&lt;li&gt;Shift the register holding the bitmap 1 bit to the left.&lt;/li&gt;&#xA;&lt;li&gt;JMP to instruction 4.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Is there a simpler or more efficient way to do this? It seems as though even something as simple as rendering a single small text character takes quite a large number of operations and would take around 200 CPU cycles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, are there any good books or resources on machine-level code for displaying images from scratch, because I haven't been able to find any as they either gloss over this particular subject or the code is written in a high level language or an assembler using macros, all of which are &quot;cheating&quot; and don't explain what is fundamentally going on at the lowest level.&lt;/p&gt;&#xA;" OwnerUserId="1735" LastActivityDate="2015-09-30T10:08:55.720" Title="Fundamentally, how are 2D bitmaps rendered?" Tags="&lt;rendering&gt;&lt;2d&gt;&lt;bitmap-graphics&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="1547" PostTypeId="2" ParentId="1546" CreationDate="2015-09-27T19:22:57.333" Score="7" Body="&lt;p&gt;You have to distinguish the text and graphical modes of the graphics board of your machine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the old days, mostly the text mode was supported. In this mode the board took care of storing the bitmap definition of the characters and displaying them at the current cursor position. All you had to do was to provide the ASCII code of the character (one byte per character) in a small text buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, a high-resolution raster buffer is provided, which is pixel accessible and to which you write color information in some supported format (in the &quot;highest&quot; mode, 3 bytes (RGB) per pixel, for a megapixel or more).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Originally, simple (packed) binary bitmaps of different sizes were used and &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bit_blit&quot;&gt;blittted&lt;/a&gt;&quot; to the raster memory via a request to the device driver, with possible format translation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nowadays, characters are mostly defined as &lt;a href=&quot;https://en.wikipedia.org/wiki/Vector_graphics&quot;&gt;vector drawings&lt;/a&gt;, which are a resolution-independent description of the outlines, and need to undergo a complex rendering process that includes &lt;a href=&quot;https://en.wikipedia.org/wiki/Spatial_anti-aliasing&quot;&gt;antialiasing&lt;/a&gt; for smooth results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The rendered output can be cached for fast display, again by blitting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The overall process is complex, can be hardware accelerated, and is handled by the operating system (GUI management) in a transparent way, together with other graphical primitive drawing operations.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2015-09-27T20:29:01.950" LastActivityDate="2015-09-27T20:29:01.950" CommentCount="0" />
  <row Id="1548" PostTypeId="2" ParentId="1535" CreationDate="2015-09-28T22:41:47.203" Score="2" Body="&lt;p&gt;This answer tries to give a short overview over some important aspects. Since the HSH definition is rather complex and I couldn't find an overview over some pre-evaluated functions, I didn't provide examples simply because it would take me too much time right now.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Problem Description &amp;amp; Brute Force&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;To determine the convolution any with any set of basis function and thus computing the coefficients we generally need to compute the integral over the domain (= sphere for SH, hemisphere for HSH). Everything we need to do, to represent the hemispherical function &lt;em&gt;f&lt;/em&gt;, which is defined over the angles &lt;em&gt;theta&lt;/em&gt; (&quot;up/down&quot;) and &lt;em&gt;phi&lt;/em&gt; (&quot;left/right&quot;), via a coefficient &lt;em&gt;c&lt;/em&gt; for HSH basis functions &lt;em&gt;H&lt;/em&gt; is the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/gkb7K.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gkb7K.gif&quot; alt=&quot;\int_0^{2\pi}\int_0^{\frac{2}{\pi}} f(\theta,\phi) \cdot H_l^m(\theta,\phi) \cdot sin(\theta) \,\, \mathrm{d}\theta\mathrm{d}\phi&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;em&gt;sin(theta)&lt;/em&gt; is there because we integrate over the surface of a (hemi-)sphere. Conceptually, the size of a piece of area that comes from changing &lt;em&gt;phi&lt;/em&gt; is bigger or smaller on the current theta. More on this &lt;a href=&quot;http://tutorial.math.lamar.edu/Classes/CalcIII/TISphericalCoords.aspx&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we don't care too much about accuracy or computing time we can solve this simply by sampling: Generate equally distributed (!) directions on the hemisphere, compute the product of f and H and average the outcomes (if you have truly equally distributed points you don't need the &lt;em&gt;sin(theta)&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Get Started with an Analytical Solution&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Of course we would love to have an analytical solution for our function, but this is where things can get very difficult. As a first step we may need to convert a function that is given on Cartesian directions into spherical coordinates. This part is still easy, just replace all your x, y and z as following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/m0jJv.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/m0jJv.gif&quot; alt=&quot;(x,y,z) \rightarrow (\sin\theta \cos\phi, \sin\theta \sin\phi, \cos\theta)&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this gives us a system where the z-axis is the &quot;up&quot; of the hemisphere (theta=0) which should be represented by the HSH. After that it may already be possible to insert everything into a computer algebra system and solve the equation. Do not try to solve for all &lt;em&gt;m&lt;/em&gt; &amp;amp; &lt;em&gt;l&lt;/em&gt; but rather try one coefficient at a time, since it is unlikely that there is a compact expression that describes all of them at once. The definition of HSH is relatively complex, which makes it very tedious to evaluate these functions. In &lt;a href=&quot;http://www.researchgate.net/publication/221121831_On_the_use_of_hemispherical_harmonics_for_modeling_images_of_objects_under_unknown_distant_illumination&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; the zero and 1st order HSH basis functions are mentioned in cartesian coordinates.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Notes on Rotations &amp;amp; Zonal Harmonics&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Functions that are rotational symmetric around this z-axis are very good candidates for a successful analytic derivation, since they only affect the &lt;em&gt;zonal&lt;/em&gt; coefficients, which are all coefficients with index &lt;em&gt;m&lt;/em&gt; equals zero. This is especially helpful for the more general Spherical Harmonics where an easy formula exists that allows to rotate any Zonal Spherical Harmonics representation to an arbitrary direction, resulting in a Spherical Harmonics representation without any data loss (see &lt;a href=&quot;http://simonstechblog.blogspot.de/2011/12/spherical-harmonic-lighting.html&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). This means that you can derive ZSH coefficients by assuming that your radial symmetric &quot;function points to z&quot; and rotate it then it into any desired direction. This works perfectly for example with various cosine lobe variations and also gives you the factors you mentioned in the question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the bad news: For HSH, any rotation of a function around another axis than z is lossy, since your function will &quot;touch&quot; the lower undefined hemisphere after the rotation. Therefore, there is also no convenient &quot;Hemi Zonal to HSH&quot; rotation formula. Instead, there are multiple ways to do it with different drawbacks. For more details see the &lt;a href=&quot;http://www.cs.ucf.edu/~ceh/Publications/Papers/Rendering/EGSR04GautronEtAl.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; and the &lt;a href=&quot;https://www.google.de/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;ved=0CDEQFjABahUKEwj6joP13ZrIAhVFVxQKHQliBik&amp;amp;url=http%3A%2F%2Fcgg.mff.cuni.cz%2F~jaroslav%2Fpapers%2Fegsr2004%2Fgautron-egsr2004-hemispherical.ppt&amp;amp;usg=AFQjCNG7U0b1JYorPpGQ3qTkr8C4AOqDpg&amp;amp;sig2=yHEuWgzlvroc65C7wxD2uA&amp;amp;cad=rja&quot; rel=&quot;nofollow&quot;&gt;presentation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;By the way: All this is easier with the &lt;a href=&quot;https://www.cg.tuwien.ac.at/research/publications/2010/Habel-2010-EIN/&quot; rel=&quot;nofollow&quot;&gt;H-Basis&lt;/a&gt;, which is hemispherical as well (but originally only defined for a limited number of frequency-bands).&lt;/p&gt;&#xA;" OwnerUserId="528" LastActivityDate="2015-09-28T22:41:47.203" CommentCount="0" />
  <row Id="1550" PostTypeId="2" ParentId="1531" CreationDate="2015-09-29T07:42:29.213" Score="2" Body="&lt;p&gt;I reckon your instructor is encouraging you to implement low-level rendering algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The obstacle to draw shapes using just points is the contradiction between continuous and discrete. Shapes are logically continuous if in its area every pixel is shaded. But you can not draw an actually continuous line using discrete points because infinite points are needed. If you draw points directly in the &quot;world&quot;, you can not guarantee every pixel on the screen can be covered after projection, unless you set the interval small enough(which may be of quite low performance).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My advice is to maintain the world coordinates of the vertices of the shape but not draw it. Get the &quot;projected&quot; coordinates on screen and fill the area later using &lt;a href=&quot;https://en.wikipedia.org/wiki/Digital_differential_analyzer_%28graphics_algorithm%29&quot; rel=&quot;nofollow&quot;&gt;DDA&lt;/a&gt; or some other interpolation algorithms. &quot;Projected&quot; is the coordinates obtained by the three matrices(Projection, View, Model), I think you've heard of these. So in the filling process GLUT is used to draw points on the 2D screen, and cover every pixel in the area.&lt;/p&gt;&#xA;" OwnerUserId="1752" LastActivityDate="2015-09-29T07:42:29.213" CommentCount="1" />
  <row Id="1551" PostTypeId="2" ParentId="1523" CreationDate="2015-09-29T13:36:53.633" Score="20" Body="&lt;p&gt;&lt;em&gt;&quot;How (hardware) texture compression works&quot;&lt;/em&gt; is a large topic. Hopefully I can provide some insights without duplicating the content of &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/1523/how-does-hardware-texture-compression-work/1545#1545&quot;&gt;Nathan's answer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Requirements&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Texture compression typically differs from 'standard' image compression techniques e.g. JPEG/PNG in four main ways, as outlined in Beers et al's &lt;a href=&quot;https://graphics.stanford.edu/projects/flashg/papers/texture_compression/&quot; rel=&quot;nofollow&quot;&gt;Rendering from Compressed Textures&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Decoding Speed&lt;/em&gt;: You don't want texture compression to be slower (at least not noticeably so) than using uncompressed textures. It should also be relatively simple to decompress since that can help achieve fast decompression without excessive hardware and power costs.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Random Access&lt;/em&gt;: You can't easily predict which texels will be required during a given render. If some subset, &lt;em&gt;M&lt;/em&gt;, of the accessed texels come from, say, the middle of the image, it's essential that you don't have to decode all of the 'previous' lines of the texture in order to determine &lt;em&gt;M&lt;/em&gt;; with JPEG and PNG this is necessary as pixel decoding depends on the previously decoded data.&lt;br&gt;&#xA;Note, having said this, just because you have &quot;random&quot; access, &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/357/is-using-many-texture-maps-bad-for-caching/419#419&quot;&gt;doesn't mean you should try to sample completely arbitrarily&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Compression Rate and Visual Quality&lt;/em&gt;: Beers et al argue (convincingly) that losing some quality in the compressed result in order to improve compression rate is a worthwhile trade-off. In 3D rendering, the data is probably going to be manipulated (e.g. filtered &amp;amp; shaded etc) and so some loss of quality may well be masked.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;em&gt;Asymmetric encoding/decoding&lt;/em&gt;: Though perhaps slightly more contentious, they argue that it is acceptable to have the encoding process much slower than the decoding. Given that the decoding needs to be at HW fill rates, this is generally acceptable. (I will admit that compression of PVRTC, ETC2 and some others at maximum quality could be faster)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h2&gt;Early History &amp;amp; Techniques&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;It may surprise some to learn that texture compression has been around for over three decades. Flight simulators from the 70s and 80s needed access to &lt;em&gt;relatively&lt;/em&gt; large amounts of texture data and given that &lt;a href=&quot;http://www.jcmit.com/memoryprice.htm&quot; rel=&quot;nofollow&quot;&gt;1MB of RAM in 1980 was &gt; $6000&lt;/a&gt;, reducing the texture footprint was essential. As another example, in the mid 70s, even a small amount of high speed memory and logic, e.g. enough for &lt;a href=&quot;https://en.wikipedia.org/wiki/Framebuffer#History&quot; rel=&quot;nofollow&quot;&gt;a modest 512x512 RGB frame buffer&lt;/a&gt;) could set you back the price of small house.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though, AFAIK, not explicitly referred to as texture compression, in the literature and patents you can find references to techniques including:&lt;br&gt;&#xA; a. simple forms of mathematical/procedural texture synthesis,&lt;br&gt;&#xA; b. use of a single channel texture (e.g. 4bpp) that is then multiplied by a per-texture RGB value,&lt;br&gt;&#xA; c. YUV, and&lt;br&gt;&#xA; d. palettes (the literature suggesting use of &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.1334&quot; rel=&quot;nofollow&quot;&gt;Heckbert's approach&lt;/a&gt; to do the compression) &lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Modelling Image Data&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;As noted above, texture compression is nearly always lossy and thus the problem becomes one of trying to represent the important data in a compact way whilst disposing of the less significant information. The various schemes that will be described below all have an implicit 'parameterised' model that approximates the typical behaviour of texture data and of the eye's response. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Further, since texture compression tends to use fixed-rate encoding, the compression process usually includes a search step to find the set of parameters which, when fed in to the model, will generate a good approximation of the original texture. That search step, however, can be time consuming.&lt;br&gt;&#xA;&lt;em&gt;(With the possible exception of tools such as &lt;a href=&quot;http://optipng.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;optipng&lt;/a&gt;, this is another area where typical use of PNG &amp;amp; JPEG differ from texture compression schemes)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Before progressing further, to help with further understanding of TC it's worth taking a look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot; rel=&quot;nofollow&quot;&gt;Principal Component Analysis (PCA)&lt;/a&gt; - a very useful mathematical tool for data compression.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Example texture&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;To compare the various methods, we'll use the following image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/SM000.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SM000.png&quot; alt=&quot;small lorikeet+text&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;Note that this is a fairly tough image, especially for palette and VQTC methods as it spans much of the RGB colour cube and only 15% of the texels use repeated colours.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;PC and (post mid 90s) Console Texture compression&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;To reduce data costs, some PC games and early games consoles also made use of palette images, which is a form of Vector Quantisation (VQ).&#xA;Palette-based approaches make the assumption that a given image only uses relatively small portions of the RGB(A) colour cube.  A problem with palette textures are that the compression rates for the achieved quality are generally rather modest. The example texture compressed to &quot;4bpp&quot; (using GIMP) produced&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Vpv0t.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Vpv0t.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;Note again that this is a relatively tough image for VQ schemes.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;VQ with larger vectors (e.g. 2BPP ARGB)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Inspired by Beers et al, the Dreamcast console used VQ to encode 2x2  or even 2x4 pixel blocks with single bytes. While the &quot;vectors&quot; in the palette textures are 3 or 4 dimensional, the 2x2 pixel blocks can be considered to be 16 dimensional. The compression scheme assumes there is sufficient, approximate repetition of these vectors. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even though VQ can achieve satisfactory quality with ~2bpp, the problem with these schemes is that it requires dependent memory reads: An initial read from the index map to determine the code for the pixel is followed by a second to actually fetch the pixel data associated with that code. Additional caches can help alleviate some of the incurred latency, but adds complexity to the hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example image compressed with the 2bpp Dreamcast scheme is &#xA;&lt;a href=&quot;http://i.stack.imgur.com/8tnag.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8tnag.png&quot; alt=&quot;2bpp VQ result&quot;&gt;&lt;/a&gt;. &#xA;The index map is: &lt;a href=&quot;http://i.stack.imgur.com/YNJww.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YNJww.png&quot; alt=&quot;2bpp VQ index map&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compression of VQ data can be done in various ways however, &lt;em&gt;IIRC&lt;/em&gt;, the above was done using PCA to derive and then partition the 16D vectors along the principal vector into 2 sets such that two representative vectors minimised the mean squared error. The process then recursed until 256 candidate vectors were produced. &#xA;A global &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm&quot; rel=&quot;nofollow&quot;&gt;k-means/Lloyd's algorithm&lt;/a&gt; approach was then applied to improve the representatives.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Colour Space Transformations&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Colour space transformations also make use of PCA noting that the global distribution of colour is often spread along a major axis with far less spread along the other axes. For YUV representations, the assumptions are that a) the major axis is often in the luma direction and that b) the eye is more sensitive to changes in this direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 3dfx Voodoo system provided &lt;a href=&quot;http://www.gamers.org/dEngine/xf3D/glide/glidepgm.htm&quot; rel=&quot;nofollow&quot;&gt;&quot;YAB&quot;&lt;/a&gt;, an 8bpp, &quot;Narrow Channel&quot; compression system that split each 8 bit texel into a 322 format, and applied a user selected colour transform to that data to map it into RGB. The main axis thus had 8 levels and the smaller axes, 4 each.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The S3 Virge chip had a slightly simpler, 4BPP, scheme that allowed the user to specify, for &lt;em&gt;the entire texture&lt;/em&gt;, two end colours, which should lie on the principal axis, along with a 4bpp monochrome texture. The per-pixel value then blended the end colours with appropriate weights to produce the RGB result.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;BTC-based schemes&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Rewinding some number of years, Delp and Mitchell designed a simple (monochrome) image compression scheme called &lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1094560&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Block Truncation Coding&lt;/em&gt;, (BTC)&lt;/a&gt;. This also included the method of compression but, for our purposes, we are mainly interested in the resulting compressed data and decompression algorithm.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this scheme, images are broken into, typically, 4x4 pixel blocks, which can be compressed independently with, what is in effect, a localised VQ algorithm. Each block is represented by two &quot;values&quot;, &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;, and a 4x4 set of index bits, which identify which of the two values to use for each pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;S3TC&lt;/em&gt;:  4BPP RGB (+1bit alpha)&lt;br&gt;&#xA;Although several colour-variants of BTC for image compression were proposed, of interest to us is &lt;a href=&quot;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;amp;Sect2=HITOFF&amp;amp;d=PALL&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&amp;amp;r=1&amp;amp;f=G&amp;amp;l=50&amp;amp;s1=5,956,431.PN.&amp;amp;OS=PN/5,956,431&amp;amp;RS=PN/5,956,431&quot; rel=&quot;nofollow&quot;&gt;Iourcha et al's S3TC&lt;/a&gt;, some of which appears to be a rediscovery of the somewhat forgotten work of &lt;a href=&quot;http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;amp;Sect2=HITOFF&amp;amp;d=PALL&amp;amp;p=1&amp;amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&amp;amp;r=1&amp;amp;f=G&amp;amp;l=50&amp;amp;s1=5046119.PN.&amp;amp;OS=PN/5046119&amp;amp;RS=PN/5046119&quot; rel=&quot;nofollow&quot;&gt;Hoffert et al&lt;/a&gt; that was used in Apple's Quicktime.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original S3TC, without the DirectX variants, compresses either RGB RGB+1bit Alpha to 4BPP. Each 4x4 block in the texture is replaced by two end colours, &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;, from which up to two other colours are derived by fixed-weight, linear blends. Further, each texel in the block has a 2-bit index that determines how to select one of these four colours. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example the following is a 4x4 pixel section of the test image compressed with the AMD/ATI Compressenator tool. (&lt;em&gt;Technically it's taken from a 512x512 version of the test image but forgive my lack of time to update the examples&lt;/em&gt;).&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/YCI3n.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YCI3n.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;This illustrates the compression process: The average and the principal axis of the colours are calculated.  A best fit is then performed to find two end points that 'lie on' the axis which, along with the two derived 1:2 and 2:1 blends (or in some cases a 50:50 blend) of those end points, that minimises the error.  Each original pixel is then mapped to one of those colours to produce the result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If, as in this case, the colours are reasonable approximated by the principal axis, the error will be relatively low. However if, like in the neighbouring 4x4 block shown below, the colours are more diverse, the error will be higher.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/d5dZ8.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/d5dZ8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example image, compressed with the &lt;a href=&quot;http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/the-compressonator/&quot; rel=&quot;nofollow&quot;&gt;AMD Compressonator&lt;/a&gt; produces:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/ZFrSa.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZFrSa.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the colours are determined independently per-block, there can be discontinuities at block boundaries but, as long as the resolution is kept sufficiently high, these block artefacts can often go unnoticed:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Fa779.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Fa779.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;ETC1&lt;/em&gt;:  4BPP RGB&lt;br&gt;&#xA;Ericsson Texture Compression also works with 4x4 blocks of texels but makes the assumption that, much like YUV, the principal axis of a local set of texels is often very strongly correlated with &quot;luma&quot;. The set of texels can then be represented by just an average colour and a highly quantised, scalar 'length' of the projection of the texels onto that assumed axis.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since this reduces the data storage costs relative to say, S3TC, it allows ETC to introduce a partitioning scheme, whereby the 4x4 block is subdivided into a pair of horizontal 4x2 or vertical 2x4 sub-blocks. These each have their own average colour. &#xA;The example image produces:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/cYOmr.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cYOmr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;The area around the beak also illustrates the horizontal and vertical partitioning of the 4x4 blocks.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Uwd5F.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Uwd5F.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;    &lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Global+Local&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;There are some texture compression systems that are a cross between global and local schemes, such as that of the distributed palettes of &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00420/abstract&quot; rel=&quot;nofollow&quot;&gt;Ivanov and Kuzmin&lt;/a&gt; or the method of &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.7956&quot; rel=&quot;nofollow&quot;&gt;PVRTC&lt;/a&gt;.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;PVRTC&lt;/em&gt;:  4 &amp;amp; 2 BPP RGBA&lt;br&gt;&#xA;PVRTC assumes that an (in practice, bilinearly) upscaled image is a good approximation to the full-resolution target and that the difference between the approximation and the target, i.e. the delta image, is locally monochromatic, i.e. has a dominant principal axis. Further, it assumes the local principal axis can be interpolated across the image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(to do: Add images showing breakdown)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The example texture, compressed with PVRTC1 4BPP produces:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/5jKnQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5jKnQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;with the area around the beak:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/ODTd4.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ODTd4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;Compared to BTC-schemes, the block artefacts typically are eliminated but there can sometimes be &quot;overshoot&quot; if there are strong discontinuities in the source image, for example around the silhouette of the lorikeet's head.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 2BPP variant has, naturally, higher error than the 4BPP (note loss of precision around the blue, high frequency areas near the neck) but arguably still of reasonably quality:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/ASiRE.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ASiRE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;A note on decompression costs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Although the compression algorithms for the schemes described above have a moderate to high evaluation cost, the decompression algorithms, especially for hardware implementations, are relatively inexpensive. ETC1, for example, requires little more than a few MUXes and low-precision adders; S3TC effectively slightly more addition units to perform the blending; and PVRTC, slightly more again.  In theory, these simple TC schemes could allow a GPU architecture to avoid decompression until just prior to the filtering stage, thus maximising the effectiveness of internal caches.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Other Schemes&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Other common TC modes that should be mentioned are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;ETC2 - is a (4bpp) superset of ETC1 that improves the handling of regions with colour distributions that don't align well with 'luma'. There are also a 4bpp variant that supports 1 bit alpha, and an 8bpp format for RGBA.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;ATC - Is effectively a small &lt;a href=&quot;http://www.guildsoftware.com/papers/2012.Converting.DXTC.to.ATC.pdf&quot; rel=&quot;nofollow&quot;&gt;variation on S3TC&lt;/a&gt;.  &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;FXT1 (3dfx) was a more ambitious &lt;a href=&quot;https://www.opengl.org/registry/specs/3DFX/texture_compression_FXT1.txt&quot; rel=&quot;nofollow&quot;&gt;variant of the S3TC theme&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;BC6 &amp;amp; BC7 : An 8bpp, block-based system supporting ARGB. Apart from HDR modes, these use a more complex partitioning system than that of ETC to attempt to better model image colour distribution.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;PVRTC2 : 2&amp;amp;4BPP ARGB. This introduces additional modes including one to overcome limitations with strong boundaries in the images.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;ASTC : This is also a block based system but is somewhat more complicated in that it has a large number of possible block sizes targeting a wide range of BPP. It also includes features such as up to 4 partition regions with a pseudo-random partition generator, and variable resolution for the index data and/or colour precision and colour models.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-01-18T13:04:53.083" LastActivityDate="2016-01-18T13:04:53.083" CommentCount="2" />
  <row Id="1552" PostTypeId="1" AcceptedAnswerId="1553" CreationDate="2015-09-30T05:40:15.270" Score="1" ViewCount="187" Body="&lt;p&gt;&lt;em&gt;[I got a warning that the question will be closed].&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have identified that CG is the place where all the advanced algorithms and computations are taking place. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the best way to start from rudimentary and progress gradually in CG programming?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please express language choices (C, C++, etc.) and other topics you find relevant. I also found out  Michael Abrash's book.&lt;/p&gt;&#xA;" OwnerUserId="1695" LastActivityDate="2015-09-30T07:17:01.497" Title="What are the best ways to start graphics programming?" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2015-09-30T20:21:58.790" />
  <row Id="1553" PostTypeId="2" ParentId="1552" CreationDate="2015-09-30T07:17:01.497" Score="1" Body="&lt;p&gt;There are many possible answers depending on your OS, goals and wanted level of abstraction. For low-level programming you can start by learning OpenGL, Metal or Direct3D. Most used shader languages are HLSL based. For higher level there are things like ShaderToy and engines like Unity (C#) or Unreal (C++) where you can experiment with graphics coding.&lt;/p&gt;&#xA;" OwnerUserId="67" LastActivityDate="2015-09-30T07:17:01.497" CommentCount="4" />
  <row Id="1554" PostTypeId="2" ParentId="1546" CreationDate="2015-09-30T09:46:04.110" Score="0" Body="&lt;p&gt;The short answer is YES, you can not avoid doing a lot of bit manipulation if your format needs it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Drawing a bitmap basically mean copy a pixel from a source to a destination and you have to do what it takes to do it. (Quoting Captain Obvious)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A longer answer is that if you write a software rasterizer, you can have some algorithm and trick to save you cpu time (by knowing which part you DO NOT NEED TO DRAW (transparency optimization), by having a pixel format of the source the same as the destination (directly or in a form of cache), optimally perform memcopy, etc... Basically consider the drawing loop of your rasterizer and see how you can restructure them to save CPU time. (ex : you could generate at runtime a piece of assembler code just specifically to print the letter A or have meta to your source bitmap info to tell you how to skip transparent area, etc..)&#xA;Each use case may have a different solution based on CPU instruction set, buffer format, the algorithm of your rendering primitive (rotating ? stretching bitmaps ? what kind of filtering ? etc...), CPU register and cache etc...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So yes anyway, it TOOK a lot of CPU cycle to write single pixel in the old days when weird encoding &amp;amp; small memory were the norm. :-)&#xA;But it didn't forbid 16 / 32 bit machines with 8 MHZ CPU to do things like that : &lt;a href=&quot;https://www.youtube.com/watch?v=GWwPJU6pp30&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=GWwPJU6pp30&lt;/a&gt; (And a big chunk of the CPU was used also for the music)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of HW rendering, HW will perform the conversion from the source format to destination format and while it will not use much of the tricks available to SW rasterizer, its generic implementation in HW will most likely beat most of the SW implementation.&lt;/p&gt;&#xA;" OwnerUserId="105" LastEditorUserId="105" LastEditDate="2015-09-30T10:08:55.720" LastActivityDate="2015-09-30T10:08:55.720" CommentCount="0" />
  <row Id="1555" PostTypeId="1" CreationDate="2015-09-30T10:57:29.540" Score="4" ViewCount="54" Body="&lt;p&gt;Asking it here and not on SO as it seems to be appropriate question for CG.&#xA;I am learning &lt;a href=&quot;https://developer.nvidia.com/nvidia-video-codec-sdk&quot; rel=&quot;nofollow&quot;&gt;NVIDIA NVENC&lt;/a&gt; API.The SDK supplies a sampled called &quot;NvEncoderCudaInterop&quot; .There is a chunk of code which copies YUV plane arrays from CPU to GPU buffers.&#xA;This is the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; // copy luma&#xA; CUDA_MEMCPY2D copyParam;&#xA;memset(&amp;amp;copyParam, 0, sizeof(copyParam));&#xA;copyParam.dstMemoryType = CU_MEMORYTYPE_DEVICE;&#xA;copyParam.dstDevice = pEncodeBuffer-&amp;gt;stInputBfr.pNV12devPtr;&#xA;copyParam.dstPitch = pEncodeBuffer-&amp;gt;stInputBfr.uNV12Stride;&#xA;copyParam.srcMemoryType = CU_MEMORYTYPE_HOST;&#xA;copyParam.srcHost = yuv[0];&#xA;copyParam.srcPitch = width;&#xA;copyParam.WidthInBytes = width;&#xA;copyParam.Height = height;&#xA;__cu(cuMemcpy2D(&amp;amp;copyParam));&#xA;&#xA;// copy chroma&#xA;&#xA;__cu(cuMemcpyHtoD(m_ChromaDevPtr[0], yuv[1], width*height / 4));&#xA;__cu(cuMemcpyHtoD(m_ChromaDevPtr[1], yuv[2], width*height / 4));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I do understand the rationale behind the procedure.The memory is copied to GPU  for the kernel to process it.What I don't understand is why,in order to copy Y plane, cuMemcpy2D is used and for UV &lt;a href=&quot;http://developer.download.nvidia.com/compute/cuda/4_1/rel/toolkit/docs/online/group__CUDA__MEM_g4d32266788c440b0220b1a9ba5795169.html&quot; rel=&quot;nofollow&quot;&gt;cuMemcpyHtoD&lt;/a&gt;?Why Y can't be copied using cuMemcpyHtoD as well?As far as I understand,YUV planes have the same linear memory layout.The only difference is their size.&lt;/p&gt;&#xA;" OwnerUserId="213" LastEditorUserId="385" LastEditDate="2015-09-30T14:26:47.203" LastActivityDate="2015-09-30T14:26:47.203" Title="CUDA cuMemcpuHtoD vs cuMemcpy2D" Tags="&lt;gpu&gt;&lt;gpgpu&gt;&lt;cuda&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="1556" PostTypeId="1" AcceptedAnswerId="1560" CreationDate="2015-09-30T11:27:36.453" Score="12" ViewCount="173" Body="&lt;p&gt;I've been trying to understand some of the &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/1513/how-physically-based-is-the-diffuse-and-specular-distinction&quot;&gt;physical principles&lt;/a&gt; behind light and material interaction lately. In his talk &lt;a href=&quot;http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf&quot;&gt;Physics and Math of Shading&lt;/a&gt;, Naty Hoffman explains Fresnel reflectance and defines the characteristic specular color &lt;strong&gt;F&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt; of a material as the Fresnel reflectance at 0° incident light angle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On slide 65, &lt;strong&gt;F&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt; of gold is given as &lt;em&gt;1.022, 0.782, 0.344&lt;/em&gt; (linear). Hoffman adds: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;its red channel value is greater than 1 (it’s outside sRGB gamut)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;All of this doesn't make too much sense to me. A value greater than 1 would mean that in the wavelengths contributing to the red channel, &lt;em&gt;more energy is reflected than is received&lt;/em&gt;. Does this really happen, and if so, how and why?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;In addition, here is a reflectance curve from &lt;a href=&quot;https://en.wikipedia.org/wiki/Reflectance&quot;&gt;Wikipedia&lt;/a&gt; for some materials including gold (Au). The curve is certainly high for red wavelengths around 600nm but does not seem to go over 100%.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/P2Vwr.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/P2Vwr.png&quot; alt=&quot;Reflectance (Au)&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2015-10-07T07:53:43.003" Title="Fresnel reflectance of gold: red channel greater than 1?" Tags="&lt;physics&gt;&lt;brdf&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="1557" PostTypeId="1" CreationDate="2015-09-30T13:21:43.827" Score="8" ViewCount="127" Body="&lt;p&gt;&lt;strong&gt;Photorealistic rendering&lt;/strong&gt; has the goal of rendering an image as a &lt;em&gt;real camera&lt;/em&gt; would capture it. While this is already an ambitious goal, for certain scenarios you might want to take it further: render an image as the &lt;em&gt;human eye&lt;/em&gt; would capture it or even as the &lt;em&gt;human being&lt;/em&gt; would perceive it. You could call it &lt;em&gt;visiorealistic&lt;/em&gt; or &lt;em&gt;perceptiorealistic rendering&lt;/em&gt;, but if anyone could come up with a catchier term (or tell me that there is one already in existence) I'd appreciate that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some examples to make my point clear. When you take a picture with a camera at a low illumination level, you either have a good lens or get a noisy image. For a human observer, &lt;a href=&quot;https://en.wikipedia.org/wiki/Scotopic_vision&quot; rel=&quot;nofollow&quot;&gt;scotopic vision&lt;/a&gt; kicks in and gives rise to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Purkinje_effect&quot; rel=&quot;nofollow&quot;&gt;Purkinje effect&lt;/a&gt; (colors are shifted towards blue). This effect depends on the HDR luminance information, which are lost when I display the image on a LDR display. In addition, the human brain may use depth information to 'filter' the perceived image - information which are lost in a final (non-stereo) rendering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assembling an exhaustive list is probably an elusive goal. Could you suggest some of the effects of the eye and the brain that I would need to consider?&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-10-01T08:34:38.210" LastActivityDate="2015-10-16T08:20:53.373" Title="Realistic rendering: which processes of the human eye and brain do I need to consider?" Tags="&lt;human-vision&gt;" AnswerCount="2" CommentCount="5" FavoriteCount="1" />
  <row Id="1560" PostTypeId="2" ParentId="1556" CreationDate="2015-10-01T05:34:20.937" Score="9" Body="&lt;p&gt;RGB color is a bit more complicated a subject than readily seems apparent. The reflectance wavelength diagram shows the reason quite well actually.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RGB color model has several central problems:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What the colors represent: They represent 3 spikes in a continuous spectrum. The R, G and B aren't energetically equivalent let alone evenly spaced.&lt;/li&gt;&#xA;&lt;li&gt;What their range is: The colors do not actually mean anything without information of what space they span. In the assumed sRGB color space the space does not span the entire sensable range. So energetically equal but more vivid colors exist.&lt;/li&gt;&#xA;&lt;li&gt;Human sensory apparatus isn't actually reading 3 color spikes but the sensors nonlinearly overlap.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;As a result one can not draw the conclusion that a reflectance color channel of greater than 1 automatically means that energy is inserted into the system. That is simply one of the possible interpretations. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another interpretation is that the color is more intense than your color space allows. As a result your color vector component could be over 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Human eyes may also bleed color from one sensor to another due to overlap of the sensors. Such things happen with the sky which seems light blue, but is actually far darker blue but so intense that we see it as light blue. But in 50% reflections it would look wrong if we wouldn't account for this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, it can also mean energy is inserted into the system. Either the energy comes from elsewhere or is generated by the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rendering is often not a scientific measure of things. No energy principle needs to be broken to achieve this.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Summa summarum (tl;dr)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Color is often a compound attribute at the same time as it measures energy levels it also measures something other. Namely the location in the color space. Thus you can not differentiate the two signals (energy and color intensity) easily.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case it is a more intense color because the source says so: Outside of sRGB gamut = more intense color than the color space can make.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="231" LastEditDate="2015-10-07T07:53:43.003" LastActivityDate="2015-10-07T07:53:43.003" CommentCount="0" />
  <row Id="1561" PostTypeId="2" ParentId="1556" CreationDate="2015-10-01T05:35:46.373" Score="10" Body="&lt;p&gt;Here's a chromaticity diagram that includes a projection of the sRGB color space: a triangle whose vertices are red (1,0,0), green (0,1,0), and blue (0,0,1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eYuuN.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eYuuN.png&quot; alt=&quot;CIE chart with sRGB gamut from Wikipedia&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Encoding the reflectance of a surface as the color at &lt;strong&gt;F&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt; and getting a value that is outside of the (somewhat arbitrary chosen) sRGB gamut is totally reasonable. It just means that gold is &quot;more red&quot; than sRGB can represent, because it reserves valuable area of its dynamic range for other colors.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-10-01T05:35:46.373" CommentCount="3" />
  <row Id="1562" PostTypeId="1" AcceptedAnswerId="1563" CreationDate="2015-10-02T00:14:52.133" Score="7" ViewCount="375" Body="&lt;p&gt;I have a class that generates a 3D shape based on inputs from the calling code. The inputs are things like length, depth, arc, etc. My code generates the geometry perfectly, however I am running into trouble when calculating the surface normals. When lit, my shape has very bizzare coloring/texture from the incorrect surface normals that are being calculated. From all my research I believe my math is correct, it seems that something is wrong with my technique or method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At a high level how does one go about programmatically calculating the surface normals for a generated shape? I am using Swift/SceneKit on iOS for my code but a generic answer is fine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have two arrays that represent my shape. One is an array of 3D points that represents the vertices that make up the shape. The other array is a list of indexes of the first array that map the vertices into triangles. I need to take that data and generate a 3rd array that is a set of surface normals that aid in the lighting of the shape. (see &lt;a href=&quot;https://developer.apple.com/library/prerelease/ios/documentation/SceneKit/Reference/SceneKit_Constants/index.html#//apple_ref/c/data/SCNGeometrySourceSemanticNormal&quot;&gt;&lt;code&gt;SCNGeometrySourceSemanticNormal&lt;/code&gt; in SceneKit`&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The list of vertices and indexes is always different depending on the inputs to the class so I cannot pre-calculate or hard code the surface normals.&lt;/p&gt;&#xA;" OwnerUserId="1774" LastEditorUserId="127" LastEditDate="2015-10-02T07:15:44.110" LastActivityDate="2015-10-14T11:43:16.537" Title="How to Calculate Surface Normals for Generated Geomtery" Tags="&lt;lighting&gt;&lt;geometry&gt;" AnswerCount="2" CommentCount="7" FavoriteCount="1" />
  <row Id="1563" PostTypeId="2" ParentId="1562" CreationDate="2015-10-02T06:01:39.200" Score="8" Body="&lt;p&gt;You simply dont want fully smooth results. While the commented method by Nathan Reed: &quot;Calculate each vertex to face normal, sum them, normalize sum&quot;, generally works it sometimes fails spectacularly. But that is of no importance here, we can use that method by adding a rejection clause to it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case you simply want certain parts not to be smoothed against certain other parts. You want selective hard edges. So for example the flat top and bottom is separate form the triangle strip on the side, as is each flat area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JLntk.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JLntk.png&quot; alt=&quot;Image we are after&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: The result you want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In effect you only want to average the vertices of the curved area all others can use the normal they get form their triangle alone. So you are better of thinking of the mesh as 9 separate regions that are handled without the others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ityq6.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ityq6.png&quot; alt=&quot;Showing mesh and normals]&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Image showing the mesh structure and the normals. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can certainly automatically deduce this by not including normals that are outside certain angle from the primary vertexes normal. Pseudocode:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;For vertex in faceVertex:&#xA;    normal = vertex.normal&#xA;    For adjVertex in adjacentVertices:&#xA;        if anglebetween(vertex.normal, adjVertex.normal )  &amp;lt; treshold:&#xA;            normal += adjVertex.normal&#xA;    normal = normalize(normal)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That works, but you can simply avoid all of this at creation time because you understand that separate planes are working differently. So only the curved sides need normal direction merging. And in fact you can just directly calulate them from the underlying mathematical shape.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-10-02T08:59:22.163" LastActivityDate="2015-10-02T08:59:22.163" CommentCount="0" />
  <row Id="1564" PostTypeId="1" AcceptedAnswerId="1565" CreationDate="2015-10-02T08:10:48.497" Score="3" ViewCount="44" Body="&lt;p&gt;I have a shader which has several uniform variables:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;uniform vec4 a;&#xA;uniform vec4 b;&#xA;uniform vec4 c;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm getting these handles using &lt;code&gt;glGetUniformLocation()&lt;/code&gt; and storing them in the array. The game was suspended, then resumed and &lt;a href=&quot;http://developer.android.com/reference/android/opengl/GLSurfaceView.Renderer.html&quot; rel=&quot;nofollow&quot;&gt;onSurfaceCreated()&lt;/a&gt; has been called. The question is: whether these handles will be valid or I need to re-get them using &lt;code&gt;glGetUniformLocation()&lt;/code&gt; once again?&lt;/p&gt;&#xA;" OwnerUserId="333" LastEditorUserId="231" LastEditDate="2015-10-07T07:15:14.157" LastActivityDate="2015-10-07T07:15:14.157" Title="Can the uniform id be changed from run to run on the same machine?" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;android&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1565" PostTypeId="2" ParentId="1564" CreationDate="2015-10-02T08:53:07.807" Score="5" Body="&lt;p&gt;The uniform locations are set when the program is linked and are tied to the program.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that when you create a new program then the uniform locations can change. &lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-10-02T08:53:07.807" CommentCount="0" />
  <row Id="1566" PostTypeId="1" AcceptedAnswerId="1567" CreationDate="2015-10-02T09:36:58.310" Score="8" ViewCount="135" Body="&lt;p&gt;I'm in the process of making a tool that requires rendered texture to follow the contours of a piece of clothing. An example would be this website &lt;a href=&quot;https://knyttan.com/editor/jumper-editor/&quot;&gt;https://knyttan.com/editor/jumper-editor/&lt;/a&gt;. The effect here is achieved by using a colour map:&lt;a href=&quot;http://i.stack.imgur.com/pcCgx.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pcCgx.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked at the shaders that are used for this and it seems that the texture offset is calculated based on the colour channels from this map. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I was wondering if this is a complete bespoke way of doing this, or if this is a known technique and if it is what is it called ?&lt;/p&gt;&#xA;" OwnerUserId="1778" LastEditorUserId="231" LastEditDate="2015-10-07T07:13:04.297" LastActivityDate="2015-10-07T07:13:04.297" Title="Help me find out what this texture mapping technique is called" Tags="&lt;texture&gt;&lt;webgl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1567" PostTypeId="2" ParentId="1566" CreationDate="2015-10-02T11:00:20.730" Score="8" Body="&lt;p&gt;What you see in the image called a &lt;a href=&quot;https://en.wikipedia.org/wiki/UV_mapping&quot;&gt;UV map&lt;/a&gt;. That is, it is simply texture coordinates to be looked up encoded in a image. Same thing happens in all texture lookup in 3D there is a underlying sampler that picks where to pick texture color from.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BhNr6.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BhNr6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Image showing UV map of two overlapped triangles and sampled texture with same UV coords&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the sources for those images, please do not overwrite the sources.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://webglplayground.net/saved/CZTXXQeZFH&quot;&gt;Demo&lt;/a&gt; showing the UV map&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://webglplayground.net/saved/tTnh7zvUc6&quot;&gt;Demo&lt;/a&gt; showing the texture&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-10-02T11:39:23.227" LastActivityDate="2015-10-02T11:39:23.227" CommentCount="0" />
  <row Id="1568" PostTypeId="1" AcceptedAnswerId="1569" CreationDate="2015-10-03T15:44:17.840" Score="7" ViewCount="248" Body="&lt;p&gt;I am wondering if the various PBR models used in modern game engines for real time are heavier in terms of computation, that more classic approaches to rendering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Worded differently, is PBR just a different way to organize one's rendering pipeline, or does it add serious complexity (especially in regards to lighting) ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on the answer, what would be the bottleneck for a low-resource implementation ?&lt;/p&gt;&#xA;" OwnerUserId="34" LastActivityDate="2015-10-05T20:55:39.827" Title="Does PBR incur a performance penalty by design?" Tags="&lt;rendering&gt;&lt;real-time&gt;&lt;lighting&gt;&lt;physically-based&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1569" PostTypeId="2" ParentId="1568" CreationDate="2015-10-03T18:06:30.183" Score="7" Body="&lt;p&gt;As mentioned in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1468/310&quot;&gt;this&lt;/a&gt; answer, Physically-Based Rendering isn't a set number of things. It's a &lt;em&gt;concept&lt;/em&gt;. It's akin to saying something is 'Environmentally Friendly'. There are many different techniques to be environmentally friendly &lt;em&gt;and&lt;/em&gt; someone can implement those techniques to varying degrees. The same is for PBR.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, Physically Based Rendering is just that, based on Physics in some way, shape, or form. One of the most basic 'physical' rendering techniques I can think of is using &lt;a href=&quot;https://en.wikipedia.org/wiki/Lambertian_reflectance&quot; rel=&quot;nofollow&quot;&gt;Lambert shading&lt;/a&gt;. While you may say, &quot;Well that doesn't take into account specular light, or etc, etc. etc.&quot;, I agree. Lambert shading is a crude representation of the physics of light. But, pedantically, it &lt;em&gt;is&lt;/em&gt; PBR, because it is based off the physical observation that light intensity decreases with viewing angle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A renderer can implement as many, or as few PBR techniques as they choose. Some can be very computationally expensive, others, quite cheap. They can also choose to enable or disable certain techniques for different targets. For example, UE4 has a different set of techniques for mobile games, than for high end PC games. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, that all said, when today's modern renderers talk about being &quot;Physically-based&quot;, they are generally referring to using a modern BRDF, generally some form of &lt;a href=&quot;http://ruh.li/GraphicsCookTorrance.html&quot; rel=&quot;nofollow&quot;&gt;Cook-Torrance&lt;/a&gt;, and physically plausible inputs. Inputs being: materials with physically-plausible color values and lights with physically based intensity values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using a modern BRDF can be very computationally expensive, but, there are many approximations that can make it more friendly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using physically-plausible inputs is generally not that difficult to implement on the renderer side, rather it's more difficult for artists. IE. teaching them what the different material values mean, and what are physically plausible values. This can be a huge task, depending on your artists, as seen in the presentations &lt;a href=&quot;http://www.frostbite.com/wp-content/uploads/2014/11/course_notes_moving_frostbite_to_pbr.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;http://www.slideshare.net/guerrillagames/lighting-of-killzone-shadow-fall&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, for example. In my opinion, this is the hardest task in switching a studio.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One last issue, as brought up by @NathanReed in the comments:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Implementing a modern BRDF in the shader doesn't necessarily immediately make the rendering better. In many cases, it can make other parts of the pipeline a glaring issue. Below are some additional things that are important in getting the most out of a modern BRDF:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HDR lights and doing all rendering calculations in HDR&lt;/li&gt;&#xA;&lt;li&gt;Gamma correct rendering&lt;/li&gt;&#xA;&lt;li&gt;Indirect lighting of some kind (skybox lighting, static environment probes, screen space reflections, etc)&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This is huge for metals. &lt;a href=&quot;http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf&quot; rel=&quot;nofollow&quot;&gt;Metals have no diffuse reflectance&lt;/a&gt;, so most of the surface of the metal will be black if you don't have indirect reflections&lt;/li&gt;&#xA;&lt;li&gt;This is also one of the more difficult things to get real-time, especially for dynamic scenes. &lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2015-10-05T20:55:39.827" LastActivityDate="2015-10-05T20:55:39.827" CommentCount="4" />
  <row Id="1571" PostTypeId="2" ParentId="1557" CreationDate="2015-10-05T05:25:54.713" Score="5" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;you might want to take it further: render an image as the human eye would capture it or even as the human being would perceive it.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;There are two ways to interpret this.  I'll do both.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Interpretation 1: Render an image that looks perceptually realistic.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the end of the day, your image still needs to be &lt;em&gt;displayed&lt;/em&gt; somewhere.  Here's the key: you want to &lt;em&gt;render your image in such a way that when you *display* that image on a particular display device, it will produce the same sensation the original radiometric image would have produced.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's how to unpack that idea.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the real world, radiometric spectra (i.e., real distributions of light) enter your eye and stimulate approximately&lt;sup&gt;1&lt;/sup&gt; four light receptors.  The stimulations of the receptors produce the sensations of color we associate with images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In rendering, we don't have arbitrary control over the spectra we produce.  Fortunately, since we (usually) have only three cones, each of which produces only a scalar value, color vision can be reproduced by using exactly three primaries.  The bottom line is you can &lt;em&gt;produce any color sensation by using a linear combination of three wavelengths only&lt;/em&gt; (up to a few colors that might have to be negative, in which case, you just use different primaries).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You don't have a choice of primaries.  Almost all color display devices use the sRGB standard, which provides three primaries (which actually usually don't have a single wavelength).  That's fine because it turns out it's all abstracted and you don't have to care.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To clarify the mess that is perceptually accurate rendering, here's the algorithm:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Render your image using correct radiometric calculations.  You trace individual wavelengths of light or buckets of wavelengths.  Whatever.  In the end, you have an image that has a representation of the spectrum received at every point.&lt;/li&gt;&#xA;&lt;li&gt;At each pixel, you take the spectrum you rendered, and convert it to the &lt;a href=&quot;https://en.wikipedia.org/wiki/CIE_1931_color_space&quot;&gt;CIE XYZ color space&lt;/a&gt;.  This works out to be integrating the product of the spectrum with the &lt;a href=&quot;https://en.wikipedia.org/wiki/CIE_1931_color_space#Color_matching_functions&quot;&gt;standard observer functions (see CIE XYZ definition)&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;This produces three scalar values, which are the CIE XYZ colors.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB#The_forward_transformation_.28CIE_xyY_or_CIE_XYZ_to_sRGB.29&quot;&gt;Use a matrix transform to convert this to linear RGB, and then from there use a linear/power transform to convert linear RGB to sRGB&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Convert from floating point to uint8 and save, clamping values out of range (your monitor can't represent them).&lt;/li&gt;&#xA;&lt;li&gt;Send the uint8 pixels to the framebuffer.&lt;/li&gt;&#xA;&lt;li&gt;The display takes the sRGB colors, does the inverse transform to produce three primaries of particular intensities.  Each scales the output of whatever picture element it is responsible for.  The picture elements light up, producing a spectrum.  This spectrum will be (hopefully) a &lt;a href=&quot;https://en.wikipedia.org/wiki/Metamerism_%28color%29&quot;&gt;metamer&lt;/a&gt; for the original spectrum you rendered.&lt;/li&gt;&#xA;&lt;li&gt;You perceive the spectrum as you would have perceived the rendered spectrum.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Interpretation 2: Attempt to simulate the end data the human eye might receive for visualization purposes or compensation for LDR displays.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This one has a less useful meaning, I think.  Essentially, you're trying to produce an image that tweaks the way the brain perceives it for fun/profit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, there was a &lt;a href=&quot;http://graphics.stanford.edu/papers/gazehdr/&quot;&gt;paper at SIGGRAPH this year&lt;/a&gt; where they simulated afterimages and color reduction to make images appear perceptually different.  Of course, the only reason they do this at all is because the displays we're working with are all low-dynamic range (LDR).  The point is to simulate the effects someone might see if exposed to a real high-dynamic range (HDR) display as actual image data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice, this turns out to not work very well.  For afterimages, for example, we see afterimages because of a very bright stimulus exhausting color cells.  If you instead try to stimulate the effect with a fake afterimage, it might look kindof similar--but since it's a completely different mechanism, it's not very convincing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This sort of graphics is actually underexplored in the literature if you want to make a go at it.  The mentioned paper is an example of more-or-less the most state-of-the-art approaches we have.  I think the current consensus, though, is that it is not really worth trying to simulate (at least at this time), since at best you'd only be approximating real vision effects by substituting different ones, and that this doesn't really work.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;Rod+3*cones, the usual case.  Approximate because humans may have as few as zero functional light receptors up to a conjectured maximum of seven (with the highest ever observed being five).&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-10-05T05:25:54.713" CommentCount="1" />
  <row Id="1572" PostTypeId="1" AcceptedAnswerId="1573" CreationDate="2015-10-05T09:16:15.527" Score="7" ViewCount="323" Body="&lt;p&gt;Display lists were an OpenGL feature that could, in theory, accelerate any part of the API by storing a group of commands for later use. In my understanding, this makes a lot of sense with regards to the current effort to reduce the driver overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Display lists were deprecated in version 3.1 though. What is the modern API equivalent and does DirectX expose a similar feature? If there is no such functionality anymore, what is the rationale?&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-10-05T12:46:08.927" Title="What is the modern equivalent of display lists?" Tags="&lt;opengl&gt;&lt;display-lists&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1573" PostTypeId="2" ParentId="1572" CreationDate="2015-10-05T09:30:15.083" Score="9" Body="&lt;p&gt;Nvidia has an &lt;a href=&quot;http://developer.download.nvidia.com/opengl/specs/GL_NV_command_list.txt&quot;&gt;extension for creating command buffers in modern GL&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason for the lack of similar functionality is that there is a lot of state involved regarding how to render and the display list be affected by a lot of different state.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;NVidia solved it by capturing &lt;em&gt;all&lt;/em&gt; state and reseting to the state after a dispatch:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;1) What motivates the design?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The primary goal is to be able to reuse pre-validated command buffers.&#xA;  Other APIs and proposals have addressed this with various incarnations&#xA;  of command  lists or state objects, but a recurring problem is that&#xA;  interactions between various stages of the pipeline prevent this&#xA;  prevalidation and reuse. These  interactions are often&#xA;  hardware-specific (and differ from vendor to vendor  or even&#xA;  generation to generation) and new interactions are introduced by  new&#xA;  features that were not imagined when the prevalidation scheme was &#xA;  proposed.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;We attempt to address this by having a monolithic state object that &#xA;  encompasses (almost) the entire state of the pipeline. This should&#xA;  provide enough information for all implementations to do any needed&#xA;  cross- validation. We try to create these in a way that minimizes the&#xA;  new API  footprint - since we want ALL state (including any added in&#xA;  the future), we just capture it from the current state of the context.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[...]&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;23) In what condition is the state left, that is modified by tokens,&#xA;  after the dispatch call?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;RESOLVED: state is reset.  &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;(from the extension text linked above)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However the true successor is the command buffer functionality in DX12 and vulkan. Those will also capture all render state into a single object to use when creating and filling the command buffer. The NVidia extension is based on that architecture as a result of NVidia's involvement in the Vulkan design.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-10-05T12:46:08.927" LastActivityDate="2015-10-05T12:46:08.927" CommentCount="0" />
  <row Id="1574" PostTypeId="1" CreationDate="2015-10-05T14:41:15.337" Score="1" ViewCount="69" Body="&lt;p&gt;I'd like to know (if it is possible), how I would go about getting at the rendered frames from an application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, say I am using some 3D modelling tool (eg Blender, 3DS Max, Maya) and I have my model (eg a coffee mug that rotates constantly). Animation rendering has completed remotely (perhaps on AWS EC2) and so this coffee mug is rotating... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I were to log into the remote server using some remote desktop application, I'd see the cup rotating, however, I'd like to rather grab the output from that application (the frames) and stream it to my laptop (or another client).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know of NVidia's AppStream, but I'd like to know how to get at that rendered frames on the remote server and pipe them to a client, without using some remote desktop software.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realise my question may seem vague (possibly even not making sense), but I'm very new to this and so any help or pointers would be great.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="1800" LastEditorUserId="48" LastEditDate="2015-10-05T18:35:52.187" LastActivityDate="2015-10-05T18:35:52.187" Title="How to capture rendered frames from an application and stream over network" Tags="&lt;rendering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1575" PostTypeId="2" ParentId="1574" CreationDate="2015-10-05T17:05:33.907" Score="1" Body="&lt;p&gt;I think you're confused on how rendering and animation work. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A traditional model is nothing more than a bunch of triangles. So a model file coming from say Blender, etc. is just a list of vertices for the triangles. (with some added stuff if you want)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rendering takes the scene definition and transforms a 3D scene (aka the models oriented somewhere in space) and creates a 2D picture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Animation creates the illusion of movement by showing static pictures very fast (24 frames per second or more). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, to have a spinning mug, your application would render each frame, then present them, at, say 30 fps.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, to stream the data, you can take two paths:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Render each frame on the server and stream them to the client with something like H.264&lt;/li&gt;&#xA;&lt;li&gt;Stream the graphics commands to the client, and let them render them.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The first option is what something like OBS or Fraps and (maybe) Nvidia GameStream is doing. &#xA;There's a paper that covers the 2nd option &lt;a href=&quot;https://www.informatik.hu-berlin.de/de/forschung/gebiete/viscom/papers/icip08a.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2015-10-05T17:05:33.907" CommentCount="4" />
  <row Id="1576" PostTypeId="2" ParentId="1562" CreationDate="2015-10-06T02:53:06.210" Score="8" Body="&lt;p&gt;I see mainly three ways of computing normals for a generated shape.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Analytic normals&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;In some cases you have enough information about the surface to generate the normals. For example, the normal of any point on a sphere is trivial to compute. Put simply, when you know the derivative of the function, you also know the normal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your case is narrow enough to allow you to use analytic normals, they will probably give the best result in terms of precision. The technique doesn't scale too well though: if you also need to handle cases where you cannot use analytic normals, it may be easier to keep the technique that handles the general case and drop the analytic one altogether.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Vertex normals&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The cross product of two vectors gives a vector perpendicular to the plane they belong to. So getting the normal of a triangle is straightforward:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec3 computeNormal(vec3 a, vec3 b, vec3 c)&#xA;{&#xA;    return normalize(crossProduct(b - a, c - a));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Moreover, in the above example, the length of the cross product is proportional to the area inside &lt;em&gt;abc&lt;/em&gt;. So the smoothed normal at a vertex shared by several triangles can be computed by summing up the cross products and normalizing as a last step, thus weighting each triangle by its area.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec3 computeNormal(vertex a)&#xA;{&#xA;    vec3 sum = vec3(0, 0, 0);&#xA;    list&amp;lt;vertex&amp;gt; adjacentVertices = getAdjacentVertices(a);&#xA;    for (int i = 1; i &amp;lt; adjacentVertices; ++i)&#xA;    {&#xA;        vec3 b = adjacentVertices[i - 1];&#xA;        vec3 c = adjacentVertices[i];&#xA;        sum += crossProduct(b - a, c - a);&#xA;    }&#xA;    if (norm(sum) == 0)&#xA;    {&#xA;        // Degenerate case&#xA;        return sum;&#xA;    }&#xA;    return normalize(sum);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you are working with quads, there is a nice trick you can use: for a quad &lt;em&gt;abcd&lt;/em&gt;, use &lt;code&gt;crossProduct(c - a, d - b)&lt;/code&gt; and it will handle nicely cases where the quad is in fact a triangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Iñigo quilez wrote a few short articles on the topic: &lt;a href=&quot;http://www.iquilezles.org/www/articles/normals/normals.htm&quot; rel=&quot;nofollow&quot;&gt;clever normalization of a mesh&lt;/a&gt;, and &lt;a href=&quot;http://iquilezles.org/www/articles/areas/areas.htm&quot; rel=&quot;nofollow&quot;&gt;normal and area of n sided polygons&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Normals from partial derivatives&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Normals can be computed in the fragment shader from the partial derivatives. The math behind is the same, except this time it is done in screen space. This article by Angelo Pesce describes the technique: &lt;a href=&quot;http://c0de517e.blogspot.jp/2008/10/normals-without-normals.html&quot; rel=&quot;nofollow&quot;&gt;Normals without normals&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="182" LastEditDate="2015-10-14T11:43:16.537" LastActivityDate="2015-10-14T11:43:16.537" CommentCount="3" />
  <row Id="1578" PostTypeId="1" AcceptedAnswerId="1582" CreationDate="2015-10-06T06:56:25.257" Score="10" ViewCount="74" Body="&lt;p&gt;In the section &lt;em&gt;6.4 Constant Buffers&lt;/em&gt; of the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/B00DIKUB7G&quot;&gt;&lt;em&gt;Practical Rendering &amp;amp; Computation with Direct3D 11&lt;/em&gt;&lt;/a&gt; (pages 325, 326) it is mentioned:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;By default, the HLSL compiler will attempt to align constants such that they don't span multiple float4 registers. [...] The packing for an HLSL constant buffer can also be manually specified through the packoffset keyword.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I assume a similar rule will apply to the OpenGL equivalent, Uniform Buffer Objects, since they map to the same hardware feature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What about vanilla uniforms though? What are the rules that apply when declaring uniforms?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;uniform vec2 xy; // Can we expect the compiler to pack xy&#xA;uniform vec2 zw; // into a same four component register?&#xA;&#xA;uniform vec2 rg;&#xA;uniform float foo; // Will this prevent from packing rg and ba?&#xA;uniform vec2 ba;   // If so, will foo eat up a full four components register?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the compiler can do such optimizations, how good are they? Can we explicitly tell the compiler to pack or not, and when should we?&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2015-10-06T15:52:54.030" Title="Do the alignement and declaration order of uniforms matter?" Tags="&lt;opengl&gt;&lt;directx11&gt;&lt;constant-buffer&gt;&lt;uniform-buffer-object&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1579" PostTypeId="1" CreationDate="2015-10-06T10:43:35.327" Score="3" ViewCount="128" Body="&lt;p&gt;Raytracing &quot;sees&quot; the pixel on the other side of the screen;&#xA;but doesn't this mean the rays traced can be computationally reflected and refracted imitating the properties of actual light?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It'd be exciting if it were possible. Any visual scene possible outside computer graphics will be possible just using physics equations.&lt;/p&gt;&#xA;" OwnerUserId="1792" LastActivityDate="2015-10-06T22:06:01.717" Title="Can raytracing be used to imitate the behavior of light to a higher degree?" Tags="&lt;raytracing&gt;&lt;reflection&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="1581" PostTypeId="1" AcceptedAnswerId="1586" CreationDate="2015-10-06T15:50:25.017" Score="6" ViewCount="139" Body="&lt;p&gt;Computer graphics files are in Windows rendered by components such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_Device_Interface&quot;&gt;Graphics Device Interface&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Direct2D&quot;&gt;Direct2D&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to create a graphic file that, besides displaying a graphic also utilizes custom code inside the graphic file?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With custom code I am referring to functionality such as:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Incrementing a counter or date that is displayed on the graphic.&lt;/li&gt;&#xA;&lt;li&gt;Accessing a web URL to retrieve data that is displayed on the graphic.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="1806" LastActivityDate="2015-10-13T05:32:32.323" Title="To what degree, if at all, can custom code be embedded in a PNG, GIF, or JPG?" Tags="&lt;rendering&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="1582" PostTypeId="2" ParentId="1578" CreationDate="2015-10-06T15:52:54.030" Score="4" Body="&lt;p&gt;I went looking for an answer, so I downloaded AMD's shader analyzer to view the assembly produced when compiled for GCN. In the assembly below vector registers are v# and scalar registers are s#.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would appear that the uniforms even vector uniforms are passed into the shader as separate scalars, so a vec3 would use 3 scalar registers. The bit I found confusing was the v0 to v4, I'm not sure if v0 is a full 4 float register or a single float in a register, with a full vector register spanning v0 to v3. One way or another it didn't appear to change between the two versions so I can assume the definition order didn't affect the assembly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/07/AMD_GCN3_Instruction_Set_Architecture.pdf&quot; rel=&quot;nofollow&quot;&gt;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/07/AMD_GCN3_Instruction_Set_Architecture.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 450&#xA;&#xA;uniform vec2 xy; &#xA;uniform vec2 zw;&#xA;&#xA;out vec4 v;&#xA;&#xA;void main(){ &#xA;    v.xy = xy; &#xA;    v.zw = zw; &#xA;}&#xA;&#xA;shader &#xA;  asic(VI)&#xA;  type(VS)&#xA;&#xA;  v_cndmask_b32  v0, s0, v0, vcc               &#xA;  v_mov_b32     v0, 0                          &#xA;  v_mov_b32     v1, 1.0                        &#xA;  exp           pos0, v0, v0, v0, v1 done      &#xA;  s_andn2_b32   s0, s5, 0x3fff0000             &#xA;  s_mov_b32     s1, s0                         &#xA;  s_mov_b32     s2, s6                         &#xA;  s_mov_b32     s3, s7                         &#xA;  s_mov_b32     s0, s4                         &#xA;  s_buffer_load_dwordx2  s[4:5], s[0:3], 0x00  &#xA;  s_buffer_load_dwordx2  s[0:1], s[0:3], 0x10  &#xA;  s_waitcnt     expcnt(0) &amp;amp; lgkmcnt(0)         &#xA;  v_mov_b32     v0, s4                         &#xA;  v_mov_b32     v1, s5                         &#xA;  v_mov_b32     v2, s0                         &#xA;  v_mov_b32     v3, s1                         &#xA;  exp           param0, v0, v1, v2, v3         &#xA;end&#xA;&#xA;#version 450&#xA;&#xA;uniform vec2 xy;&#xA;uniform float z;&#xA;uniform vec2 zw;&#xA;&#xA;out vec4 v;&#xA;&#xA;void main(){ &#xA;    v.xy = xy; &#xA;    v.zw = zw;&#xA;    v.w += z;&#xA;}&#xA;&#xA;shader &#xA;  asic(VI)&#xA;  type(VS)&#xA;&#xA;  v_cndmask_b32  v0, s0, v0, vcc              &#xA;  v_mov_b32     v0, 0                         &#xA;  v_mov_b32     v1, 1.0                       &#xA;  s_andn2_b32   s0, s5, 0x3fff0000            &#xA;  exp           pos0, v0, v0, v0, v1 done     &#xA;  s_mov_b32     s1, s0                        &#xA;  s_mov_b32     s2, s6                        &#xA;  s_mov_b32     s3, s7                        &#xA;  s_mov_b32     s0, s4                        &#xA;  s_buffer_load_dword  s4, s[0:3], 0x10       &#xA;  s_buffer_load_dwordx2  s[6:7], s[0:3], 0x00 &#xA;  s_buffer_load_dwordx2  s[0:1], s[0:3], 0x20 &#xA;  s_waitcnt     expcnt(0) &amp;amp; lgkmcnt(0)        &#xA;  v_mov_b32     v0, s4                        &#xA;  v_add_f32     v0, s1, v0                    &#xA;  v_mov_b32     v1, s6                        &#xA;  v_mov_b32     v2, s7                        &#xA;  v_mov_b32     v3, s0                        &#xA;  exp           param0, v1, v2, v3, v0        &#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1801" LastActivityDate="2015-10-06T15:52:54.030" CommentCount="1" />
  <row Id="1583" PostTypeId="2" ParentId="1536" CreationDate="2015-10-06T19:25:49.710" Score="3" Body="&lt;p&gt;complement: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;homogeneous coordinates also allows to represent infinity: $(x,y, z, 0) = \frac{x,y,z}0$ in 3D, i.e., the point at infinity in direction $x,y,z$. Typically, light sources at finite or infinite position can be represented the same way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;About perspective transform, it even allows to interpolate correctly with no perspective distortion (contrary to early graphics hardware on PC ).&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="137" LastEditDate="2015-11-03T15:15:14.507" LastActivityDate="2015-11-03T15:15:14.507" CommentCount="0" />
  <row Id="1584" PostTypeId="2" ParentId="1579" CreationDate="2015-10-06T19:49:41.333" Score="2" Body="&lt;p&gt;Raytracing is one of the techniques that trace the light as it moves through the scene.&#xA;This can, depending on the implementation, also include effects such as refraction, reflection and scattering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Raytracing and similar techniques (see Monte-Carlo Pathtracing for example) all are based on the idea to solve the so called rendering equation.&#xA;The equation is a long known equation that describes the physically correct amount of light we can see for at each point (this means mathematical infinitesimal point, not pixel!).&#xA;By (approximally) solving this equation the mentioned rendering techniques are indeed able to render &quot;all&quot; scenes. The only real limiting factors are that it is hard to solve the equation for some effects and the huge amount of computation you need to solve it.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2015-10-06T19:49:41.333" CommentCount="1" />
  <row Id="1585" PostTypeId="2" ParentId="1579" CreationDate="2015-10-06T22:06:01.717" Score="7" Body="&lt;p&gt;You are late by about 35 years. This was addressed in the historical paper &quot;Whitted, T., An improved illumination model for shaded display. Communications of the ACM, Volume 23 Issue 6, June 1980, Pages 343-349&quot;, using  ray tracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qKlTI.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qKlTI.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since then radiosity has been introduced to better deal with diffuse phenomena. Correct physical modeling of the image formation process was discussed six years later in the seminal &quot;Kajiya, James T. (1986), &quot;The rendering equation&quot;, Siggraph 1986: 143&quot;.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastActivityDate="2015-10-06T22:06:01.717" CommentCount="3" />
  <row Id="1586" PostTypeId="2" ParentId="1581" CreationDate="2015-10-06T22:34:12.600" Score="7" Body="&lt;p&gt;Common graphics formats such as JPEG, PNG and GIF are not designed to have any form of code inside them. They just store a compressed array of pixels&amp;mdash;they can't generate anything on command or contact a web service, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(There have occasionally been vulnerabilities discovered in image decoders, that would allow a maliciously constructed image to cause code execution when viewed, via a buffer overflow exploit or similar. But this is of course not part of the design of the format.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some vector graphics formats such as EPS and SVG actually do allow embedding code. However, being vector formats instead of bitmap ones, these formats often aren't supported by ordinary image viewers (although SVG is supported by browsers), and can't be edited in ordinary paint programs. One needs specialized vector graphics programs to work with them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to generate an image that updates with new data, you could create a web service that regenerates the image on the server each time it's requested. However, if a client caches the resulting image rather than re-requesting it each time it's viewed, they would see old data.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-10-06T22:34:12.600" CommentCount="0" />
  <row Id="1587" PostTypeId="1" AcceptedAnswerId="1588" CreationDate="2015-10-07T00:07:08.770" Score="4" ViewCount="188" Body="&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=LKnqECcg6Gw&quot; rel=&quot;nofollow&quot;&gt;In this video&lt;/a&gt; from about 1:15, it is stated that if you have an RGB value of (0.5,0.5,0.5) it is only 22% as bright as (1,1,1) rather than the expected 50% as bright.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does this mean that RGB is adjusted for a logarithmic scale so that we perceive the (0.5,0.5,0.5) to be half as bright, or have a misunderstood the video?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Despite searching around a lot I can not find any other source of information that discusses this.&lt;/p&gt;&#xA;" OwnerUserId="1812" LastEditorUserId="54" LastEditDate="2015-10-07T08:36:47.367" LastActivityDate="2015-11-03T13:56:29.593" Title="Actual vs Perceived Brightness of RGB Colour" Tags="&lt;color&gt;&lt;brightness&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1588" PostTypeId="2" ParentId="1587" CreationDate="2015-10-07T02:46:58.663" Score="6" Body="&lt;p&gt;Two different effects are causing the observation mentioned in the video.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On one side, the vast majority of screens have a non linear response: if the RGB value is half as much, the emitted light intensity is not half as much. This behavior originally comes from &lt;a href=&quot;https://en.wikipedia.org/wiki/Cathode_ray_tube&quot; rel=&quot;nofollow&quot;&gt;cathode ray tube (CRT)&lt;/a&gt; displays, which produced different light intensities by varying the voltage used to generate the electron beam. From the article:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;CRTs have a pronounced triode characteristic, which results in significant gamma (a nonlinear relationship in an electron gun between applied video voltage and beam intensity).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;As CRT became ubiquitous, systems assumed this behavior to be present and took it into account. Nowadays CRT have mostly been replaced by LCD technology, which emulate this behavior for compatibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gamma is usually considered to be around $2.2$, which means the light intensity follows a function $I=x^{2.2}$. This is where the 22% mentioned in the video comes from: ($0.5^{2.2} \approx 0.218$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other side, the human vision is not linear either. &lt;a href=&quot;https://en.wikipedia.org/wiki/Lux#Illuminance&quot; rel=&quot;nofollow&quot;&gt;According to Wikipedia&lt;/a&gt;, a typical sunset on a clear day is about 400 lux, and clear sky daylight is about 10000 to 25000 lux. Yet the latter doesn't feel over 25 times as bright as the former.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, the non linear response of screens is a good thing, because it gives more precision where it is needed. Note that it is not logarithmic though, but closer to a square function. If you want to read more on the topic, you can look for articles on &lt;em&gt;&quot;gamma correction&quot;&lt;/em&gt;.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="137" LastEditDate="2015-11-03T13:56:29.593" LastActivityDate="2015-11-03T13:56:29.593" CommentCount="0" />
  <row Id="1589" PostTypeId="1" CreationDate="2015-10-07T08:36:55.203" Score="1" ViewCount="42" Body="&lt;p&gt;Say I am running a game in the cloud and I am playing that game on my client (the game is being streamed from the cloud to the client), be it a laptop, phone or calculator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I work out how long a frame (of the game being played) takes to arrive at the client?&lt;/p&gt;&#xA;" OwnerUserId="1800" LastEditorUserId="1800" LastEditDate="2015-10-07T11:30:11.603" LastActivityDate="2015-10-07T11:30:11.603" Title="Measure how long a rendered frame takes to arrive on a client from the cloud" Tags="&lt;gpu&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2015-10-09T19:18:07.210" />
  <row Id="1590" PostTypeId="2" ParentId="1589" CreationDate="2015-10-07T11:11:51.073" Score="1" Body="&lt;p&gt;Try putting a timestamp over some key frames as the last step on your render pipeline. Then compare it to the local systems time. If both clocks are correct you should get a delay in ms (using unix rime-stamps).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another idea to measure the round-trip-delay (might be important for UX reasons):&#xA;input to server,&#xA;server render,&#xA;result from to server on your screen&#xA;is to have a bot give an input than measure the time till the result is back&lt;/p&gt;&#xA;" OwnerUserId="1818" LastActivityDate="2015-10-07T11:11:51.073" CommentCount="2" />
  <row Id="1591" PostTypeId="2" ParentId="283" CreationDate="2015-10-07T12:12:45.563" Score="8" Body="&lt;p&gt;As usual with numerical methods and samplings, it also depends of your quality threshold of what you consider &quot;isotropic&quot;. And of what you would consider as a being or not a &quot;grid-based noise algorithm&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance Gabor Noise reproduces a target spectrum, for instance blue noise, which in Fourier domain is a simple isotropic ring.&#xA;Now if you consider that this ring is not analytic but rasterized, as such it is not perfectly symmetrical. Also if the ring radius (i.e., frequency) get too close to the window size (i.e., maximum frequency), it will be truncated (and thus no longer symmetrical). Up to you to accept or not these as anisotropic ;-)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vFj2M.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vFj2M.png&quot; alt=&quot;&amp;quot;this is not a circle&amp;quot; - Magritte&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/c4Xaj.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/c4Xaj.png&quot; alt=&quot;&amp;quot;this is not a circle&amp;quot; - Nyquist&quot;&gt;&lt;/a&gt;&#xA;&lt;em&gt;&quot;This is not a circle&quot; - Magritte . . . . . . . . . . . . . . . .   &quot;This is not a circle&quot; - Nyquist&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You might or might not accept a rasterized ring in Fourier space to be &quot;isotropic&quot;. Still, in the extreme cases where the ring get thinner than the resolution, or larger than the window, isotropy is objectively lost.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2015-10-08T08:08:20.757" LastActivityDate="2015-10-08T08:08:20.757" CommentCount="2" />
  <row Id="1592" PostTypeId="1" AcceptedAnswerId="1593" CreationDate="2015-10-07T13:15:33.013" Score="6" ViewCount="96" Body="&lt;p&gt;I am working on Silverlight app. I am trying to render floor covered with tiles. Although I am using mip maps I am still getting awful Moire patterns, when trying to render big area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I am doing is creating cuboid and then cover it with single tile:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/fZhKQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fZhKQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my pixel shader I am multiplying texture coordinates in order to create tiled floor (otherwise I would get one tile stretched over my cuboid). Pixel shader code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float Width                 : register(c3);                  //Width of cuboid&#xA;float Height                : register(c4);                  //Height of cuboid&#xA;&#xA;texture texTexture;&#xA;sampler textureSampler  : register(s0) = sampler_state {&#xA;    Texture = (texTexture);&#xA;};&#xA;&#xA;struct VsOutput&#xA;{&#xA;    float4 position : POSITION;&#xA;    float3 dirLightPosition : COLOR;&#xA;    float2 texCoord : TEXCOORD0;&#xA;    float3 normal   : TEXCOORD1;&#xA;    float3 view : TEXCOORD2;&#xA;};&#xA;&#xA;float4 main(VsOutput IN) : COLOR&#xA;{&#xA;    float2 texCoord = float2(IN.texCoord.x * Width / 4.0f, &#xA;                             IN.texCoord.y * Height / 4.0f); //multiplying texture coordinates&#xA;&#xA;&#xA;    float4 texColor = tex2D(textureSampler, texCoord);&#xA;&#xA;    return float4(color.r, color.g, color.b, 1.0f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My output is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/pPOY1.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pPOY1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What else can I do to prevent from creating Moires patterns on texture?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="205" LastEditorUserId="205" LastEditDate="2015-10-09T15:56:02.890" LastActivityDate="2015-10-09T15:56:19.853" Title="Moires patterns despite using mipmaps" Tags="&lt;rendering&gt;&lt;texture&gt;&lt;pixel-shader&gt;&lt;artifacts&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="1593" PostTypeId="2" ParentId="1592" CreationDate="2015-10-07T13:54:55.197" Score="5" Body="&lt;p&gt;Two things come to mind:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;When generating your smaller mip map levels try to avoid using a simple 2x2 box filter because, though cheap and cheerful, they do a really poor job of removing high frequency information (that exceeds the Nyquist limit) as well as &lt;em&gt;over filtering&lt;/em&gt; some of the lower frequency information you need to keep. (Also, as an aside, you need to perform the MIP map generation in linear space. If your source data is sRGB, you thus need to map to and from linear).  FWIW, In Williams' original paper on MIP mapping, he said he used a &quot;box (Fourier) window&quot; to generate the prefiltered levels, which would be a sinc function in image space.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Standard Trilinear filtering will also over- and under-filter parts of the texture. If you don't like the aliasing you may have to a) adjust the bias to increase the filtering (which will increase blurring) and/or b) try using anisotropic filtering.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2015-10-07T14:21:16.043" LastActivityDate="2015-10-07T14:21:16.043" CommentCount="7" />
  <row Id="1594" PostTypeId="2" ParentId="1557" CreationDate="2015-10-07T16:59:14.693" Score="4" Body="&lt;p&gt;When it comes to perception, there is also the issue of what we are almost blind to (in terms of properties or artifacts), and what we have enforced perception of.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance as for sound, you have contrast or frequencies that make you less or not aware of other contents (an old SIGGRAPH paper illustrated how texture can mask mesh resolution), plus all the time aspects (google for &quot;change blindness&quot;). Similarly, details of surface, normals, BRDF might or might not be seen depending of values and relative values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also since our perceptive system tends to adapt locally and globally, for low frequency values it is important to have maxima and minima at the right places, but their exact value won't really be noticed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sometimes cognition might be there to, like you allow a lot of errors in clouds and trees but surely less in human faces. (Sometimes you might downgrade this to statistics of parameter for a given category.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's why I prefer using the word &quot;plausible&quot; rather than &quot;photorealistic&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Contrarily, we are ultra-sensitive to artifacts such as false pixels or flickering pixels, ultra-sensitive to correlations like fronts of error of 1 grey level only, unwanted patterns such as Moiré, aliasing or bad random, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way it's one of the reasons that optimization-based solutions summing everything in a simple energy can be a very bad idea perceptively speaking as in all paradoxical situations this can be prone to concentration of errors in lines or points. For the same reason early global illumination people were really disappointed that energy exact solutions were less accepted than anti-aliased approximate shadows (and then came from quad-tree based to mesh-based adaptive methods).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A (quite general) overview about perception for graphics can be found in this SigAsia'11 course on &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2077448&quot; rel=&quot;nofollow&quot;&gt;&quot;Perception in Graphics, Visualization, Virtual Environments and Animation&quot;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="231" LastEditDate="2015-10-16T08:20:53.373" LastActivityDate="2015-10-16T08:20:53.373" CommentCount="0" />
  <row Id="1595" PostTypeId="1" AcceptedAnswerId="1599" CreationDate="2015-10-07T18:10:08.277" Score="6" ViewCount="219" Body="&lt;p&gt;Is there any strategy to detect the region of an image that is sharp and in-focus, in order to separate the out-of-focus background? I've tried using &lt;a href=&quot;https://en.wikipedia.org/wiki/Edge_detection&quot; rel=&quot;nofollow&quot;&gt;edge detection&lt;/a&gt; methods but without any success. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, is it possible to determine if an area of an image is blurred (out of focus) or not?&lt;/p&gt;&#xA;" OwnerUserId="18" LastEditorUserId="48" LastEditDate="2015-10-09T18:36:05.377" LastActivityDate="2015-10-09T18:36:05.377" Title="Detect in-focus regions of an image" Tags="&lt;algorithm&gt;&lt;blur&gt;&lt;image-processing&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="1" />
  <row Id="1597" PostTypeId="2" ParentId="1434" CreationDate="2015-10-07T18:53:23.187" Score="4" Body="&lt;p&gt;Radiosity, by definition, handles only the diffuse component. You cannot 'limit' Radiosity to diffuse, because it already is handling just that diffuse component (remember - the diffuse lighting is just one (albeit popular) application of the energy distribution).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, you just misinterpreted the quote.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, contrary to popular misconception, you &lt;strong&gt;do &lt;em&gt;not&lt;/em&gt; have to distribute all energy&lt;/strong&gt; to get great results. Look into &lt;strong&gt;Progressive Refinement method&lt;/strong&gt;, which only processes the largest emitors, thus allowing to converge to 'close enough' solution much sooner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Usually, you set some threshold (e.g. I want to redistribute 85% if all energy) and before processing the next Shooter patch, you do a simple check for the running total of the distributed energy (and quit the loop). This is usually few orders of magnitude faster (for a very minor implementation cost) than the reference brute-force method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, to get true benefits of Radiosity (e.g. color bleeding) it's best to redistribute as much as possible (given the resources available).&lt;/p&gt;&#xA;" OwnerUserId="1820" LastActivityDate="2015-10-07T18:53:23.187" CommentCount="0" />
  <row Id="1598" PostTypeId="2" ParentId="1529" CreationDate="2015-10-07T20:34:05.963" Score="3" Body="&lt;p&gt;Google said how in 2012, avoiding explaining too many details&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://googleblog.blogspot.it/2012/06/never-ending-quest-for-perfect-map.html&quot; rel=&quot;nofollow&quot;&gt;The never-ending quest for the perfect map&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;...new imagery rendering techniques and computer vision that let us&#xA;  automatically create 3D cityscapes, complete with buildings, terrain&#xA;  and even landscaping, from 45-degree aerial imagery&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and here it's a &lt;a href=&quot;https://www.youtube.com/watch?t=44&amp;amp;v=N6Douyfa7l8&quot; rel=&quot;nofollow&quot;&gt;video&lt;/a&gt; of the feature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we know that &quot;Computer Vision&quot; is used. I guess the edges are detected, along with the shadows, to build the elevation of the object. Then the segmented image is used as a texture on the built mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, i saw &lt;a href=&quot;http://iris.usc.edu/outlines/papers/1998/lin-cviu98.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt; linked, and i think a similar technique is used in Google to extract the 3d mesh from different 45° images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a lot of papers (a book too) about &quot;building extraction/detection&quot;.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.asprs.org/a/publications/proceedings/Louisville2014/Jabari_1.pdf&quot; rel=&quot;nofollow&quot;&gt;Detection of buildings in urban and suburban areas using very high resolution satellite&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://scholar.google.it/scholar?q=Building+Detection&quot; rel=&quot;nofollow&quot;&gt;Google Scholar: Building Detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://scholar.google.it/scholar?q=Building%20Detection%20from%20Satellite%20Images&amp;amp;btnG=&amp;amp;hl=en&amp;amp;as_sdt=0%2C5&amp;amp;as_vis=1&quot; rel=&quot;nofollow&quot;&gt;Google Scholar: Building Detection from Satellite Images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;src: &lt;a href=&quot;http://programmers.stackexchange.com/questions/202599/how-does-the-new-google-maps-make-buildings-and-cityscapes-3d&quot;&gt;How does the new google maps make buildings and cityscapes 3D?&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="316" LastActivityDate="2015-10-07T20:34:05.963" CommentCount="0" />
  <row Id="1599" PostTypeId="2" ParentId="1595" CreationDate="2015-10-07T21:07:45.853" Score="9" Body="&lt;p&gt;it is quite easy to measure the local max frequency in an image (at least as a low resolution mask, with some regularization).&#xA;Several papers of the MIT graphics group have been around detecting and processing from this kind of clue, with regular or coded aperture cameras.&#xA;e.g. &lt;a href=&quot;http://people.csail.mit.edu/soonmin/dof/&quot;&gt;Defocus Magnification&lt;/a&gt; and &lt;a href=&quot;http://groups.csail.mit.edu/graphics/CodedAperture/&quot;&gt;Image and Depth from a Conventional Camera with a Coded Aperture&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2015-10-07T21:07:45.853" CommentCount="3" />
  <row Id="1601" PostTypeId="2" ParentId="1434" CreationDate="2015-10-08T07:48:39.857" Score="1" Body="&lt;p&gt;The sentence says it: radiosity precomputes an &quot;image&quot; for all potential viewpoints at the same time, i.e. it doesn't focus on just the rays that hit a particular observer. Hence there are naturally many more rays to consider, as you are actually rendering a multitude of views simultaneously.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whether the surfaces are specular or diffuse isn't really relevant in regard to this statement.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To make the approach viable, radiosity performs very gross rendering, as if using large beams instead of thin rays.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2015-10-08T08:19:00.467" LastActivityDate="2015-10-08T08:19:00.467" CommentCount="0" />
  <row Id="1602" PostTypeId="2" ParentId="361" CreationDate="2015-10-08T08:27:22.417" Score="2" Body="&lt;p&gt;What one has to do in this matter is first define the quantities that are physically at play here, so that everybody speaks about the same thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;radiance (&lt;a href=&quot;https://en.wikipedia.org/wiki/Radiance&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;)&lt;br&gt;&#xA;Flux emitted by a surface per solid angle per projected area.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/N2ziI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/N2ziI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;Unit is &lt;strong&gt;W·sr−1·m−2&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;radiant intensity (&lt;a href=&quot;https://en.wikipedia.org/wiki/Radiant_intensity&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;)&lt;br&gt;&#xA;Origin of radiance, you take the surface area out of the unit.&lt;br&gt;&#xA;Unit is &lt;strong&gt;W·sr−1&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;intensity (&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminous_intensity&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;)&lt;br&gt;&#xA;A perception-based unit of power per solid angle.&lt;br&gt;&#xA;Unit is &lt;strong&gt;candela&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;luminance (&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminance&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;)&lt;br&gt;&#xA;&lt;em&gt;The luminance is normally obtained by dividing the luminous intensity by the light source area&lt;/em&gt; (&lt;a href=&quot;http://www.rohm.com/web/eu/faq_search/-/faq_search/FaqId/260&quot; rel=&quot;nofollow&quot;&gt;source&lt;/a&gt;)&lt;br&gt;&#xA;therefore this is also perception based.&lt;br&gt;&#xA;Unit is &lt;strong&gt;cd·m−2&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;luminuous flux (&lt;a href=&quot;https://en.wikipedia.org/wiki/Luminous_flux&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;)&lt;br&gt;&#xA;Same thing but not related to solid angle.&lt;br&gt;&#xA;quote:&lt;br&gt;&#xA;&lt;em&gt;Luminous flux is a measure of the total amount of light a lamp puts out. The luminous intensity (in candelas) is a measure of how bright the beam in a particular direction is&lt;/em&gt;&lt;br&gt;&#xA;Unit is &lt;strong&gt;lumen&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;You can also talk of irradiance (&lt;a href=&quot;https://en.wikipedia.org/wiki/Irradiance&quot; rel=&quot;nofollow&quot;&gt;wiki&lt;/a&gt;) when speaking of received radiance.&lt;br&gt;&#xA;And one can also talk of total-irradiance when speaking of the irradiance taken for the whole hemisphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;refer to: &lt;a href=&quot;http://www.crompton.com/light/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.crompton.com/light/index.html&lt;/a&gt;&lt;br&gt;&#xA;and: &lt;a href=&quot;https://pathtracing.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;https://pathtracing.wordpress.com/&lt;/a&gt;&lt;br&gt;&#xA;and why not: &lt;a href=&quot;http://www.nvc-lighting.com/showuseInfo.Aspx?typeID=42&amp;amp;ID=94&quot; rel=&quot;nofollow&quot;&gt;http://www.nvc-lighting.com/showuseInfo.Aspx?typeID=42&amp;amp;ID=94&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, there are two classes of units, the perception based and the absolute physical units.&lt;br&gt;&#xA;the &lt;strong&gt;radiance&lt;/strong&gt; is the measure you want to look at to understand Lambert, you can actually see the cosine falloff directly in the formula.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see the intuition of this in this blog: &lt;a href=&quot;https://pathtracing.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;https://pathtracing.wordpress.com/&lt;/a&gt; chapter &quot;Lambert's cosine law&quot;&lt;/p&gt;&#xA;" OwnerUserId="1614" LastEditorUserId="1614" LastEditDate="2015-10-09T01:49:20.690" LastActivityDate="2015-10-09T01:49:20.690" CommentCount="0" />
  <row Id="1604" PostTypeId="2" ParentId="210" CreationDate="2015-10-09T01:44:16.853" Score="2" Body="&lt;p&gt;I'll add some guidelines to help readers understand Benedikt Bitterli's statement &quot;Make sure to gamma correct your images&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gamma correcting images does not mean applying a power filter at the end.&#xA;It means working in linear space during all calculations, and finally encoding the output into gamma space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gamma space is the color space into which all display machines of this age expect RGB values to be presented to them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Linear space in opposition, is the fact of working with values which are proportional to physical emissions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, when the artist edit the values of the albedos colors of the world surfaces, and the colors of the lights of the scene, he does it in gamma space because no edition software dare to modify what the user has input and just saves as-is. So the artist was looking at something that looked good for him, while stored in gamma space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the first step of all rendering engine, should be to convert all human edited input to linear space first. In your case that means all sphere colors and emissions values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, you do your usual raytracing, brdf evaluation, monte carlo sampling, whatever process, in HDR if possible using &lt;code&gt;float32&lt;/code&gt; components if possible. (usually works nice with 128 bits SIMD as &lt;code&gt;r,g,b,a&lt;/code&gt; vectors).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally you tone filter the HDR image according to the exposure value you choose, heuristically or manually. This operation could be very simple like a &lt;code&gt;clamp&lt;/code&gt;. And then and only then, you encode the final image to gamma space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The base is: a monitor will take an input value x and apply the formula x^2.2 to create the physical radiance on the output pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus, the conversion formulas goes as follow:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;linear to gamma: x^(1/2.2)&lt;br&gt;&#xA;gamma to linear: x^2.2&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please note as trivia that sRGB space has a complex formula, a &lt;code&gt;if&lt;/code&gt;, and some offset. But viewed from afar the curve is very very close to simple gamma.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second thing to take into account, are you sure your sampling is fair ?&lt;br&gt;&#xA;You could be sampling too much in the directions that privileges more rays to reach the sky. check the uniformity of your sampling. You said you use a gaussian that seems to me like you are going to ask for bias. Just use the classic Lambert distribution sampling: &lt;a href=&quot;https://pathtracing.wordpress.com/&quot; rel=&quot;nofollow&quot;&gt;https://pathtracing.wordpress.com/&lt;/a&gt;&#xA;this way you reduce the number of samples necessary to get a variance equivalent with a purely uniform sphere. and you also save the cosine evaluation because its embedded in the distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, if your spheres are really mirrors and your GI is supposedly actually working, it will accumulate reflected energy on the ground, therefore compensating your shadows from occlusion. I even expect to see more energy than darkening in such cases.&lt;/p&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2015-10-09T01:44:16.853" CommentCount="2" />
  <row Id="1605" PostTypeId="2" ParentId="1592" CreationDate="2015-10-09T15:56:19.853" Score="2" Body="&lt;p&gt;&lt;strong&gt;SOLUTION&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I managed to solve my problem by increacing size of texture 2048x2048px so there would be generated more mipmaps. Also it seems like changing my SamplerState form anisotropic to something like that helped:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SamplerState MirrorTexCoord = new SamplerState()&#xA;        {&#xA;            AddressU = TextureAddressMode.Mirror,&#xA;            AddressV = TextureAddressMode.Mirror,&#xA;            .&#xA;            .&#xA;            .&#xA;        };&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Actually I don' t know what is TextureAdressMode.Mirror, but it helped a lot, now it looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2acRT.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2acRT.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="205" LastActivityDate="2015-10-09T15:56:19.853" CommentCount="4" />
  <row Id="1606" PostTypeId="1" CreationDate="2015-10-09T18:03:51.633" Score="2" ViewCount="191" Body="&lt;p&gt;I have the following problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Give a matrix that will transform the point $(x,y,z)$ into the point $(\frac{2}{x+y}, \frac{5y + z}{2x + 2y}, 3)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been reading my book to find a way to solve it without success. I will very much appreciate your feedback about how to solve it.&lt;/p&gt;&#xA;" OwnerUserId="1836" LastEditorUserId="16" LastEditDate="2015-11-03T12:56:46.733" LastActivityDate="2015-11-04T09:32:20.727" Title="Transform a point into another point" Tags="&lt;transformations&gt;" AnswerCount="3" CommentCount="4" />
  <row Id="1607" PostTypeId="2" ParentId="1606" CreationDate="2015-10-09T18:37:37.273" Score="2" Body="&lt;p&gt;The hint is to use the perspective divide inherent in homegeneous coordinate. So the resulting point you are looking for is $(2, (5y+z)/2, 3x+3y, x+y)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That matrix then ends up as&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}&#xA;    0 &amp;amp;  0 &amp;amp;  0 &amp;amp;  2  \\&#xA;    0 &amp;amp;  2.5&amp;amp; 0.5&amp;amp; 0  \\&#xA;    3 &amp;amp;  3 &amp;amp;  0 &amp;amp;  0  \\&#xA;    1 &amp;amp;  1 &amp;amp;  0 &amp;amp;  0  \\&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-11-03T12:54:16.087" LastActivityDate="2015-11-03T12:54:16.087" CommentCount="5" />
  <row Id="1608" PostTypeId="1" CreationDate="2015-10-09T19:40:42.943" Score="5" ViewCount="272" Body="&lt;p&gt;I already know about the matrices I have to use in order to perform rotations. If I have to rotate in z-axis and then in x-axis, I would do it in 2 steps. My question is, is it possible to combine both rotations into a single matrix? I will appreciate your feedback.&lt;/p&gt;&#xA;" OwnerUserId="1836" LastActivityDate="2015-10-18T06:09:20.450" Title="How to combine rotation in 2 axis into one matrix" Tags="&lt;transformations&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="1609" PostTypeId="2" ParentId="1608" CreationDate="2015-10-09T19:58:32.230" Score="6" Body="&lt;p&gt;Yes, just multiply them in reverse order:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Matrix myrotation = Matrix.CreateRotationX(xrot) * Matrix.CreateRotationZ(zrot);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT.&lt;/strong&gt; &#xA;My answer only applies if you are using column vectors. Please see Martin Büttner detailed answer.&lt;/p&gt;&#xA;" OwnerUserId="1826" LastEditorUserId="1826" LastEditDate="2015-10-13T11:58:18.750" LastActivityDate="2015-10-13T11:58:18.750" CommentCount="4" />
  <row Id="1610" PostTypeId="1" CreationDate="2015-10-10T17:25:56.540" Score="3" ViewCount="89" Body="&lt;p&gt;I have the following question in an exam review and I am looking for feedback about how to approach the solution. Any advice will be highly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Using a parametric line intersection test, give the parameter values of the endpoints of a line from (3,3,4) to (11,8,7) clipped to the cube extending from 0 to 5 in x, y, and z&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="1836" LastEditorUserId="4" LastEditDate="2015-10-12T16:31:21.303" LastActivityDate="2015-11-03T15:15:16.903" Title="Parametric line intersection test" Tags="&lt;clipping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1611" PostTypeId="2" ParentId="1610" CreationDate="2015-10-11T08:59:56.547" Score="4" Body="&lt;p&gt;&lt;strong&gt;Hint&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The three coordinates can be handled independently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance in $x$, you need to solve&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$x_0 + t (x_1-x_0) = 0$$&#xA; and&lt;br&gt;&#xA;$$x_0 + t (x_1-x_0) = x_{max}$$&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2015-11-03T15:15:16.903" LastActivityDate="2015-11-03T15:15:16.903" CommentCount="0" />
  <row Id="1613" PostTypeId="2" ParentId="1608" CreationDate="2015-10-12T15:17:48.963" Score="6" Body="&lt;p&gt;(This answer is essentially the same as Stefan's but I wanted to add some detail about row and column vectors, and how to determine which you are using.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yes, this is possible, but the details depend on whether you represent your vectors as rows or columns.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Column vectors&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If you are using &lt;em&gt;column&lt;/em&gt; vectors, you will normally transform them by &lt;em&gt;left&lt;/em&gt;-multiplying your matrices:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vector = mRotateZ * vector;&#xA;vector = mRotateX * vector;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Of course, you can also do this in one step:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vector = mRotateX * mRotateZ * vector;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But matrix multiplication is associative, which means it doesn't matter which multiplication is performed first:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A * B * C = (A * B) * C = A * (B * C)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So we can write&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Matrix mRotate = mRotateX * mRotateZ;&#xA;vector = mRotate * vector;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We have now created a single matrix, which is equivalent to &lt;em&gt;first&lt;/em&gt; rotating about &lt;code&gt;Z&lt;/code&gt; and &lt;em&gt;second&lt;/em&gt; about &lt;code&gt;X&lt;/code&gt;. This generalises trivially for any number of transformations. Notice that transformations are applied from right to left.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Row vectors&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If, on the other hand, you are using &lt;em&gt;row&lt;/em&gt; vectors, you will normally &lt;em&gt;right&lt;/em&gt;-multiply your matrices:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vector = vector * mRotateZ;&#xA;vector = vector * mRotateX;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Again, writing it in one step, we get&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vector = vector * mRotateZ * mRotateX;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which can be rewritten as&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Matrix mRotate = mRotateZ * mRotateX;&#xA;vector = vector * mRotate;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Notice that in this case, the transformations applied from left to right.&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-10-13T07:36:02.573" LastActivityDate="2015-10-13T07:36:02.573" CommentCount="4" />
  <row Id="1614" PostTypeId="1" AcceptedAnswerId="1616" CreationDate="2015-10-12T21:04:12.770" Score="5" ViewCount="131" Body="&lt;p&gt;I have a sequence of compute shaders that generates an indexed mesh.The last one of those writes the generated indices like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void addTriangle (uint i0, uint i1, uint i2) {&#xA;    uint ic = atomicCounterIncrement(indirectIndexCount);&#xA;    meshIndices[ic*3+0] = i0;&#xA;    meshIndices[ic*3+1] = i1;&#xA;    meshIndices[ic*3+2] = i2;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After the mesh has been generated, it is drawn with &lt;a href=&quot;http://docs.gl/gl4/glDrawElementsIndirect&quot;&gt;glDrawElementsIndirect&lt;/a&gt;. The &lt;em&gt;indirectIndexCount&lt;/em&gt; in the code above is an &lt;em&gt;atomic_uint&lt;/em&gt; counter at position 0 inside the &lt;em&gt;GL_DRAW_INDIRECT_BUFFER&lt;/em&gt; (see the struct called &lt;em&gt;DrawElementsIndirectCommand&lt;/em&gt;). This counter is now obviously too small by a factor of three, since it was incremented only once for each triangle. Currently, I multiply it by 3 afterwards just before issuing the draw call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(At the moment this is done by mapping the buffer and multiplying on the CPU, which is of course nonsense, but shows that the whole thing basically works. Everything is drawn correctly. I could do it with an invocation of a single 1x1x1x1x1x1 compute shader, but that seems only slightly less silly.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I get rid of this extra multiplication step?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since this seems to be an obvious problem whenever meshes with variable index count are generated, I guess there must be some easy solution that I'm overlooking?&lt;/p&gt;&#xA;" OwnerUserId="1699" LastActivityDate="2015-10-13T14:44:20.010" Title="OpenGL Compute Shader generating triangle indices: How to get correct element count for glDrawElementsIndirect?" Tags="&lt;opengl&gt;&lt;compute-shader&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="1615" PostTypeId="2" ParentId="1581" CreationDate="2015-10-13T05:32:32.323" Score="0" Body="&lt;p&gt;I don't know if you would consider this to be a stretch for an answer to your question seeing as how I am a bit unsure how it was intended to be interpreted. That being considered, 2d barcodes can be graphics with which store code and interact with variable data elements defined throughout such symbologies as PDF417, DataMatrix, etc. &lt;/p&gt;&#xA;" OwnerUserId="1846" LastActivityDate="2015-10-13T05:32:32.323" CommentCount="2" />
  <row Id="1616" PostTypeId="2" ParentId="1614" CreationDate="2015-10-13T08:36:08.430" Score="6" Body="&lt;p&gt;If you have access to glsl 4.3+ (or glsl ES 3.1) you can use &lt;a href=&quot;http://docs.gl/sl4/atomicAdd&quot; rel=&quot;nofollow&quot;&gt;atomicAdd&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The next option is to use a &lt;code&gt;barrier()&lt;/code&gt; after all vertices are generated in the compute and then multiplying the value in the counter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;main(){&#xA;&#xA;    // generate vertices&#xA;&#xA;    barrier();&#xA;    if(gl_localInvocationID == vec3(0)){&#xA;        indirectCount = atomicCounter(indirectIndexCount)*3;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This emulates running another 1x1x1 compute shader to multiply the index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Otherwise you can use a second atomic to hold the vertexcount:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void addTriangle (uint i0, uint i1, uint i2) {&#xA;    uint ic = atomicCounterIncrement(indirectIndex);&#xA;    meshIndices[ic*3+0] = i0;&#xA;    meshIndices[ic*3+1] = i1;&#xA;    meshIndices[ic*3+2] = i2;&#xA;    atomicCounterIncrement(indirectIndexCount);&#xA;    atomicCounterIncrement(indirectIndexCount);&#xA;    atomicCounterIncrement(indirectIndexCount);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Both &lt;code&gt;indirectIndex&lt;/code&gt; and &lt;code&gt;indirectIndexCount&lt;/code&gt; are initialized to 0 and &lt;code&gt;indirectIndexCount&lt;/code&gt; is what is passed through to  &lt;code&gt;glDrawElementsIndirect&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-10-13T14:44:20.010" LastActivityDate="2015-10-13T14:44:20.010" CommentCount="2" />
  <row Id="1618" PostTypeId="1" CreationDate="2015-10-14T16:51:06.533" Score="10" ViewCount="114" Body="&lt;p&gt;My raytracer supports a wide variety of objects.  To intersect them, I use the standard technique of transforming rays into object-space.  This works fantastically until I add motion blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I model motion blur as a sequence of transforms (to simplify discussion, let's say exactly two) instead of one.  My approach is to take the inverse transform of the ray at both keyframes and lerp the positions/directions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This seems to work fine for translations, but it breaks down for rotations.  E.g. here are two triangles undergoing 30 and 90 degree rotations:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/DRHf1.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/DRHf1.png&quot; alt=&quot;rotation1&quot;&gt;&lt;/a&gt;&lt;br/&gt;&#xA;&lt;sup&gt;(4 samples, MN reconstruction, the red samples came from near the two keyframes)&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the corners, I would expect the lerped samples to lie on a straight line between the the two vertices.  Instead, they bulge outward.  This is wrong.  In more interesting scenes with more interesting transformations, it causes a variety of failure modes.  E.g. here's a propeller undergoing a 45 rotation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8kcIi.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8kcIi.png&quot; alt=&quot;rotation 2&quot;&gt;&lt;/a&gt;&lt;br/&gt;&#xA;&lt;sup&gt;(100 samples, normals visualized)&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some problems are due to the BVH breaking (it assumes the extrema of objects lie at keyframes), but even a brute force render is incorrect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can fix all this by doing forward transforms only (transform object, not the ray), but this only works for objects where that is possible (only triangles, really).&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I make my raytracer produce linear approximations to transformation (especially rotation) by transforming rays, not objects?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="523" LastEditorUserId="523" LastEditDate="2015-10-14T20:56:33.537" LastActivityDate="2015-10-14T20:56:33.537" Title="Ray Transformation to Object Space for Motion Blur" Tags="&lt;raytracing&gt;&lt;transformations&gt;&lt;motion&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1619" PostTypeId="2" ParentId="1618" CreationDate="2015-10-14T17:48:09.203" Score="7" Body="&lt;p&gt;Lerping the ray positions/directions between keyframes should be equivalent to lerping the inverse matrices between keyframes and transforming by the lerped matrix. Trouble is, if the keyframes have different rotations, that lerped matrix will in general be something &quot;weird&quot;, with shearing, nonuniform scale, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wouldn't be surprised if some of your intersection and shading routines don't work properly in such a distorted coordinate system, unless you've specifically tested and hardened them against such cases. (For example, taking the dot product of two unit vectors doesn't give the same answer in a sheared coordinate system as in an orthonormal one.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is just a guess, but it might work better if you choose an interpolation method where the translation, rotation, and scale (if applicable) are lerped separately (using quaternions for the rotation part) and re-combined.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-10-14T17:48:09.203" CommentCount="3" />
  <row Id="1620" PostTypeId="2" ParentId="1618" CreationDate="2015-10-14T17:56:16.040" Score="3" Body="&lt;p&gt;I don't think you'll get terribly far with, AFAICS, a single linear approximation to a rather non-linear interpolation, but perhaps this &lt;a href=&quot;http://fileadmin.cs.lth.se/graphics/research/papers/2013/highorder/&quot; rel=&quot;nofollow&quot;&gt;paper / presentation&lt;/a&gt; by Gribel et al on motion blur in rasterisation may help.&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2015-10-14T17:56:16.040" CommentCount="1" />
  <row Id="1621" PostTypeId="2" ParentId="1606" CreationDate="2015-10-14T18:02:21.180" Score="5" Body="&lt;p&gt;The hint with the perspective division was already &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1607/127&quot;&gt;mentioned by ratchet freak&lt;/a&gt;, but I'd like to add some explanation of how to come up with the solution.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;First of all, remember that homogenous coordinates add a fourth value $w$ to your 3D vector. For points in the 3D space, $w \neq 0$ holds and $[x, y, z, w] = [n*x, n*y, n*z, n*w]$ also holds for every $n$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the goal is to find a transformation matrix $T$, which solves&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;    \begin{bmatrix} \frac{2}{x + y} \\ \frac{5y + z}{2x + 2y} \\ 3 \\ 1 \end{bmatrix}&#xA;= T \cdot &#xA;\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}&#xA;=&#xA;\begin{bmatrix} t_{11} &amp;amp; t_{21} &amp;amp; t_{31} &amp;amp; t_{41}  \\ t_{12} &amp;amp; t_{22} &amp;amp; t_{32} &amp;amp; t_{42} \\ t_{13} &amp;amp; t_{23} &amp;amp; t_{33} &amp;amp; t_{43} \\ t_{14} &amp;amp; t_{24} &amp;amp; t_{34} &amp;amp; t_{44} \end{bmatrix}&#xA;\cdot \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are basically four equations, but having a look at the first row, being&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\frac{2}{x + y} = t_{11}\cdot x + t_{21}\cdot y + t_{31} \cdot z + t_{41}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and trying to find values for the $t$s, you'll see that this is not possible without further restructuring. The main problems are the $x$ and $y$ in the denominator. But as we can make use of the fourth value in the vector, we can expand the $w$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix} \frac{2}{x + y} \\ \frac{5y + z}{2x + 2y} \\ 3 \\ 1 \end{bmatrix}&#xA;=&#xA;\begin{bmatrix} \frac{2}{x + y} \\ \frac{2.5y + 0.5z}{x + y} \\ 3 \\ 1 \end{bmatrix}&#xA;=&#xA;\begin{bmatrix} 2 \\ 2.5y + 0.5z \\ 3(x + y) \\ x + y \end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the first step, I removed the factor of $2$ from the second values denominator, leading to every values denominator either being $1$ or $(x + y)$. In the second step, I made use of the above mentioned rule ($[x, y, z, w] = [n\cdot x, n\cdot y, n\cdot z, n\cdot w]$), setting $n = (x + y)$, thus eliminating the denominator of the first two values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now we've got&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix} 2 \\ 2.5y + 0.5z \\ 3(x + y) \\ x + y \end{bmatrix}&#xA;=&#xA;\begin{bmatrix} t_{11} &amp;amp; t_{21} &amp;amp; t_{31} &amp;amp; t_{41}  \\ t_{12} &amp;amp; t_{22} &amp;amp; t_{32} &amp;amp; t_{42} \\ t_{13} &amp;amp; t_{23} &amp;amp; t_{33} &amp;amp; t_{43} \\ t_{14} &amp;amp; t_{24} &amp;amp; t_{34} &amp;amp; t_{44} \end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;which is not that hard anymore.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix} 2 \\ 2.5y + 0.5z \\ 3(x + y) \\ x + y \end{bmatrix}&#xA;=&#xA;\begin{bmatrix} 0  &amp;amp;  0 &amp;amp;   0  &amp;amp;  2   \\ 0 &amp;amp;  2.5 &amp;amp; 0.5 &amp;amp;  0 \\ 3 &amp;amp;   3  &amp;amp;  0 &amp;amp;   0 \\ 1  &amp;amp;  1  &amp;amp;  0  &amp;amp;  0 \end{bmatrix}&#xA;\cdot&#xA;\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;TL;DR&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Remember the advantage of homogenous coordinates $w$.&lt;/li&gt;&#xA;&lt;li&gt;$[x, y, z, w] = [n\cdot x, n\cdot y, n\cdot z, n\cdot w]$ is the key.&lt;/li&gt;&#xA;&lt;li&gt;Push everything that's in the denominator to the $w$ coordinate.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-11-03T15:32:59.350" LastActivityDate="2015-11-03T15:32:59.350" CommentCount="2" />
  <row Id="1622" PostTypeId="1" AcceptedAnswerId="1643" CreationDate="2015-10-15T07:33:36.113" Score="5" ViewCount="115" Body="&lt;p&gt;I'm trying to learn GLSL, I'm following &lt;a href=&quot;https://www.youtube.com/playlist?list=PLRwVmtr-pp06qT6ckboaOhnm9FxmzHpbY&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; tutorial. I've done everything according to the tutorials but I can't get any output from my code. As you see I'm checking for both compile errors and linker errors and everything is fine, GLSL code compiles and links fine but there is no output but a red window. Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;main.cpp &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;QApplication&amp;gt;&#xA;#include &amp;lt;GlWindow.h&amp;gt;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;int main(int argc, char *argv[]){&#xA;    QApplication app(argc, argv);&#xA;&#xA;    GlWindow glWindow;&#xA;    glWindow.show();&#xA;&#xA;    return app.exec();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;GLWindow.h&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#ifndef GLWINDOW_H&#xA;#define GLWINDOW_H&#xA;#include &amp;lt;QtOpenGL/QGLWidget&amp;gt;&#xA;#include &amp;lt;QOpenGLWidget&amp;gt;&#xA;&#xA;class GlWindow : public QGLWidget{&#xA;public:&#xA;&#xA;protected:&#xA;    void initializeGL();&#xA;    void paintGL();&#xA;};&#xA;&#xA;#endif // GLWINDOW_H&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;GLWindow.cpp&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;GL/glew.h&amp;gt;&#xA;#include &quot;GlWindow.h&quot;&#xA;#include &amp;lt;iostream&amp;gt;&#xA;&#xA;extern const char* vertexShaderCode;&#xA;extern const char* fragmentShaderCode;&#xA;&#xA;void sendDataToOpenGL(){&#xA;    GLfloat verts[] = {&#xA;        +0.0f, +0.0f,&#xA;             1.0f, 0.0f, 0.0f,&#xA;        +1.0f, +1.0f,&#xA;             1.0f, 0.0f, 0.0f,&#xA;        -1.0f, +1.0f,&#xA;             1.0f, 0.0f, 0.0f,&#xA;        -1.0f, -1.0f,&#xA;             1.0f, 0.0f, 0.0f,&#xA;        +1.0f, -1.0f,&#xA;             1.0f, 0.0f, 0.0f,&#xA;&#xA;    };&#xA;&#xA;    GLuint verexBufferID;&#xA;    glGenBuffers(1, &amp;amp;verexBufferID);&#xA;    glBindBuffer(GL_ARRAY_BUFFER, verexBufferID);&#xA;    glBufferData(GL_ARRAY_BUFFER, sizeof(verts), verts, GL_STATIC_DRAW);&#xA;&#xA;    glEnableVertexAttribArray(0);&#xA;    glVertexAttribPointer(0,2, GL_FLOAT, GL_FALSE, sizeof(GLfloat) * 5,0);&#xA;    glEnableVertexAttribArray(1);&#xA;    glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, sizeof(GLfloat) * 5, (char*)(sizeof(GLfloat) * 2));&#xA;&#xA;    GLuint indices [] = {0,1,2, 0,3,4};&#xA;    GLuint indexBufferID;&#xA;    glGenBuffers(1,&amp;amp;indexBufferID);&#xA;    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, indexBufferID);&#xA;    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);&#xA;}&#xA;&#xA;&#xA;bool checkShaderStatus(GLuint shaderID){&#xA;    GLint compileStatus;&#xA;    glGetShaderiv(shaderID, GL_COMPILE_STATUS, &amp;amp;compileStatus);&#xA;&#xA;&#xA;    if(compileStatus != GL_TRUE){&#xA;        std::cerr&amp;lt;&amp;lt;&quot;failed&quot;&amp;lt;&amp;lt;std::endl;&#xA;        GLint infoLogLength;&#xA;        glGetShaderiv(shaderID, GL_INFO_LOG_LENGTH, &amp;amp;infoLogLength);&#xA;        GLchar* buffer = new GLchar [infoLogLength];&#xA;&#xA;        GLsizei bufferSize;&#xA;        glGetShaderInfoLog(shaderID, infoLogLength, &amp;amp;bufferSize, buffer);&#xA;        std::cout&amp;lt;&amp;lt;buffer&amp;lt;&amp;lt;std::endl;&#xA;        return false;&#xA;        delete []buffer;&#xA;    }&#xA;    return true;&#xA;}&#xA;&#xA;bool checkProgramStatus(GLuint programID){&#xA;    GLint linkStatus;&#xA;    glGetProgramiv(programID, GL_LINK_STATUS, &amp;amp;linkStatus);&#xA;&#xA;&#xA;    if(linkStatus != GL_TRUE){&#xA;        std::cerr&amp;lt;&amp;lt;&quot;failed&quot;&amp;lt;&amp;lt;std::endl;&#xA;        GLint infoLogLength;&#xA;        glGetProgramiv(programID, GL_INFO_LOG_LENGTH, &amp;amp;infoLogLength);&#xA;        GLchar* buffer = new GLchar [infoLogLength];&#xA;&#xA;        GLsizei bufferSize;&#xA;        glGetProgramInfoLog(programID, infoLogLength, &amp;amp;bufferSize, buffer);&#xA;        std::cout&amp;lt;&amp;lt;buffer&amp;lt;&amp;lt;std::endl;&#xA;        return false;&#xA;&#xA;        delete []buffer;&#xA;    }&#xA;    return true;&#xA;}&#xA;&#xA;&#xA;void installShaders(){&#xA;&#xA;    GLuint vertexShaderID = glCreateShader(GL_VERTEX_SHADER);&#xA;    GLuint fragmentShaderID = glCreateShader(GL_FRAGMENT_SHADER);&#xA;&#xA;    const char *adapter[1];&#xA;    adapter[0] = vertexShaderCode;&#xA;&#xA;    glShaderSource(vertexShaderID, 1, adapter, 0);&#xA;    adapter[0] = fragmentShaderCode;&#xA;    glShaderSource(fragmentShaderID, 1, adapter, 0);&#xA;&#xA;&#xA;&#xA;    glCompileShader(vertexShaderID);&#xA;    glCompileShader(fragmentShaderID);&#xA;&#xA;    if(!checkShaderStatus(vertexShaderID)){&#xA;        std::cerr&amp;lt;&amp;lt;&quot;Sth wrong with vertex shader&quot;&amp;lt;&amp;lt;std::endl;&#xA;        return;&#xA;    }&#xA;&#xA;    if(!checkShaderStatus(fragmentShaderID)){&#xA;       std::cerr&amp;lt;&amp;lt;&quot;Sth wrong with fragment shader&quot;&amp;lt;&amp;lt;std::endl;&#xA;       return;&#xA;    }&#xA;&#xA;&#xA;    GLuint programID = glCreateProgram();&#xA;    glAttachShader(programID, vertexShaderID);&#xA;    glAttachShader(programID, fragmentShaderID);&#xA;&#xA;    glLinkProgram(programID);&#xA;&#xA;    if(!checkProgramStatus(programID)){&#xA;        std::cerr&amp;lt;&amp;lt;&quot;failed to link&quot;&amp;lt;&amp;lt;std::endl;&#xA;        return;&#xA;    }&#xA;&#xA;    glUseProgram(programID);&#xA;}&#xA;&#xA;void GlWindow::paintGL(){&#xA;    glClear(GL_COLOR_BUFFER_BIT);&#xA;    glClearColor(1.0f, 0.0f, 0.0f, 1.0f);&#xA;    glViewport(0,0,width(), height());&#xA;&#xA;    glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_SHORT, 0);&#xA;&#xA;}&#xA;&#xA;&#xA;void GlWindow::initializeGL(){&#xA;    glewInit();&#xA;&#xA;    const GLubyte *shaderVersion = glGetString(GL_SHADING_LANGUAGE_VERSION);&#xA;    std::cout&amp;lt;&amp;lt;shaderVersion&amp;lt;&amp;lt;std::endl;&#xA;&#xA;    sendDataToOpenGL();&#xA;    installShaders();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and finally my shader code: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;const char* vertexShaderCode  =&#xA;        &quot;#version 430\r\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;in layout(location=0) vec2 position;\n&quot;&#xA;        &quot;in layout(location=1) vec3 vertexColor;\n&quot;&#xA;        &quot;&quot;&#xA;        &quot;out vec3 theColor;\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;void main()\n&quot;&#xA;        &quot;{\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;gl_Position = vec4(position, 0.0, 1.0);\n&quot;&#xA;        &quot;theColor = vertexColor; \n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;}\n&quot;&#xA;        ;&#xA;&#xA;&#xA;const char* fragmentShaderCode =&#xA;        &quot;#version 430\r\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;out vec4 daColor;\n&quot;&#xA;        &quot;in vec3 theColor;\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot;void main()\n&quot;&#xA;        &quot;{\n&quot;&#xA;        &quot;\n&quot;&#xA;        &quot; daColor = vec4(theColor,1.0);\n&quot;&#xA;        &quot;}\n&quot;&#xA;        ;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I know that my shader code looks stupid but I'm just trying to learn. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I don't know if thats relevant, but I'm using Ubuntu 15.0 and &lt;code&gt;glGetString(GL_SHADING_LANGUAGE_VERSION)&lt;/code&gt; returns: &lt;code&gt;4.4 nvidia via cg compiler&lt;/code&gt;.  &lt;/p&gt;&#xA;" OwnerUserId="1861" LastEditorUserId="127" LastEditDate="2015-10-15T15:12:19.413" LastActivityDate="2015-10-20T21:21:24.110" Title="Can't get output from a GLSL code" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1623" PostTypeId="1" CreationDate="2015-10-15T08:06:54.277" Score="11" ViewCount="71" Body="&lt;p&gt;Spherical harmonics appear in several computer graphics techniques.&#xA;I feel that in order to be a better computer graphics developer, I need to have a deep understanding of what they are and how they are used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems that the reference most often recommended to understand Spherical Harmonics is &lt;a href=&quot;http://www.ppsloan.org/publications/StupidSH36.pdf&quot;&gt;&quot;Stupid Spherical Harmonics Tricks&quot; by Peter-Pike Sloan&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I started reading it but did not find a &quot;satisfying&quot; definition of SH, it seems like the document mostly relies on other references for the &quot;basics&quot;.&#xA;Other references introduce the Fourier Basis functions as a &quot;simpler version&quot; of SH, but once again it seems hard to find good material explaining them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are thorough, accessible references to understand Fourier basis functions and Spherical Harmonics ?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2015-10-16T06:07:31.000" Title="Thorough, accessible material about Fourier basis functions and Spherical Harmonics?" Tags="&lt;maths&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="1624" PostTypeId="1" CreationDate="2015-10-15T09:09:14.390" Score="1" ViewCount="35" Body="&lt;p&gt;So I need to print out &lt;a href=&quot;https://www.myminifactory.com/object/engine-1406&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; model, however with default sizes, one of the pieces is too large for my printer (M3D). I will need to scale it down.. the question is, do I need to scale the rest of the individual objects (pieces) in the same proportions, so I will be able to assemble all the parts afterwards ? This might be a bit of a stupid question, but I don't have any experience with 3D printing yet.. thank you.&lt;/p&gt;&#xA;" OwnerUserId="1863" LastActivityDate="2015-10-15T11:16:19.893" Title="Scaling models for 3D print" Tags="&lt;3d&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2015-10-16T20:22:49.063" />
  <row Id="1625" PostTypeId="2" ParentId="1624" CreationDate="2015-10-15T11:16:19.893" Score="1" Body="&lt;p&gt;Just like for any reduction or drawing, if you scale one part and not the other, their relative proportion is changed, so yes, aesthetic, matching, functionality are affected: scale everything in the same proportion. Nothing specific to 3D printing here.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2015-10-15T11:16:19.893" CommentCount="0" />
  <row Id="1626" PostTypeId="1" CreationDate="2015-10-15T20:44:30.027" Score="7" ViewCount="88" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Sketchpad&quot;&gt;Scratchpad was a constraint-based graphics system.&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What does it mean  by a Constraint-based Graphics System?&lt;/p&gt;&#xA;" OwnerUserId="464" LastEditorUserId="231" LastEditDate="2016-01-19T13:05:40.117" LastActivityDate="2016-01-19T13:05:40.117" Title="What does it mean by a Constraint-based Graphics System?" Tags="&lt;constraint-based&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1627" PostTypeId="1" CreationDate="2015-10-15T22:31:29.023" Score="2" ViewCount="47" Body="&lt;p&gt;What is the basic difference between Radar Screen, PC CRT Monitor, and, Oscilloscope displays?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of them use CRT. So, what is the difference?&lt;/p&gt;&#xA;" OwnerUserId="464" LastActivityDate="2015-10-16T21:05:01.800" Title="What is the basic difference between Radar Screen, CRT and Oscilloscope displays?" Tags="&lt;display-lists&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2015-10-16T20:22:59.230" />
  <row Id="1628" PostTypeId="2" ParentId="1626" CreationDate="2015-10-16T02:54:45.827" Score="5" Body="&lt;p&gt;Sketchpad was a system that allowed you to draw simple shapes using lines and curves in 2D.  It maintained constraints between the shapes. For example, an endpoint of one line could be constrained to lie on another line. Also, as mentioned in the Wikipedia article, the user could set a fixed length for a line, or an angle between two connected lines. These constraints would be automatically applied by the system while editing the diagrams&amp;mdash;e.g., when moving one line, other lines would be adjusted to maintain the constraints.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-10-16T02:54:45.827" CommentCount="0" />
  <row Id="1629" PostTypeId="2" ParentId="1623" CreationDate="2015-10-16T06:07:31.000" Score="3" Body="&lt;p&gt;wil, you largely have enough scholar background, you must have done Fourier and Laplace transforms in second year, and maybe in your engineering school again as part of signal processing classes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you read &quot;stupid tricks&quot; there is not much more you can do to find a condensed course at this point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second most famous paper that goes with SH for graphics is by Robin Green called &quot;the gritty details&quot;:&lt;br&gt;&#xA;&lt;strong&gt;&lt;a href=&quot;http://www.research.scea.com/gdc2003/spherical-harmonic-lighting.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.research.scea.com/gdc2003/spherical-harmonic-lighting.pdf&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the third most important is the one by Ramamoorthi (the original paper preceding &quot;&lt;a href=&quot;http://graphics.stanford.edu/papers/envmap/&quot; rel=&quot;nofollow&quot;&gt;An Efficient Representation for Irradiance Environment Maps&lt;/a&gt;&quot;), which was called &lt;a href=&quot;http://graphics.stanford.edu/papers/invlamb/josa.pdf&quot; rel=&quot;nofollow&quot;&gt;&lt;strong&gt;On the relationship between radiance and&#xA;irradiance: determining&#xA;the illumination from&#xA;images of a convex Lambertian object&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And I think they mention somewhere that SH were previously most used by another science field, forgot which one, physics maybe, and that most of their base material came from these papers. So if you want to dig in the roots, you've got to pull out these mid last century references.&lt;/p&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2015-10-16T06:07:31.000" CommentCount="1" />
  <row Id="1630" PostTypeId="2" ParentId="1626" CreationDate="2015-10-16T15:13:25.457" Score="10" Body="&lt;p&gt;If you have a geometric drawing with measurements you can solve what the shape has to be. This is valuable for example in engineering as it allows you specify the requirements and the computer can then solve the constrained drawing via a solver. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/80tMB.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/80tMB.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Constraint solver in action, red constraints are fixed while blue ones are free to change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are several ways how the solver might work, it could be a numeric nonlinear based on gradient descent. Or it can be fully algebraic. Usually its a gradient descent solver as this is easier to implement, and possibly faster.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-10-16T15:13:25.457" CommentCount="0" />
  <row Id="1631" PostTypeId="2" ParentId="1627" CreationDate="2015-10-16T18:55:16.347" Score="4" Body="&lt;p&gt;At many places in CG hardwares and algorithms you have the choice between vector description and raster description. Oscilloscops, radar, lasers shows, plotting tables, laser cuts directly follow lines and curves; these are the vectorials devices. LCD, inkjet and laser printers, stereolithographic 3D printing, follow pixels and voxels; these are the raster devices. The CRT, but also some laser shows, use the oscilloscope technology to implement raster: the &quot;curve&quot; is a repeated Z narrowly sweeping the screen line after line, with intensity modulation making the &quot;pixels&quot;. Radars are indeed between this and pure vectors since rotation angle is swept. Details about various forms of radar display and oscilloscope is available on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Radar_display&quot; rel=&quot;nofollow&quot;&gt;wikipedia page&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;( About representations, the vector/raster dichotomy corresponds to inkscape/illustrator vs gimp/photoshop, 3D meshes vs 3D voxels, list of lobes vs angular or SH decomposition, Lagrangian animation vs Eulerian animation, etc. )&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2015-10-16T21:05:01.800" LastActivityDate="2015-10-16T21:05:01.800" CommentCount="0" />
  <row Id="1632" PostTypeId="1" CreationDate="2015-10-16T19:54:28.523" Score="6" ViewCount="283" Body="&lt;p&gt;I am trying to sample from the Blinn-Phong BRDF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For testing, I use a spherical light source (Lambertian emission) sitting on a specular plane.  Here's a reference image using energy-conserving Phong with exponent &lt;code&gt;30000&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/R3uLF.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/R3uLF.png&quot; alt=&quot;Phong&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Notice that no energy is lost.  &lt;strong&gt;I would now like to achieve the same thing for Blinn-Phong&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's my attempt (same exponent, but on Blinn-Phong BRDF):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xKlyz.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xKlyz.png&quot; alt=&quot;Blinn-Phong&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, a substantial portion of energy is lost.  The normalization term I am using comes from &lt;a href=&quot;http://www.thetenthplanet.de/archives/255&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and is $\frac{n+1}{2 \pi}$.  The problem is this is a &lt;em&gt;normalization&lt;/em&gt; term, not a &lt;em&gt;conservation&lt;/em&gt; term.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe this is the expected result.  It is common in graphics to make BRDFs reflect less than the input amount of energy, as opposed to exactly the right amount, as this is usually less difficult.&lt;sup&gt;1, 2&lt;/sup&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is there a version that conserves all energy exactly?&lt;/li&gt;&#xA;&lt;li&gt;Iff not, can someone at least confirm this is expected?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;N.B. I am fairly confident this is not a ray generation/PDF issue.  These images were importance sampled using a method given in the PBRT book pg. 695 - pg. 699.  Moreover, a non-importance sampled version looks very similar (though I had to use a lower exponent to get it to converge fast enough, and it still took 10,000 s/p).&lt;/sub&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt;This can be argued from a shadowing/masking argument on microfacets.  Unfortunately, it ignores multiple scattering.  The correct answer is somewhere in-between.&lt;/sub&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;2&lt;/sup&gt;Indeed, I had to derive the Phong conservation term myself.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="523" LastEditorUserId="137" LastEditDate="2015-11-03T15:18:26.990" LastActivityDate="2015-11-03T15:18:26.990" Title="Energy Conservation for Blinn-Phong BRDF" Tags="&lt;raytracing&gt;&lt;brdf&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1633" PostTypeId="1" AcceptedAnswerId="1634" CreationDate="2015-10-17T02:04:20.943" Score="6" ViewCount="129" Body="&lt;p&gt;In PC game development, after loading models, textures, shaders, etc in a loading screen, some games will render the models once to an off screen target to make sure the driver and gpu have done all the work required to keep there from being a hitch the first time the models render.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know how much that actually helps? Does it only affect certain cards and drivers?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-10-17T20:38:19.193" Title="Does prerendering actually help performance?" Tags="&lt;gpu&gt;&lt;real-time&gt;&lt;direct3d&gt;&lt;directx11&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="1634" PostTypeId="2" ParentId="1633" CreationDate="2015-10-17T07:29:49.867" Score="9" Body="&lt;p&gt;As far as I know, this sort of thing is mainly about shader compilation. One of the main reasons why a game may experience hitches the first time something renders is that the shaders necessary to render it haven't finished compiling yet, and the driver has to finish that work before the frame can proceed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A little bit of background. When you write shaders in HLSL, you compile them with Microsoft's HLSL compiler (fxc.exe), which outputs a hardware-independent bytecode. When you call CreateVertexShader etc. in D3D, that bytecode gets passed to the underlying driver, which ultimately compiles it into the machine code for the specific GPU you're running on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This internal compilation step isn't trivial&amp;mdash;it's a full-blown optimizing compiler. So it can take a little while on a complicated shader. Because of this, when you call D3D's CreateVertexShader, the driver doesn't necessarily &lt;em&gt;finish&lt;/em&gt; compiling the shader right then and there; it may add it to a queue, then return control to the application. Meanwhile, a background thread (perhaps more than one) consumes the queue and compiles the shaders. (The driver may also have a disk cache of previously-compiled shaders, so that it doesn't have to redo all this work every time the game is launched.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This background-thread compilation means the main game thread can get on with other work, e.g. loading other data or building a scene graph or what-have-you, so it's generally a win for overall loading times. But the consequence of it is that the shaders may not all be finished compiling when the game starts rendering. Unfortunately, there is no way with DX11 and earlier APIs for the app to tell when the shaders are done. The best you can do is to render something with the shader&amp;mdash;this effectively makes the app wait for the compiler to finish. That's why games sometimes do this offscreen pre-rendering business.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't have any hard numbers handy, but qualitatively, one can reduce the impact of shader compilation by front-loading it during loading. Create all your shaders first, then the compiler will run through those while you load everything else. However, with real-time streaming  (e.g. open-world games) it can still be quite difficult to ensure no hitches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth noting that there are some cases where state changes can cause a shader recompile. Things like vertex buffer formats, render target formats, and shader linking details (e.g. re-using the same pixel shader with different vertex shaders, or with tessellation shaders) can potentially cause the driver to have to generate different versions of the shader under the hood. The details vary by vendor and hardware. This isn't necessarily a full from-scratch recompile&amp;mdash;often it's just sticking on some prologue or epilogue code&amp;mdash;but it's something to be aware of.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is one place where the new APIs (DX12, Metal, and Vulkan) are in a good position to solve the problem. These APIs have &quot;pipeline state&quot; objects where all shader stages and all possibly relevant states are rolled up into a single object, and when you create one of these, the driver has all the information it needs to actually and completely finalize all the shaders. DX12 also makes shader caching explicit. Also, at least for NVIDIA DX12 drivers (I don't know about others), shader compilation is done fully inside the pipeline state creation method, and the app can use multiple threads itself in order to parallelize shader compilation and other work. So, the practice of offscreen pre-rendering to ensure shaders are finalized should no longer be necessary in DX12.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="504" LastEditDate="2015-10-17T20:38:19.193" LastActivityDate="2015-10-17T20:38:19.193" CommentCount="0" />
  <row Id="1635" PostTypeId="2" ParentId="1632" CreationDate="2015-10-17T09:22:33.657" Score="7" Body="&lt;p&gt;First off, this is probably not the best way of testing energy conservation, because not all incident inclinations are visible in the image. Most of the environment is also black, so a less &quot;peaky&quot; BRDF such as Lambert would come out very dark with no way of telling whether it's energy conserving or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is more commonly used is a so-called &quot;furnace test&quot;, where you place a single sphere into a white environment map. When you render this sphere using the BRDF of interest, it should blend into the environment perfectly if the BRDF is energy conserving (make sure no color channels are clipping). Because it is a sphere, you will be able to see the BRDF at all incident inclinations. Additionally, the sphere is convex, so lacking any other objects in the scene, no reflected ray will be occluded, and every pixel of the sphere in the image will be an integral over the BRDF at a different inclination.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you did this test, you would also notice that your Phong BRDF is most likely not energy conserving. The commonly used normalization term of $\frac{n+1}{2 \pi}$ (or $\frac{n+2}{2 \pi}$, depending on who you ask) only holds at normal incidence. At other inclinations, and particularly at grazing angle, up to half of the reflected rays will land below the surface and are usually terminated (because transmitting through a reflective surface does not make sense).&lt;br&gt;&#xA;The correct normalization term that accounts for the part of the Phong lobe that is clipped below the hemisphere is very difficult to compute. Jim Arvo derived a closed-form solution for integer exponents using double-axis moments, summarised in the &lt;a href=&quot;http://people.cs.kuleuven.be/~philip.dutre/GI/TotalCompendium.pdf&quot; rel=&quot;nofollow&quot;&gt;Global Illumination Compendium&lt;/a&gt;, paragraph 31a) and 31b) (page 17/18). These formulas are not very practical for use in a renderer though and I don't suggest implementing them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It should be clear that if a simple model such as Phong is already this difficult to normalize, then Blinn-Phong is even more difficult. First of all, the $\frac{n+1}{2 \pi}$ normalization term you mention is only correct if you use Blinn-Phong as a microfacet distribution inside a full microfacet BRDF (e.g. Torrance-Sparrow or Walter). If you directly use the Blinn-Phong BRDF, then a more correct normalization term is $\frac{(n+2)(n+4)}{(8 \pi 2^{-n/2} + n)}$, derived in &lt;a href=&quot;http://www.farbrausch.de/~fg/stuff/phong.pdf&quot; rel=&quot;nofollow&quot;&gt;this short article&lt;/a&gt; by Fabian Giesen (second page) and also listed on the page you mention. However, this normalization term is again only correct at normal incidence and will not be exact at other inclinations. From what I can tell, no exact normalization term exists.&lt;/p&gt;&#xA;" OwnerUserId="79" LastEditorUserId="137" LastEditDate="2015-11-03T15:18:17.490" LastActivityDate="2015-11-03T15:18:17.490" CommentCount="1" />
  <row Id="1636" PostTypeId="2" ParentId="1608" CreationDate="2015-10-18T06:09:20.450" Score="3" Body="&lt;p&gt;From math:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;There is a 2:1 homomorphism from the unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Quaternion&quot; rel=&quot;nofollow&quot;&gt;quaternions&lt;/a&gt; to &lt;a href=&quot;https://en.wikipedia.org/wiki/Rotation_group_SO%283%29&quot; rel=&quot;nofollow&quot;&gt;SO(3)&lt;/a&gt; (the rotation group).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What this (essentially) means is that:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Every orientation can be represented as a quaternion&lt;/li&gt;&#xA;&lt;li&gt;Quaternions represent a single rotation&lt;/li&gt;&#xA;&lt;li&gt;Multiplication of quaternions produces another quaternion (closure), and is equivalent to composing the rotations.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Therefore &lt;em&gt;any&lt;/em&gt; number of rotations can be represented as a single rotation!&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Think about that.  Starting from object space, you can rotate your object into &lt;em&gt;any&lt;/em&gt; orientation using only a &lt;em&gt;single&lt;/em&gt; rotation.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I would like to point out that bringing quaternions in wasn't just random math-ese.  In contrast to the other answers, the &lt;em&gt;favored&lt;/em&gt; approach in graphics is actually to represent rotations as quaternions, since they take up less space and are faster to combine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are easily-Googleable ways to convert between rotation matrices and quaternions, depending on which you prefer.  The point is that rotations &lt;em&gt;are&lt;/em&gt; the quaternions in a mathematical sense, so combinations thereof are also single rotations.&lt;/p&gt;&#xA;" OwnerUserId="523" LastActivityDate="2015-10-18T06:09:20.450" CommentCount="0" />
  <row Id="1637" PostTypeId="1" CreationDate="2015-10-19T04:39:43.037" Score="3" ViewCount="85" Body="&lt;p&gt;Given a 3D space and a light source, I use ray casting to determine the shadows generated in the scene. If I'm to store the shadow data (boundary of the shadow, object which cast the shadow, etc.) what is an efficient way to go about it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Its basically for an application where I want to be able to move the shadow around manually to see the change in the 3d objects position. Say I apply a force to the shadow of a ball, I can actually see the ball move and also the change in the shadow. So I need to be able to set up colliders for the shadow and hence need a way to be able to store the boundary of the shadow and information about the object that cast it to carry out the actions.&lt;/p&gt;&#xA;" OwnerUserId="1879" LastEditorUserId="1879" LastEditDate="2015-11-03T19:03:47.043" LastActivityDate="2015-11-03T19:03:47.043" Title="Storing shadow data" Tags="&lt;shadow-mapping&gt;&lt;shadow&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="1639" PostTypeId="2" ParentId="1637" CreationDate="2015-10-20T08:45:52.520" Score="3" Body="&lt;p&gt;If the goal is mouse interactivity, you can do with a simple &quot;shadow buffer&quot;, i.e. an image that holds the identity of the occluding object (if any) on every rendered pixel. You will compute this map during the casting process as you shoot the rays to the source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case there are several occluding objects, it is up to you to choose which one to consider (or keep a linked list of all occluding objects). If there are several sources, you can keep one map per source.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastActivityDate="2015-10-20T08:45:52.520" CommentCount="4" />
  <row Id="1640" PostTypeId="1" AcceptedAnswerId="1642" CreationDate="2015-10-20T13:38:20.027" Score="3" ViewCount="138" Body="&lt;p&gt;I've written an implementation of the sphere tracing algorithm in OpenGL 4+.&#xA;As an experiment/toy project, i'm reimplementing it using the OpenGL 4.3 compute shader, but i'm having trouble with the whole local/global invocation ID thing.&#xA;The basic idea is to use the compute shader to calculate the image and output it in a texture, then use a trivial program to copy it onto the framebuffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the compute shader i'm using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;layout (binding = 0, rgba32f) writeonly uniform image2D output_image;&#xA;layout (local_size_x = 16, local_size_y = 16, local_size_z = 1) in;&#xA;&#xA;void main()&#xA;{&#xA;    ivec2 coord = ivec2(gl_GlobalInvocationID.xy);&#xA;    imageStore(output_image, coord, vec4(0.0, 0.0, 1.0, 1.0));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is how I initialize the texture:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLuint offscreen_texture;&#xA;glGenTextures(1, &amp;amp;offscreen_texture);&#xA;&#xA;glActiveTexture(GL_TEXTURE0);&#xA;glBindTexture(GL_TEXTURE_2D, offscreen_texture);&#xA;glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);&#xA;glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);&#xA;glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, WINDOW_WIDTH, WINDOW_HEIGHT, 0, GL_RGBA, GL_FLOAT, nullptr);&#xA;glBindImageTexture(0, offscreen_texture, 0, GL_FALSE, 0, GL_WRITE_ONLY, GL_RGBA32F);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And these are the trivial vertex shader i'm using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;void main()&#xA;{&#xA;    const vec4 verts[4] = vec4[4](vec4(-1.0, -1.0, 0.5, 1.0),&#xA;                                  vec4( 1.0, -1.0, 0.5, 1.0),&#xA;                                  vec4(-1.0,  1.0, 0.5, 1.0),&#xA;                                  vec4( 1.0,  1.0, 0.5, 1.0));&#xA;&#xA;    gl_Position = verts[gl_VertexID];&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And fragment:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;layout (location = 0) out vec4 color_out;&#xA;layout(binding = 0) uniform sampler2D source_image;&#xA;&#xA;void main()&#xA;{&#xA;    color_out = texture(source_image, gl_FragCoord.xy);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And this is the render code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;compute_program.use();&#xA;glDispatchCompute(WINDOW_WIDTH / 16, WINDOW_HEIGHT / 16, 1);&#xA;glMemoryBarrier(GL_SHADER_IMAGE_ACCESS_BARRIER_BIT);&#xA;&#xA;copy_program.use();&#xA;glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Up to this point everything works correctly and i see my screen completely blue instead of the usual clear color.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Things start to break down when i try to use the Invocation ID to generate actual data (like the rays from the camera).&#xA;I tried to switch to a compute shader like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;layout (binding = 0, rgba32f) writeonly uniform image2D output_image;&#xA;layout (local_size_x = 16, local_size_y = 16, local_size_z = 1) in;&#xA;&#xA;void main()&#xA;{&#xA;    ivec2 coord = ivec2(gl_GlobalInvocationID.xy);&#xA;    imageStore(output_image, coord, vec4(0.0, 0.0, coord.x / 1280.0, 1.0));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which uses the global ID to generate the blue tint of the pixel.&#xA;As I understand from the OpenGL Suberbible book and the OpenGL wiki, I should be able to use the global ID as screen space coordinates (it's pretty easy to do the math using local IDs and work groups IDs and confirm that), much like gl_FragCoord, and this is confirmed by the previous trivial compute shader which paint every pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would expect a sort of horizontal gradient going from black to blue as an output of this compute shader, but instead everything turns black.&#xA;I've tried several combination of &quot;painting&quot; based on the local/global IDs of the threads, but i haven't got any luck no matter what.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Did I misunderstood the way the whole threads ID work?&#xA;What is the correct way to use them/map them to screen space coordinates like gl_FragCoord?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Just to add, i've tested this with both my integrated Intel HD 5100 and my discrete Nvidia 765M GPUs and the result is the same, so there is clearly something wrong on my side.&lt;/p&gt;&#xA;" OwnerUserId="281" LastEditorUserId="281" LastEditDate="2015-10-20T18:16:43.390" LastActivityDate="2015-10-20T20:06:11.753" Title="How to convert a thread ID into Screen Space Coord in an OpenGL Compute Shader?" Tags="&lt;opengl&gt;&lt;glsl&gt;&lt;compute-shader&gt;&lt;c++&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1641" PostTypeId="1" CreationDate="2015-10-20T15:04:28.460" Score="8" ViewCount="218" Body="&lt;p&gt;I would like to implement a Maya plugin (this question is independent from Maya) to create 3D Voronoi patterns, Something like &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8RYgU.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8RYgU.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I just know that I have to start from point sampling (I implemented the adaptive poisson sampling algorithm described in &lt;a href=&quot;http://link.springer.com/article/10.1007%2Fs11432-011-4322-8#/page-1&quot; rel=&quot;nofollow&quot;&gt;this paper&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought that, from those points, I should create the 3D wire of the mesh applying Voronoi (I tried to use (Python) scipy.spatial.Voronoi but the result was something different from what I expected).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am missing something? Can anyone suggest the proper pipeline and algorithms I have to implement to create such patterns?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;[EDIT] Here are a few example of what I get handling the result i get from scipy.spatial.Voronoi like this (as suggested &lt;a href=&quot;http://stackoverflow.com/questions/23658776/voronoi-diagram-edges-how-to-get-edges-in-the-form-point1-point2-from-a-scip&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vor = Voronoi(points)&#xA;for vpair in vor.ridge_vertices:&#xA;    for i in range(len(vpair) - 1):&#xA;        if all(x &amp;gt;= 0 for x in vpair):&#xA;            v0 = vor.vertices[vpair[i]]&#xA;            v1 = vor.vertices[vpair[i+1]]&#xA;            create_line(v0.tolist(), v1.tolist())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The grey vertices are the sampled points (the original shape was a simple sphere):&#xA;&lt;a href=&quot;http://i.stack.imgur.com/gz8hD.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gz8hD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a more complex shape (an arm)&#xA;&lt;a href=&quot;http://i.stack.imgur.com/mCkPm.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/mCkPm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1895" LastEditorUserId="1895" LastEditDate="2015-10-21T11:38:38.460" LastActivityDate="2015-10-21T11:38:38.460" Title="Pipeline to create Voronoi Meshes" Tags="&lt;3d&gt;&lt;geometry&gt;&lt;mesh&gt;&lt;polygon&gt;" AnswerCount="0" CommentCount="11" />
  <row Id="1642" PostTypeId="2" ParentId="1640" CreationDate="2015-10-20T20:06:11.753" Score="2" Body="&lt;p&gt;The problem is actually in your fragment shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;color_out = texture(source_image, gl_FragCoord.xy);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The texture() function accepts normalized coordinates which range from 0.0 to 1.0. The gl_FragCoord built in contains window coordinates which range from (0.0, 0.0) to (window width, window height).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To fix this, change the fragment shader to this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;color_out = texture(source_image, gl_FragCoord.xy / vec2(1280.0, 720.0));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That is assuming your window is 1280x720, change as necessary.&lt;/p&gt;&#xA;" OwnerUserId="40" LastActivityDate="2015-10-20T20:06:11.753" CommentCount="1" />
  <row Id="1643" PostTypeId="2" ParentId="1622" CreationDate="2015-10-20T21:21:24.110" Score="3" Body="&lt;p&gt;There are two problems:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Your indices are unsigned integers (4 bytes per index) but the call to glDrawElements specifies GL_UNSIGNED_SHORT. You need to change this to GL_UNSIGNED_INT.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;You are drawing two red triangles onto a buffer that has been cleared to red.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="40" LastActivityDate="2015-10-20T21:21:24.110" CommentCount="0" />
  <row Id="1644" PostTypeId="1" CreationDate="2015-10-21T15:46:02.827" Score="2" ViewCount="49" Body="&lt;p&gt;Most information about memory mapped displays on the net are about those in which there is essentially a location in main memory for each pixel on the display. A hypothetical 1024 x 512 display would therefore have 524,288 locations each mapped to a unique pixel. To set a pixel to a particular color, all you basically need to do is write an RGB value into the corresponding memory location for that pixel, eg: MOV 0xFF3C0A, 0x00BB75. The first argument in that instruction is the address holding the RGB value and the second is the address of the pixel you are copying it to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, this can use up a quite a lot of main memory, which might not be ideal for certain systems. In a machine with a 32 bit or higher word size, it could instead be possible to have a single &quot;display output register&quot; into which you would write a single word of data containing the X and Y coordinates as well as the RGB value for the pixel you want to set. For a 1024 x 512 display with a 12-bit color depth, 31 bits would suffice to specify all this. The data 'written' into that register would either be sent to a display adapter or sent directly to the display and decoded there. This would save over half a megabyte of memory while still retaining the memory-mapped model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So is this actually a thing or not?&lt;/p&gt;&#xA;" OwnerUserId="1902" LastActivityDate="2015-10-23T17:33:02.260" Title="Does this type of memory-mapping for a display exist?" Tags="&lt;pixels&gt;&lt;memory&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1648" PostTypeId="2" ParentId="1644" CreationDate="2015-10-23T17:26:14.500" Score="2" Body="&lt;p&gt;In your 31 bit solution, the thing most likely to happen there is that you'd give an extra bit to green and make a 32 bit solution.  In 16 bit color for instance, r and b get 5 bits while g gets 6 bits, because your eye can distinguish more greens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or maybe you'd use that extra bit to have 1 bit alpha? Not sure if that'd be very useful though...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as the rest, whether you have a gpu or not, writing to single pixels at a time is bad form.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead, you ideally want to do block operations that either fill or copy large strips of memory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For some things, like alpha blending, that isn't able to be done as a block operation very easily so is a more expensive operation in general.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a solution to this, modern computers have a processor that is closer to the video memory, so there isn't too much latency when pushing pixels back and forth across the main bus to do things like calculate alpha blended values involving the source and destination pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, instead of fewer more powerful cores, there are many less powerful cores, which helps address the situation of needing &quot;one off&quot; pixel operations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That processor I'm referring to is the gpu and the programs you write for it are called shaders (;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you are asking about doesn't exist to my knowledge and seems like a bad usage pattern, so seems unlikely to exist IMO.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-10-23T17:33:02.260" LastActivityDate="2015-10-23T17:33:02.260" CommentCount="0" />
  <row Id="1649" PostTypeId="1" AcceptedAnswerId="1650" CreationDate="2015-10-24T02:42:58.883" Score="4" ViewCount="152" Body="&lt;p&gt;I'm playing around with directX 12 and I'm having some problems loading geometry in. I have a std::vector vertices definition and I do exactly what the dx12 template does with it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  const UINT vertexBufferSize = sizeof(vertices);&#xA;&#xA;  // Create the vertex buffer resource in the GPU's default heap and copy vertex data into it using the upload heap.&#xA;  // The upload resource must not be released until after the GPU has finished using it.&#xA;  Microsoft::WRL::ComPtr&amp;lt;ID3D12Resource&amp;gt; vertexBufferUpload;&#xA;&#xA;  CD3DX12_HEAP_PROPERTIES defaultHeapProperties(D3D12_HEAP_TYPE_DEFAULT);&#xA;  CD3DX12_RESOURCE_DESC vertexBufferDesc = CD3DX12_RESOURCE_DESC::Buffer(vertexBufferSize);&#xA;  DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateCommittedResource(&#xA;     &amp;amp;defaultHeapProperties,&#xA;     D3D12_HEAP_FLAG_NONE,&#xA;     &amp;amp;vertexBufferDesc,&#xA;     D3D12_RESOURCE_STATE_COPY_DEST,&#xA;     nullptr,&#xA;     IID_PPV_ARGS(&amp;amp;mVertexBuffer)));&#xA;&#xA;  CD3DX12_HEAP_PROPERTIES uploadHeapProperties(D3D12_HEAP_TYPE_UPLOAD);&#xA;  DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateCommittedResource(&#xA;     &amp;amp;uploadHeapProperties,&#xA;     D3D12_HEAP_FLAG_NONE,&#xA;     &amp;amp;vertexBufferDesc,&#xA;     D3D12_RESOURCE_STATE_GENERIC_READ,&#xA;     nullptr,&#xA;     IID_PPV_ARGS(&amp;amp;vertexBufferUpload)));&#xA;&#xA;  mVertexBuffer-&amp;gt;SetName(L&quot;Vertex Buffer Resource&quot;);&#xA;  vertexBufferUpload-&amp;gt;SetName(L&quot;Vertex Buffer Upload Resource&quot;);&#xA;&#xA;  // Upload the vertex buffer to the GPU.&#xA;  {&#xA;     D3D12_SUBRESOURCE_DATA vertexData = {};&#xA;     vertexData.pData = reinterpret_cast&amp;lt;BYTE*&amp;gt;(vertices.data());&#xA;     vertexData.RowPitch = vertexBufferSize;&#xA;     vertexData.SlicePitch = vertexData.RowPitch;&#xA;&#xA;     UpdateSubresources(mCommandList.Get(), mVertexBuffer.Get(), vertexBufferUpload.Get(), 0, 0, 1, &amp;amp;vertexData);&#xA;&#xA;     CD3DX12_RESOURCE_BARRIER vertexBufferResourceBarrier =&#xA;        CD3DX12_RESOURCE_BARRIER::Transition(mVertexBuffer.Get(), D3D12_RESOURCE_STATE_COPY_DEST, D3D12_RESOURCE_STATE_VERTEX_AND_CONSTANT_BUFFER);&#xA;     mCommandList-&amp;gt;ResourceBarrier(1, &amp;amp;vertexBufferResourceBarrier);&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But this is what I get in the graphics debugger when it was suppossed to be a sphere. I can confirm that the data in vertices is correct.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/a6cL7.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/a6cL7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I change vertices to be what the template code had it works and renders the default cube. The only real difference mine is using a vector instead of an array.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;VertexPositionColor cubeVertices[] =&#xA;{&#xA;    { XMFLOAT3(-0.5f, -0.5f, -0.5f), XMFLOAT3(0.0f, 0.0f, 0.0f) },&#xA;    { XMFLOAT3(-0.5f, -0.5f,  0.5f), XMFLOAT3(0.0f, 0.0f, 1.0f) },&#xA;    { XMFLOAT3(-0.5f,  0.5f, -0.5f), XMFLOAT3(0.0f, 1.0f, 0.0f) },&#xA;    { XMFLOAT3(-0.5f,  0.5f,  0.5f), XMFLOAT3(0.0f, 1.0f, 1.0f) },&#xA;    { XMFLOAT3(0.5f, -0.5f, -0.5f), XMFLOAT3(1.0f, 0.0f, 0.0f) },&#xA;    { XMFLOAT3(0.5f, -0.5f,  0.5f), XMFLOAT3(1.0f, 0.0f, 1.0f) },&#xA;    { XMFLOAT3(0.5f,  0.5f, -0.5f), XMFLOAT3(1.0f, 1.0f, 0.0f) },&#xA;    { XMFLOAT3(0.5f,  0.5f,  0.5f), XMFLOAT3(1.0f, 1.0f, 1.0f) },&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="113" LastActivityDate="2016-01-16T16:32:24.677" Title="Dx12 Vertex Buffer incorrect" Tags="&lt;directx12&gt;&lt;vertex-buffer-object&gt;&lt;c++&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1650" PostTypeId="2" ParentId="1649" CreationDate="2015-10-24T03:58:23.510" Score="5" Body="&lt;p&gt;Vector has a few different semantics from static arrays. For one it's a struct containing a pointer a capacity and a length (at the very least). That means that &lt;code&gt;sizeof&lt;/code&gt; will not reflect how much data is actually stored in there (that only works on static arrays).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need to get the size in bytes of the data then you need&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;const UINT vertexBufferSize = sizeof(vertices::value_type) * vertices.size();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2016-01-16T16:32:24.677" LastActivityDate="2016-01-16T16:32:24.677" CommentCount="0" />
  <row Id="1651" PostTypeId="1" AcceptedAnswerId="1652" CreationDate="2015-10-24T15:06:21.040" Score="7" ViewCount="419" Body="&lt;p&gt;I'd like to &quot;hook&quot; into a running game, say Mario Bros, and capture each frame rendered... saving that frame to an image file. A good example of something similar is FRAPS. --Note: I don't want to capture the whole screen/desktop. I want to capture a targeted window.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have had a look at &lt;a href=&quot;https://obsproject.com/&quot; rel=&quot;nofollow&quot;&gt;OBS&lt;/a&gt; (Open Broadcasting Software) but it is not particularly fast. Don't get me wrong, it's great software, but unfortunately there is no/poor documentation, making a massive project written in c and c++ nearly inaccessible for a new-to-c++ programmer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also had a look at &lt;a href=&quot;http://gaminganywhere.org/perf.html&quot; rel=&quot;nofollow&quot;&gt;GamingAnywhere&lt;/a&gt;, but unfortunately, I am unable to get it to work, there is very little/no documentation, runs in VS2010 only and is messy (with poor variable naming). However, it is a research project and so it is understandably undocumented and messy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that this can be done with OpenGL, GDI and with Direct3D, but I am unable to find some good examples on the net. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I read that glReadlPixels (using OpenGL) can be used and I've read the &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glReadPixels.xml&quot; rel=&quot;nofollow&quot;&gt;documentation&lt;/a&gt;, but the post mentioned nothing about hooking into a running game/application graphics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Can I hook into the graphics of a game developed with OpenGL, using,&#xA;say, Direct3D? Does the library used for hooking have to be the same&#xA;as the one used by the game?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How do I hook into the game's rendered frames so that I can output&#xA;those frames to image files or to a video file? (Just some links or&#xA;brief explanation of what I need to do would be great)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;BackBuffer - I read that it is very fast accessing the BackBuffer to&#xA;retrieve the frames. Does someone have an example for me on how to&#xA;do this with the latest libraries? I have found that most examples&#xA;are out of date.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;For my purposes, is there any clearly &quot;this is faster than that&quot;? What I mean is, would, say, OpenGL, be faster for my purposes? &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;If someone knows of an open-source project (that does essentially what I need) that is actively developed and well documented, I'd love to know about it.&lt;/p&gt;&#xA;" OwnerUserId="1800" LastEditorUserId="1800" LastEditDate="2015-10-25T17:07:58.357" LastActivityDate="2015-10-25T17:07:58.357" Title="Screen capture of game video" Tags="&lt;opengl&gt;&lt;direct3d&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="2" />
  <row Id="1652" PostTypeId="2" ParentId="1651" CreationDate="2015-10-24T17:49:58.717" Score="6" Body="&lt;p&gt;It is possible to do what you describe, but I'm afraid it is not a trivial process. Actually, this will be very tied to the Operating Systems you are targeting and possibly also requiring specific tweaks for the given game/application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would start looking into &lt;a href=&quot;https://en.wikipedia.org/wiki/DLL_injection&quot;&gt;DLL injection&lt;/a&gt; techniques, which should allow, for instance, intercepting calls to the Direct3D (Windows) or OpenGL APIs, which you can then use to copy de framebuffers of the application. A quick Internet search turns up &lt;a href=&quot;http://blog.opensecurityresearch.com/2013/01/windows-dll-injection-basics.html&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;http://www.evilsocket.net/2015/05/01/dynamically-inject-a-shared-library-into-a-running-process-on-androidarm/&quot;&gt;this&lt;/a&gt; with detailed explanation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I once wrote a small-scale OpenGL interceptor on MacOS by injecting a custom shared module, but it was under very controlled circumstances and making a lot of assumptions, such as having root privileges in the system, so for a production tool, it would be a lot more complicated. You can find another very interesting project &lt;a href=&quot;https://graphics.stanford.edu/~mdfisher/D3D9Interceptor.html&quot;&gt;here&lt;/a&gt; about intercepting the D3D API to install an automated AI bot on Starcraft, which should touch on very similar concepts to what you are looking for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you manage to intercept the underlying rendering API, you could then perhaps hook your code into each &lt;code&gt;SwapBuffers&lt;/code&gt; or equivalent call to just copy the previous framebuffer before the swap, then save it (this is where something like &lt;code&gt;glReadPixels&lt;/code&gt; would come into play).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Copying and saving the framebuffer in an efficient manner is also challenging. The easy way is to just dump each frame as an uncompressed RGB image, but then you'll wind up with hundreds of gigabytes of data for just a couple of minutes of gameplay, so unless you have a nice HDD array sitting at the corner of your table &lt;code&gt;;)&lt;/code&gt;, you'll need to look into compressing the frames somehow. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The downside of compressing the frames now is that it takes a lot of processing, so a naive approach might turn the once interactive application you are trying to record into an interactive slide-show &lt;code&gt;:(&lt;/code&gt;. So at this stage you'd have to start thinking about optimizations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Don't know of any projects out there providing a library for efficient frame capture for games, but this would certainly be something nice to have! I think one thing that might be holding such project back is, like I mentioned at the beginning, that this is a very system dependent thing, so cases of use will most likely be limited to a single platform or OS.&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="54" LastEditDate="2015-10-24T17:57:52.740" LastActivityDate="2015-10-24T17:57:52.740" CommentCount="6" />
  <row Id="1656" PostTypeId="1" AcceptedAnswerId="1659" CreationDate="2015-10-25T21:03:06.477" Score="4" ViewCount="95" Body="&lt;p&gt;I'm trying to use &lt;code&gt;glMultiDrawArraysIndirect&lt;/code&gt; to render a batch of object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my example I have a cube and a sphere &lt;strong&gt;(in this order)&lt;/strong&gt; in a &lt;code&gt;VBO&lt;/code&gt; and a &lt;code&gt;GL_SHADER_STORAGE_BUFFER (SSBO)&lt;/code&gt; that contains matrix transforms as well as a &lt;code&gt;GL_DRAW_INDIRECT_BUFFER (DIB)&lt;/code&gt; that contains the draw commands.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The allocation of both the &lt;code&gt;SSBO&lt;/code&gt; and &lt;code&gt;DIB&lt;/code&gt; are managed by a custom class.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can currently render both objects successfully, each using their own transform. The problem I'm encountering is rendering just the sphere by specifying an offset. As per the OpenGL spec:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;If a buffer is bound to the &lt;code&gt;GL_DRAW_INDIRECT_BUFFER&lt;/code&gt; binding at the&#xA;  time of a call to &lt;strong&gt;glMultiDrawArraysIndirect&lt;/strong&gt;, &lt;em&gt;indirect&lt;/em&gt; is interpreted&#xA;  as an offset, in basic machine units, into that buffer and the&#xA;  parameter data is read from the buffer rather than from client memory.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;but doing the following code to render results in nothing being rendered:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Calculates the offset &amp;amp; size and calls BindBufferRange &#xA;this-&amp;gt;_indrMatrixBuffer.BindBufferRange(0, 1, 1);&#xA;// A simple call to glBindBuffer, binding to GL_DRAW_INDIRECT_BUFFER&#xA;this-&amp;gt;_indrBuffer.BindBuffer();&#xA;// Render&#xA;glMultiDrawArraysIndirect(GL_TRIANGLES, (void*)sizeof(DAICmd), 1, 0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;The &lt;code&gt;DAICmd&lt;/code&gt; is a struct of four &lt;code&gt;unsigned int&lt;/code&gt; (as per the &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/glMultiDrawArraysIndirect.xhtml&quot; rel=&quot;nofollow&quot;&gt;spec&lt;/a&gt;)&lt;/sub&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At first I though it might be my custom class but calling &lt;code&gt;glMultiDrawArraysIndirect(GL_TRIANGLES, 0, 1, 0);&lt;/code&gt; and keeping the matrix buffer offset renders the cube in the spheres position!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I missing to get this to work?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Extra notes&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Both the &lt;code&gt;SSBO&lt;/code&gt; &amp;amp; &lt;code&gt;DIB&lt;/code&gt; are mapped using the flags &lt;code&gt;GL_MAP_WRITE_BIT, GL_MAP_PERSISTENT_BIT and GL_MAP_COHERENT_BIT&lt;/code&gt; and are created with the same flags plus &lt;code&gt;GL_DYNAMIC_STORAGE_BIT&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;I'm using OpenGL 4.5&lt;/li&gt;&#xA;&lt;li&gt;Using &lt;code&gt;glMultiDrawArraysIndirect(GL_TRIANGLES, (void*)sizeof(DAICmd), 1, 0);&lt;/code&gt; does not produce an error when using &lt;code&gt;glGetError()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Using KHR_debug produces no errors either&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="214" LastEditorUserId="214" LastEditDate="2015-10-26T09:10:22.040" LastActivityDate="2015-10-26T16:24:09.740" Title="glMultiDrawArraysIndirect does not work when specifying an offset" Tags="&lt;opengl&gt;&lt;rendering&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1657" PostTypeId="1" CreationDate="2015-10-26T01:10:09.947" Score="2" ViewCount="108" Body="&lt;p&gt;3D math covers vector algebra and matrix algebra, which are easily calculated on paper, but not so easy when coding them effectively. I only leant some basic algorithms on matrix in my Data Structure course, which is not enough when it comes to accurate and effective operations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there some books on this topic?&lt;/p&gt;&#xA;" OwnerUserId="1886" LastActivityDate="2015-10-26T07:07:56.683" Title="Is there some book about how to design a 3D math lib?" Tags="&lt;algorithm&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2015-10-28T05:37:33.750" />
  <row Id="1658" PostTypeId="2" ParentId="1657" CreationDate="2015-10-26T06:33:58.150" Score="3" Body="&lt;p&gt;Books: &lt;a href=&quot;http://www.essentialmath.com/book.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.essentialmath.com/book.htm&lt;/a&gt; is the one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best algos on the real 3D topics (and some tricky 2D like image resize and premult-alpha ), explained and with source most of the time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The website is very nice too, the &quot;tutorial&quot; full of gems&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Adding &lt;a href=&quot;http://www.geometrictools.com/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.geometrictools.com/index.html&lt;/a&gt; which is the most complete source code repository on the topic, with loads of very precise algos implemented.&lt;/p&gt;&#xA;" OwnerUserId="212" LastEditorUserId="212" LastEditDate="2015-10-26T07:07:56.683" LastActivityDate="2015-10-26T07:07:56.683" CommentCount="1" />
  <row Id="1659" PostTypeId="2" ParentId="1656" CreationDate="2015-10-26T16:24:09.740" Score="2" Body="&lt;p&gt;The problem was my class handling the population fo the &lt;code&gt;draw indirect commands&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each time I added a new command to the indirect buffer I also incremented the &lt;code&gt;baseInstance&lt;/code&gt;, so the cube had &lt;code&gt;baseInstance = 0&lt;/code&gt; and the sphere had &lt;code&gt;baseInstance = 1&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason why it doesn't render the sphere is because &lt;code&gt;glMultiDrawArraysIndirect&lt;/code&gt; pretty much evaluates as &lt;code&gt;glDrawArraysInstancedBaseInstance(mode,  cmd-&amp;gt;first,  cmd-&amp;gt;count,  cmd-&amp;gt;instanceCount, cmd-&amp;gt;baseInstance);&lt;/code&gt; (as per the &lt;a href=&quot;https://www.opengl.org/wiki/GLAPI/glMultiDrawArraysIndirect&quot; rel=&quot;nofollow&quot;&gt;OGL wiki&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The drawID is calculated as &lt;code&gt;(gl_InstanceID / divisor) +&lt;/code&gt;&lt;strong&gt;&lt;code&gt;baseInstance&lt;/code&gt;&lt;/strong&gt; (&lt;a href=&quot;https://www.opengl.org/wiki/GLAPI/glDrawArraysInstancedBaseInstance&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;), which means that the drawID is out of range when trying to render with an offset, as the sphere now appears as the first element in the indirect buffer.&lt;/p&gt;&#xA;" OwnerUserId="214" LastActivityDate="2015-10-26T16:24:09.740" CommentCount="0" />
  <row Id="1660" PostTypeId="5" CreationDate="2015-10-26T20:29:26.973" Score="0" Body="" OwnerUserId="-1" LastEditorUserId="-1" LastEditDate="2015-10-26T20:29:26.973" LastActivityDate="2015-10-26T20:29:26.973" CommentCount="0" />
  <row Id="1661" PostTypeId="4" CreationDate="2015-10-26T20:29:26.973" Score="0" Body="For questions related to textures: procedural generation, encodings, aspect characterisation, filtering, mapping, storage..." OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2015-10-27T08:33:48.850" LastActivityDate="2015-10-27T08:33:48.850" CommentCount="0" />
  <row Id="1662" PostTypeId="1" AcceptedAnswerId="2465" CreationDate="2015-10-27T06:52:32.947" Score="8" ViewCount="107" Body="&lt;p&gt;I'm exploring isosurface algorithms on GPU for a bachelor's project (specifically concentrating on just binary in/out voxel data rather than real-valued fields). So I have a CPU implementation of good old marching cubes up and running in OpenFrameworks, and now at the stage of trying to port it to GLSL compute shaders, and considering the pitfalls before I dive in. I've only written vert and frag shaders before so it's all new to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first issue is how to efficiently use a lookup table across dozens or hundreds of threads in a workgroup? I understand a GPU has different kinds of memory for different tasks but not fully sure on how each operates or which type to use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Paul Bourke's classic copypasta table is a 256*16 array so if using a scalar byte type this can presumably be packed into a 4kb texture or SSBO.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is, how to stop the different threads from tripping each other up? Many cubes in each work group can potentially have the same configuration therefore trying to access the same location in the buffer at the same time. Is there a workaround or optimization to deal with this?&lt;/p&gt;&#xA;" OwnerUserId="1937" LastEditorUserId="385" LastEditDate="2015-10-28T13:29:36.397" LastActivityDate="2016-05-19T22:40:47.670" Title="Optimal memory access when using lookup tables on GPU?" Tags="&lt;opengl&gt;&lt;algorithm&gt;&lt;gpgpu&gt;&lt;mesh&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1663" PostTypeId="1" CreationDate="2015-10-27T08:05:36.913" Score="2" ViewCount="120" Body="&lt;p&gt;Previously, to render a bunch of quads, I was simply using a few uniforms (one for a model matrix and another for the texture layer ID). However, I'd rather not have to loop through each quad and set both uniforms each time, each frame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I went ahead and looked for better alternatives with which I could render everything with a single call. Now I'm using instanced rendering:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// In VAO definition            &#xA;glGenBuffers(1, &amp;amp;instanceVBO);&#xA;glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);&#xA;glBufferData(GL_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;glEnableVertexAttribArray(3);&#xA;glVertexAttribPointer(3, 4, GL_FLOAT, GL_FALSE, sizeof(RenderingData), nullptr);&#xA;glVertexAttribDivisor(3, 1);&#xA;glEnableVertexAttribArray(4);&#xA;glVertexAttribPointer(4, 4, GL_FLOAT, GL_FALSE, sizeof(RenderingData), (GLvoid*)(sizeof(glm::vec4)));&#xA;glVertexAttribDivisor(4, 1);&#xA;glEnableVertexAttribArray(5);&#xA;glVertexAttribPointer(5, 4, GL_FLOAT, GL_FALSE, sizeof(RenderingData), (GLvoid*)(sizeof(glm::vec4) * 2));&#xA;glVertexAttribDivisor(5, 1);&#xA;glEnableVertexAttribArray(6);&#xA;glVertexAttribPointer(6, 4, GL_FLOAT, GL_FALSE, sizeof(RenderingData), (GLvoid*)(sizeof(glm::vec4) * 3));&#xA;glVertexAttribDivisor(6, 1);&#xA;glEnableVertexAttribArray(7);&#xA;glVertexAttribIPointer(7, 1, GL_UNSIGNED_INT, sizeof(RenderingData), (GLvoid*)(sizeof(glm::mat4)));&#xA;glVertexAttribDivisor(7, 1);&#xA;&#xA;// In quad adding function&#xA;instanceData.push_back(quad-&amp;gt;data);&#xA;glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);&#xA;glBufferData(GL_ARRAY_BUFFER, sizeof(RenderingData) * quads.size(), &amp;amp;instanceData[0], GL_DYNAMIC_DRAW);&#xA;&#xA;// Rendering&#xA;glDrawElementsInstanced(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0, quads.size());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm using 2 VBOs: one for per-vertex data (position, normal, etc vectors) and another for per-instance data (model matrix and texture layer ID).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is the following:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9UAOQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9UAOQ.png&quot; alt=&quot;middle one is way too stretched, whereas bottom one&amp;#39;s UV are incorrect, since they aren&amp;#39;t filling the whole quad&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, the middle and bottom images aren't rendering correctly (middle one is way too stretched, whereas bottom one's UV are incorrect, since they aren't filling the whole quad), however, I'm 98% sure both my vertex and instance data are correct, since when I was using the previously mentioned uniforms, they were rendering correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I &lt;strong&gt;&lt;em&gt;might&lt;/em&gt;&lt;/strong&gt; 've spotted the problem: the indices. Somehow, if I change my &lt;code&gt;glDrawElementsInstanced&lt;/code&gt;'s count value to say, 12 indices, the middle one renders correctly, whereas the other 2, do not (see picture below). Same thing happens if I change them to 18 (the last one renders correctly and the other 2, do not).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2QSBs.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2QSBs.png&quot; alt=&quot;middle one renders correctly, whereas the other 2, do not&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What might the problem be?&lt;/p&gt;&#xA;" OwnerUserId="1938" LastEditorUserId="127" LastEditDate="2015-11-01T14:29:34.893" LastActivityDate="2015-11-01T14:29:34.893" Title="Incorrect instanced rendering" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;c++&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="1664" PostTypeId="1" AcceptedAnswerId="1665" CreationDate="2015-10-27T21:03:54.563" Score="5" ViewCount="78" Body="&lt;p&gt;I'm developing with OpenGL 4.1 and have good understanding of the modern pipeline. I was thinking on doing it with GL_LINE_LOOP for each face, but I think this will require several draw calls which certainly is not optimal. What could be a better approach?&lt;/p&gt;&#xA;" OwnerUserId="116" LastActivityDate="2015-10-27T22:11:26.847" Title="How to render the wireframe of a cube without the diagonals on the faces?" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1665" PostTypeId="2" ParentId="1664" CreationDate="2015-10-27T22:11:26.847" Score="5" Body="&lt;p&gt;Things like that are usually done using an &lt;a href=&quot;https://www.opengl.org/wiki/Vertex_Specification#Index_buffers&quot;&gt;index buffer&lt;/a&gt;. The idea is that you have you have two buffers: One for all the vertices and one that determines the topology of what to draw. Then you can draw all the lines at once with a single &lt;a href=&quot;https://www.opengl.org/wiki/GLAPI/glDrawElements&quot;&gt;glDrawElements&lt;/a&gt; call, using GL_LINES as mode.&lt;/p&gt;&#xA;" OwnerUserId="528" LastActivityDate="2015-10-27T22:11:26.847" CommentCount="1" />
  <row Id="1666" PostTypeId="1" CreationDate="2015-10-28T06:18:44.837" Score="8" ViewCount="129" Body="&lt;p&gt;I can't understand math equations. I'm a graphic designer.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;What is &quot;importance sampling&quot;?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;What is &quot;multiple importance sampling&quot;?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Could you explain easily, using illustrations and no math equations? What is the difference between &quot;importance sampling&quot; and &quot;multiple importance sampling&quot;?&lt;/p&gt;&#xA;" OwnerUserId="1944" LastEditorUserId="231" LastEditDate="2015-10-28T10:11:30.387" LastActivityDate="2015-10-28T13:37:44.837" Title="What is the difference between importance sampling and mutiple importance sampling?" Tags="&lt;raytracing&gt;&lt;sampling&gt;&lt;importance-sampling&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1667" PostTypeId="2" ParentId="1666" CreationDate="2015-10-28T10:12:19.223" Score="10" Body="&lt;p&gt;When shading a point on an opaque surface, you need to gather incoming light and weight it with the bidirectional reflectance distribution function (BRDF) of the material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The naive approach is to distribute samples equally over the hemisphere and probe all directions equally for incoming light. This is called &lt;strong&gt;uniform sampling&lt;/strong&gt; (Fig. 1). While this works in most cases, it can take a long time to converge towards the correct result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sG2z5.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sG2z5.jpg&quot; alt=&quot;Uniform sampling on the hemisphere&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Figure 1:&lt;/strong&gt; Uniform sampling on the hemisphere&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To speed things up, let's consider the properties of the surface in question. A diffuse surface needs to take light from all directions on the hemisphere into account, where light coming from flat angles has less influence. A perfect mirror only needs a single direction according to the law of reflection. &lt;strong&gt;Importance sampling&lt;/strong&gt; takes this into account and distributes samples where light is expected to have a greater influence, judging from the BRDF. For the diffuse surface, a cosine sampling would be used (Fig. 2), resulting in fewer samples at flat angles that presumably have a lower influence. For the perfect mirror, only a single ray is shot because all other directions have zero influence anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uJksp.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uJksp.jpg&quot; alt=&quot;Cosine sampling on the hemisphere&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Figure 2:&lt;/strong&gt; Cosine sampling on the hemisphere&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, the BRDF is not the only thing that can be taken into account. When you have known light sources with strong intensity, it is often desirable to direct more rays towards these light sources, as these directions presumably have a greater impact on the result (i.e. there is more light coming from these directions). The idea to combine various different importance sampling strategies into one is called &lt;strong&gt;multiple importance sampling&lt;/strong&gt;. This technique is not limited to the given example of BRDF + known lights, but can be used with any kind of sensible heuristic you can come up with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Images were generated using Holger Dammertz' &lt;a href=&quot;http://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html&quot;&gt;Hammersley Points on the Hemisphere&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-10-28T13:37:44.837" LastActivityDate="2015-10-28T13:37:44.837" CommentCount="0" />
  <row Id="1669" PostTypeId="1" CreationDate="2015-10-30T14:08:45.240" Score="8" ViewCount="107" Body="&lt;p&gt;Why for perfect reflections a surface must have G2 continuity (class A surface)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like a mathematical answer.&lt;/p&gt;&#xA;" OwnerUserId="1636" LastActivityDate="2015-11-03T13:30:51.730" Title="Why for perfect reflections a surface must have G2 continuity?" Tags="&lt;rendering&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="1670" PostTypeId="2" ParentId="1669" CreationDate="2015-10-30T20:11:09.003" Score="5" Body="&lt;p&gt;What you see on reflects is the n-continuity of normals, which are the derivative of positions. -&gt; a G1-only surface would have G0-only normal field, i.e.,  with sudden change of gradient in the normals (and thus, the reflects), that the eyes can notice. G2 surfaces have G1 normals fields, which is smooth enough for your eyes.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2015-10-30T20:11:09.003" CommentCount="0" />
  <row Id="1671" PostTypeId="1" CreationDate="2015-10-31T06:50:49.570" Score="3" ViewCount="34" Body="&lt;p&gt;I am pursuing my research in Texture Interpolation. After digging deep into it, I'm stuck at a point. It is proven that textures can be modeled as Markov Random Fields and that each texture can be synthesized from a collection of neighborhoods. How exactly do we get these neighborhoods that can characterize a texture and if we have them, how can we synthesize the texture back ? And, how does MRF help in any of the above ? Can anyone please provide me with a explanation that helps a layman understand concepts above ?&lt;/p&gt;&#xA;" OwnerUserId="1957" LastActivityDate="2015-10-31T06:50:49.570" Title="MRF and Textures" Tags="&lt;texture&gt;&lt;interpolation&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="1672" PostTypeId="1" CreationDate="2015-10-31T11:10:42.477" Score="1" ViewCount="48" Body="&lt;p&gt;How are objects moved smoothly from point A to point B given  matrix transformations that describe next position? &lt;/p&gt;&#xA;" OwnerUserId="1958" LastEditorUserId="137" LastEditDate="2015-10-31T21:10:16.213" LastActivityDate="2015-10-31T21:10:16.213" Title="How is smooth animation accomplished?" Tags="&lt;transformations&gt;&lt;animation&gt;" AnswerCount="0" CommentCount="4" ClosedDate="2015-10-31T14:54:48.140" />
  <row Id="1673" PostTypeId="1" AcceptedAnswerId="1680" CreationDate="2015-11-01T07:12:43.850" Score="2" ViewCount="73" Body="&lt;p&gt;I had posted about this issue in a &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/1663/incorrect-instanced-rendering&quot;&gt;previous question&lt;/a&gt;, however, I thought instancing was causing it, whereas even using normal uniforms or a SSBO (which's what I'm using right now) causes it, which means the problem is elsewhere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been trying to figure out the cause, and I might've found out something: only the first quad's vertex data seems to be uploaded / used, somehow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, if I were to place random position and texcoord vectors to my other quads, they would still render somewhat correctly. I said &lt;strong&gt;somewhat&lt;/strong&gt; because the output is the following: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ULWbw.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ULWbw.png&quot; alt=&quot;middle one is way too stretched and the last one&amp;#39;s UV coords are wrong&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The middle one is way too stretched and the last one's UV coords are wrong, since they aren't filling the whole quad, as you can see by the white background. So long as the first quad's were right, and whenever I changed the first quad's vertex data, it was propagated to the other ones as well, leading me to believe the other quads' vertex data are getting ignored and the first quad is being re-used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// VAO definition&#xA;glGenVertexArrays(1, &amp;amp;VAO);&#xA;glBindVertexArray(VAO);&#xA;/*** VBO ***/&#xA;    glGenBuffers(1, &amp;amp;VBO);&#xA;    glBindBuffer(GL_ARRAY_BUFFER, VBO);&#xA;    glBufferData(GL_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;/*** Vertex Format ***/&#xA;    //Position Vertex&#xA;    glEnableVertexAttribArray(0);&#xA;    glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr);&#xA;    //Normal Vertex&#xA;    glEnableVertexAttribArray(1);&#xA;    glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), (GLvoid*)(sizeof(glm::vec3)));&#xA;    //Texture Coordinate Vertex&#xA;    glEnableVertexAttribArray(2);&#xA;    glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, sizeof(Vertex), (GLvoid*)(sizeof(glm::vec3) * 2));&#xA;/*** EBO ***/&#xA;    glGenBuffers(1, &amp;amp;EBO);&#xA;    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);&#xA;    glBufferData(GL_ELEMENT_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;/*** SSBO ***/&#xA;    glGenBuffers(1, &amp;amp;SSBO);&#xA;    glBindBuffer(GL_SHADER_STORAGE_BUFFER, SSBO);&#xA;    glBufferData(GL_SHADER_STORAGE_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;&#xA;&#xA;// Quad adding function&#xA;auto iter = std::find(quads.begin(), quads.end(), quad);&#xA;&#xA;if(iter == quads.end())&#xA;{&#xA;    quads.push_back(quad);&#xA;&#xA;    for(unsigned int i = 0; i &amp;lt; 4; i++)&#xA;        vertices.push_back(quad-&amp;gt;vertices[i]);&#xA;&#xA;    GLuint start = (indices.empty()) ? 0 : indices[indices.size() - 1] + 1;&#xA;&#xA;    GLuint newIndices[] = {&#xA;        start, start + 1, start + 2,&#xA;        start, start + 2, start + 3&#xA;    };&#xA;&#xA;    for(unsigned int i = 0; i &amp;lt; 6; i++)&#xA;        indices.push_back(newIndices[i]);&#xA;&#xA;    glBindBuffer(GL_ARRAY_BUFFER, VBO);&#xA;    glBufferData(GL_ARRAY_BUFFER, sizeof(Vertex) * vertices.size(), &amp;amp;vertices[0], GL_DYNAMIC_DRAW);&#xA;    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);&#xA;    glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(GLuint) * indices.size(), &amp;amp;indices[0], GL_DYNAMIC_DRAW);&#xA;&#xA;    instanceData.push_back(quad-&amp;gt;data);&#xA;    glBindBuffer(GL_SHADER_STORAGE_BUFFER, SSBO);&#xA;    glBufferData(GL_SHADER_STORAGE_BUFFER, sizeof(RenderingData) * quads.size(), &amp;amp;instanceData[0], GL_DYNAMIC_DRAW);&#xA;    glBindBufferRange(GL_SHADER_STORAGE_BUFFER, 0, SSBO, 0, sizeof(RenderingData) * quads.size());&#xA;}&#xA;&#xA;&#xA;// Rendering&#xA;program-&amp;gt;Use();&#xA;&#xA;program-&amp;gt;SetStorageBlock(&quot;RenderingData&quot;, 0);&#xA;&#xA;glActiveTexture(GL_TEXTURE0);&#xA;glBindTexture(GL_TEXTURE_2D_ARRAY, texture-&amp;gt;GetTexture());&#xA;program-&amp;gt;SetInteger(&quot;diffuseTextureArray&quot;, 0);&#xA;&#xA;glDrawElementsInstanced(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0, quads.size());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;NOTE: I also tried to limit the vertex data uploading to the first quad, effectively making it the only one with valid data, a part from the per-instance data (&lt;em&gt;RenderingData&lt;/em&gt; struct: a model matrix and an uint), which were all fed to the SSBO. This still lead to 3 quads being rendered, which shouldn't be the case under normal circumstances, right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I made a sample code which replicates the problem; it should render a green quad and a red quad, however, it just renders 2 red quads. Also, even if you try to change the second quads' attributes (even like placing all 0s), it'll still just render. Somehow only the first quads' vertices are getting read and used for each subsequent quad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;C++ code:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &amp;lt;fstream&amp;gt;&#xA;#include &amp;lt;string&amp;gt;&#xA;#include &amp;lt;sstream&amp;gt;&#xA;#include &amp;lt;vector&amp;gt;&#xA;#define GLEW_STATIC&#xA;#include &quot;GL/glew.h&quot;&#xA;#include &quot;GLFW/glfw3.h&quot;&#xA;#include &amp;lt;IL/il.h&amp;gt;&#xA;#include &amp;lt;IL/ilu.h&amp;gt;&#xA;#include &amp;lt;glm/glm.hpp&amp;gt;&#xA;#include &amp;lt;glm/gtc/matrix_transform.hpp&amp;gt;&#xA;#include &amp;lt;glm/gtc/type_ptr.hpp&amp;gt;&#xA;&#xA;struct Vertex{&#xA;    glm::vec2 pos, tex;&#xA;    glm::vec3 color;&#xA;};&#xA;&#xA;struct Quad{&#xA;    std::vector&amp;lt;Vertex&amp;gt; vertices;&#xA;    glm::mat4 model;&#xA;&#xA;    Quad(glm::vec2 pos, glm::vec2 size, const std::vector&amp;lt;Vertex&amp;gt; &amp;amp;v){&#xA;        vertices = v;&#xA;        model = glm::mat4();&#xA;        model = glm::translate(model, glm::vec3(pos, 0.f));&#xA;        model = glm::scale(model, glm::vec3(size, 1.f));&#xA;    }&#xA;};&#xA;&#xA;class Renderer{&#xA;    private:&#xA;        GLuint VAO, VBO, instanceVBO, EBO;&#xA;        std::vector&amp;lt;Quad*&amp;gt; quads;&#xA;        std::vector&amp;lt;Vertex&amp;gt; vertices;&#xA;        std::vector&amp;lt;glm::mat4&amp;gt; models;&#xA;        std::vector&amp;lt;GLuint&amp;gt; indices;&#xA;&#xA;    public:&#xA;        Renderer(){&#xA;            /*** VAO ***/&#xA;            glGenVertexArrays(1, &amp;amp;VAO);&#xA;            glBindVertexArray(VAO);&#xA;            /*** Per-vertex VBO ***/&#xA;            glGenBuffers(1, &amp;amp;VBO);&#xA;            glBindBuffer(GL_ARRAY_BUFFER, VBO);&#xA;            glBufferData(GL_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;            glEnableVertexAttribArray(0);   // Position&#xA;            glVertexAttribPointer(0, 2, GL_FLOAT, GL_FALSE, sizeof(Vertex), nullptr);&#xA;            glEnableVertexAttribArray(1);   // Tex Coord&#xA;            glVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, sizeof(Vertex), (GLvoid*)(sizeof(glm::vec2)));&#xA;            glEnableVertexAttribArray(2);   // Color&#xA;            glVertexAttribPointer(2, 3, GL_FLOAT, GL_FALSE, sizeof(Vertex), (GLvoid*)(sizeof(glm::vec2) * 2));&#xA;            /*** Per-instance VBO ***/&#xA;            glGenBuffers(1, &amp;amp;instanceVBO);&#xA;            glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);&#xA;            glBufferData(GL_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;            glEnableVertexAttribArray(3);   // Model matrix - row1&#xA;            glVertexAttribPointer(3, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), nullptr);&#xA;            glVertexAttribDivisor(3, 1);&#xA;            glEnableVertexAttribArray(4);   // Model matrix - row2&#xA;            glVertexAttribPointer(4, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (GLvoid*)(sizeof(glm::vec4)));&#xA;            glVertexAttribDivisor(4, 1);&#xA;            glEnableVertexAttribArray(5);   // Model matrix - row3&#xA;            glVertexAttribPointer(5, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (GLvoid*)(sizeof(glm::vec4) * 2));&#xA;            glVertexAttribDivisor(5, 1);&#xA;            glEnableVertexAttribArray(6);   // Model matrix - row4&#xA;            glVertexAttribPointer(6, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (GLvoid*)(sizeof(glm::vec4) * 3));&#xA;            glVertexAttribDivisor(6, 1);&#xA;            /*** EBO ***/&#xA;            glGenBuffers(1, &amp;amp;EBO);&#xA;            glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);&#xA;            glBufferData(GL_ELEMENT_ARRAY_BUFFER, 0, nullptr, GL_DYNAMIC_DRAW);&#xA;        }&#xA;        ~Renderer(){&#xA;            glDeleteBuffers(1, &amp;amp;VBO);&#xA;            glDeleteBuffers(1, &amp;amp;instanceVBO);&#xA;            glDeleteBuffers(1, &amp;amp;EBO);&#xA;            glDeleteVertexArrays(1, &amp;amp;VAO);&#xA;        }&#xA;        void AddQuad(Quad *const quad){&#xA;            quads.push_back(quad);&#xA;&#xA;            for(unsigned int i = 0; i &amp;lt; 4; i++)&#xA;                vertices.push_back(quad-&amp;gt;vertices[i]);&#xA;&#xA;            models.push_back(quad-&amp;gt;model);&#xA;&#xA;            GLuint start = ((indices.empty()) ? 0 : indices[indices.size() - 1] + 1);&#xA;&#xA;            GLuint newIndices[] = {&#xA;                start,  start + 1,  start + 2,&#xA;                start,  start + 3,  start + 2&#xA;            };&#xA;&#xA;            for(unsigned int i = 0; i &amp;lt; 6; i++)&#xA;                indices.push_back(newIndices[i]);&#xA;&#xA;            glBindBuffer(GL_ARRAY_BUFFER, VBO);&#xA;            glBufferData(GL_ARRAY_BUFFER, sizeof(Vertex) * vertices.size(), &amp;amp;vertices[0], GL_DYNAMIC_DRAW);&#xA;&#xA;            glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);&#xA;            glBufferData(GL_ARRAY_BUFFER, sizeof(glm::mat4) * quads.size(), &amp;amp;models[0], GL_DYNAMIC_DRAW);&#xA;&#xA;            glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);&#xA;            glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(GLuint) * indices.size(), &amp;amp;indices[0], GL_DYNAMIC_DRAW);&#xA;        }&#xA;        void Render(GLuint &amp;amp;program){&#xA;            glUseProgram(program);&#xA;&#xA;            glDrawElementsInstanced(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0, quads.size());&#xA;        }&#xA;        void CheckQuads(){&#xA;            for(unsigned int i = 0; i &amp;lt; quads.size(); i++)&#xA;            {&#xA;                std::cout &amp;lt;&amp;lt; &quot;Quad n.&quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;\n&quot;;&#xA;                for(unsigned int j = 0; j &amp;lt; 4; j++)&#xA;                {&#xA;                    std::cout &amp;lt;&amp;lt; &quot;\tVert &quot; &amp;lt;&amp;lt; j &amp;lt;&amp;lt; &quot;:\n&quot;;&#xA;                    std::cout &amp;lt;&amp;lt; &quot;\t&quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].pos.x &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].pos.y &amp;lt;&amp;lt; &quot;\n&quot;;&#xA;                    std::cout &amp;lt;&amp;lt; &quot;\t&quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].tex.x &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].tex.y &amp;lt;&amp;lt; &quot;\n&quot;;&#xA;                    std::cout &amp;lt;&amp;lt; &quot;\t&quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].color.r &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].color.g &amp;lt;&amp;lt; &quot; &quot; &amp;lt;&amp;lt; quads[i]-&amp;gt;vertices[j].color.b &amp;lt;&amp;lt; &quot;\n&quot;;&#xA;                }&#xA;            }&#xA;        }&#xA;};&#xA;&#xA;GLuint GenerateShader(const std::string &amp;amp;filename, GLenum shaderType)&#xA;{&#xA;    GLuint shader;&#xA;    std::ifstream file(filename.c_str());&#xA;    std::stringstream ss;&#xA;    ss &amp;lt;&amp;lt; file.rdbuf();&#xA;    file.close();&#xA;    const GLchar *source = ss.str().c_str();&#xA;    shader = glCreateShader(shaderType);&#xA;    glShaderSource(shader, 1, &amp;amp;source, nullptr);&#xA;    glCompileShader(shader);&#xA;    return shader;&#xA;}&#xA;&#xA;int main()&#xA;{&#xA;    /*** Window + Context ***/&#xA;    glfwInit();&#xA;    glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3);&#xA;    glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3);&#xA;    glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);&#xA;    glfwWindowHint(GLFW_RESIZABLE, GL_FALSE);&#xA;    GLFWwindow *window = glfwCreateWindow(640, 480, &quot;OpenGL Incorrect Rendering Test&quot;, nullptr, nullptr);&#xA;    glfwMakeContextCurrent(window);&#xA;    glewExperimental = GL_TRUE;&#xA;    glewInit();&#xA;    glViewport(0, 0, 640, 480);&#xA;    glClearColor(0.f, 0.f, 0.f, 1.f);&#xA;&#xA;    glm::mat4 projection = glm::ortho(0.f, 640.f, 480.f, 0.f, -1.f, 1.f);&#xA;&#xA;    /*** Shader Program ***/&#xA;    GLuint program = glCreateProgram();&#xA;    glAttachShader(program, GenerateShader(&quot;vertex_shader.glsl&quot;, GL_VERTEX_SHADER));&#xA;    glAttachShader(program, GenerateShader(&quot;fragment_shader.glsl&quot;, GL_FRAGMENT_SHADER));&#xA;    glLinkProgram(program);&#xA;&#xA;    glUseProgram(program);&#xA;    glUniformMatrix4fv(glGetUniformLocation(program, &quot;projection&quot;), 1, GL_FALSE, glm::value_ptr(projection));&#xA;    /*** Renderer ***/&#xA;    Renderer *renderer = new Renderer();&#xA;    // Quad 1&#xA;    Vertex v1 = {glm::vec2(0.f, 0.f), glm::vec2(0.f, 0.f), glm::vec3(1.f, 0.f, 0.f)};&#xA;    Vertex v2 = {glm::vec2(1.f, 0.f), glm::vec2(0.f, 0.f), glm::vec3(1.f, 0.f, 0.f)};&#xA;    Vertex v3 = {glm::vec2(1.f, 1.f), glm::vec2(0.f, 0.f), glm::vec3(1.f, 0.f, 0.f)};&#xA;    Vertex v4 = {glm::vec2(0.f, 1.f), glm::vec2(0.f, 0.f), glm::vec3(1.f, 0.f, 0.f)};&#xA;    renderer-&amp;gt;AddQuad(new Quad(glm::vec2(300, 10), glm::vec2(150, 150), {v1, v2, v3, v4}));&#xA;    // Quad 2&#xA;    Vertex v5 = {glm::vec2(0.f, 0.f), glm::vec2(0.f, 0.f), glm::vec3(0.f, 1.f, 0.f)};&#xA;    Vertex v6 = {glm::vec2(1.f, 0.f), glm::vec2(0.f, 0.f), glm::vec3(0.f, 1.f, 0.f)};&#xA;    Vertex v7 = {glm::vec2(1.f, 1.f), glm::vec2(0.f, 0.f), glm::vec3(0.f, 1.f, 0.f)};&#xA;    Vertex v8 = {glm::vec2(0.f, 1.f), glm::vec2(0.f, 0.f), glm::vec3(0.f, 1.f, 0.f)};&#xA;    renderer-&amp;gt;AddQuad(new Quad(glm::vec2(30, 10), glm::vec2(150, 150), {v5, v6, v7, v8}));&#xA;&#xA;    while(!glfwWindowShouldClose(window))&#xA;    {&#xA;        glfwPollEvents();&#xA;&#xA;        if(glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS)&#xA;            glfwSetWindowShouldClose(window, true);&#xA;&#xA;        glClear(GL_COLOR_BUFFER_BIT);&#xA;&#xA;        renderer-&amp;gt;Render(program);&#xA;&#xA;        glfwSwapBuffers(window);&#xA;    }&#xA;&#xA;    delete renderer;&#xA;&#xA;    glfwDestroyWindow(window);&#xA;    glfwTerminate();&#xA;&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;vertex_shader.glsl:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;layout (location = 0) in vec2 position;&#xA;layout (location = 1) in vec2 texCoord;&#xA;layout (location = 2) in vec3 color;&#xA;layout (location = 3) in mat4 model;&#xA;&#xA;out vec2 uv;&#xA;out vec3 col;&#xA;&#xA;uniform mat4 projection;&#xA;&#xA;void main()&#xA;{&#xA;    uv = texCoord;&#xA;    col = color;&#xA;&#xA;    gl_Position = projection * model * vec4(position, 0.f, 1.f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;fragment_shader.glsl:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 430 core&#xA;&#xA;in vec2 uv;&#xA;in vec3 col;&#xA;out vec4 color;&#xA;&#xA;void main()&#xA;{&#xA;    color = vec4(col, 1.f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1938" LastEditorUserId="1938" LastEditDate="2015-11-02T16:30:19.460" LastActivityDate="2015-11-03T10:01:38.730" Title="First quad being re-used?" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;c++&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1674" PostTypeId="2" ParentId="1669" CreationDate="2015-11-01T12:56:51.173" Score="4" Body="&lt;ul&gt;&#xA;&lt;li&gt;G0 Continuity means that the separate surfaces meet,&lt;/li&gt;&#xA;&lt;li&gt;G1 Continuity that the surfaces meet at same angle,&lt;/li&gt;&#xA;&lt;li&gt;G2 Continuity means that the change in angle matches in point of contact.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The G2 requirement does not mean that the surface is good quality. Just means that without this the surface is not going to have a continuous reflection flow so humans can see the difference. That may or may not be a good thing depends on what you want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically the surface normal is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$f(u,v) \frac{\partial}{\partial u} \times f(u,v) \frac{\partial}{\partial v}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since both sides are derived that means that the function field of the surface normal has on degree less than the original surface. So for the reflection to be first degree continuous it has to have a second degree continuity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far we have established the relationship between the continuity of the surface and the continuity of the reflection. Nothing thus far proves that the surface reflection needs to be first degree continuous. To understand why we must exit the realm of mathematics and enter the realm of biology.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They eye is equipped with a edge detection algorithm on a structural level right on the retina. This edge detection algorithm in essence works as a discrete derivative of the input signal. So, if your surface is not G2 continuous then the human edge detection kicks in and shows itself up. For references read on &lt;a href=&quot;https://en.wikipedia.org/wiki/Mach_bands&quot; rel=&quot;nofollow&quot;&gt;Mach Bands&lt;/a&gt; and so forth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the edge detection is discrete G2 continuity is not enough. The change has to not only be locally satisfied but also satisfied on the retina. So the change should still be shallow enough not to cause problems.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-03T13:30:51.730" LastActivityDate="2015-11-03T13:30:51.730" CommentCount="4" />
  <row Id="1675" PostTypeId="1" CreationDate="2015-11-02T11:05:12.347" Score="1" ViewCount="56" Body="&lt;p&gt;I am very eager to know what technology might been used to implement this piece of wonder:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/MFCAK.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MFCAK.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think, it's ray tracing. What do you think?&lt;/p&gt;&#xA;" OwnerUserId="1970" LastActivityDate="2015-11-03T02:18:02.977" Title="How to design something like this?" Tags="&lt;animation&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2015-11-03T08:57:47.263" />
  <row Id="1678" PostTypeId="2" ParentId="1675" CreationDate="2015-11-03T02:18:02.977" Score="2" Body="&lt;p&gt;It does look like raytracing to me as well.  It could also possibly be ray marching, which would be easier to make the tubes with since ray marching is all based on distances from things (center of tube in this case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Refraction is what's used to bend the light that goes through the tubes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You are then just left with how to model the tubes. One way could be to use 3d bezier curves (: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may not have been made in real time, but may have been rendered frame by frame and then assembled into the smooth &quot;movie&quot; you are showing.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-11-03T02:18:02.977" CommentCount="0" />
  <row Id="1679" PostTypeId="2" ParentId="1637" CreationDate="2015-11-03T03:12:17.680" Score="0" Body="&lt;p&gt;You might want to look into signed distance field textures.  They are very efficient and high quality for monochromatic images like shadows.  They work by storing a clamped, signed distance from the pixel to the shape.  Working this way, the distances play nicely with the bilinear interpolation of texture sampling, so you can have very nice results for very low resolution.  You can also give them a soft edge to have a soft shadow look.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-11-03T03:12:17.680" CommentCount="1" />
  <row Id="1680" PostTypeId="2" ParentId="1673" CreationDate="2015-11-03T09:05:16.970" Score="1" Body="&lt;p&gt;Seems like I had misunderstood instancing, that's why my code was acting strange: I was trying to use instancing while also trying to render different quads using different vertex data.&lt;/p&gt;&#xA;" OwnerUserId="1938" LastEditorUserId="1938" LastEditDate="2015-11-03T10:01:38.730" LastActivityDate="2015-11-03T10:01:38.730" CommentCount="0" />
  <row Id="1681" PostTypeId="1" AcceptedAnswerId="1682" CreationDate="2015-11-03T20:49:40.587" Score="11" ViewCount="162" Body="&lt;p&gt;I'm trying to implement an ocean scene with C++ and DirectX11. Currently I have a projected grid, Gerstner waves and a basic shading. My problem is that when I aim my camera horizontally, so I can see the water horizon, in the distance, the projected grid becomes insufficient, even at high vertex numbers. These screenshots illustrate the problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/F1Nqx.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/F1Nqx.jpg&quot; alt=&quot;shaded water surface&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1qsKc.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1qsKc.jpg&quot; alt=&quot;wireframe water surface&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know the cause of the problem is in the concept of the projected grid (the grid is detailed near the camera, rough far from it), but there has to be a best practice to solve this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas?&lt;/p&gt;&#xA;" OwnerUserId="1987" LastActivityDate="2015-11-06T15:49:43.377" Title="Projected grid water horizon detail" Tags="&lt;real-time&gt;&lt;geometry&gt;&lt;c++&gt;&lt;directx11&gt;&lt;grid&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="2" />
  <row Id="1682" PostTypeId="2" ParentId="1681" CreationDate="2015-11-04T02:55:53.690" Score="7" Body="&lt;p&gt;I believe a common solution is to split the camera transform used to project the grid from the camera transform that is used to render the grid. At perspectives close to top-down, the two cameras coincide, but as the viewing camera gets close to a horizontal perspective, the projection camera deviates and tries to keep a minimum inclination, i.e. it hovers somewhere above the view camera and looks down slightly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The tricky bit is making sure that the field of view of the projection camera always covers the region of the scene seen from the render camera. I don't have a resource at hand that details how to compute the appropriate transforms, and it might be tedious to derive by hand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A different solution is to grab the signal processing toolbox: The artifacts seen in your image are essentially aliasing, caused by insufficient sampling of the wave heightfield by the projected grid. Therefore, one solution is to filter the heightfield appropriately, depending on the projected area of a grid cell. I believe this is used in offline rendering of oceans, and it essentially makes sure that waves at the horizon go flat. However, I'm not sure how feasible this is in real-time rendering, since you would need high-quality anisotropic filtering to make this approach look reasonable.&lt;/p&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-11-04T02:55:53.690" CommentCount="1" />
  <row Id="1683" PostTypeId="2" ParentId="1681" CreationDate="2015-11-04T05:25:47.107" Score="3" Body="&lt;p&gt;Some software like Maya, solve this by using a polar (or actually cartesian that turns polar at a distance) much in the same way as you grid centered on the camera position. This setup adds more detail where it counts most Then they rely on the shaders normal processing at further ranges. There is room for improvemenet offcourse. You cold modify this approach a bit, and have any other shape that increases the mesh density towards the camera. The benefit of this is you can stretch the  effect up to the horizon without worrying about the seam. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The trick to not get the displacenent jumbled up in this case is that you gradually reduce the displacement as you move further away. You then just use normal modification in the pixel shader as you get further. This is easier to filter than having to filter an accurate shiluette edge. Also if you can see that far away then your vawes are likely sufficiently flat anyway.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-04T21:52:47.867" LastActivityDate="2015-11-04T21:52:47.867" CommentCount="0" />
  <row Id="1685" PostTypeId="2" ParentId="1606" CreationDate="2015-11-04T09:32:20.727" Score="1" Body="&lt;p&gt;In homogeneous coordinates i would do something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\vec{p} = \left(\frac{2}{x+y}, \frac{5y + z}{2x + 2y}, 3\right)^T = \left(\frac{2}{x+y}, \frac{\frac{5y + z}{2}}{x + y}, \frac{3x + 3y}{x + y}\right)^T$$ adding the fourth coordinate&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\vec{p}^H = \left(2, \frac{5y + z}{2}, 3x + 3y, x + y\right)^T = &#xA;\begin{pmatrix} &#xA; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 2 \\ &#xA; 0 &amp;amp; \frac{5}{2} &amp;amp; \frac{1}{2} &amp;amp; 0 \\&#xA; 3 &amp;amp; 3 &amp;amp; 0 &amp;amp; 0 \\&#xA; 1 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0&#xA;\end{pmatrix} \cdot \begin{pmatrix}x \\ y \\ z \\ 1\end{pmatrix}$$,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want a systematic way, since a linear transformation is completely characterized from the transformation of a basis, take the canonical bases i.e.&#xA;($\vec{e}^H_0 = (1,0,0,0), \vec{e}^H_1 = (0,1,0,0),...$) apply the transformation in $\vec{p}^H$ it will give you for $i = 0,...,3$ the columns of the matrix.&lt;/p&gt;&#xA;" OwnerUserId="228" LastActivityDate="2015-11-04T09:32:20.727" CommentCount="0" />
  <row Id="1686" PostTypeId="1" AcceptedAnswerId="1689" CreationDate="2015-11-04T15:47:36.557" Score="8" ViewCount="119" Body="&lt;p&gt;The &lt;a href=&quot;https://eheitzresearch.wordpress.com/240-2/&quot;&gt;Multiple Scattering Microfacet BSDFs with the Smith Model&lt;/a&gt; paper describes a statistical model for replacing the masking-shadowing functions in microfacet BSDFs (which account for paths with more than one surface intersection by setting their contribution to 0) with a distribution which can be path traced and allows the a ray to intersect a microfacet surface several times before exiting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They do this by modifying a volumetric (microflake) model to behave like a heightfield: to never collide with anything &quot;above&quot; the surface and to always collide with anything &quot;below&quot; the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kTAw4.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kTAw4.png&quot; alt=&quot;Microsurface vs modified microflake volume&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The result, using wording from their slides, is that &quot;the ray can never go through the Smith volume; the model creates an opaque surface-like interface.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this model compatible with traditional diffuse subsurface scattering, where a path can travel a perceptible (macro-scale) distance through a surface before exiting? Or is this simply a specular BSDF with no intention of modeling long paths internal to a surface, to be combined with a diffuse BSDF that adds that component?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Does the microflake model of volumetric rendering normally separate diffuse and specular, or is &quot;diffuse&quot; simply a path that bounces around so many times that the outgoing direction is uniformly distributed?)&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-11-05T01:57:04.717" Title="How does Smith multiple scattering interact with diffuse subsurface scattering?" Tags="&lt;brdf&gt;&lt;scattering&gt;&lt;subsurface-scattering&gt;&lt;microfacet&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1687" PostTypeId="1" CreationDate="2015-11-04T21:41:15.603" Score="4" ViewCount="26" Body="&lt;p&gt;I need a source code that takes as input a COLLADA file and/or an OBJ file, and gives as output basic information about the file, such as file size, model dimension, volume and if possible texture issues.&lt;/p&gt;&#xA;" OwnerUserId="1994" LastActivityDate="2015-11-04T22:31:27.753" Title="Is there any open code to get information about COLLADA or OBJ files?" Tags="&lt;implementation&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1688" PostTypeId="2" ParentId="1687" CreationDate="2015-11-04T22:31:27.753" Score="5" Body="&lt;p&gt;&lt;strong&gt;For OBJ Files&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Free tools like &lt;a href=&quot;http://meshlab.sourceforge.net/&quot;&gt;MeshLab&lt;/a&gt;, &lt;a href=&quot;https://www.blender.org/&quot;&gt;Blender&lt;/a&gt;, and &lt;a href=&quot;http://www.glc-player.net/&quot;&gt;GLC_Player&lt;/a&gt; are able to load OBJ files and give you basic information about their content.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apparently, GLC_Player can also load COLLADA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In code you can use the following standard tools:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/assimp/assimp&quot;&gt;Assimp&lt;/a&gt; - Library for loading many common formats including OBJ&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/syoyo/tinyobjloader&quot;&gt;TinyObjLoader&lt;/a&gt; - .h/.cc only, no external dependencies&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-11-04T22:31:27.753" CommentCount="0" />
  <row Id="1689" PostTypeId="2" ParentId="1686" CreationDate="2015-11-05T01:57:04.717" Score="6" Body="&lt;p&gt;The goal of Heitz et al.'s model is pretty much the opposite of subsurface scattering: They only consider surface scattering, i.e. the ray can never enter the material.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because microfacets are statistical in nature, they can recast their problem in such a way that it can be solved by microflakes, which allows them to compute properties such as the mean free path to derive a heightfield sampling procedure. However, even though they use microflake theory, they still solve the same problem of multiple scattering on microfacets, and their result is still a BSDF, not a subsurface scattering model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because it is a BSDF, the path exits at the same location as it hit the surface. This is also mentioned in the introduction:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Note that our volume-scattering process models the interaction with a surface, but its application in rendering is virtual in that no displacements occur, i.e. the incident and exitant location are the same and the resulting plane-parallel radiometry produces a BSDF.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="79" LastActivityDate="2015-11-05T01:57:04.717" CommentCount="0" />
  <row Id="1691" PostTypeId="1" CreationDate="2015-11-05T10:08:52.910" Score="5" ViewCount="113" Body="&lt;p&gt;I have a spectral power distribution (SPD, 5 nm steps) for all light sources in the scene and SPDs for the reflectance of all surfaces under any light / viewer angle in question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm going to calculate in floating point precision and not clamp any values. Except for pathological cases like monochromatic light or special things like dispersion effects, does it make a difference whether I render using the spectrum, XYZ color space or linear RGB color space? Note that I will convert the result to sRGB as a last step in any case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specifically, is it invariant at which point (before or after multiplication of surface BRDF with light intensity) I integrate to convert spectrum to XYZ? Does the introduction of reference white in XYZ / RGB distort the colors when working with light sources of a different color / SPD?&lt;/p&gt;&#xA;" OwnerUserId="385" LastEditorUserId="385" LastEditDate="2015-11-05T11:36:46.390" LastActivityDate="2015-11-05T11:36:46.390" Title="How correct is rendering in XYZ / linear RGB space?" Tags="&lt;rendering&gt;&lt;physically-based&gt;&lt;maths&gt;" AnswerCount="0" CommentCount="6" FavoriteCount="1" />
  <row Id="1692" PostTypeId="1" AcceptedAnswerId="1693" CreationDate="2015-11-05T12:57:12.430" Score="7" ViewCount="320" Body="&lt;p&gt;This a follow up for a question I answered on GameDev SE. The question was simply &lt;a href=&quot;http://gamedev.stackexchange.com/questions/110781/is-gldrawarraysinstanced-in-opengl-parallel-when-drawing-those-instances&quot;&gt;Is glDrawArraysInstanced in OpenGL parallel when drawing those instances?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My answer was in the lines of &quot;the GPU might execute multiple draw calls in parallel depending on multiple things, for instance if two draw calls use the same vertex shaders.&quot; &lt;a href=&quot;http://gamedev.stackexchange.com/questions/110781/is-gldrawarraysinstanced-in-opengl-parallel-when-drawing-those-instances/110806#110806&quot;&gt;Here is the full answer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the comments the OP mentioned, &quot;I think it should be sequential. After all, some of blending mode depends of sequence&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I am little bit confused. Is my answer of &lt;em&gt;it will still parallelize them when it can&lt;/em&gt; valid? Can someone clarify this?&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="127" LastEditDate="2015-11-05T20:20:08.097" LastActivityDate="2015-11-05T20:20:08.097" Title="Are draw calls executed in parallel or sequentially or both?" Tags="&lt;opengl&gt;&lt;gpu&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1693" PostTypeId="2" ParentId="1692" CreationDate="2015-11-05T13:30:39.553" Score="8" Body="&lt;p&gt;The result should be &lt;em&gt;as if&lt;/em&gt; it was executed sequentially one triangle at a time. This is important so that each frame is deterministic. If it wasn't then drawing the same frame multiple times could create different results and create a flickering image on screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However that &lt;em&gt;does not mean parallelization is impossible&lt;/em&gt;. Vertices can be computed in parallel. Their results will be ordered and fed to the rasterizer which will feed the fragment shader stage.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same happens with pixels. Typically a group of pixels from a triangle can be computed in parallel and blended. A non-overlapping triangle can then also be filled at the same time however if triangle overlaps one currently busy then the result of the fragment shader must be buffered until the corresponding pixels of the previous triangle are done so blending can happen in the correct order.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2015-11-05T14:46:13.837" LastActivityDate="2015-11-05T14:46:13.837" CommentCount="5" />
  <row Id="1694" PostTypeId="1" AcceptedAnswerId="1717" CreationDate="2015-11-06T09:26:09.643" Score="4" ViewCount="128" Body="&lt;p&gt;I have some material which I want to represent as a collection of spheres (atoms) and a particle of light (photon). I'm firing this particle and checking for the collision against atoms. What should happen if collision occurs? I see two possible solutions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Light particle can be either absorbed or reflected, depends on some probability.&lt;/li&gt;&#xA;&lt;li&gt;Light particle reflected, but loses some part of it's energy. When the energy is zero, particle is absorbed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Update: I want to see the general behavior of the particle. The particle itself I want to simulate as a ray. Also I want to simulate a variety of materials - I think small spheres placed near each other can lead to specular material. Big spheres placed far from each other can lead to Lambertian material and subsurface scattering. The light transport is nor stochastic nor full valuated - a ray after collision will reflect correctly, not randomly. In the end I want to see the number of photons reflected, transmitted, scattered and their direction.&lt;/p&gt;&#xA;" OwnerUserId="386" LastEditorUserId="386" LastEditDate="2015-11-06T16:25:41.327" LastActivityDate="2015-11-13T16:50:33.490" Title="Does light particle loses energy when it hits something?" Tags="&lt;lighting&gt;&lt;material&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1696" PostTypeId="2" ParentId="1681" CreationDate="2015-11-06T15:49:43.377" Score="5" Body="&lt;p&gt;You can be both realistical and real-time. the secret is to change representation each time the information get under the Shannon-Nyquist (i.e. grid) scale: from geometry to normal maps to shading models. This paper is made for you: &lt;a href=&quot;http://maverick.inria.fr/Publications/2010/BNH10/index.php&quot;&gt;http://maverick.inria.fr/Publications/2010/BNH10/index.php&lt;/a&gt; (see also Yoube videos)&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2015-11-06T15:49:43.377" CommentCount="0" />
  <row Id="1698" PostTypeId="1" AcceptedAnswerId="1699" CreationDate="2015-11-07T10:17:34.330" Score="2" ViewCount="308" Body="&lt;p&gt;I can't really seem to figure out how to bind two constant buffers to my shaders. I have them described like so. One in slot b0 and the other in slot b1.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cbuffer WVPData : register(b0)&#xA;{&#xA;    matrix model;&#xA;    matrix view;&#xA;    matrix projection;&#xA;};&#xA;&#xA;cbuffer DirLightData : register(b1)&#xA;{&#xA;   float4 Ambient;&#xA;   float4 Diffuse;&#xA;   float4 Specular;&#xA;   float3 Direction;&#xA;   float pad;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then for the root signature it's described like so.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CD3DX12_DESCRIPTOR_RANGE range[2];&#xA;CD3DX12_ROOT_PARAMETER parameter[1];&#xA;&#xA;range[0].Init(D3D12_DESCRIPTOR_RANGE_TYPE_CBV, 1, 0);&#xA;range[1].Init(D3D12_DESCRIPTOR_RANGE_TYPE_CBV, 1, 1);&#xA;parameter[0].InitAsDescriptorTable(_countof(range), range, D3D12_SHADER_VISIBILITY_ALL);&#xA;&#xA;D3D12_ROOT_SIGNATURE_FLAGS rootSignatureFlags =&#xA;D3D12_ROOT_SIGNATURE_FLAG_ALLOW_INPUT_ASSEMBLER_INPUT_LAYOUT | // Only the input assembler stage needs access to the constant buffer.&#xA;D3D12_ROOT_SIGNATURE_FLAG_DENY_DOMAIN_SHADER_ROOT_ACCESS |&#xA;D3D12_ROOT_SIGNATURE_FLAG_DENY_GEOMETRY_SHADER_ROOT_ACCESS |&#xA;D3D12_ROOT_SIGNATURE_FLAG_DENY_HULL_SHADER_ROOT_ACCESS;&#xA;&#xA;CD3DX12_ROOT_SIGNATURE_DESC descRootSignature;&#xA;descRootSignature.Init(_countof(parameter), parameter, 0, nullptr, rootSignatureFlags);&#xA;&#xA;ComPtr&amp;lt;ID3DBlob&amp;gt; pSignature;&#xA;ComPtr&amp;lt;ID3DBlob&amp;gt; pError;&#xA;DX::ThrowIfFailed(D3D12SerializeRootSignature(&amp;amp;descRootSignature, D3D_ROOT_SIGNATURE_VERSION_1, pSignature.GetAddressOf(), pError.GetAddressOf()));&#xA;DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateRootSignature(0, pSignature-&amp;gt;GetBufferPointer(), pSignature-&amp;gt;GetBufferSize(), IID_PPV_ARGS(&amp;amp;mRootSignature)));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I believe that's correct. I think the problem I have is when I create the constant buffer and the cbv below. Specifically right under the &quot;Describe and create the constant buffer view.&quot; comment. I don't really understand what's going on.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Create a descriptor heap for the constant buffers.&#xA;{&#xA;    D3D12_DESCRIPTOR_HEAP_DESC heapDesc = {};&#xA;    heapDesc.NumDescriptors = 2;&#xA;    heapDesc.Type = D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV;&#xA;    // This flag indicates that this descriptor heap can be bound to the pipeline and that descriptors contained in it can be referenced by a root table.&#xA;    heapDesc.Flags = D3D12_DESCRIPTOR_HEAP_FLAG_SHADER_VISIBLE;&#xA;    DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateDescriptorHeap(&amp;amp;heapDesc, IID_PPV_ARGS(&amp;amp;mCbvHeap)));&#xA;&#xA;    mCbvHeap-&amp;gt;SetName(L&quot;Constant Buffer View Descriptor Heap&quot;);&#xA;}&#xA;&#xA;// Create the constant buffer.&#xA;DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateCommittedResource(&#xA;  &amp;amp;uploadHeapProperties,&#xA;  D3D12_HEAP_FLAG_NONE,&#xA;  &amp;amp;CD3DX12_RESOURCE_DESC::Buffer(CAlignedWVPDataSize),&#xA;  D3D12_RESOURCE_STATE_GENERIC_READ,&#xA;  nullptr,&#xA;  IID_PPV_ARGS(&amp;amp;mWVPConstantBuffer)));&#xA;&#xA;DX::ThrowIfFailed(d3dDevice-&amp;gt;CreateCommittedResource(&#xA;  &amp;amp;uploadHeapProperties,&#xA;  D3D12_HEAP_FLAG_NONE,&#xA;  &amp;amp;CD3DX12_RESOURCE_DESC::Buffer(CAlignedDirLightDataSize),&#xA;  D3D12_RESOURCE_STATE_GENERIC_READ,&#xA;  nullptr,&#xA;  IID_PPV_ARGS(&amp;amp;mDirLightConstantBuffer)));&#xA;&#xA;// Describe and create a constant buffer view.&#xA;D3D12_CONSTANT_BUFFER_VIEW_DESC cbvDesc[2];// = {};&#xA;cbvDesc[0].BufferLocation = mWVPConstantBuffer-&amp;gt;GetGPUVirtualAddress();&#xA;cbvDesc[0].SizeInBytes = CAlignedWVPDataSize;&#xA;cbvDesc[1].BufferLocation = mDirLightConstantBuffer-&amp;gt;GetGPUVirtualAddress();&#xA;cbvDesc[1].SizeInBytes = CAlignedDirLightDataSize;&#xA;&#xA;CD3DX12_CPU_DESCRIPTOR_HANDLE cbvHandle0(mCbvHeap-&amp;gt;GetCPUDescriptorHandleForHeapStart(), 0, 0);&#xA;d3dDevice-&amp;gt;CreateConstantBufferView(cbvDesc, cbvHandle0);&#xA;&#xA;CD3DX12_CPU_DESCRIPTOR_HANDLE cbvHandle1(mCbvHeap-&amp;gt;GetCPUDescriptorHandleForHeapStart(), d3dDevice-&amp;gt;GetDescriptorHandleIncrementSize(D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV), 1);&#xA;d3dDevice-&amp;gt;CreateConstantBufferView(cbvDesc, cbvHandle1);&#xA;&#xA;// Initialize and map the constant buffers. We don't unmap this until the&#xA;// app closes. Keeping things mapped for the lifetime of the resource is okay.&#xA;DX::ThrowIfFailed(mWVPConstantBuffer-&amp;gt;Map(0, nullptr, reinterpret_cast&amp;lt;void**&amp;gt;(&amp;amp;mMappedWVPBuffer)));&#xA;memcpy(mMappedWVPBuffer, &amp;amp;mWVPData, sizeof(mWVPData));  &#xA;&#xA;DX::ThrowIfFailed(mDirLightConstantBuffer-&amp;gt;Map(0, nullptr, reinterpret_cast&amp;lt;void**&amp;gt;(&amp;amp;mMappedDirLightBuffer)));&#xA;memcpy(mMappedDirLightBuffer, &amp;amp;mDirLightData, sizeof(mDirLightData));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note: Slot b0 works perfect. I can change world view projection data just fine. But b1 does not work at all. What am I doing wrong?&lt;/p&gt;&#xA;" OwnerUserId="113" LastActivityDate="2015-11-07T18:26:36.093" Title="DirectX 12 Constant Buffer Binding" Tags="&lt;c++&gt;&lt;directx12&gt;&lt;constant-buffer&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1699" PostTypeId="2" ParentId="1698" CreationDate="2015-11-07T18:26:36.093" Score="2" Body="&lt;p&gt;The problem looks like it's in this line:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;d3dDevice-&amp;gt;CreateConstantBufferView(cbvDesc, cbvHandle1);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The first parameter should be &lt;code&gt;&amp;amp;cbvDesc[1]&lt;/code&gt;. As it is now, you're setting up two copies of &lt;code&gt;cbvDesc[0]&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, it looks like you've reversed the second and third arguments to the &lt;code&gt;cbvHandle1&lt;/code&gt; constructor: the second argument should be the offset (1) and the third should be the increment size. Not that it really matters, since those two values just get multiplied together anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way, I don't think you need to set up two separate descriptor ranges when creating the root signature; since they're contiguous, you could just use a single range of two descriptors. But it shouldn't make a difference to the results.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-11-07T18:26:36.093" CommentCount="0" />
  <row Id="1700" PostTypeId="1" CreationDate="2015-11-08T23:43:16.020" Score="3" ViewCount="84" Body="&lt;p&gt;If I want to model a large environment, how can I solve the problem of earth curvature and horizon?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use an environment map for the sky, but for the ground what can I do?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do I have to model the ground with a big plane, disk or a spherical cap?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a general procedure to solve this problem in computer graphics?&lt;/p&gt;&#xA;" OwnerUserId="1636" LastEditorUserId="127" LastEditDate="2015-11-10T16:57:18.720" LastActivityDate="2015-11-10T16:57:18.720" Title="Earth curvature and horizon in modeling large scenes" Tags="&lt;3d&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1701" PostTypeId="2" ParentId="1700" CreationDate="2015-11-09T00:13:20.613" Score="5" Body="&lt;p&gt;Having the horizon fall off is simply dropping the ground in the distance down somewhat.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A point $x$ km away if you follow the curve of the planet will be $r-r\cdot \cos \frac{x}{r}$ down and $r\cdot \sin \frac{x}{r}$ out horizontally where $r$ is the radius of the planet you are modeling (~6.3k km for earth). In the vertex shader you can account for that before you apply the view and projection matrix.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2015-11-09T00:13:20.613" CommentCount="2" />
  <row Id="1702" PostTypeId="1" CreationDate="2015-11-10T06:18:54.503" Score="4" ViewCount="82" Body="&lt;p&gt;Could anyone please explain to me how drawing works with respect to scrolling?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose there is a window, which has an area where one can draw to (a &quot;canvas&quot;). Are there two copies of this canvas? One for CPU and one for GPU? I mean, I know one could copy from main memory to graphic card memory very quickly with OpenGL (PBO) so one way to draw is first draw with software in main memory and the &quot;blit&quot; it to GPU, right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I would assume that when the window scrolls there is some GPU functionality which would rapidly copy over the non-dirty parts in the appropriate positions without leaving GPU-land and then ask the CPU for redrawing the dirty/missing parts. But now I have two un-synced copies of the canvas! So how does it work? Do I do the scrolling in main memory in software and upload the whole canvas to GPU as the first time? Or perhaps I scroll the canvas in GPU and &quot;download&quot; then download it to CPU-land and &quot;repair&quot; it there? Or something else entirely? Perhaps somehow I don't have to care that the copies are unsynchronized?&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2015-11-13T10:38:50.640" Title="Blit and scroll" Tags="&lt;gpu&gt;&lt;drawing&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1703" PostTypeId="1" CreationDate="2015-11-10T15:27:38.280" Score="6" ViewCount="62" Body="&lt;p&gt;I am trying to implement (in C#) an image perturbation algorithm presented in the book &quot;Texturing and modeling - K. Perlin et al&quot; (page 91 if anyone has it), which distorts an image.&#xA;The following code is in Renderman language:&#xA;The texture access&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Ct = texture(&quot;example.tx&quot;, s, t);&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;is replaced by&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;point Psh;&#xA;float ss, tt;&#xA;Psh = transform(&quot;shader&quot;, P);&#xA;ss = s + 0.2 * snoise(Psh);&#xA;tt = t + 0.2 * snoise(Psh+(l.5,6.7,3.4));&#xA;Ct = texture(&quot;example.tx&quot;, ss, tt);&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;transforming the image on the left to that on the right.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/vxxJX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vxxJX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I undestood, instead of accessing coordinate $(s,t)\in[0,1]$ we access slighty perturbed coordinates $(ss,tt)$ and display them at place $(s,t)$, thus creating an image that looks slightly perturbed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$snoise(x)$ is defined as $(noise(x)*2)-1$, mapping noise from $[0,1]$ to $[-1,1]$, and in the RenderMan documentation $noise(P)$ where P is a point, returns a value based on some noise (most likely perlin or lattice).&#xA;(&lt;a href=&quot;http://renderman.pixar.com/resources/current/RenderMan/noiseFunctions.html&quot; rel=&quot;nofollow&quot;&gt;http://renderman.pixar.com/resources/current/RenderMan/noiseFunctions.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I don't understand is what the transform function does, which is supposed to map the 3d point P into the &quot;shader&quot; space, and how can it be implemented.&#xA;Also, I'm not sure whether noise(x) returns a 3d point, a float (would make more sense) and if I can use a simple 2d implementation of Perlin's noise to reach the same desired effect.&lt;/p&gt;&#xA;" OwnerUserId="2024" LastEditorUserId="16" LastEditDate="2015-11-12T14:54:50.120" LastActivityDate="2015-11-12T14:54:50.120" Title="Perturbed image texture implementation from renderman language" Tags="&lt;rendering&gt;&lt;texture&gt;&lt;algorithm&gt;&lt;noise&gt;&lt;mapping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1704" PostTypeId="2" ParentId="1702" CreationDate="2015-11-10T16:52:41.847" Score="2" Body="&lt;p&gt;Traditionally, there are two ways to display something on the screen:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Re-draw it from scratch every frame (or, every time something changes)&lt;/li&gt;&#xA;&lt;li&gt;Track the sections of the screen (ie. &quot;dirty rectangles&quot;) that change each frame, and update only those portions.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The first option is what most modern games / animations will do. Each frame, you re-submit all the draw calls / re-upload any textures to the GPU that have changed, then ask the GPU to draw everything and show it on the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second option is what most old 2D games did, as well as most window applications (ie. GUI programs that don't have a lot of change going on). One implementation would be to have a local CPU copy of the screen, a GPU copy, and a series of buffers to transfer data between them.&#xA;So for every frame where there were changes:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Blit pixel data of the dirty rectangle areas to one of the data transfer buffers. &lt;/li&gt;&#xA;&lt;li&gt;Copy the buffer to the GPU&lt;/li&gt;&#xA;&lt;li&gt;Use the GPU to blit the pixel data from the data transfer buffer to the GPU copy.&lt;/li&gt;&#xA;&lt;li&gt;Present&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For more info, I suggest looking up how some window managers do compositing. There is a decent explaination of how WPF works &lt;a href=&quot;https://jeremiahmorrill.wordpress.com/2011/02/14/a-critical-deep-dive-into-the-wpf-rendering-system/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. And an excellent post about how Chrome accelerates their browser rendering &lt;a href=&quot;https://www.chromium.org/developers/design-documents/gpu-accelerated-compositing-in-chrome&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2015-11-10T16:52:41.847" CommentCount="0" />
  <row Id="1706" PostTypeId="1" CreationDate="2015-11-11T04:18:18.883" Score="5" ViewCount="96" Body="&lt;p&gt;With directX12 they introduced heap descriptors. A way for us to describe the table for resources we wanted to send to the shaders. I'm admittedly very new at computer graphics and only tinkered a bit in directX11. I have not toyed with instancing or any more complicated things right now so I have a object for every mesh that has these defintions.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Microsoft::WRL::ComPtr&amp;lt;ID3D12Resource&amp;gt; mVertexBuffer;&#xA;D3D12_VERTEX_BUFFER_VIEW mVertexBufferView;&#xA;&#xA;// Index Buffer&#xA;Microsoft::WRL::ComPtr&amp;lt;ID3D12Resource&amp;gt; mIndexBuffer;&#xA;D3D12_INDEX_BUFFER_VIEW mIndexBufferView;&#xA;&#xA;// World View Projection Constant Buffer&#xA;Microsoft::WRL::ComPtr&amp;lt;ID3D12Resource&amp;gt; mWVPConstantBuffer;&#xA;WVPData mWVPData;      &#xA;UINT8* mMappedWVPBuffer;&#xA;&#xA;// Directional Light Constant Buffer&#xA;Microsoft::WRL::ComPtr&amp;lt;ID3D12Resource&amp;gt; mDirLightConstantBuffer;&#xA;DirLightData mDirLightData;&#xA;UINT8* mMappedDirLightBuffer;&#xA;&#xA;// CBVHeap&#xA;Microsoft::WRL::ComPtr&amp;lt;ID3D12DescriptorHeap&amp;gt; mCbvHeap;&#xA;UINT mCbvDescriptorSize;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A vertex buffer, index buffer, the cbvHeap, and two constant buffers. One for the transformation matrices and one for the directional light data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wasn't really sure what the cbvHeap (constant buffer view heap) was really doing. I just knew how to use it to my content to the screen. So I experimented. I took the cbvHeap and mCbvDescriptorSize out of the mesh object and put it in the scene object (where the array of meshes were contained) and then used the same cbvheap across all my meshes. This did not work as it gave one consistent color across the whole mesh. (ie: The same constant buffer was used for every mesh giving them the same diffuse and ambient light data.) What exactly is the descriptor heap doing that caused this? And are my definitions right per mesh?&lt;/p&gt;&#xA;" OwnerUserId="113" LastActivityDate="2015-11-11T22:47:47.257" Title="DirectX12 CbvHeap" Tags="&lt;c++&gt;&lt;directx12&gt;&lt;constant-buffer&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1707" PostTypeId="2" ParentId="1706" CreationDate="2015-11-11T22:47:47.257" Score="6" Body="&lt;p&gt;In DX12, a descriptor is a small record, basically a pointer, that tells the GPU where to find some data such as a constant buffer. Since each object is going to have its own constant buffer data with its own particular transforms, lighting/material properties, etc., each object also has to end up with a separate set of descriptors to point to its individual data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a couple of ways to set up descriptors in DX12. First, you can add commands in a command list that update descriptors directly in the root table. For example, you can use &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn903911.aspx&quot;&gt;&lt;code&gt;SetGraphicsRootConstantBufferView()&lt;/code&gt;&lt;/a&gt; to set up descriptors for an object's constant buffers, then draw the object. Repeat for multiple objects. These descriptors will be saved in the command list, and will get applied in-order as the command list is executed on the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other way to do it is use descriptor heaps. A descriptor heap is basically a buffer that stores an array of descriptors. With this approach, the command list doesn't save descriptors for you; you have to manage them yourself. If you overwrite any descriptors before the GPU consumes them, then you'll get wrong results. This sounds like what's happening in your program: you're likely overwriting the same descriptor in the heap every time you issue an object's draw commands on the CPU, so by the time the GPU executes the command list, only the last descriptor written is still there, and that descriptor gets applied to all of the objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To fix this, you have to allocate enough space in the descriptor heap to store all the descriptors you're going to use for the frame. Write all the objects' descriptors to the heap. Then when drawing each object, point the shader at the that object's data using &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn903912.aspx&quot;&gt;&lt;code&gt;SetGraphicsRootDescriptorTable()&lt;/code&gt;&lt;/a&gt; with an offset handle that points to the spot in the heap where that object's descriptors live.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As long as you're still just trying to learn and get a simple app running, I would probably stick with the root table approach as much as possible. For a more serious high-performance app, there are probably optimizations you can do with descriptor heaps, such as keeping all the constants and textures for a given material together so you can reuse them across many objects and bind them all in one call. You'd probably also want to avoid having a constant buffer and descriptor heap per object, but rather aggregate many objects' data into a few large buffers; this will be more efficient when you have lots of objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also worth noting that a serious 3D app will want to have multiple frames' command lists in flight at the same time, to obtain CPU/GPU parallelism, and this will require constant buffers and descriptor heaps to be lengthened to store multiple frames' worth of data. Again, this is something you can gloss over for learning/bringup purposes by just building a single command list at a time, submitting it, and waiting for it to finish before going on to the next frame.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-11-11T22:47:47.257" CommentCount="0" />
  <row Id="1708" PostTypeId="1" AcceptedAnswerId="1709" CreationDate="2015-11-12T10:42:17.850" Score="2" ViewCount="145" Body="&lt;p&gt;Let's say I have an image of an object against a white background. I want to crop so that the image fills the available space.&#xA;This takes 10px off:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;convert original.png -shave 10x10 shaved.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How would I implement something like this that detects the edges of the object, thus cropping a variable number of pixels from each side?&lt;/p&gt;&#xA;" OwnerUserId="2040" LastActivityDate="2015-11-12T13:36:52.010" Title="How to crop with edge-detection using imagemagick" Tags="&lt;edge-detection&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2015-11-16T09:49:11.447" />
  <row Id="1709" PostTypeId="2" ParentId="1708" CreationDate="2015-11-12T13:36:52.010" Score="1" Body="&lt;p&gt;Try &lt;code&gt;-trim&lt;/code&gt; instead of &lt;code&gt;-shave&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;convert original.png -trim trimmed.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;See the &lt;a href=&quot;http://www.imagemagick.org/Usage/crop/#trim&quot; rel=&quot;nofollow&quot;&gt;docs&lt;/a&gt; for this and other options.&lt;/p&gt;&#xA;" OwnerUserId="192" LastActivityDate="2015-11-12T13:36:52.010" CommentCount="1" />
  <row Id="1710" PostTypeId="2" ParentId="1703" CreationDate="2015-11-12T13:40:36.583" Score="6" Body="&lt;p&gt;As you've surmised, the &lt;code&gt;transform()&lt;/code&gt; function transforms points from one co-ordinate space to another. (There are also &lt;code&gt;vtransform()&lt;/code&gt; and &lt;code&gt;ntransform()&lt;/code&gt; for transforming direction vectors and normal vectors, respectively.) The string argument names the co-ordinate space to transform into.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://renderman.pixar.com/resources/current/RenderMan/rslGuidelines97.html#part-2-coordinate-spaces&quot;&gt;Renderman Shading Guidelines&lt;/a&gt; have this to say about it:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;At the start of shader execution, all point, vector, normal, and matrix variables are expressed in the &quot;current&quot; coordinate system. Exactly which coordinate system is &quot;current&quot; is implementation-dependent. It just so happens to be that &quot;current&quot; is &quot;camera&quot; for PRMan*, but you should never count on this behavior - it is entirely possible that other RenderMan compliant renderers (including future renderers from Pixar) may use some other space (like &quot;world&quot;) as &quot;current&quot; space.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It goes on to give a case like this as an example. Most lighting calculations should be done in camera space, but evaluating a noise function should be in the object's co-ordinate system, because you want the noise to stay the same as the object moves through world space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your C# implementation, you'll also need to transform the point being shaded from camera space to the object's co-ordinate system. Maybe you've done this already before computing the texture co-ordinates. If not, you'll need to multiply by the object's transformation matrix. Remember that the only use of this transformed point is as an input (like a seed) to the Perlin noise generator. It sets the domain that the noise varies over: world space co-ordinates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In RSL, the &lt;code&gt;noise()&lt;/code&gt; function can return any type you like: a &lt;code&gt;float&lt;/code&gt;, a &lt;code&gt;color&lt;/code&gt;, a &lt;code&gt;point&lt;/code&gt;, or a &lt;code&gt;vector&lt;/code&gt;. As you're adding it to another &lt;code&gt;float&lt;/code&gt; (&lt;code&gt;u&lt;/code&gt; or &lt;code&gt;v&lt;/code&gt;), you'll get a &lt;code&gt;float&lt;/code&gt; in this code. Really, the two &lt;code&gt;noise()&lt;/code&gt; calls, added to &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt;, are acting to generate a single 2D noise vector. In your own code, if you are using a 2D vector to store your texture co-ordinates, you can use a single noise function that returns a 2D vector, to get the same effect in one line of code.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If you're interested in making a nice noise generator, &lt;a href=&quot;https://www.shadertoy.com/results?query=noise&quot;&gt;Shadertoy&lt;/a&gt; has a lot of noise shaders featuring different variants of Perlin noise with different properties (isotropic or not, configurable smoothness and bandwidth) and is worth looking at for inspiration as well as implementation hints.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2015-11-12T13:40:36.583" CommentCount="0" />
  <row Id="1711" PostTypeId="2" ParentId="166" CreationDate="2015-11-12T17:01:58.687" Score="1" Body="&lt;p&gt;You can use OpenGl 4.x tessellation shaders to convert Bezier control points into polygons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A google search for &quot;tessellation shader bezier&quot; found this outline describing the tessellation of Bezier surfaces and curves:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://web.engr.oregonstate.edu/~mjb/cs519/Handouts/tessellation.1pp.pdf&quot; rel=&quot;nofollow&quot;&gt;http://web.engr.oregonstate.edu/~mjb/cs519/Handouts/tessellation.1pp.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This offloads the Bezier evaluation from the CPU to the GPU and reduces the data flow across the bus.&lt;/p&gt;&#xA;" OwnerUserId="2049" LastEditorUserId="2049" LastEditDate="2015-11-13T19:53:20.723" LastActivityDate="2015-11-13T19:53:20.723" CommentCount="2" />
  <row Id="1713" PostTypeId="1" CreationDate="2015-11-13T06:01:34.150" Score="2" ViewCount="64" Body="&lt;p&gt;Please help me tackle the below question:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Explain THREE forms of affine transformations using relevant examples for each case.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The answers I want to see are scaling, rotation and reflection.&lt;/p&gt;&#xA;" OwnerUserId="2055" LastEditorUserId="231" LastEditDate="2015-11-13T09:52:29.547" LastActivityDate="2015-11-13T13:08:52.040" Title="What are forms of affine transformations?" Tags="&lt;vector-graphics&gt;&lt;affine-transformations&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1714" PostTypeId="2" ParentId="1713" CreationDate="2015-11-13T09:32:27.343" Score="4" Body="&lt;p&gt;&lt;em&gt;Note: I have answered before the edit from trichoplax and I thought you were searching for other transformations other than the one you mentioned. The informations below are still useful so I will keep the answer here, but it does not directly answer your question.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Affine transformations (surprise!) map affine spaces to affine spaces. An affine space is substantially a vector space where you can establish an origin and define points as tuples of their coordinates. This is far from a formal definition, if you are interested in one I can edit the answer. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now an affine transformation $T$ transform points into points and must preserve affine combinations: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$T(\lambda_1 P_1 + \lambda_2 P_2 + ...  +  \lambda_n P_n) = \lambda_1 T(P_1) + \lambda_2 T(P_2) + ... + \lambda_n T(P_n)  $   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given $\sum_{i}^{n} \lambda_i = 1 $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However these transformations can't be arbitrary as the following must be preserved: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Parallelism is preserved&lt;/em&gt;&lt;/strong&gt;. This means that if you transform parallel lines they remain parallel after the affine transformation.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Relative ratios are preserved&lt;/em&gt;&lt;/strong&gt;. This means that if you have $R = (1-\beta) P + \beta Q$  then $T(R) = (1 - \beta) T(P) + \beta T(Q)$ &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Addition between vector and points are preserved&lt;/em&gt;&lt;/strong&gt;. Meaning that $T( P + \vec{v} ) = T(P) + T(\vec{v})$&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;With these properties in mind you can come up with a very big number of affine transforms yourself. A couple of obvious ones other the ones you mentioned are: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Translation&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$T_vp = &#xA;\begin{bmatrix}&#xA;1&amp;amp;0&amp;amp;0&amp;amp;v_x\\0&amp;amp;1&amp;amp;0&amp;amp;v_y\\0&amp;amp;0&amp;amp;1&amp;amp;v_z\\0&amp;amp;0&amp;amp;0&amp;amp;1\end{bmatrix}&#xA;\begin{bmatrix}&#xA;p_x+v_x\\p_x+v_y\\p_z+v_z\\1\end{bmatrix}=p+v$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That moves a point into a specific direction by a specific amount specified by a displacement vector. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Shearing&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/fpg2D.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fpg2D.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;em&gt;image from wikipedia&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is a transform that displace all points in a given direction by an amount that is proportional to their perpendicular distance to a line parallel to that direction. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example the transform matrix for an horizontal shear in a 2D space is given by: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix}x+my\\y\end{bmatrix} = &#xA;\begin{bmatrix}1&amp;amp;m\\0&amp;amp;1\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Again, these two are just example, the important information you should really keep is the definition (and properties) of an affine transform; with that in mind it shouldn't be too hard to recognize an affine transform.  Also note that combining affine transforms will give you an affine transform! &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-11-13T13:08:52.040" LastActivityDate="2015-11-13T13:08:52.040" CommentCount="1" />
  <row Id="1715" PostTypeId="2" ParentId="1702" CreationDate="2015-11-13T10:38:50.640" Score="2" Body="&lt;p&gt;Let's take web browsers as an example. On mobile, both Chrome and Firefox draw the page content on the GPU, and they typically have a lot of scrolling, so it's a relevant example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They work a little like Google Maps or Open Street Map's tileserver. They slice the page up into tiles and draw each tile into a texture. (The textures may well be in a texture atlas; I'm afraid I don't know the specifics.) Drawing the final screen simply consists of drawing a small number of textured squares in the correct positions. This approach makes scrolling very easy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you scroll, it doesn't have to regenerate the whole page starting from the DOM (the web page equivalent of the scene graph): it just moves the quads around so that each textured tile moves around the viewport. When a new row or column of tiles is about to come into view, the page renderer has to draw the new tiles before you want to see them. At the other end of the screen, you can keep tiles that are no longer in view, but they can be disposed of if you start to run out of texture memory. Because all the tiles are the same size, you can simply draw the new tiles over old textures you don't want any more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's just the same as in Google Maps: when you scroll in that, you can usually see a placeholder texture if new tiles are visible before they've finished downloading. And the page keeps a cache of tiles you've looked at before, but it can destroy any that are no longer visible in order to free some memory if necessary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The tile technique also allows progressive refinement, which is important to let you scroll at 60 fps on a mobile platform without enough compute power. If it's rendering the tiles too slowly, it can draw the textures at lower resolution first (while you're scrolling), and then redraw them at full resolution later, if they stay on screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, there isn't any special functionality for blitting some of the last frame into a new position in the new frame, but by rendering to texture, we can access the same texture from the next frame, throwing away any parts that are no longer useful. Some games also use this technique for mostly static content backgrounds, 2D world maps, &amp;amp;c.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have to update small portions of the scrolling content, it's easy: just redraw the tiles that changed. The code that updates the tiles doesn't care what screen position they're being displayed, because the final frame renderer puts each tile in the right place. You don't have &quot;two un-synced copies of the canvas&quot; and it doesn't matter whether you're drawing each tile on the CPU or GPU (or both).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The word &quot;static&quot; there was important. If much of the background is changing every frame, you don't get any advantage from tiles, because you have to update every tile every frame. In that case, you should just throw away the old frame and draw the new frame from scratch. That's why many 2D games don't use this technique even if they show scrolling content (e.g. a Mario game).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;In summary:&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If your scrolling content is completely static and small enough to fit in memory, draw it in a big texture and render a textured quad per-frame.&lt;/li&gt;&#xA;&lt;li&gt;If your scrolling content is too big to fit in memory, or small regions of it need updating sometimes, draw it in tiles, render all the visible tiles per-frame, and add something to load and unload tiles when necessary.&lt;/li&gt;&#xA;&lt;li&gt;If most tiles change most frames, just draw the visible region every frame.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Knowing which case you're in, and being able to separate out the static parts from the changing parts, constitutes a lot of the work of the graphics developer :-)&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2015-11-13T10:38:50.640" CommentCount="0" />
  <row Id="1716" PostTypeId="1" AcceptedAnswerId="1724" CreationDate="2015-11-13T10:48:15.710" Score="3" ViewCount="47" Body="&lt;p&gt;I have a small project that needs to extract the perceptually-salient colors of an image, in javascript. There are a few libraries out there (color-thief.js, vibrant.js). The trouble is, I'm finding that &lt;a href=&quot;https://color.adobe.com/create/image/&quot; rel=&quot;nofollow&quot;&gt;adobe color's&lt;/a&gt; image extraction does a noticeably better job of recognizing the little high-contrast highlight colors which make a big visual aesthetic difference, even though they represent a small number of pixels. (This is the &quot;color mood&quot; option). For example, the color of lipstick a person wears, or small decorations on a big cake.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think (?) Adobe doesn't provide an API for their Color CC any more, but in any case I'd rather not rely on it. Is there a javascript library out there with similar functionality that provides access to some sort of functionality that maximizes dissimilarity between hues?&lt;/p&gt;&#xA;" OwnerUserId="2040" LastEditorUserId="2040" LastEditDate="2015-11-13T10:59:19.023" LastActivityDate="2015-11-16T10:33:47.740" Title="Javascript color extraction library" Tags="&lt;color&gt;&lt;color-separation&gt;&lt;javascript&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1717" PostTypeId="2" ParentId="1694" CreationDate="2015-11-13T16:50:33.490" Score="3" Body="&lt;p&gt;Both of your two proposed solutions are valid, with different properties. Which you choose is purely a design issue. The difference is that your probabilistic rays/particles (1) don't need to store their energy: they all have the same amount of energy, so you can just count particles to sum the energy. Your partial particles (2) need to store an energy so you can split them up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The advantage of 1 is that each ray is smaller. Also, it's easy to keep track of where the energy ended up, because you can use the intersections where the ray was dropped.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because you're only reflecting or discarding rays, the number of in-flight rays will decrease with each bounce, so you might find it harder to keep batches of rays together for better SIMD or parallel performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, because you're probabilistically discarding rays, you'll get a lot more noise in your results for the same number of rays.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second solution is closer to a traditional ray-tracer, so you may find it easier to use existing algorithms and code. You can use fewer primary rays, and the number of rays in each batch is constant at first (but gets worse when the brightnesses start to reach zero).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also a lot more efficient to track multiple frequencies this way. If you give each ray a spectrum or RGB energies, the material can have a different effect on different colours without you having to fire multiple batches of rays (duplicating the ray intersection work).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand, you'll need a second data structure to count up where energy was absorbed, if you want to track that.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your place, I'd probably use the second solution, but it depends on what other software you're trying to integrate with e.g. an existing ray-tracing library or photon representation, and what you need to do with the output/results.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2015-11-13T16:50:33.490" CommentCount="0" />
  <row Id="1718" PostTypeId="1" AcceptedAnswerId="1719" CreationDate="2015-11-13T17:38:28.863" Score="8" ViewCount="265" Body="&lt;p&gt;I have a mesh and in the region around each triangle, I want to compute an estimate of the principal curvature directions. I have never done this sort of thing before and &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_curvature&quot;&gt;Wikipedia&lt;/a&gt; does not help a lot. Can you describe or point me to a simple algorithm that can help me compute this estimate?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Assume that I know positions and normals of all vertices.&lt;/p&gt;&#xA;" OwnerUserId="14" LastActivityDate="2015-11-14T09:54:46.410" Title="What is the simplest way to compute principal curvature for a mesh triangle?" Tags="&lt;mesh&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1719" PostTypeId="2" ParentId="1718" CreationDate="2015-11-14T00:43:02.683" Score="6" Body="&lt;p&gt;When I needed an estimate of mesh curvature for a skin shader, the algorithm I ended up settling on was this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, I computed a scalar curvature for each edge in the mesh. If the edge has positions $p_0, p_1$ and normals $n_0, n_1$, then I estimated its curvature as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\frac{(n_1 - n_0) \cdot (p_1 - p_0)}{(p_1 - p_0)^2}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This calculates the difference in normals, projected along the edge, as a fraction of the length of the edge. You can verify that if applied to two points on a circle of radius $r$, this gives the correct curvature $1/r$. It also correctly gives a negative curvature if you invert the normals.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, for each vertex I looked at the curvatures of all the edges touching it. In my case, I just wanted a scalar estimate of &quot;average curvature&quot;, so I ended up taking the geometric mean of the absolute values of all the edge curvatures at each vertex. For your case, you might find the minimum and maximum curvatures, and take those edges to be the principal curvature directions (maybe orthonormalizing them with the vertex normal). That's a bit rough, but it might give you a good enough result for what you want to do.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another approach I can imagine using, but haven't tried, would be to estimate the &lt;a href=&quot;https://en.wikipedia.org/wiki/Second_fundamental_form&quot;&gt;second fundamental form&lt;/a&gt; of the surface at each vertex. This could be done by setting up a tangent basis at the vertex, then converting all neighboring vertices into that tangent space, and using least-squares to find the best-fit 2FF matrix. Then the principal curvature directions would be the eigenvectors of that matrix. This seems interesting as it could let you find curvature directions &quot;implied&quot; by the neighboring vertices without any edges explicitly pointing in those directions, but on the other hand is a lot more code, more computation, and perhaps less numerically robust.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-11-14T00:43:02.683" CommentCount="0" />
  <row Id="1720" PostTypeId="1" AcceptedAnswerId="1722" CreationDate="2015-11-14T03:57:21.970" Score="11" ViewCount="136" Body="&lt;p&gt;I am trying to understand why meshes that are smoothed in 3D studio (Modifiers/Smoother) end up having the same amount of vertices/faces before or after that process, as well as the exact same geometry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the following example, both meshes have 32 vertices and 60 faces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JQ1KW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JQ1KW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dvqdb.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dvqdb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although I have experience working with programming (c++ and c#), I am quite a beginner with computer graphics. Thus my expectation was that the smoothed look of a smoothed mesh would require additional vertices, i.e. subdivision of triangles, in order to end up having a more detailed final mesh. However, that seems to be not the case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore I ask:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) how is the smoothing possible without increasing the detailing of the mesh geometry?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) does the smoothing at least increase the memory allocated/used for storing the new mesh in comparison to the original one?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Explanations are super welcome, but references (academic or not) are also appreciated.&lt;/p&gt;&#xA;" OwnerUserId="2061" LastActivityDate="2015-11-14T13:52:00.097" Title="Why smoothed meshes in 3D studio end up with the same number of vertices/triangles? How then can they be smoothed with the same geometry?" Tags="&lt;3d&gt;&lt;geometry&gt;&lt;mesh&gt;&lt;memory&gt;&lt;model&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1721" PostTypeId="2" ParentId="1718" CreationDate="2015-11-14T09:36:56.117" Score="9" Body="&lt;p&gt;Just to an add another way to the excellent @NathanReed answer, you can use mean and gaussian curvature that can be obtained with a discrete Laplace-Beltrami.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So suppose that the 1-ring neighbourhood of $v_i$ in your mesh looks like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;http://i.stack.imgur.com/aWaSm.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/aWaSm.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$A(v_i)$ can be simply a $\frac{1}{3}$ of the areas of the triangles that form this ring and the indicated $v_j$ is one of the neighbouring vertices. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now let's call $f(v_i)$ the function defined by your mesh (must be a differentiable manifold) at a certain point. The most popular discretization of the Laplace-Beltrami operator that I know is the cotangent discretization and is given by: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\Delta_S f(v_i) = \frac{1}{2A(v_i)} \sum_{v_j \in N_1(v_i)} (cot \alpha_{ij} + cot \beta_{ij}) (f(v_j) - f(v_i)) $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $v_j \in N_1(v_i)$ means every vertex in the one ring neighbourhood of $v_i$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;With this is pretty simple to compute the mean curvature (now for simplicity let's call the function of your mesh at the vertex of interest simply $v$ ) is&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$H = \frac{1}{2} || \Delta_S v || $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now let's introduce the angle $\theta_j$ as &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;http://i.stack.imgur.com/ysAHy.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ysAHy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Gaussian curvature is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$K = (2\pi - \sum_j \theta_j) / A$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After all of this pain, the principal discrete curvatures are given by: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$k_1 = H + \sqrt{H^2 - K} \ \  \text{and} \ \  k_2 = H - \sqrt{H^2 - K}$$&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If you are interested in the subject (and to add some reference to this post) an excellent read is:   &lt;a href=&quot;http://http.cs.berkeley.edu/~jrs/meshpapers/MeyerDesbrunSchroderBarr.pdf&quot;&gt;Discrete Differential-Geometry Operators&#xA;for Triangulated 2-Manifolds&lt;/a&gt; [Meyer et al. 2003]. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the images I thank my ex-professor Niloy Mitra as I found them in some notes I took for his lectures.&lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2015-11-14T09:54:46.410" LastActivityDate="2015-11-14T09:54:46.410" CommentCount="2" />
  <row Id="1722" PostTypeId="2" ParentId="1720" CreationDate="2015-11-14T13:52:00.097" Score="13" Body="&lt;p&gt;Smooth in this case just makes the surface normals at vertices point the same way, when interpolated it looks smooth. Meshsmooth would add vertices.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;1) how is the smoothing possible without increasing the detailing of the mesh geometry?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Human eyes cant actually see curvature except on the edges of objects. All they can do is approximate the smoothness and process the gradient slope. So having a continuous field does give a air of smoothness. The eye however is extremely sensitive to abrupt changes in color, and interprets that as a hard crease.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By interpolating the vertex normals your surface will get the appearance of smooth flowing. Since this normal is used to calculate the final reflected color you get a smooth color field.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EqJkQ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EqJkQ.png&quot; alt=&quot;Flat shaded vs smooth shaded&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: a flat shaded normal versus the normals of a smooth interplation. The black normal's lie on a vertex. The colored ones are interpolated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is nothing that says we need to do a linear interpolation. In fact by perturbing the normals we can cause the flat surface to change appearance. This is how bump mapping and normal mapping works. The effect can be convincing unless the surfaces edge plays a too big part in which place the illusion breaks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/l62Ov.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/l62Ov.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: a flat shaded surface (back), Smooth shaded (middle) and a mapped smooth normal. The illusion of a wavy surface breaks because the edge plays so prominent part in the image, you could instead increase the normals&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;2) does the smoothing at least increase the memory allocated&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Hard to say definitive things about the underlying graphics engine. The normals need to be emitted to the graphics card anyway, most likely this data is cached, but could be calculated on the fly (in both cases).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since Max uses smoothing groups it seems to me that memory usage is constant regardless. Hard to say, even if its not cached then it wouldn't make a big difference. It makes the shader tiny a bit more complicated, but only just most likely this complexity is present use it or not.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2015-11-14T13:52:00.097" CommentCount="5" />
  <row Id="1723" PostTypeId="2" ParentId="166" CreationDate="2015-11-15T12:12:19.667" Score="0" Body="&lt;p&gt;One can do curved drawing with hardware. There is a method described in &lt;strong&gt;GPU Gems 3&lt;/strong&gt; that describes how to do this. User @yuriks actually comments this. I have actually made a quick a dirty demo for you to take a look at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/D9F9k.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/D9F9k.png&quot; alt=&quot;curve&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: HW accelerated curve shapen (drawn usung a triangle) and &lt;a href=&quot;http://webglplayground.net/saved/9ZD4Uoi72Q&quot; rel=&quot;nofollow&quot;&gt;webgl source&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-15T12:17:42.913" LastActivityDate="2015-11-15T12:17:42.913" CommentCount="0" />
  <row Id="1724" PostTypeId="2" ParentId="1716" CreationDate="2015-11-16T10:33:47.740" Score="1" Body="&lt;p&gt;In the end I decided to use Leon Sorokin's RGBQuant.js for quantization, because it offered better flexibility over the color histogram/clustering method. I'm probably going to convert the quantized palette RGB output to HSV and implement custom clustering that picks out the &quot;highlight&quot; colors, favoring diversity of hue for colors with high value and saturation, and low occurrence frequency.&lt;/p&gt;&#xA;" OwnerUserId="2040" LastActivityDate="2015-11-16T10:33:47.740" CommentCount="0" />
  <row Id="1725" PostTypeId="1" AcceptedAnswerId="2004" CreationDate="2015-11-16T15:25:18.107" Score="12" ViewCount="274" Body="&lt;p&gt;There have been lots of papers over the years on different techniques for drawing height-field terrain in a ray-tracer. Some algorithms ray-march the grid directly (or via a quadtree); others transform the terrain into a polygon mesh and use a standard ray-triangle intersection test. The research seems to have moved on in the last few years, and it's hard to find papers written in the last decade, but the balance between memory and compute (both CPU and GPU) is still changing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What kind of algorithm gives best performance on high-end desktop machines nowadays? Or if there isn't a single answer, how do the performance characteristics of the current best algorithms differ?&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2016-06-10T14:16:48.143" Title="What's the current state-of-the-art algorithm for ray-tracing height-fields?" Tags="&lt;raytracing&gt;&lt;height-field&gt;" AnswerCount="3" CommentCount="5" FavoriteCount="2" />
  <row Id="1726" PostTypeId="5" CreationDate="2015-11-16T15:46:10.380" Score="0" Body="&lt;p&gt;A height-field is a way of defining a surface with an image, where the co-ordinates of the image represent co-ordinates on a plane, and the intensity of the image at each point represents the height of the surface from the plane at that point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's a compact representation of 2.5D surfaces such as mountainous terrain, and is easier to create and modify in use than a polygon-based representation, but can't represent holes in the height field (such as a cave system or bridge).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compare &lt;em&gt;displacement map&lt;/em&gt;, which is essentially a height field on a smaller scale to represent microgeometry on a polygon mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See also:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Heightmap&quot; rel=&quot;nofollow&quot;&gt;Heightmap on Wikipedia&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-16T15:51:57.453" LastActivityDate="2015-11-16T15:51:57.453" CommentCount="0" />
  <row Id="1727" PostTypeId="4" CreationDate="2015-11-16T15:46:10.380" Score="0" Body="Questions about image-based height-field representations, commonly used to represent a terrain." OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-16T15:52:11.630" LastActivityDate="2015-11-16T15:52:11.630" CommentCount="0" />
  <row Id="1728" PostTypeId="2" ParentId="1725" CreationDate="2015-11-16T18:19:51.997" Score="3" Body="&lt;p&gt;The best I've personally seen is the stuff inigo quillez does, which is used in demoscene stuff. Ray March the terrain, taking larger steps the farther you get from the camera since (usually) detail matters less at a distance (exception = thin walls!). He uses penetration info and other easily gotten metrics to simulate ambient occlusion and other sophisticated lighting techniques.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a demo of the stuff in action:&#xA;&lt;a href=&quot;https://www.youtube.com/watch?v=_YWMGuh15nE&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=_YWMGuh15nE&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here is IQ's page on the terrain raymarching which is a pretty interesting read:&#xA;&lt;a href=&quot;http://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&quot; rel=&quot;nofollow&quot;&gt;http://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, in modern games, the technique of &quot;screen space reflection&quot; often is just a ray march against the Z buffer of the rendered scene.  The Z buffer is really just a heightfield.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw some talks on this at siggraph 2014, and while some of the people were using similar techniques as IQ, a few were doing things not even as well as IQ, which was interesting to see :P&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2015-11-16T19:00:24.480" LastActivityDate="2015-11-16T19:00:24.480" CommentCount="6" />
  <row Id="1729" PostTypeId="1" CreationDate="2015-11-16T22:52:48.660" Score="3" ViewCount="54" Body="&lt;p&gt;I'm using hardware tessellation to render a terrain mesh with variable LOD.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have this snippet on my tessellation control shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;int edge1 = gl_InvocationID;&#xA;int edge2 = (gl_InvocationID + 1) % 3;&#xA;tcs_out[gl_InvocationID].color = vec4(computeLODColor(gl_TessLevelOuter[edge1], gl_TessLevelOuter[edge2]), 1.0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This should compute a color for the control point based on the tessellation level of the two edges that intersect on the point (using quad patches).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's all working fine except for this: &lt;code&gt;gl_TessLevelOuter[edge1]&lt;/code&gt; and &lt;code&gt;gl_TessLevelOuter[edge1]&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This indexing always return 0 (I've tried using it directly as a color). To test the problem, I tried this: &lt;code&gt;gl_TessLevelOuter[gl_InvocationID]&lt;/code&gt;. It also has the same behavior.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I do something like this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(gl_InvocationID == 0) {&#xA;    tcs_out[gl_InvocationID].color = vec4(computeLODColor(gl_TessLevelOuter[0], gl_TessLevelOuter[1]), 1.0);&#xA;}&#xA;else if(gl_InvocationID == 1) {&#xA;    tcs_out[gl_InvocationID].color = vec4(computeLODColor(gl_TessLevelOuter[1], gl_TessLevelOuter[2]), 1.0);&#xA;}&#xA;else if(gl_InvocationID == 2) {&#xA;    tcs_out[gl_InvocationID].color = vec4(computeLODColor(gl_TessLevelOuter[2], gl_TessLevelOuter[3]), 1.0);&#xA;}&#xA;else if(gl_InvocationID == 3) {&#xA;    tcs_out[gl_InvocationID].color = vec4(computeLODColor(gl_TessLevelOuter[3], gl_TessLevelOuter[0]), 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;then it all works as expected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've checked the OpenGL specification and couldn't find anything about undefined behavior when indexing &lt;code&gt;gl_TessLevelOuter&lt;/code&gt; by a non-constant value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is, am I misunderstanding something, doing something against the specification or is it a GL implementation bug?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using an AMD Radeon HD 6700 with updated drivers, and there are no warnings compiling the shaders. (I've never seen a single shader compiler warning  though, so not sure if that's another problem)&lt;/p&gt;&#xA;" OwnerUserId="2064" LastActivityDate="2015-11-16T22:52:48.660" Title="Indexing gl_TessLevelOuter[] with gl_InvocationID on a tessellation control shader" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="1730" PostTypeId="1" CreationDate="2015-11-17T16:21:39.400" Score="3" ViewCount="94" Body="&lt;p&gt;I have been trying to implement the following functionality in my code:&#xA;&lt;br&gt;There are 2 flags editMode and deleteMode, both set to zero.&#xA;&lt;br&gt;with the help of keys function in glutKeyBoardFunc(keys) the user can press keys ( 'c' and 'd' respectively) to set editMode and deleteMode to 1.The following is the algo for my mouse fucntion.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;when ( editMode == 0)&#xA;       left click plots the point&#xA;       right click plots beizer curve&#xA;when ( editMode == 1)&#xA;       left click to displace the point to new postion and draw new beizer curve&#xA;       right click to exit editMode(i.e set editMode to 0)&#xA;when (deleteMode = 1)&#xA;       left click to click any point to delete it and draw new beizer curve&#xA;       right click to exit delete mode(i.e set deleteMode to 0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is when I am in editMode == 1, on first instance of left click my delete function is called to delete a point in beizer curve.&#xA;&lt;br&gt; Is there anything specific to the mouse listener which I am missing?&#xA;&lt;br&gt; Any new way to set controls for the above functions is welcome..&#xA;&lt;br&gt; Thank you.. &#xA;&lt;br&gt; EDIT  1:&#xA;&lt;br&gt; I modified the logic a bit. So I have:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;when( deleteMode == 0)&#xA;{       when (editMode == 0)&#xA;           left click to plot points &#xA;           right click to plot beizer&#xA;        else when (editFlag == 1)&#xA;           left click to displace points and draw new beizer curve&#xA;           right click to exit editMode(i.e set editMode to 0)&#xA;}&#xA;else&#xA;{&#xA;       when ( left_mouse_button == TRUE)&#xA;       { if(editMode == 0 ) delete chosen point and draw new beizer curve}&#xA;       when ( right_mouse_button == TRUE)&#xA;       { exit deleteMode e(i.e set deleteMode to 0) }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This works fairly for most of the time.And is serving my purpose for now.I have the assumption that the user goes to one mode and exits from that mode before going into other mode.&lt;/p&gt;&#xA;" OwnerUserId="1588" LastEditorUserId="1588" LastEditDate="2015-11-19T07:41:36.640" LastActivityDate="2015-11-19T07:41:36.640" Title="How does function for mouse in glutMouseFunc(mouse) work in openGL?" Tags="&lt;opengl&gt;" AnswerCount="0" CommentCount="7" ClosedDate="2015-12-02T18:12:39.340" />
  <row Id="1731" PostTypeId="1" AcceptedAnswerId="1732" CreationDate="2015-11-17T21:51:32.030" Score="6" ViewCount="96" Body="&lt;p&gt;I have two rectangles — one with a fill (blue) and one with a stroke (red). The red rectangle is being offset (depending on the stroke width) so that it appears snug and outside the edge of the blue rectangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This set up works great except when I need to round the corners. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see I'm getting gaps between the rectangles and I'm stuck trying to figure out the math to calculate the right roundness amount for the outside rectangle (red).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://dl.dropboxusercontent.com/u/1414976/rectangles.png&quot; alt=&quot;Rectangles&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="2076" LastEditorUserId="127" LastEditDate="2015-11-18T08:22:57.373" LastActivityDate="2015-11-18T08:22:57.373" Title="How to calculate matching roundness of two offset rectangles?" Tags="&lt;curve&gt;&lt;2d&gt;&lt;bezier-curve&gt;&lt;vector-graphics&gt;&lt;line-drawing&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1732" PostTypeId="2" ParentId="1731" CreationDate="2015-11-18T07:56:05.453" Score="7" Body="&lt;p&gt;What you (probably) want to achieve is something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Y1crM.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Y1crM.png&quot; alt=&quot;box&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When having a closer look at one of the corners and add a few lines, we see this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4w6mA.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4w6mA.png&quot; alt=&quot;corner&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The black lines indicate that the center points of the circles along the borders of the red and blue boxes is the same. If the outer radius of the red box, for example, is $50px$, and the distance between the outer borders of the red and blue boxes is $25px$, this leads to a border radius of $50px - 25px = 25px$ for the blue box. The distance, obviously, is dependent on the stroke width of your red stroke.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Note: When creating the above images, I simply drew two filled boxes with rounded corners. So first a filled red box and then a smaller filled blue box above it. This also avoids some pixels being left white due to rounding errors (no pun intended).&lt;/p&gt;&#xA;" OwnerUserId="127" LastActivityDate="2015-11-18T07:56:05.453" CommentCount="4" />
  <row Id="1734" PostTypeId="1" AcceptedAnswerId="1738" CreationDate="2015-11-19T01:58:30.133" Score="3" ViewCount="290" Body="&lt;p&gt;After spending a few days making very little headway with a simple Raytracing program that implements Phong illumination (with shadows and no attenuation), I'm convinced I've made a logic error that I'm overlooking.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I understand it, the pseudo-code should look like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ambient light&#xA;for each light source&#xA;    direction of light = light point - point on the object&#xA;    test for an intersection starting at the point on the object, until you hit something&#xA;    if (the intersection test returned a distance &amp;gt; distance from point on object to light point)&#xA;    (do  diffuse and specular shading)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A one quick thing to mention about my implementation: the intersection test will return 9999999 if there is no intersection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the things I've tried are: inverting the direction of the shadow ray - no effect. Making the comparison intersection distance - light distance &gt; (some small epsilon value) - no change.  Manually moving the light source to the opposite side - the specular reflection moved to a new spot, but otherwise, no change.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a picture of my most recent output, I'm trying to get a shadow in the bottom left corner of the light gray plane.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/jnsiR.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jnsiR.jpg&quot; alt=&quot;The shadow should be in the lower left corner of the plane&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be greatly appreciated.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the relevant part of my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    //iterate through all lights&#xA;    if (myLights.size() &amp;gt; 0)&#xA;    {&#xA;        for (int i = 0; i &amp;lt; myLights.size(); i++)&#xA;        {&#xA;            //calculate the direction of the light&#xA;            glm::vec3 ldir = (myLights[i].position - surfacePoint);&#xA;            //this the length of the distance between the light and the surface&#xA;            float ldist= sqrt(pow(ldir.x, 2.0f) + pow(ldir.y, 2.0f) + pow(ldir.z, 2.0f)); &#xA;//check if the light can see the surface point&#xA;            float intersections = myObjGroup-&amp;gt;testIntersections(surfacePoint, normalize(ldir));&#xA;            if (abs(intersections-ldist)  &amp;gt; 0.0000001f)&#xA;            {&#xA;                //diffuse and specular shading have been cut for brevity&#xA;            }&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;edit: New image:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/2pAxT.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2pAxT.jpg&quot; alt=&quot;New image!&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2084" LastEditorUserId="2084" LastEditDate="2015-11-19T23:55:07.770" LastActivityDate="2015-11-20T09:15:00.770" Title="Shadow rays in Raytracing" Tags="&lt;raytracing&gt;&lt;c++&gt;&lt;shadow&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1736" PostTypeId="1" AcceptedAnswerId="1737" CreationDate="2015-11-19T04:10:30.597" Score="8" ViewCount="202" Body="&lt;p&gt;When rendering VR ( stereo view ) environments, do you guys recommend just doing 2 frustum checks for determining what is to be drawn during the frustum culling pass or is there some other check that would be able to combine both frusta?&lt;/p&gt;&#xA;" OwnerUserId="288" LastActivityDate="2015-11-19T05:09:30.257" Title="VR and frustum culling" Tags="&lt;virtual-reality&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1737" PostTypeId="2" ParentId="1736" CreationDate="2015-11-19T05:09:30.257" Score="7" Body="&lt;p&gt;It's pretty easy to create a single frustum that encloses both of the individual eye frusta, as shown in this diagram created by Cass Everitt at Oculus. You just have to place the vertex of this frustum between the eyes and behind them a little bit. You can also increase the near plane distance to match the original near plane of the eye frusta.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TpHYa.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TpHYa.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Such a frustum includes a little space that wasn't in the union of the original two frusta. There's a small volume horizontally between the two eyes; also, the diagram doesn't show the vertical dimension, but pulling the frustum vertex back makes it include some space above and below the original frusta. However, for practical culling purposes it probably doesn't matter. Frustum culling is usually conservative anyway, and the extra space is only a few centimeters in thickness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also possible to simply cull against the original frustum top and bottom planes (which are shared between eyes) together with the outer side planes. These planes don't form a frustum, technically, but if using &quot;the usual&quot; culling algorithm that tests against one plane at a time, the algorithm will happily work on any set of planes you care to give it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sharing frustum culling, LOD selection, etc. between eyes, rather than re-doing it twice every frame, is a great way to optimize the CPU cost of stereo rendering. This is also a prerequisite if you want to use a rendering method that draws both eyes in a single pass, such as &lt;a href=&quot;https://docs.google.com/presentation/d/19x9XDjUvkW_9gsfsMQzt3hZbRNziVsoCEHOn4AercAc/&quot;&gt;instanced stereo rendering&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-11-19T05:09:30.257" CommentCount="2" />
  <row Id="1738" PostTypeId="2" ParentId="1734" CreationDate="2015-11-19T10:11:47.387" Score="4" Body="&lt;p&gt;Your &lt;code&gt;if&lt;/code&gt; condition makes me suspicious. You should include the diffuse and specular shading if the intersection test &lt;strong&gt;didn't&lt;/strong&gt; hit an object; that is, if &lt;code&gt;intersections &amp;gt; ldist&lt;/code&gt;. So, your code should look as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (intersections &amp;gt; ldist)&#xA;{&#xA;    colour += diffuse(...);&#xA;    colour += specular(...);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Your comparison with 0.0000001f suggests that you've also been having trouble with self-intersections (that is, with the ray intersecting the object being shaded). In a one-hit tracer, you can't fix that problem by simply discarding the intersection, because you still don't know whether the shadow ray would have intersected another object further along. To avoid self-intersections, you need to push the start point of the ray along its direction by epsilon:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glm::vec3 direction = normalize(ldir);&#xA;glm::vec3 start = surfacePoint + epsilon * direction;&#xA;float intersections = myObjGroup-&amp;gt;testIntersections(start, direction);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One more thing: don't forget to test that the light is in front of the surface that you're illuminating. You can do this with the dot product of &lt;code&gt;ldir&lt;/code&gt; and the surface normal: if the dot product is positive, the light is on the correct side; if it's negative, the light is behind the surface and thus shadowed before you even cast a ray.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-20T09:15:00.770" LastActivityDate="2015-11-20T09:15:00.770" CommentCount="2" />
  <row Id="1739" PostTypeId="5" CreationDate="2015-11-19T13:01:01.153" Score="0" Body="&lt;p&gt;A shadow is a dark region caused when light from an emitter is blocked by a shadowing object, causing a contrast between the dark region and the surrounding region which is fully lit. The emitter might be a point, cone, cylindrical light, an area light or image-based light. The shadowing object might be fully opaque or partially transparent, in which case it will cast coloured shadows (like stained glass). The dark region might be the lighting on another surface (the shadowed object), or it might be an environment shadow in some kind of volumetric medium.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This tag is appropriate whatever technique you're using to simulate shadows, whether by shadow rays in a raytracer, some kind of shadow maps or volumes, or something else completely.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your problem is with shadowing indirect light in a global illumination system, you probably want a more specific tag. If your problem is with self-shadowing of micro-geometry, e.g. in the derivation of a shader, you should use a tag about that.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-19T13:48:46.187" LastActivityDate="2015-11-19T13:48:46.187" CommentCount="0" />
  <row Id="1740" PostTypeId="4" CreationDate="2015-11-19T13:01:01.153" Score="0" Body="Shadows cast by objects in front of lights, whether raytraced, shadow maps, shadow volumes, or any other technique" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2015-11-19T13:48:52.383" LastActivityDate="2015-11-19T13:48:52.383" CommentCount="0" />
  <row Id="1741" PostTypeId="1" CreationDate="2015-11-19T18:17:37.720" Score="6" ViewCount="54" Body="&lt;p&gt;I'm doing a quick and dirty automated deformation routine on a polygon body.  If it was a tree, and my axis was in the center of the tree, I would like to bend the body by bending the axis.  I would use a simple influencing algorithm to determine vertex point xyz displacement.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the concave side of the bend, how can I check for polygons that have overlapped or passed through each other, and how can I fix such polygons?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ObYbr.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ObYbr.png&quot; alt=&quot;Rough Illustration of concept&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2091" LastEditorUserId="2091" LastEditDate="2015-11-20T18:26:08.183" LastActivityDate="2015-11-20T18:26:08.183" Title="After a deformation operation on polygons, how can I check for and fix inverted polys?" Tags="&lt;algorithm&gt;&lt;3d&gt;&lt;transformations&gt;&lt;mesh&gt;&lt;computational-geometry&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="1743" PostTypeId="1" CreationDate="2015-11-19T22:31:31.463" Score="4" ViewCount="202" Body="&lt;p&gt;I've been trying to implement the &lt;a href=&quot;https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm&quot; rel=&quot;nofollow&quot;&gt;Moller-Trumbore ray-triangle intersection algorithm&lt;/a&gt; in my raytracing code. The code is supposed to read in a mesh and light sources, fire off rays from the light source, and return the triangle from the mesh which each ray intersects. Here is my implementation of the algorithm:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//Moller-Trumbore intersection algorithm&#xA;void getFaceIntersect(modelStruct m, ray r, hitFaceStruct&amp;amp; hitFaces)&#xA;{&#xA;    // Constant thoughout loop&#xA;    point origin = r.p0;&#xA;    point direction = r.u;&#xA;    hitFaces.isHit = false;&#xA;&#xA;    for (int i = 0; i &amp;lt; m.faces; i++)&#xA;    {&#xA;        // Get face vertices&#xA;        point v1 = m.vertList[m.faceList[i].v1];&#xA;        point v2 = m.vertList[m.faceList[i].v2];&#xA;        point v3 = m.vertList[m.faceList[i].v3];&#xA;&#xA;        // Get two edgess&#xA;        point edge1 = v2 - v1;&#xA;        point edge2 = v3 - v1;&#xA;&#xA;        // Get p&#xA;        point p = direction.cross(direction, edge2);&#xA;        // Use p to find determinant&#xA;        double det = p.dot(edge1, p);&#xA;&#xA;        // If the determinant is about 0, the ray lies in the plane of the triangle&#xA;        if (abs(det) &amp;lt; 0.00000000001)&#xA;        {&#xA;            continue;&#xA;        }&#xA;&#xA;        double inverseDet = 1 / det;&#xA;&#xA;        point v1ToOrigin = (origin - v1);&#xA;&#xA;        double u = v1ToOrigin.dot(v1ToOrigin, p) * inverseDet;&#xA;&#xA;        // If u is not between 0 and 1, no hit&#xA;        if (u &amp;lt; 0 || u &amp;gt; 1)&#xA;        {&#xA;            continue;&#xA;        }&#xA;&#xA;        // Used for calculating v&#xA;        point q = v1ToOrigin.cross(v1ToOrigin, edge1);&#xA;        double v = direction.dot(direction, q) * inverseDet;&#xA;&#xA;        if (v &amp;lt; 0 || (u + v) &amp;gt; 1)&#xA;        {&#xA;            continue;&#xA;        }&#xA;&#xA;        double t = q.dot(edge2, q) * inverseDet;&#xA;&#xA;        // gets closest face&#xA;        if (t &amp;lt; abs(hitFaces.s)) &#xA;        {&#xA;            hitFaceStruct goodStruct = hitFaceStruct();&#xA;            goodStruct.face = i;&#xA;            goodStruct.hitPoint = p;&#xA;            goodStruct.isHit = true;&#xA;            goodStruct.s = t;&#xA;            hitFaces = goodStruct;&#xA;            break;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The relevant code for hitFaceStruct and modelStruct is as follows:       &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;typedef struct _hitFaceStruct&#xA;{&#xA;    int face; //the index of the sphere in question in the list of faces&#xA;    float s; //the distance from the ray that hit it&#xA;    bool isHit;&#xA;    point hitPoint;&#xA;} hitFaceStruct;&#xA;&#xA;typedef struct _modelStruct {&#xA;    char *fileName;&#xA;    float scale;&#xA;    float rot_x, rot_y, rot_z;&#xA;    float x, y, z;&#xA;    float r_amb, g_amb, b_amb;&#xA;    float r_dif, g_dif, b_dif;&#xA;    float r_spec, g_spec, b_spec;&#xA;    float k_amb, k_dif, k_spec, k_reflective, k_refractive;&#xA;    float spec_exp, index_refraction;&#xA;    int verts, faces, norms = 0;    // Number of vertices, faces, normals, and spheres in the system&#xA;    point *vertList, *normList; // Vertex and Normal Lists&#xA;    faceStruct *faceList;       // Face List&#xA;} modelStruct;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Whenever I shoot a ray, the values of &lt;code&gt;u&lt;/code&gt; or &lt;code&gt;v&lt;/code&gt; in the algorithm code always come out to a large negative number, rather than the expected small, positive one. The direction vector of the ray is normalized before I pass it on to the intersection code, and I'm positive I'm firing rays that would normally hit the mesh. Can anyone please help me spot my error here?&lt;/p&gt;&#xA;" OwnerUserId="2092" LastEditorUserId="127" LastEditDate="2015-12-04T21:46:16.450" LastActivityDate="2015-12-04T21:46:16.450" Title="Ray-triangle intersection algorithm not intersecting (C++)" Tags="&lt;opengl&gt;&lt;raytracing&gt;&lt;algorithm&gt;&lt;c++&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1744" PostTypeId="1" CreationDate="2015-11-20T12:41:12.840" Score="1" ViewCount="64" Body="&lt;p&gt;I am looking for smooth way to enhance scans of golden emboss on. For now it takes sometimes hours to select out the golden part and then manipulate into suitable color. But I hope to learn some more neat way to do it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So here I provide 2 sample images, one before and other after:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vQvZi.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vQvZi.jpg&quot; alt=&quot;image after scan&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LFW5N.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LFW5N.jpg&quot; alt=&quot;image with enhanced golden emboss&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, after lot of annoying work to select out the golden part from background it still looks out clumsy. So, how it might work out smoothly? Is there some tutorial for that?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basicly, what I did now? &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;made duplicate layer of background&lt;/li&gt;&#xA;&lt;li&gt;selected and erased background on duplicate layer with different tools (most work of which I'd like to get rid of)&lt;/li&gt;&#xA;&lt;li&gt;selected background and inverted selection&lt;/li&gt;&#xA;&lt;li&gt;feathered selection 5px&lt;/li&gt;&#xA;&lt;li&gt;with Colorize got it to some goldish tone&lt;/li&gt;&#xA;&lt;li&gt;slide layer opacity ca 60%&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2100" LastEditorUserId="16" LastEditDate="2015-11-20T15:53:18.737" LastActivityDate="2015-11-20T15:53:18.737" Title="How to enhance scans of golden emboss?" Tags="&lt;image-processing&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="1745" PostTypeId="1" AcceptedAnswerId="1746" CreationDate="2015-11-19T16:45:14.177" Score="6" ViewCount="66" Body="&lt;p&gt;I'm trying to make a HSV representation of the xyY color space. To calculate hue from an $(x, y)$ color, I use the angle between that color and red (wavelength 745) on the xy chromacity diagram, with white $(\frac{1}{3}, \frac{1}{3})$ as the center.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The saturation is the ratio between the distance between white and $(x, y)$, and white and a fully-saturated version of $(x, y)$ (which is the intersection between the line between $(\frac{1}{3}, \frac{1}{3})$ and $(x, y)$ and the edge of the chromacity diagram).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;xy chromacity diagram:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/SzQiv.png&quot; width=&quot;300&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem that I'm having is that when I plot my color space (at value=1) and compare it to the HSV representation of RGB, the saturation (distance from center) doesn't seem to match how &quot;colorful&quot; the color actually is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;My color space (saturation seems wrong):&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/t1rfO.png&quot; width=&quot;300&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;HSV color space of RGB:&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/g0a9x.png&quot; width=&quot;300&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I calculate saturation instead?&lt;/p&gt;&#xA;" OwnerUserId="2110" OwnerDisplayName="abcd5555" LastEditorUserId="127" LastEditDate="2015-11-21T08:33:17.890" LastActivityDate="2015-11-21T13:18:12.253" Title="What would be the correct way to calculate saturation in this case?" Tags="&lt;color&gt;&lt;color-science&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1746" PostTypeId="2" ParentId="1745" CreationDate="2015-11-21T12:04:06.943" Score="2" Body="&lt;p&gt;There is unfortunately no good answer to this question. Simply it wont work. There is no good way to define colorful, it this context. Cie is trying to capture the physical measurement. It however does not succeed very well in relating the colors to each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Colors on the very outer arc represent spectral distributions of close to &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot; rel=&quot;nofollow&quot;&gt;Dirac delta function&lt;/a&gt;. So one could construct a model that says that a color is very colorful when it is a Dirac delta. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a unforeseen consequence of this definition though. Namely the magenta colors do not exist as Dirac Deltas. As these colors do not exist in the spectrum. So they consist of mixture of 2 wavelengths only. This would mean they are less colorful than most other colors.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Other problems&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Unfortunately, xyY is not linear. So a straight line on the xyY does not represent interpolations between 2 color mixtures. Therefore making a polar transformation means you will have different color bases on same coordinates. Also precieved color do not really move over to your model. To do this properly you would need to do a extremely sophisticated transformation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many problems with converting color to a polar coordinates in that that is exactly contrary how vision works. White is also a bit problematic in this context. The distance to full saturated signal is different for each of the 3 different cones in the eye. Hell, even what is while depends on the surrounding colors and ambient color conditions. So aim afraid your trying to force a worldview that does not exist.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Finally&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;What would this be useful for? &lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-21T13:18:12.253" LastActivityDate="2015-11-21T13:18:12.253" CommentCount="0" />
  <row Id="1747" PostTypeId="1" AcceptedAnswerId="1751" CreationDate="2015-11-22T04:39:39.110" Score="9" ViewCount="144" Body="&lt;p&gt;For example, while it's the current top-of-the-line GPU, the GTX 980 has a staggering 72.1 gigapixels/second fillrate, which with back-to-front rendering and/or Z buffer checks, seems almost ridiculously large, possibly even at 4k resolutions. As far as polygon counts go, modern GPUs can do tens to hundreds of millions of textured triangles without a hitch if you batch and/or instantiate them properly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With forward rendering, the amount of fragments that shaders will run on can quickly become overwhelming, but with deferred rendering, the cost is usually more-or-less constant, depending on the resolution, and we long since passed a point where most shading or post-processing effects can be done in realtime in 1080p.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either way, the limiting factors nowadays are most commonly draw call counts, and shading costs, both of which are kept relatively low by proper deferred rendering and geometry batching, so with that in mind, is culling more than just backfaces and out-of-frustrum polygons of any substantial benefit? Wouldn't the costs(CPU/GPU time, programmer time) outweight the benefits, a lot of the time?&lt;/p&gt;&#xA;" OwnerUserId="2111" LastEditorUserId="127" LastEditDate="2015-11-22T08:37:40.507" LastActivityDate="2015-11-23T07:36:55.853" Title="With modern fillrates and deferred rendering, is occlussion culling still relevant?" Tags="&lt;performance&gt;&lt;occlusion&gt;&lt;space-filling&gt;&lt;deferred-rendering&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="1748" PostTypeId="1" AcceptedAnswerId="1749" CreationDate="2015-11-22T09:00:02.117" Score="5" ViewCount="131" Body="&lt;p&gt;I'm trying to create a RYB color wheel with a smooth-looking gradient across the RYB spectrum. The angles of the HSV wheel are incorrect (yellow at 60deg instead of 120deg etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/0u3Ea.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0u3Ea.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I just do a few linear transformations the results is still not appropriate; perceptually red occupies too many angles and yellow not enough. Even if I do a bit of a hack and try a non-linear polynomial gradient it's difficult to get the right &quot;spread&quot;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/bq1cd.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bq1cd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an &quot;official&quot; function that maps HSV hue angle to RYB hue angle?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: Reading &lt;a href=&quot;http://blufiro.blogspot.fr/2013/02/ryb-color-wheel.html&quot;&gt;this&lt;/a&gt; gives some ideas, but the part about excluding cyan confused me more because of Adobe's RYB color wheel: Is this true RYB or is it CMY with hue angles shifted in the blue range?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/U7kPu.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/U7kPu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2040" LastEditorUserId="2040" LastEditDate="2015-11-22T09:23:12.730" LastActivityDate="2015-11-22T15:21:37.757" Title="Function to convert HSV angle to RYB angle" Tags="&lt;color&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1749" PostTypeId="2" ParentId="1748" CreationDate="2015-11-22T14:56:34.147" Score="5" Body="&lt;p&gt;I just discovered that Adobe color source includes HSV-RYB hue mapping functions (replicated in Ben Knight's Kuler-d3). Apparently Adobe uses uses piecewise linear gradients rather than the polynomial that I was trying to use (and technically it is a CMY wheel not a RBY one, I believe). Here are the relevant stops:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;RYBstop HSVstop&#xA;60      35&#xA;122     60&#xA;165     120&#xA;218     180&#xA;275     240&#xA;330     300&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the graph of these showing the perceptual mapping:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/PGcnP.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/PGcnP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So there'll be artifacts at the junctions where the mapping function is not smooth (which can be seen in the original Adobe image). And here is an imagemagick script for creating the color wheel.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/bash&#xA;#assemble gradient pieces (pre-caculated stops), then join them all&#xA;convert -size 600x300 gradient:#000000-#181818 -rotate -90 grad1.png&#xA;convert -size 600x310 gradient:#181818-#2A2A2A -rotate -90 grad2.png&#xA;convert -size 600x215 gradient:#2A2A2A-#555555 -rotate -90 grad3.png&#xA;convert -size 600x265 gradient:#555555-#7F7F7F -rotate -90 grad4.png&#xA;convert -size 600x285 gradient:#7F7F7F-#AAAAAA -rotate -90 grad5.png&#xA;convert -size 600x275 gradient:#AAAAAA-#D4D4D4 -rotate -90 grad6.png&#xA;convert -size 600x150 gradient:#D4D4D4-#FFFFFF -rotate -90 grad7.png&#xA;convert grad?.png +append grad.png&#xA;#create hue&#xA;convert grad.png -alpha set -virtual-pixel Transparent -rotate 180 -distort Arc '360 -90 300' +repage -gravity center -crop 600x600+0+0 +repage h.png&#xA;#create saturation&#xA;convert -size 600x600 -alpha on radial-gradient:white-none s.png&#xA;#create value&#xA;convert -size 600x600 -alpha on radial-gradient:none-white v.png&#xA;#combine h,s,v in hsv colorspace&#xA;convert h.png s.png v.png -combine -set colorspace HSV -colorspace sRGB ryb.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ds5KS.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ds5KS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2040" LastEditorUserId="2040" LastEditDate="2015-11-22T15:21:37.757" LastActivityDate="2015-11-22T15:21:37.757" CommentCount="0" />
  <row Id="1750" PostTypeId="2" ParentId="1747" CreationDate="2015-11-22T16:08:24.660" Score="2" Body="&lt;p&gt;It depends on the game style just how much culling is needed. For instance first person shooters benefit from this a lot, having a lot of stuff in the frustrum at any one time, while an overhead view RTS does not since you are effectively looking at a plane with things on that plane.  Even in an RTS though, doing a &quot;depth only render&quot; to eliminate overdraw is still useful.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2015-11-22T16:08:24.660" CommentCount="0" />
  <row Id="1751" PostTypeId="2" ParentId="1747" CreationDate="2015-11-23T07:36:55.853" Score="9" Body="&lt;p&gt;Yes, occlusion culling is still worth it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At minimum, a draw call that you skipped due to culling is a draw call that doesn't have to run the vertex shader. Triangle count goes up as quickly as GPUs start supporting more triangles, because why not? With unified architectures, vertex shaders use the exact same hardware that pixel shaders do, so every vertex that you skip because of culling is more compute time for the stuff that you &lt;em&gt;can&lt;/em&gt; see. Not to mention all the other stuff you're skipping (CPU draw call processing, and throwing the tris far enough through the pipeline that the rasterizer realizes that it doesn't need to shade them).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There was a &lt;a href=&quot;http://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf&quot;&gt;great presentation from two Ubisoft studios at SIGGRAPH 2015&lt;/a&gt; about GPU-driven rendering pipelines. On the surface it's about some of the things you mentioned: batching and instancing, reducing draw call counts. But one of the major advantages they get out of a GPU-driven pipeline is incredibly fine-grained occlusion culling: better culling than you would normally see at the draw-call level. It's all in the service of asymptotically approaching the goal: processing only what you can see, which means that what you can see looks better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Also: consider console, mobile, VR, and desktop-without-the-latest-and-greatest-GPU-that-money-can-buy. Even if all your tris disappear into the gaping maw of your top-of-the-line GPU, you may not be the primary target.)&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2015-11-23T07:36:55.853" CommentCount="8" />
  <row Id="1753" PostTypeId="1" CreationDate="2015-11-23T19:04:20.070" Score="2" ViewCount="47" Body="&lt;p&gt;I have an analysis management web system developed using Ruby on Rails and I need to analyse an uploaded image in my system. Using Java I used to have ImageJ included im my projects. Since I'm new in Ruby on Rails I don't know any library/gem to manage Image processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know any image processing gem for Rails like ImageJ on Java?&lt;/p&gt;&#xA;" OwnerUserId="2123" LastEditorUserId="231" LastEditDate="2015-11-28T16:58:17.920" LastActivityDate="2015-11-28T16:58:17.920" Title="ImageJ alternative for Ruby on Rails Development" Tags="&lt;image-processing&gt;&lt;image-comparison&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2015-12-12T11:44:58.767" />
  <row Id="1754" PostTypeId="1" CreationDate="2015-11-25T19:09:52.097" Score="4" ViewCount="65" Body="&lt;p&gt;I am currently doing a research on projection on a sphere, specifically for NOAA's project Science on a Sphere. What I'm trying to do is to find out how exactly do you create a video material for such a device, because it's not as simple as making a regular video since the projection on the sphere warps the final image a lot etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I'm struggling with finding any reasonable material source. I only found NOAA's creation guidelines, and even though they do describe all the problems with warping and having a seamless video pretty well, they do not describe specific methods, or the whole process of creating this kind of a video. They only mention several programs and plugins for Adobe's AE, but that's it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Link: &lt;a href=&quot;http://sos.noaa.gov/_downloads/docs/sos-content-creation-guidelines.pdf&quot; rel=&quot;nofollow&quot;&gt;http://sos.noaa.gov/_downloads/docs/sos-content-creation-guidelines.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't get any reasonable source from Google either, since all links are directing me to topics related to projecting a sphere (Earth) onto a plane, which is not what I'm trying to find at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I decided to give this site a shot and ask you whether you know any source/tutorial that could help me with this issue, or whether is here someone who understands this topic better than I do.&lt;/p&gt;&#xA;" OwnerUserId="2138" LastEditorUserId="16" LastEditDate="2015-11-26T09:16:59.913" LastActivityDate="2015-11-28T20:44:34.907" Title="Materials for research on spherical projection" Tags="&lt;projections&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1755" PostTypeId="1" CreationDate="2015-11-26T03:08:06.657" Score="8" ViewCount="91" Body="&lt;p&gt;I am interested in duplicating a figure (shown below, ch 1 fig 1.21) in the book Algorithmic Beauty of Plants. The book is available here&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://algorithmicbotany.org/papers/#abop&quot;&gt;http://algorithmicbotany.org/papers/#abop&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/V6EHc.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/V6EHc.png&quot; alt=&quot;Plant generated with an axial L-system&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This image appears in several resources but I have been unable to find the exact rules for the axial system that produced it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the book, this figure is presented in the context of L-systems and is referenced in the text as follows.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Of special interest are methods proposed by Horton [70, 71] and Strahler, &#xA;  which served as a basis for synthesizing botanical trees [37, 152] (Figure &#xA;  1.21).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am unable to find a copy of the PhD thesis (ref 37) and ref 152 does not produce this figure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Performing a Google image search with this image points to material related to the book, such as slides.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone here reproduced this figure?&lt;/p&gt;&#xA;" OwnerUserId="2141" LastEditorUserId="2041" LastEditDate="2015-11-26T10:19:55.960" LastActivityDate="2015-11-26T10:19:55.960" Title="What exact algorithm and parameters reproduce L-system plant growth figure in Algorithmic Beauty of Plants" Tags="&lt;untagged&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="1756" PostTypeId="1" AcceptedAnswerId="1757" CreationDate="2015-11-26T05:11:59.980" Score="6" ViewCount="98" Body="&lt;p&gt;In what sense is it true that a NURBS surface can only have the topology of a plane, cylinder or torus?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example I can do a NURBS sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is the sphere homeomorphic to one of the above surfaces?&lt;/p&gt;&#xA;" OwnerUserId="1636" LastEditorUserId="231" LastEditDate="2015-11-28T16:41:47.460" LastActivityDate="2015-11-28T16:48:14.650" Title="NURBS topology classification" Tags="&lt;3d&gt;&lt;nurbs&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1757" PostTypeId="2" ParentId="1756" CreationDate="2015-11-26T08:04:16.070" Score="5" Body="&lt;p&gt;Before we begin, let us differentiate between two things:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The shape of a sphere,&lt;/li&gt;&#xA;&lt;li&gt;and the topology of a sphere.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;A NURBS surface can make the &lt;em&gt;shape&lt;/em&gt; of a sphere. In a typical configuration, it will be 'open' at the poles. That is the mathematical function of the surface does not wrap over the pole in that it is not a true sphere (it has the shape of a sphere). Topologically the sphere is in this typical configuration a cylinder. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is possible to leave the other direction open too in which case it is topologically a plane. Several CAD applications choose this approach. A torus is also possible if you allow a thin no volume sliver at the center of your sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kP30Z.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kP30Z.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Turning cylinder to sphere. Note this is not a topological sphere it is still a cylinder as the top is open (even though infinitesimally small).&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Is the sphere homeomorphic to one of the above surfaces?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No, but you can still have a spherical shape even if it does not satisfy the topology condition of mathematics.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Why only 3 topological families?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Simply, a NURBS surface has only 4 possible configurations of wrapping around the parameter space:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It does not wrap at all. Topology: Plane.&lt;/li&gt;&#xA;&lt;li&gt;It wraps around the U direction. Topology: Cylinder.&lt;/li&gt;&#xA;&lt;li&gt;It wraps around V direction. Same as above. Topology: Cylinder.&lt;/li&gt;&#xA;&lt;li&gt;It wraps around both U and V. Topology: Torus.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;A wrap is always periodic so it goes from - direction to + direction. It can not arbitrarily connect (on a mathematical level).&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="231" LastEditDate="2015-11-28T16:48:14.650" LastActivityDate="2015-11-28T16:48:14.650" CommentCount="3" />
  <row Id="1758" PostTypeId="1" CreationDate="2015-11-26T13:10:01.193" Score="3" ViewCount="38" Body="&lt;p&gt;Is there a library to do mesh processing (slicing, repairing, etc.) on big models in a distributed way using Hadoop?&lt;/p&gt;&#xA;" OwnerUserId="2144" LastActivityDate="2015-11-26T13:10:01.193" Title="Distributed Mesh Processing with Hadoop" Tags="&lt;mesh&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2016-03-12T12:23:42.047" />
  <row Id="1759" PostTypeId="1" CreationDate="2015-11-27T10:59:41.930" Score="17" ViewCount="186" Body="&lt;p&gt;I've got a homework in which I have to calculate and plot some points using a pespective transformation, but I'm not sure my results are correct, since the 3d plot using Camera coordinates looks very different from the 2d plot using the image coordinates. Can you help me understand what's wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is what is given:&#xA;The camera is at the point $_WT^C = [−1, 1, 5]^T$, specified in world coordinates (in meters). The camera coordinate system is rotated around the Y axis of the world reference by $\theta = 160^o$, so it's rotation matrix is $^wR_c = \begin{bmatrix}cos(\theta) &amp;amp; 0 &amp;amp; sin(\theta)\\ 0 &amp;amp; 1 &amp;amp; 0 \\ -sin(\theta) &amp;amp; 0 &amp;amp; cos(\theta)\end{bmatrix}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Camera parameter are: $f = 16mm$, $s_x = s_y = 0.01 mm/px$, $o_x = 320 px$, $o_y = 240px$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample points (in world coordinates):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$^WP_1 = [1, 1, 0.5]^T$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$^WP_2 = [1, 1.5, 0.5]^T$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$^WP_3 = [1.5, 1.5, 0.5]^T$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$^WP_4 = [1.5, 1, 0.5]^T$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have to calculate and plot the points in camera coordinates and in image coordinates, so I wrote the following code in Octave:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-matlab prettyprint-override&quot;&gt;&lt;code&gt;%camera intrinsic parameters&#xA;f = 16&#xA;Sx = 0.01&#xA;Sy = 0.01&#xA;Ox = 320&#xA;Oy = 240&#xA;&#xA;%given points, in world coordinate&#xA;wP1 = transpose([1, 1, 0.5])&#xA;wP2 = transpose([1, 1.5, 0.5])&#xA;wP3 = transpose([1.5, 1.5, 0.5])&#xA;wP4 = transpose([1.5, 1, 0.5])&#xA;&#xA;% camera translation matrix&#xA;wTc = transpose([-1, 1, 5])&#xA;&#xA;% rotation angle converted to rad&#xA;theta = 160 / 180 * pi&#xA;&#xA;%camera rotation matrix&#xA;wRc = transpose([cos(theta), 0, sin(theta); 0, 1, 0; -sin(theta), 0, cos(theta)])&#xA;&#xA;%transform the points to homogeneous coordinates&#xA;wP1h = [wP1; 1]&#xA;wP2h = [wP2; 1]&#xA;wP3h = [wP3; 1]&#xA;wP4h = [wP4; 1]&#xA;&#xA;%separate each line of the rotation matrix&#xA;R1 = transpose(wRc(1 , :))&#xA;R2 = transpose(wRc(2 , :))&#xA;R3 = transpose(wRc(3 , :))&#xA;&#xA;%generate the extrinsic parameters matrix&#xA;Mext = [wRc, [-transpose(R1) * wTc; -transpose(R2) * wTc; -transpose(R3) * wTc]]&#xA;&#xA;%intrinsic parameters matrix&#xA;Mint = [-f/Sx, 0, Ox; 0, -f/Sy, Oy; 0, 0, 1]&#xA;&#xA;% calculate coordinates in camera coordinates&#xA;cP1 = wRc * (wP1 - wTc)&#xA;cP2 = wRc * (wP2 - wTc)&#xA;cP3 = wRc * (wP3 - wTc)&#xA;cP4 = wRc * (wP4 - wTc)&#xA;&#xA;% put coordinates in a list for plotting&#xA;&#xA;x = [cP1(1), cP2(1), cP3(1), cP4(1), cP1(1)]&#xA;y = [cP1(2), cP2(2), cP3(2), cP4(2), cP1(2)]&#xA;z = [cP1(3), cP2(3), cP3(3), cP4(3), cP1(3)]&#xA;&#xA;%plot the points in 3D using camera coordinates&#xA;plot3(x, y, z, &quot;o-r&quot;)&#xA;&#xA;pause()&#xA;&#xA;% calculate the points in image coordinates&#xA;iP1 = Mint * (Mext * wP1h)&#xA;iP2 = Mint * (Mext * wP2h)&#xA;iP3 = Mint * (Mext * wP3h)&#xA;iP4 = Mint * (Mext * wP4h)&#xA;&#xA;%generate a list of points for plotting&#xA;x = [iP1(1) / iP1(3), iP2(1) / iP2(3), iP3(1) / iP3(3), iP4(1) / iP4(3), iP1(1) / iP1(3)]&#xA;y = [iP1(2) / iP1(3), iP2(2) / iP2(3), iP3(2) / iP3(3), iP4(2) / iP4(3), iP1(2) / iP1(3)]&#xA;&#xA;plot(x, y, &quot;o-r&quot;)&#xA;&#xA;pause()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And these are the plots I've got from the script:&#xA;I was expecting they were somewhat similar, but they don't look so.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/AkLN0.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/AkLN0.png&quot; alt=&quot;3D plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Plot in camera coordinates&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/p83mD.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/p83mD.png&quot; alt=&quot;2D plot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Plot in image coordinates&lt;/p&gt;&#xA;" OwnerUserId="2154" LastActivityDate="2015-11-27T10:59:41.930" Title="Is my perspective math correct?" Tags="&lt;3d&gt;&lt;maths&gt;&lt;perspective&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="2" />
  <row Id="1760" PostTypeId="2" ParentId="1754" CreationDate="2015-11-28T09:51:24.587" Score="1" Body="&lt;p&gt;You need to use a cylindrical projection. So the key issue is how to prepare data for a cylindrical projection. Basically what you do is: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You project your data on a sphere, it may be that this process was already done for you.&lt;/li&gt;&#xA;&lt;li&gt;And then you project it back to a cylinder&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Both steps can be easily taken care of by your cartography software. Will the data distort... yes but since you project information onto a sphere from whatever source onto a sphere and to cylindrical coordinates it will just interpolate the distortion away. If anything your just going to have slight over resolution at poles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quite a lot of data is in fact in this format to begin with. But nearly all 3D applications can use projected textures and texture baking to do these steps for you. Also most 2D applications like Photoshop have tools for turning rectangular coordinates to polar coordinates which do the same thing&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The real question is what your original image data is like?&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-28T20:44:34.907" LastActivityDate="2015-11-28T20:44:34.907" CommentCount="0" />
  <row Id="1761" PostTypeId="1" CreationDate="2015-11-29T12:55:29.020" Score="8" ViewCount="69" Body="&lt;p&gt;I have to connect point pairs without intersection. &#xA;Let's say I have two given points that I connect with a segment of a curve. Then again two new endpoints are selected and these new points have to be connected as well however without intersecting previously drawn curves and so on for any number of given point pairs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the easiest way of finding and drawing these segments of curves?&lt;/p&gt;&#xA;" OwnerUserId="2161" LastEditorUserId="2161" LastEditDate="2015-11-29T22:30:35.507" LastActivityDate="2015-11-30T03:58:02.933" Title="Strategy for connecting 2 points without intersecting previously drawn segments of curves" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1762" PostTypeId="1" CreationDate="2015-11-29T15:00:13.707" Score="7" ViewCount="106" Body="&lt;p&gt;Given the 4 coordinates of a 2D shape in a 3D space I want to calculate its aspect ratio.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 3D space is created with 2 vanishing points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 4 coordinates - marked blue - are the 2D coordinates on the display.&#xA;in the example they should be roughly (14, 5.5), (19, 5), (20.3, 7.3), (25.3, 6).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not sure if this is possible at all, if someone could find proof that for 2 different aspect ratios the 2D coordinates are the same this problem would be unsolvable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My exapmle:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/omyRz.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/omyRz.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2162" LastEditorUserId="2162" LastEditDate="2015-12-09T21:59:15.067" LastActivityDate="2015-12-09T21:59:15.067" Title="Calculate aspect ratio from 2D shape in 3D space" Tags="&lt;transformations&gt;&lt;geometry&gt;&lt;projections&gt;&lt;perspective&gt;&lt;space&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1763" PostTypeId="2" ParentId="1762" CreationDate="2015-11-29T20:19:22.653" Score="5" Body="&lt;p&gt;The ratio is with a quick and dirty visual measurement $665:501$ which is approximately $5:4$. You can measure it by taking the ratio of the vanishing angles $\alpha/\beta$ (&lt;em&gt;see picture 1&lt;/em&gt;) because we are so close to the center.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IeTey.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IeTey.png&quot; alt=&quot;Angles&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Ratio of the inbound angles&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can check the situation visually by drawing a 2 point perspective grid. For this we need the center line between the vanishing points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9gjvk.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9gjvk.png&quot; alt=&quot;Perspective grid&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Seems about right.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-11-29T20:30:21.200" LastActivityDate="2015-11-29T20:30:21.200" CommentCount="4" />
  <row Id="1764" PostTypeId="2" ParentId="1761" CreationDate="2015-11-30T00:51:28.830" Score="6" Body="&lt;p&gt;The general problem is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_drawing&quot;&gt;graph drawing&lt;/a&gt; and is not an easy problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The graphs that can be drawn on the plane without crossings are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Planar_graph&quot;&gt;planar&lt;/a&gt;, but not all graphs are planar: the typical graphs that are not planar are the complete graph on $5$ vertices $K_5$ and the complete bipartite graph on 6 vertices $K_{3,3}$, famous because of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Three_utilities_problem&quot;&gt;three utilities problem&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may want to try &lt;a href=&quot;http://www.graphviz.org/&quot;&gt;Graphviz - Graph Visualization Software&lt;/a&gt; to produce nice drawings.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See also &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Szirmay-Kalos, László, &lt;a href=&quot;http://www.iit.bme.hu/~szirmay/lsgraph.ps.gz&quot;&gt;Dynamic layout algorithm to display general graphs&lt;/a&gt;, in &lt;em&gt;Graphics Gems IV&lt;/em&gt;, 1994. &lt;a href=&quot;http://www.realtimerendering.com/resources/GraphicsGems/gemsiv/graph_layout/&quot;&gt;code&lt;/a&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Rosati, Claudio, A simple connection algorithm for 2-d drawing, &lt;em&gt;Graphics Gems III&lt;/em&gt;, 1992, &lt;a href=&quot;http://www.realtimerendering.com/resources/GraphicsGems/gemsiii/con2d.c&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="192" LastEditorUserId="38" LastEditDate="2015-11-30T03:58:02.933" LastActivityDate="2015-11-30T03:58:02.933" CommentCount="1" />
  <row Id="1765" PostTypeId="1" AcceptedAnswerId="1882" CreationDate="2015-11-30T15:36:34.443" Score="4" ViewCount="79" Body="&lt;p&gt;I am working on the move tool in a 3D modeling software. I need to know how much to move when the user drags an axis of the &lt;a href=&quot;http://www.finalclap.com/ressources/images/3dsmax-tracking6.jpg&quot; rel=&quot;nofollow&quot;&gt;gizmo&lt;/a&gt; (for example along the x axis). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the 2D vector describing how much the mouse moved on the screen &lt;code&gt;MouseDelta&lt;/code&gt;, and the vector corresponding to the selected axis &lt;code&gt;AxisVector&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like to know the best way to compute the amount of extrusion I should apply. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I try to project the &lt;code&gt;AxisVector&lt;/code&gt; in screen space, project &lt;code&gt;MouseDelta&lt;/code&gt; on it and get the ratio between the size of the &lt;code&gt;AxisVectorOnScreen&lt;/code&gt; and the computed projection. Then I have to clamp the result and multiply it by a factor. Here is a pseudo code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vector3 AxisVectorOnScreen = GetViewProjMatrix().TransformVector(AxisVector);&#xA;Vector2 AxisVectorOnScreen2D(AxisVectorOnScreen .X * 0.5f * Screen.Width(), AxisVectorOnScreen .Y * 0.5f * Screen.Height());&#xA;Vector2 MouseDelta = EndMousePosition - BeginMousePosition;&#xA;float Size = AxisVectorOnScreen2D.Length();&#xA;float ProjectionLength = MouseDelta | AxisVectorOnScreen2D.GetSafeNormal();&#xA;float Ratio = Size / ProjectionLength;&#xA;float Amount = 10.f * FMath::Clamp(Ratio, -2.5f, 2.5f);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think I could also project my mouse vector on a plane perpendicular to the camera and do the same maths on this plane. What is the best solution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note: I have the same problem to get the amount of extrusion I should apply with my extrude tool (along the normal of a face).&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="2173" LastEditorUserId="2173" LastEditDate="2015-12-02T12:16:51.070" LastActivityDate="2016-01-09T12:41:12.300" Title="How to get the transform amount from a screen vector and a direction vector?" Tags="&lt;algorithm&gt;&lt;3d&gt;&lt;transformations&gt;&lt;projections&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="1766" PostTypeId="1" AcceptedAnswerId="1772" CreationDate="2015-11-30T16:12:56.893" Score="2" ViewCount="63" Body="&lt;p&gt;I try to project a 3D vector (a direction, not a position) in screen space, but it does not return satisfying results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; FVector LineDirectionOnScreen = &#xA;   EdMode-&amp;gt;ViewMatrices.GetViewProjMatrix().TransformVector(LineDirection);&#xA; FVector2D LineDirectionOnScreen2D(&#xA;   LineDirectionOnScreen.X * 0.5f * (float)EdMode-&amp;gt;ViewRect.Width(), &#xA;   LineDirectionOnScreen.Y * 0.5f * (float)EdMode-&amp;gt;ViewRect.Height());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When &lt;code&gt;LineDirection&lt;/code&gt; is (0,0,1) it always returns a something like (0,Y) (Y can be anything), no matter how the camera is rotated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is wrong with my code?&lt;/p&gt;&#xA;" OwnerUserId="2173" LastEditorUserId="2173" LastEditDate="2015-12-02T09:58:57.457" LastActivityDate="2015-12-02T22:52:08.860" Title="How to use GetViewProjMatrix().TransformVector(LineDirection) in UE4?" Tags="&lt;projections&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1767" PostTypeId="1" CreationDate="2015-12-01T08:06:23.860" Score="3" ViewCount="104" Body="&lt;p&gt;I have three points P&lt;sub&gt;0&lt;/sub&gt;, P&lt;sub&gt;1&lt;/sub&gt;, P&lt;sub&gt;2&lt;/sub&gt;, which are located on an arbitrarily oriented ellipse in 3D space. I have a square texture map with a circle on it. I would like to render a textured quad (with the obvious UV mapping) so that the circle in the texture map projects onto the ellipse that P&lt;sub&gt;0&lt;/sub&gt;, P&lt;sub&gt;1&lt;/sub&gt;, P&lt;sub&gt;2&lt;/sub&gt; are on. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I calculate the vertices of the quad to project the ellipse correctly?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Alternately, in addition to P&lt;sub&gt;0&lt;/sub&gt;, P&lt;sub&gt;1&lt;/sub&gt;, P&lt;sub&gt;2&lt;/sub&gt;, I have the &lt;a href=&quot;https://en.wikipedia.org/wiki/Orbital_elements#Keplerian_elements&quot; rel=&quot;nofollow&quot;&gt;Keplerian elements&lt;/a&gt; of an elliptical orbit, if that's more straightforward.)&lt;/p&gt;&#xA;" OwnerUserId="1634" LastEditorUserId="1634" LastEditDate="2015-12-01T08:34:20.093" LastActivityDate="2016-01-04T15:21:26.153" Title="Project quad onto ellipse in 3D" Tags="&lt;3d&gt;&lt;projections&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1768" PostTypeId="1" AcceptedAnswerId="1773" CreationDate="2015-12-02T10:07:54.247" Score="14" ViewCount="400" Body="&lt;p&gt;For reference, what I'm referring to is the &quot;generic name&quot; for the technique first(I believe) introduced with idTech 5's &lt;a href=&quot;https://en.wikipedia.org/wiki/MegaTexture&quot;&gt;MegaTexture&lt;/a&gt; technology. See the video &lt;a href=&quot;http://holger.dammertz.org/stuff/notes_VirtualTexturing.html&quot;&gt;here&lt;/a&gt; for a quick glance on how it works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been skimming some papers and publications related to it lately, and what I don't understand is how it can possibly be efficient. Doesn't it require constant recalculation of UV coordinates from the &quot;global texture page&quot; space into the virtual texture coordinates? And how doesn't that curb most attempts at batching geometry altogether? How can it allow arbitrary zooming in? Wouldn't it at some point require subdividing polygons?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There just is so much I don't understand, and I have been unable to find any actually easily approachable resources on the topic.&lt;/p&gt;&#xA;" OwnerUserId="2111" LastEditorUserId="2111" LastEditDate="2015-12-02T18:54:07.313" LastActivityDate="2015-12-03T20:51:12.677" Title="How can virtual texturing actually be efficient?" Tags="&lt;rendering&gt;&lt;texture&gt;&lt;optimisation&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="4" />
  <row Id="1769" PostTypeId="1" AcceptedAnswerId="1771" CreationDate="2015-12-02T13:32:22.497" Score="5" ViewCount="160" Body="&lt;p&gt;Can I get to know the difference between the three? A good example would add up too.&lt;/p&gt;&#xA;" OwnerUserId="2179" LastActivityDate="2015-12-02T22:27:09.273" Title="World coordinates, Normalised device coordinates and device coordinates" Tags="&lt;2d&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1770" PostTypeId="2" ParentId="1768" CreationDate="2015-12-02T18:08:11.140" Score="7" Body="&lt;p&gt;Virtual Texturing is the logical extreme of texture atlases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A texture atlas is a single giant texture that contains textures for individual meshes inside it:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://www.gamasutra.com/features/20060126/figure1.jpg&quot; alt=&quot;Texture Atlas Example&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Texture atlases became popular due to the fact that changing textures causes a full pipeline flush on the GPU. When creating the meshes, the UVs are compressed/shifted so that they represent the correct 'portion' of the whole texture atlas. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As @nathan-reed mentioned in the comments, one of the main drawbacks of texture atlases is losing wrap modes such as repeat, clamp, border, etc. In addition, if the textures don't have enough border around them, you can accidentally sample from an adjacent texture when doing filtering. This can lead to bleeding artifacts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Texture Atlases do have one major limitation: size. Graphics APIs place a soft limit on how big a texture can be. That said, graphics memory is only so big. So there is also a hard limit on texture size, given by the size of your v-ram. Virtual textures solve this problem, by borrowing concepts from &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_memory&quot;&gt;virtual memory&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Virtual textures exploit the fact that in most scenes, you only see a small portion of all the textures. So, only that subset of textures need to be in vram. The rest can be in main RAM, or on disk.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a few ways to implement it, but I will explain the implementation described by Sean Barrett in his &lt;a href=&quot;https://www.youtube.com/watch?v=MejJL87yNgI&quot;&gt;GDC talk&lt;/a&gt;. (which I highly recommend watching)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have three main elements: the virtual texture, the physical texture, and the lookup table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/8gLuaq6.jpg&quot; alt=&quot;Virtual Texture&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The virtual texture represents the theoretical mega atlas we would have if we had enough vram to fit everything. It doesn't actually exist in memory anywhere. The physical texture represents what pixel data we actually have in vram. The lookup table is the mapping between the two. For convenience, we break all three elements into equal sized tiles, or pages. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The lookup table stores the location of the top-left corner of the tile in the physical texture. So, given a UV to the entire virtual texture, how do we get the corresponding UV for the physical texture?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, we need to find the location of the page within the physical texture. Then we need to calculate the location of the UV within the page. Finally we can add these two offsets together to get the location of the UV within the physical texture&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float2 pageLocInPhysicalTex = ...&#xA;float2 inPageLocation = ...&#xA;float2 physicalTexUV = pageLocationInPhysicalTex + inPageLocation;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Calculating pageLocInPhysicalTex&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we make the lookup table the same size as the number of tiles in the virtual texture, we can just sample the lookup table with nearest neighbor sampling and we will get the location of the top-left corner of the page within the physical texture.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float2 pageLocInPhysicalTex = lookupTable.Sample(virtTexUV, nearestNeighborSampler);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Calculating inPageLocation&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;inPageLocation is a UV coordinate that is relative to the top-left of the page, rather than to the top-left of the whole texture. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way to calculate this is by subtracting off the UV of the top left of the page, then scaling to the size of the page. However, this is quite a bit of math. Instead, we can exploit how IEEE floating point is represented. IEEE floating point stores the fractional part of a number by a series of base 2 fractions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/48VfH.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/48VfH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this example, the number is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;number = 0 + (1/2) + (1/8) + (1/16) = 0.6875&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now lets look at a simplified version of the virtual texture:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Jj8K9.png&quot; alt=&quot;Simple Virtual Texture&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 1/2 bit tells us if we're in the left half of the texture or the right. The 1/4 bit tells us which quarter of the half we're in. In this example, since the texture is split into 16, or 4 to a side, these first two bits tell us what page we're in. The remaining bits tell us the location inside the page.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can get the remaining bits by shifting the float with exp2() and stripping them out with fract()&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float2 inPageLocation = virtTexUV * exp2(sqrt(numTiles));&#xA;inPageLocation = fract(inPageLocation);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where numTiles is a int2 giving the number of tiles per side of the texture. In our example, this would be (4, 4)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So let's calculate the inPageLocation for the green point, (x,y) = (0.6875, 0.375)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;inPageLocation = float2(0.6875, 0.375) * exp2(sqrt(int2(4, 4));&#xA;               = float2(0.6875, 0.375) * int2(2, 2);&#xA;               = float2(1.375, 0.75);&#xA;&#xA;inPageLocation = fract(float2(1.375, 0.75));&#xA;               = float2(0.375, 0.75);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One last thing to do before we're done. Currently, inPageLocation is a UV coordinate in the virtual texture 'space'. However, we want a UV coordinate in the physical texture 'space'. To do this we just have to scale inPageLocation by the ratio of virtual texture size to physical texture size&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;inPageLocation *= physicalTextureSize / virtualTextureSize;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&#xA;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the finished function is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float2 CalculatePhysicalTexUV(float2 virtTexUV, Texture2D&amp;lt;float2&amp;gt; lookupTable, uint2 physicalTexSize, uint2 virtualTexSize, uint2 numTiles) {&#xA;    float2 pageLocInPhysicalTex = lookupTable.Sample(virtTexUV, nearestNeighborSampler);&#xA;&#xA;    float2 inPageLocation = virtTexUV * exp2(sqrt(numTiles));&#xA;    inPageLocation = fract(inPageLocation);&#xA;    inPageLocation *= physicalTexSize / virtualTexSize;&#xA;&#xA;    return pageLocInPhysicalTex + inPageLocation;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2015-12-03T20:51:12.677" LastActivityDate="2015-12-03T20:51:12.677" CommentCount="5" />
  <row Id="1771" PostTypeId="2" ParentId="1769" CreationDate="2015-12-02T22:27:09.273" Score="8" Body="&lt;p&gt;&lt;strong&gt;World coordinates&lt;/strong&gt; are at the bedrock of your game world. The 3D positions of all objects are ultimately specified in world space, either directly or through a node hierarchy. The ground, buildings, trees, and other stationary things will be fixed in world space. Gameplay calculations and physics will be done in world space (perhaps with some local recentering for precision purposes, if the world is large). The axes of world coordinates are often taken to represent compass directions, such as X = east, Y = north, Z = up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Local coordinates&lt;/strong&gt; are attached to an object (there is one local coordinate space for each object). The axes may represent something meaningful to the object, for instance X = forward, Y = left, Z = up for an object such as a character, vehicle, gun, etc. that has an inherent orientation. As the object moves around, the relationship between local and world space (as expressed by a transformation matrix) will change. For instance if you flip your car upside down, its local Z axis (&quot;up&quot; in local space) will now be pointing &quot;down&quot; in world space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Camera or view coordinates&lt;/strong&gt; are local coordinates attached to the camera. This is still a 3D coordinate system, without any projection or anything, but with the axes aligned to match the screen orientation: usually X = right, Y = up, Z = backward. The transformation from world to view space is often known as the &quot;view matrix&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Clip space coordinates&lt;/strong&gt; are the coordinates output by a vertex shader: coordinates to which the projection matrix has been applied, but not the perspective divide. This is a 4D (homogeneous) space. (World, local, and view space are 3D with an implicit w = 1.) It's so named because this is the space in which view frustum clipping and culling takes place (conceptually, at least).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Normalized device coordinates&lt;/strong&gt;, also commonly known as &quot;screen space&quot; although that term is a little loose, are what you get after applying the perspective divide to clip space coordinates. The 3D coordinates now represent the 2D positions of points on screen, with X and Y in [−1, 1], together with the depth within the depth buffer range, Z in [0, 1] for D3D or [−1, 1] for OpenGL. The axis orientation is X = right, Y = up, and Z can be either forward or backward depending on the depth buffer configuration.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Device coordinates&lt;/strong&gt; are 2D pixel coordinates within the render target, with (0, 0) in the upper-left corner, X = right, Y = down. One arrives at device coordinates by applying the viewport transform to the normalized device coordinates; the viewport controls at what pixel offset and resolution the image appears in the render target. One thing that's important to note is that device coordinates are not integer values; they aren't indices of pixels. They're a continuous 2D coordinate system, using pixel-sized units, but in which fractional (subpixel) values are perfectly valid. Pixel centers lie at 0.5 offsets (like 0.5, 1.5, 2.5, ...) in this coordinate system.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-02T22:27:09.273" CommentCount="0" />
  <row Id="1772" PostTypeId="2" ParentId="1766" CreationDate="2015-12-02T22:52:08.860" Score="3" Body="&lt;p&gt;I don't think you can simply use &lt;code&gt;TransformVector&lt;/code&gt; when the matrix you're transforming by involves a projection matrix. To fully project to screen space, you have to divide by W, which &lt;code&gt;TransformVector&lt;/code&gt; doesn't do (it simply multiplies by the matrix without translation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, because of the divide by W, transforming &lt;em&gt;vectors&lt;/em&gt; (as opposed to &lt;em&gt;points&lt;/em&gt;) by a projection matrix is probably not what you actually want. If what you want is the screen-space vector between two points, you should transform and divide-by-W each point, then subtract them to get a screen-space vector. Otherwise the vector will likely be inaccurate.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-02T22:52:08.860" CommentCount="6" />
  <row Id="1773" PostTypeId="2" ParentId="1768" CreationDate="2015-12-03T04:05:20.233" Score="10" Body="&lt;h2&gt;Overview&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The main reason for Virtual Texturing (VT), or &lt;em&gt;Sparse Virtual Textures&lt;/em&gt;, as it is sometimes called, is as a memory optimization. The gist of the thing is to only move into video memory the actual texels (generalized as pages/tiles) that you might need for a rendered frame. So it will allow you to have much more texture data in offline or slow storage (HDD, Optical-Disk, Cloud) than it would otherwise fit on video memory or even main memory. If you understand the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Virtual_memory&quot;&gt;Virtual Memory&lt;/a&gt; used by modern Operating Systems, it is the same thing in its essence (the name is not given by accident).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VT does not require recomputing UVs in the sense that you'd do that each frame before rendering a mesh, then resubmit vertex data, but it does require some substantial work in the Vertex and Fragment shaders to perform the indirect lookup from the incoming UVs. In a good implementation however, it should be completely transparent for the application if it is using a virtual texture or a traditional one. Actually, most of the time an application will mix both types of texturing, virtual and traditional.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Batching can in theory work very well, though I have never looked into the details of this. Since the usual criteria for grouping geometry are the textures, and with VT, every polygon in the scene can share the same &quot;infinitely large&quot; texture, theoretically, you could achieve full scene drawing with 1 draw call. But in reality, other factors come into play making this impractical.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Issues with VT&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Zooming in/out and abrupt camera movement are the hardest things to handle in a VT setup. It can look very appealing for a static scene, but once things start moving, more texture pages/tiles will be requested than you can stream for external storage. Async file IO and threading can help, but if it is a real-time system, like in a game, you'll just have to render for a few frames with lower resolution tiles until the hi-res ones arrive, every now and then, resulting in a blurry texture. There's no silver bullet here and that's the biggest issue with the technique, IMO.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Virtual Texturing also doesn't handle transparency in an easy way, so transparent polygons need a separate traditional rendering path for them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;All in all, VT is interesting, but I wouldn't recommend it for everyone. It can work well, but it is hard to implement and optimize, plus there's just too many corner cases and case-specific tweaks needed for my taste. But for large open-world games or data visualization apps, it might be the only possible approach to fit all the content into the available hardware. With a lot of work, it can be made to run fairly efficiently even on limited hardware, like we can see in the PS3 and XBOX360 versions of id's &lt;em&gt;Rage&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Implementation&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I have managed to get VT working on iOS with OpenGL-ES, to a certain degree. My implementation is not &quot;shippable&quot;, but I could conceivably make it so if I wanted and had the resources. &lt;a href=&quot;https://bitbucket.org/glampert/vt-mobile&quot;&gt;You can view the source code here&lt;/a&gt;, it might help getting a better idea of how the pieces fit together. &lt;a href=&quot;https://youtu.be/sWz45m0QKj4&quot;&gt;Here's a video&lt;/a&gt; of a demo running on the iOS Sim. It looks very laggy because the simulator is terrible at emulating shaders, but it runs smoothly on a device.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following diagram outlines the main components of the system in my implementation. It differs quite a bit from Sean's SVT demo (link down bellow), but it is closer in architecture to the one presented by the paper &lt;em&gt;Accelerating Virtual Texturing Using CUDA&lt;/em&gt;, found in the first GPU Pro book (link bellow).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/F3liK.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/F3liK.jpg&quot; alt=&quot;virtual texturing system&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;code&gt;Page Files&lt;/code&gt; are the virtual textures, already cut into tiles (AKA pages) as a preprocessing step, so they are ready to be moved from disk into video memory whenever needed. A page file also contains the whole set of mipmaps, also called &lt;em&gt;virtual mipmaps&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;code&gt;Page Cache Manager&lt;/code&gt; keeps an application-side representation of the &lt;code&gt;Page Table&lt;/code&gt; and &lt;code&gt;Page Indirection&lt;/code&gt; textures. Since moving a page from offline storage to memory is expensive, we need a cache to avoid reloading what is already available. This cache is a very simple &lt;em&gt;Least Recently Used&lt;/em&gt; (LRU) cache. The cache is also the component responsible for keeping the physical textures up-to-date with its own local representation of the data.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;code&gt;Page Provider&lt;/code&gt; is an async job queue that will fetch the pages needed for a given view of the scene and send them to the cache.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The &lt;code&gt;Page Indirection&lt;/code&gt; texture is a texture with one pixel for each page/tile in the virtual texture, that will map the incoming UVs to the &lt;code&gt;Page Table&lt;/code&gt; cache texture that has the actual texel data. This texture can get quite large, so it must use some compact format, like RGBA 8:8:8:8 or RGB 5:6:5.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;But we are still missing a key piece here, and that's how to determine which pages must be loaded from storage into the cache and consequently into the &lt;code&gt;Page Table&lt;/code&gt;. That's where the &lt;em&gt;Feedback Pass&lt;/em&gt; and the &lt;code&gt;Page Resolver&lt;/code&gt; enter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Feedback Pass is a pre-render of the view, with a custom shader and at a much lower resolution, that will write the ids of the required pages to the color framebuffer. That colorful patchwork of the cube and sphere above are actual page indexes encoded as an RGBA color. This pre-pass rendering is then read into main memory and processed by the &lt;code&gt;Page Resolver&lt;/code&gt; to decode the page indexes and fire the new requests with the &lt;code&gt;Page Provider&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After the Feedback pre-pass, the scene can be rendered normally with the VT lookup shaders. But note that we don't wait for new page request to finish, that would be terrible, because we'd simply block on synchronous file IO. The requests are asynchronous and might or might not be ready by the time the final view is rendered. If they are ready, sweet, but if not, we always keep a locked page of a low-res mipmap in the cache as a fallback, so we have some texture data in there to use, but it is going to be blurry.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Others resources worth checking out&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;The first book in the &lt;a href=&quot;http://gpupro.blogspot.com.br/&quot;&gt;GPU Pro series&lt;/a&gt;. It has two very good articles on VT.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Mandatory paper by MrElusive: &lt;a href=&quot;http://www.mrelusive.com/publications/papers/Software-Virtual-Textures.pdf&quot;&gt;Software Virtual Textures&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The Crytek paper by Martin Mittring: &lt;a href=&quot;http://developer.amd.com/wordpress/media/2013/01/Chapter02-Mittring-Advanced_Virtual_Texture_Topics.pdf&quot;&gt;Advanced Virtual Texture Topics&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;And &lt;a href=&quot;http://silverspaceship.com/src/svt/&quot;&gt;Sean's presentation on youtube&lt;/a&gt;, which I see you've already found. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;VT is still a somewhat hot topic on Computer Graphics, so there's tons of good material available, you should be able to find a lot more. If there's anything else I can add to this answer, please feel free to ask. I'm a bit rusty on the topic, haven't read much about it for the past year, but it is alway good for the memory to revisit stuff :)&lt;/p&gt;&#xA;" OwnerUserId="54" LastEditorUserId="54" LastEditDate="2015-12-03T18:43:24.437" LastActivityDate="2015-12-03T18:43:24.437" CommentCount="9" />
  <row Id="1774" PostTypeId="1" CreationDate="2015-12-03T09:47:23.183" Score="5" ViewCount="82" Body="&lt;p&gt;For example, let's assume I'm rendering cascaded shadow mapping, but for whatever reason, instead of one of the typical approach, I do the following:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Render the lowest resolution shadowmap&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Copy part of the shadowmap to the next shadowmap, covering a smaller area(and thus more dense)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Compute a stencil mask based on different points' distance from camera, e.g. more detail on edges, and on points closer to the camera, reusing the old shadowmap elsewhere.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Render the next, higher-density shadowmap with the aforementioned stencil.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What performance can I realistically expect from that? If only 50% of the pixels(or should I say fragments here?) pass the stencil test, do I save 50% of the rendering time, less, none?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this is a poor example, my fallback question scenario is a static scene, where the previous frame's result is reprojected, with an error accumulator in alpha, and a stencil mask is computed based on misses or too high error rates(e.g. position of the previous frame's reprojected pixel is too far from the new target pixel).&lt;/p&gt;&#xA;" OwnerUserId="2111" LastActivityDate="2015-12-03T09:47:23.183" Title="How much processing power does stenciling actually save?" Tags="&lt;optimisation&gt;&lt;stencil-test&gt;&lt;stencil-buffer&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="1775" PostTypeId="1" CreationDate="2015-12-03T19:44:36.010" Score="2" ViewCount="39" Body="&lt;p&gt;I am trying to draw an angled line in a raster grid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The idea is to create a function that given a start point, a length and an angle, would give me the pixel positions that combined would give me a line at the specified angle, in a raster grid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to implement this in C++ but I seem to get some weird points... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way I do it, is I compute the equation of the line, and thereby also the endpoint, and then use Bresenham's line algorithm to find the pixel positions that create the line between the start point and the end point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some of the points though, don't seem right. Some of the traversing points make no sense, or go to infinity. What is wrong with my implementation? &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &amp;lt;math.h&amp;gt;&#xA;using namespace std;&#xA;&#xA;int sign(double x ){ return (x &amp;gt; 0) ? 1 : ((x &amp;lt; 0) ? -1 : 0); }&#xA;&#xA;std::pair&amp;lt;int,int&amp;gt; endpoint(double angle, int x1 , int y1, int lenght)&#xA;{&#xA;&#xA;    if(angle == 90)&#xA;    {&#xA;        double y2 = y1+lenght;&#xA;        return std::make_pair(x1, -y2);&#xA;    }&#xA;    if(angle == 180)&#xA;    {&#xA;        double x2 = x1+lenght;&#xA;        return std::make_pair(-x2, y1);&#xA;    }&#xA;    if(angle == 270)&#xA;    {&#xA;        double y2 = y1+lenght;&#xA;        return std::make_pair(x1, y2);&#xA;    }&#xA;&#xA;&#xA;    double x2 = x1 + (lenght * cos(angle));&#xA;    double y2 = y1 + (lenght * sin(angle));&#xA;    return std::make_pair(x2,y2);&#xA;}&#xA;&#xA;&#xA;void bresenham(int x0, int y0, int x1, int y1)&#xA;{&#xA;    int dx = x1-x0;&#xA;    int dy = y1 - y0;&#xA;    if (abs(dx) &amp;gt; abs(dy))&#xA;    {&#xA;        // x-axis&#xA;        int j = y0;&#xA;        double e = dy - dx;&#xA;&#xA;        for (int i = x0; i&amp;lt;x1; i++)&#xA;        {&#xA;            std::cout &amp;lt;&amp;lt; &quot;marked: &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;,&quot;&amp;lt;&amp;lt; j &amp;lt;&amp;lt; std::endl;&#xA;            if( e &amp;gt;= 0)&#xA;            {&#xA;                j += 1;&#xA;                e -= dx;&#xA;            }&#xA;            e += dy;&#xA;&#xA;        }&#xA;    }&#xA;    else if(abs(dx)&amp;lt;abs(dy))&#xA;    {&#xA;        // y-axis&#xA;&#xA;        int j = x0;&#xA;        double e = dx - dy;&#xA;&#xA;        for (int i = y0; i&amp;lt;y1; i++)&#xA;        {&#xA;            std::cout &amp;lt;&amp;lt; &quot;marked: &quot; &amp;lt;&amp;lt; j&amp;lt;&amp;lt; &quot;,&quot;&amp;lt;&amp;lt; i &amp;lt;&amp;lt; std::endl;&#xA;            if( e &amp;gt;= 0)&#xA;            {&#xA;                j += 1;&#xA;                e -= dy;&#xA;            }&#xA;            e += dx;&#xA;&#xA;        }&#xA;&#xA;&#xA;    }&#xA;&#xA;}&#xA;&#xA;&#xA;void line(int x0, int y0, int x1, int y1)&#xA;{&#xA;&#xA;    if(x1 &amp;lt; x0)&#xA;    {&#xA;        swap(x1, x0);&#xA;    }&#xA;&#xA;    if(y1 &amp;lt; y0)&#xA;    {&#xA;        swap(y1, y0);&#xA;    }&#xA;&#xA;&#xA;    int dx = x1-x0;&#xA;    int dy = y1-y0;&#xA;    double e = 0;&#xA;    if(dx == 0)&#xA;    {&#xA;        if (sign(dy) == -1)&#xA;        {&#xA;            cout &amp;lt;&amp;lt; &quot;new&quot; &amp;lt;&amp;lt; endl;&#xA;            for (int y = y0; y != y1; y--)&#xA;            {&#xA;                cout &amp;lt;&amp;lt; &quot;mark&quot; &amp;lt;&amp;lt; x1 &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;            }&#xA;&#xA;        }&#xA;        else&#xA;        {&#xA;            for (int y = y0; y != y1; y++)&#xA;            {&#xA;                cout &amp;lt;&amp;lt; &quot;mark&quot; &amp;lt;&amp;lt; x1 &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;            }&#xA;        }&#xA;        return;&#xA;    }&#xA;&#xA;    else if(dy == 0)&#xA;    {&#xA;&#xA;        if (sign(dx) == -1)&#xA;        {&#xA;            cout &amp;lt;&amp;lt; &quot;new&quot; &amp;lt;&amp;lt; endl;&#xA;&#xA;            for (int x = x0; x != x1; x--)&#xA;            {&#xA;                cout &amp;lt;&amp;lt; &quot;mark&quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y1 &amp;lt;&amp;lt; endl;&#xA;            }&#xA;&#xA;        }&#xA;        else&#xA;        {&#xA;            for (int y = y0; y != y1; y++)&#xA;            {&#xA;                cout &amp;lt;&amp;lt; &quot;mark&quot; &amp;lt;&amp;lt; x1 &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;            }&#xA;        }&#xA;        return;&#xA;    }&#xA;    cout &amp;lt;&amp;lt; &quot;bressenham: &quot; &amp;lt;&amp;lt; endl;&#xA;    double de = abs(dy/dx);&#xA;    int y = y0;&#xA;    for (int x = x0; x != x1; x++)&#xA;    {&#xA;        cout &amp;lt;&amp;lt; &quot;mark: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;        e = e + de;&#xA;        while (e &amp;gt;= 0.5)&#xA;        {&#xA;            cout &amp;lt;&amp;lt; &quot;mark: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;            y = y + sign(y1-y0);&#xA;            e = e -1;&#xA;        }&#xA;    }&#xA;&#xA;&#xA;}&#xA;&#xA;int main(int argc, const char * argv[])&#xA;{&#xA;    for (int angle = 0; angle &amp;lt; 180; angle++)&#xA;    {&#xA;        cout &amp;lt;&amp;lt; &quot;angle: &quot; &amp;lt;&amp;lt; angle &amp;lt;&amp;lt; endl;&#xA;        pair&amp;lt;int, int&amp;gt; end = endpoint(angle, 3, 3, 20);&#xA;        cout &amp;lt;&amp;lt; end.first &amp;lt;&amp;lt; end.second &amp;lt;&amp;lt; endl;&#xA;        line(3, 3, end.first, end.second);&#xA;        cout &amp;lt;&amp;lt; end.first &amp;lt;&amp;lt; end.second &amp;lt;&amp;lt; endl;&#xA;&#xA;    }&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2187" LastEditorUserId="231" LastEditDate="2015-12-07T11:38:35.267" LastActivityDate="2015-12-07T11:38:35.267" Title="Draw lines angled from 0 to 2 pi in a raster grid" Tags="&lt;c++&gt;&lt;grid&gt;&lt;line-drawing&gt;&lt;raster-image&gt;" AnswerCount="0" CommentCount="2" ClosedDate="2015-12-11T12:34:54.760" />
  <row Id="1776" PostTypeId="1" CreationDate="2015-12-04T08:30:44.973" Score="6" ViewCount="86" Body="&lt;p&gt;Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;math.h&amp;gt;&#xA;#include &amp;lt;GL/glut.h&amp;gt;&#xA;#pragma comment(lib, &quot;opengl32&quot;)&#xA;#include &amp;lt;gl/gl.h&amp;gt;&#xA;#include &amp;lt;gl/glu.h&amp;gt;&#xA;&#xA;//Initialize OpenGL &#xA;void init(void) {&#xA;    glClearColor(0, 0, 0, 0);&#xA;&#xA;    glViewport(0, 0, 500, 500);&#xA;&#xA;    glMatrixMode(GL_PROJECTION);&#xA;    glLoadIdentity();&#xA;&#xA;    glOrtho(0, 500, 0, 500, 1, -1);&#xA;&#xA;    glMatrixMode(GL_MODELVIEW);&#xA;    glLoadIdentity();&#xA;} &#xA;&#xA;void drawLines(void) {&#xA;    glClear(GL_COLOR_BUFFER_BIT);  &#xA;    glColor3f(1.0,1.0,1.0); &#xA;&#xA;    glBegin(GL_LINES);&#xA;&#xA;    glVertex3d(0.5,         0.999,  0.0f);&#xA;    glVertex3d(499.501,     0.999,  0.0f);&#xA;&#xA;    glEnd();&#xA;&#xA;    glFlush();&#xA;} &#xA;&#xA;&#xA;int _tmain(int argc, _TCHAR* argv[])&#xA;{&#xA;    glutInit(&amp;amp;argc, argv);  &#xA;    glutInitWindowPosition(10,10); &#xA;    glutInitWindowSize(500,500); &#xA;    glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); &#xA;&#xA;    glutCreateWindow(&quot;Example&quot;); &#xA;    init(); &#xA;    glutDisplayFunc(drawLines); &#xA;    glutMainLoop();&#xA;&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Problem description:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Above code will put the entire bottom row pixels of the window client area to white.&lt;/li&gt;&#xA;&lt;li&gt;If I swap the command order, from &lt;code&gt;glVertex3d(0.5, 0.999, 0.0f);glVertex3d(499.501, 0.999, 0.0f);&lt;/code&gt; to &lt;code&gt;glVertex3d(499.501, 0.999, 0.0f);glVertex3d(0.5, 0.999, 0.0f);&lt;/code&gt;, then only the left bottom pixel will not draw.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My understandings:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The two vertices will be finally transformed to 2D pixel center coordinate which are (0.0, 0.499) and (499.001, 0.499).&lt;/li&gt;&#xA;&lt;li&gt;The line drawing algorithm only accept pixel center integer points as input.&lt;/li&gt;&#xA;&lt;li&gt;So the two vertices will using int(x + 0.5) and will be (0, 0) and (499 , 0). This conforms to first result but contradict to the result when input order changed. Why?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2190" LastActivityDate="2015-12-05T22:57:33.030" Title="Why different result when change input order in GL_LINES?" Tags="&lt;opengl&gt;&lt;line-drawing&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1777" PostTypeId="2" ParentId="1743" CreationDate="2015-12-04T11:19:36.937" Score="3" Body="&lt;p&gt;The syntax for your dot- and cross-product are a bit strange.&#xA;The methods are members of a vector, but still take two arguments. Are they static methods? Or are you really sure they do what you would expect?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Otherwise the only real difference in your implementation and the one you linked that I can see is your test against the parameter t.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The implementation you linked tests for&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(t &amp;gt; EPSILON)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;your code tests for&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(t &amp;lt; abs(hitFaces.s))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I don't see in your code how you set the value for hitFaces.s since you fire this ray against a model and don't know jet if it has hit anything.&#xA;My understanding would be, that you first test if t is large enough to signal a hit ( &gt; EPSILON ) and then in a second step sort for the smallest t (closest to the ray origin).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my head you can't get a value for hitFaces.s without calculating this intersection test as you wrote it, and the intersection test needs the hitFaces.s to determine if an intersection was made. But I could be in the wrong here.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2015-12-04T11:19:36.937" CommentCount="0" />
  <row Id="1778" PostTypeId="5" CreationDate="2015-12-04T14:12:45.167" Score="0" Body="&lt;p&gt;WebGL is &lt;a href=&quot;http://en.wikipedia.org/wiki/OpenGL&quot; rel=&quot;nofollow&quot;&gt;OpenGL&lt;/a&gt; 3D graphics for web pages. It extends the capability of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Canvas_element&quot; rel=&quot;nofollow&quot;&gt;HTML canvas element&lt;/a&gt; to allow it to render accelerated 3D graphics in any compatible web browser. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;WebGL is based on &lt;a href=&quot;http://en.wikipedia.org/wiki/OpenGL_ES&quot; rel=&quot;nofollow&quot;&gt;OpenGL ES&lt;/a&gt; and is driven via a JavaScript API. It does not require the use of plug-ins. Official specifications and additional information can be found at &lt;a href=&quot;http://www.khronos.org/webgl/&quot; rel=&quot;nofollow&quot;&gt;Khronos&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Engines:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are also many different webGL based 3D engines available:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://threejs.org/&quot; rel=&quot;nofollow&quot;&gt;Three.js&lt;/a&gt; - the most popular and the most used&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.gooengine.com/&quot; rel=&quot;nofollow&quot;&gt;Goo Engine&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://scenejs.org/&quot; rel=&quot;nofollow&quot;&gt;SceneJS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.babylonjs.com/&quot; rel=&quot;nofollow&quot;&gt;BabylonJS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.glge.org/&quot; rel=&quot;nofollow&quot;&gt;GLGE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.sencha.com/blog/introducing-philogl-a-webgl-javascript-library-from-sencha-labs/&quot; rel=&quot;nofollow&quot;&gt;PhiloGL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.ambiera.com/copperlicht/&quot; rel=&quot;nofollow&quot;&gt;CopperLicht&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.kickjs.org/&quot; rel=&quot;nofollow&quot;&gt;KickJS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.aerotwist.com/a3/&quot; rel=&quot;nofollow&quot;&gt;A3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.cubicvr.org/cubicvr-js/webgl-cubicvr&quot; rel=&quot;nofollow&quot;&gt;CubicVR&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.c3dl.org/&quot; rel=&quot;nofollow&quot;&gt;C3DL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://code.google.com/p/kuda/&quot; rel=&quot;nofollow&quot;&gt;Kuda&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://code.google.com/p/o3d/&quot; rel=&quot;nofollow&quot;&gt;O3D&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.spidergl.org/&quot; rel=&quot;nofollow&quot;&gt;SpiderGL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Getting started with webGL:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The head-first place for all new webGL developers - &lt;a href=&quot;http://learningwebgl.com/&quot; rel=&quot;nofollow&quot;&gt;LearningWebGL&lt;/a&gt;. LearningWebGL has basic tutorials required to understand the API and pipeline, and is also a blog site that publishes newest information about webGL every week.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Website with plenty of Three.js tutorials and examples - &lt;a href=&quot;http://learningthreejs.com/&quot; rel=&quot;nofollow&quot;&gt;LearningThreeJS&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="1908" LastEditorUserId="1908" LastEditDate="2015-12-04T21:37:54.457" LastActivityDate="2015-12-04T21:37:54.457" CommentCount="0" />
  <row Id="1779" PostTypeId="4" CreationDate="2015-12-04T14:12:45.167" Score="0" Body="WebGL extends the capability of the HTML canvas element to allow it to render accelerated 3D graphics in any compatible web browser." OwnerUserId="1908" LastEditorUserId="1908" LastEditDate="2015-12-04T15:40:12.640" LastActivityDate="2015-12-04T15:40:12.640" CommentCount="0" />
  <row Id="1781" PostTypeId="2" ParentId="1767" CreationDate="2015-12-05T15:20:54.810" Score="1" Body="&lt;p&gt;I assume that your texture is deemed to belong to the plane of the trajectory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at the unprojected ellipse, you can map the circle to it by an anisotropic scaling. If necessary, rotate the texture first if there is any orientation requirement.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To achieve this first mapping, you will compute the four corners of a rectangle, presumably related to the frame formed by the ellipse axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then transform those vertices to the global scene coordinates, the same way to transform the ellipse.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastActivityDate="2015-12-05T15:20:54.810" CommentCount="0" />
  <row Id="1782" PostTypeId="2" ParentId="1776" CreationDate="2015-12-05T22:17:50.043" Score="7" Body="&lt;p&gt;The difference in which pixels are covered depending on the vertex order is related to &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/cc627092.aspx&quot;&gt;rasterization rules&lt;/a&gt;. They're the rules that the GPU hardware uses to determine exactly which pixels are covered by a primitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rasterization rules are a bit subtle. One of their goals is to ensure that whenever you draw multiple primitives that are connected water-tightly, the rasterization never produces any cracks between them, nor are any pixels covered twice (that's important for blending). To achieve that, the rules have some very specific edge and corner cases. Literally...they're cases having to do with primitive edges and corners. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of lines, it works as follows. There's a diamond-shaped region around each pixel center, as shown in this snippet of the diagram from the MSDN article linked above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/GJ5Zw.png&quot; alt=&quot;diamond rule for line rasterization&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The rule is that a pixel is covered if the line &lt;em&gt;exits&lt;/em&gt; the diamond, when tracing from the start to the end of the line. Note that a pixel doesn't get covered if the line enters, but doesn't exit, the diamond. This ensures that if you have two line segments connected end to end, that the pixel at their shared endpoint only belongs to one of the segments and thus doesn't get rasterized twice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also see in the above diagram how changing the order of the vertices can affect which pixels are covered (which doesn't happen with triangles, BTW). The diagram shows two line segments that are identical except for swapping the endpoint order, and you can see it makes a difference which pixel gets covered. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your line segment is somewhat analogous to this. The left end, at (0.5, 0.999) is inside the diamond of the (0, 0) pixel, so it gets covered when it's the first vertex (the line starts inside the diamond, then exits it) and not when it's the second vertex (the line enters the diamond, and ends inside it, so it never exits). &lt;sub&gt;Actually, the vertices are snapped to fixed-point with 8 subpixel bits before rasterization, so this one ends up rounded to (0.5, 1.0), which is exactly on the top corner of the diamond. Depending on the rasterization rules, this might or might not be considered inside the diamond; it seems as if on your GPU it's considered inside, but this can vary between implementations, as the GL spec doesn't nail down the rules entirely.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-12-05T22:57:33.030" LastActivityDate="2015-12-05T22:57:33.030" CommentCount="0" />
  <row Id="1784" PostTypeId="1" AcceptedAnswerId="1792" CreationDate="2015-12-07T12:04:58.077" Score="3" ViewCount="168" Body="&lt;p&gt;In my SharpGL application I do a very simple, mouse controlled rotation of the scene.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gl.Rotate(cameraRotation.X, cameraRotation.Y, 0.0f);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cameraRotation.Y += MousePosition.X - previousMousePositionX;&#xA;cameraRotation.X += MousePosition.Y - previousMousePositionY;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem with this is that the x rotation always comes first, so if I rotate the object around the x axis by 180° and then move my mouse left, the object will rotate to the right instead of left (since the y axis is now inverted). Is there a way to change this so that the object always rotates into the mouse movement direction, regardless of its current position? (like netfabb does it for example)&lt;/p&gt;&#xA;" OwnerUserId="2219" LastEditorUserId="127" LastEditDate="2015-12-08T22:52:52.573" LastActivityDate="2015-12-09T21:44:33.037" Title="Always rotate in the direction of mouse movement" Tags="&lt;opengl&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1785" PostTypeId="1" AcceptedAnswerId="1795" CreationDate="2015-12-08T13:03:25.073" Score="3" ViewCount="97" Body="&lt;p&gt;I at the moment trying to draw some angled lines using bresenham line algorithm which can circulate a 2d array of size 21x21, as a line angled from 0 - 2pi. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/c3X52.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/c3X52.png&quot; alt=&quot;Angled line&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the idea is that the program has to output the values which the lines pass through in the grid.. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So an example with 5x5&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Angle:0&#xA;     _ _ _ _ _ &#xA;    |_|_|_|_|_|&#xA;    |_|_|_|_|_|&#xA;    |_|_|.|.|.|&#xA;    |_|_|_|_|_|&#xA;    |_|_|_|_|_|&#xA;&#xA;Angle: 45&#xA;     _ _ _ _ _ &#xA;    |_|_|_|_|.|&#xA;    |_|_|_|.|_|&#xA;    |_|_|.|_|_|&#xA;    |_|_|_|_|_|&#xA;    |_|_|_|_|_|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and so on..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem here is that it doesn't look like that my program does that.. The endpoint lies within the given radius length.. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am sure am might that I might be messing up with the math.. So i hope some of you could help me here.. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;&#xA;#include &amp;lt;math.h&amp;gt;&#xA;using namespace std;&#xA;typedef std::pair&amp;lt;int,int&amp;gt; coordinate;&#xA;&#xA;int sign(double x ){ return (x &amp;gt; 0) ? 1 : ((x &amp;lt; 0) ? -1 : 0); }&#xA;&#xA;coordinate endpoint(double angle, int x1 , int y1, int lenght)&#xA;{&#xA;    double radians = (M_PI/180)*angle;&#xA;&#xA;    double x2 = x1 + (lenght * cos(radians));&#xA;    double y2 = y1 + (lenght * sin(radians));&#xA;&#xA;    return std::make_pair(round(x2),round(y2));&#xA;}&#xA;&#xA;void bresenham(coordinate start, coordinate end)&#xA;{&#xA;    //restriction a.x &amp;lt; b.x  and 0 &amp;lt; H/W &amp;lt; 1&#xA;    int y =  start.second;&#xA;    int w = end.first - start.first;&#xA;    int h = end.second - start.second;&#xA;    int f = 2*h-w; // current error term&#xA;&#xA;    for (int x = start.first; x&amp;lt;= end.first; x++)&#xA;    {&#xA;        cout &amp;lt;&amp;lt; &quot;mark: &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl;&#xA;        if (f &amp;lt; 0)&#xA;        {&#xA;            f = f + 2*h;&#xA;        }&#xA;        else&#xA;        {&#xA;            y++;&#xA;            f=f+2*(h-w);&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;&#xA;int main(int argc, const char * argv[])&#xA;{&#xA;&#xA;    coordinate start = make_pair(0,0);&#xA;    for (int i = 0; i &amp;lt;= 45; i++)&#xA;    {&#xA;        coordinate end = endpoint(i,0,0,10);&#xA;        cout &amp;lt;&amp;lt; &quot;    endPos: &quot;&amp;lt;&amp;lt; &quot;(&quot; &amp;lt;&amp;lt; end.first &amp;lt;&amp;lt;&quot;,&quot;  &amp;lt;&amp;lt; end.second   &amp;lt;&amp;lt;&quot;)&quot;    &amp;lt;&amp;lt; &quot; Angle: &quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &quot;       &quot; &amp;lt;&amp;lt; endl;&#xA;        cout &amp;lt;&amp;lt; &quot;--------------------------------------------&quot; &amp;lt;&amp;lt; endl;&#xA;        bresenham(start, end);&#xA;        cout &amp;lt;&amp;lt; &quot;--------------------------------------------&quot; &amp;lt;&amp;lt; endl;&#xA;    }&#xA;&#xA;    return 0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the output. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    endPos: (10,0) Angle: 0       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,0&#xA;mark: 6,0&#xA;mark: 7,0&#xA;mark: 8,0&#xA;mark: 9,0&#xA;mark: 10,0&#xA;--------------------------------------------&#xA;    endPos: (10,0) Angle: 1       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,0&#xA;mark: 6,0&#xA;mark: 7,0&#xA;mark: 8,0&#xA;mark: 9,0&#xA;mark: 10,0&#xA;--------------------------------------------&#xA;    endPos: (10,0) Angle: 2       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,0&#xA;mark: 6,0&#xA;mark: 7,0&#xA;mark: 8,0&#xA;mark: 9,0&#xA;mark: 10,0&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 3       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 4       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 5       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 6       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 7       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,1) Angle: 8       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,0&#xA;mark: 4,0&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,1&#xA;mark: 9,1&#xA;mark: 10,1&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 9       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 10       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 11       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 12       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 13       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,2) Angle: 14       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,0&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,1&#xA;mark: 6,1&#xA;mark: 7,1&#xA;mark: 8,2&#xA;mark: 9,2&#xA;mark: 10,2&#xA;--------------------------------------------&#xA;    endPos: (10,3) Angle: 15       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,2&#xA;mark: 9,3&#xA;mark: 10,3&#xA;--------------------------------------------&#xA;    endPos: (10,3) Angle: 16       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,2&#xA;mark: 9,3&#xA;mark: 10,3&#xA;--------------------------------------------&#xA;    endPos: (10,3) Angle: 17       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,2&#xA;mark: 9,3&#xA;mark: 10,3&#xA;--------------------------------------------&#xA;    endPos: (10,3) Angle: 18       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,2&#xA;mark: 9,3&#xA;mark: 10,3&#xA;--------------------------------------------&#xA;    endPos: (9,3) Angle: 19       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,3&#xA;mark: 9,3&#xA;--------------------------------------------&#xA;    endPos: (9,3) Angle: 20       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,1&#xA;mark: 5,2&#xA;mark: 6,2&#xA;mark: 7,2&#xA;mark: 8,3&#xA;mark: 9,3&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 21       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 22       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 23       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 24       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 25       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,4) Angle: 26       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,0&#xA;mark: 2,1&#xA;mark: 3,1&#xA;mark: 4,2&#xA;mark: 5,2&#xA;mark: 6,3&#xA;mark: 7,3&#xA;mark: 8,4&#xA;mark: 9,4&#xA;--------------------------------------------&#xA;    endPos: (9,5) Angle: 27       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,2&#xA;mark: 5,3&#xA;mark: 6,3&#xA;mark: 7,4&#xA;mark: 8,4&#xA;mark: 9,5&#xA;--------------------------------------------&#xA;    endPos: (9,5) Angle: 28       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,2&#xA;mark: 5,3&#xA;mark: 6,3&#xA;mark: 7,4&#xA;mark: 8,4&#xA;mark: 9,5&#xA;--------------------------------------------&#xA;    endPos: (9,5) Angle: 29       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,2&#xA;mark: 5,3&#xA;mark: 6,3&#xA;mark: 7,4&#xA;mark: 8,4&#xA;mark: 9,5&#xA;--------------------------------------------&#xA;    endPos: (9,5) Angle: 30       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,2&#xA;mark: 5,3&#xA;mark: 6,3&#xA;mark: 7,4&#xA;mark: 8,4&#xA;mark: 9,5&#xA;--------------------------------------------&#xA;    endPos: (9,5) Angle: 31       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,2&#xA;mark: 5,3&#xA;mark: 6,3&#xA;mark: 7,4&#xA;mark: 8,4&#xA;mark: 9,5&#xA;--------------------------------------------&#xA;    endPos: (8,5) Angle: 32       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,3&#xA;mark: 6,4&#xA;mark: 7,4&#xA;mark: 8,5&#xA;--------------------------------------------&#xA;    endPos: (8,5) Angle: 33       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,1&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,3&#xA;mark: 6,4&#xA;mark: 7,4&#xA;mark: 8,5&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 34       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 35       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 36       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 37       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 38       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 39       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,6) Angle: 40       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,2&#xA;mark: 4,3&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,5&#xA;mark: 8,6&#xA;--------------------------------------------&#xA;    endPos: (8,7) Angle: 41       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,3&#xA;mark: 4,4&#xA;mark: 5,4&#xA;mark: 6,5&#xA;mark: 7,6&#xA;mark: 8,7&#xA;--------------------------------------------&#xA;    endPos: (7,7) Angle: 42       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,3&#xA;mark: 4,4&#xA;mark: 5,5&#xA;mark: 6,6&#xA;mark: 7,7&#xA;--------------------------------------------&#xA;    endPos: (7,7) Angle: 43       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,3&#xA;mark: 4,4&#xA;mark: 5,5&#xA;mark: 6,6&#xA;mark: 7,7&#xA;--------------------------------------------&#xA;    endPos: (7,7) Angle: 44       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,3&#xA;mark: 4,4&#xA;mark: 5,5&#xA;mark: 6,6&#xA;mark: 7,7&#xA;--------------------------------------------&#xA;    endPos: (7,7) Angle: 45       &#xA;--------------------------------------------&#xA;mark: 0,0&#xA;mark: 1,1&#xA;mark: 2,2&#xA;mark: 3,3&#xA;mark: 4,4&#xA;mark: 5,5&#xA;mark: 6,6&#xA;mark: 7,7&#xA;--------------------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What am i doing wrong?... I know that the bresenham algorithm might have to be modified to overcome slopes greater 1 and lower that 0. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Clarifying the problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying the iterate the 2d array in a circular manner, using bresenham line algorithm . &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm should start from the center of the 2d array, and &quot;shoot out&quot; a beam at angles between 0 - 2pi. The beam has to start from the center and end at the edge of the matrix, Hope it makes more sense..&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; _ _ _ _ _ _ _ _ _ _ _ &#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|.|.|.|.|.|.|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;&#xA; _ _ _ _ _ _ _ _ _ _ _ &#xA;|_|_|_|_|_|_|_|_|_|.|.|&#xA;|_|_|_|_|_|_|_|_|.|_|_|&#xA;|_|_|_|_|_|_|_|.|_|_|_|&#xA;|_|_|_|_|_|_|.|_|_|_|_|&#xA;|_|_|_|_|_|.|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;|_|_|_|_|_|_|_|_|_|_|_|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to output the values the beam hits on the matrix, at each angle..  Which i don't get..  I want to sample the points which the beam intersect in the matrix.. &lt;/p&gt;&#xA;" OwnerUserId="2226" LastEditorUserId="2226" LastEditDate="2015-12-09T16:44:25.213" LastActivityDate="2015-12-09T18:40:17.690" Title="Draw angles lines in raster graphics using bresenham line algorithm" Tags="&lt;c++&gt;&lt;line-drawing&gt;&lt;maths&gt;&lt;raster-image&gt;&lt;triangulation&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1786" PostTypeId="1" CreationDate="2015-12-08T14:23:11.243" Score="7" ViewCount="84" Body="&lt;p&gt;Whew, that was a long title.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either way, I'm asking this question, since I like to think about various things, and it occurred to me that there isn't really any simple, open source layers on top of GLSL, even if only to add simple things such as includes, or commonly used functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As research of sorts, I'm asking this question, since my own awareness of such languages is minuscule to say the least - I know of bgfx's shading language, and Unity's ShaderLab, but I don't really know what they accomplish - or why - being a relative newbie to computer graphics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, what would your wishlist for a shading language like this be? Mine so far is includes, some compatibility between versions, optional &quot;hidden&quot; inputs that allow easily accessing textures at pixel offsets, or image sizes, etc. and probably passes - for, say, two-pass gaussian blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thoughts?&lt;/p&gt;&#xA;" OwnerUserId="2111" LastActivityDate="2015-12-10T03:11:35.820" Title="There are many shading languages built on top of GLSL or HLSL - what problems do they typically solve, and what worthwhile advancements do they make?" Tags="&lt;shader&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1788" PostTypeId="1" AcceptedAnswerId="1807" CreationDate="2015-12-08T18:48:35.203" Score="8" ViewCount="111" Body="&lt;p&gt;First of all, I want to say that I've read a lot of post about shadow mapping using depth maps and cubemaps and I understand how they work and also, I have working experience with them using OpenGL, but, I have an issue implementing Omnidirectional Shadow Mapping technique using a single point light source in my 3D graphics engine named &quot;EZ3&quot;. My engine uses WebGL as a 3D graphics API and JavaScript as programming language, this is for my bachelor's thesis in Computer Science. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically this is how I've implemented my shadow mapping algorithm, but I'll only focus on point lights case because with them I can archive omnidirectional shadow mapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, I active front-face culling like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (this.state.faceCulling !== Material.FRONT) {&#xA;    if (this.state.faceCulling === Material.NONE)&#xA;      gl.enable(gl.CULL_FACE);&#xA;&#xA;    gl.cullFace(gl.FRONT);&#xA;    this.state.faceCulling = Material.FRONT;&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Second, I create a depth program in order to record depth values for each cubemap face, this is my depth program code in GLSL 1.0:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vertex Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;attribute vec3 position;&#xA;&#xA;uniform mat4 uModelView;&#xA;uniform mat4 uProjection;&#xA;&#xA;void main() {&#xA;  gl_Position = uProjection * uModelView * vec4(position, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fragment Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;vec4 packDepth(const in float depth) {&#xA;  const vec4 bitShift = vec4(256.0 * 256.0 * 256.0, 256.0 * 256.0, 256.0, 1.0);&#xA;  const vec4 bitMask = vec4(0.0, 1.0 / 256.0, 1.0 / 256.0, 1.0 / 256.0);&#xA;  vec4 res = mod(depth * bitShift * vec4(255), vec4(256)) / vec4(255);&#xA;  res -= res.xxyz * bitMask;&#xA;  return res;&#xA;}&#xA;&#xA;void main() {&#xA;  gl_FragData[0] = packDepth(gl_FragCoord.z);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Third, this is my JavaScript function's body that &quot;archives&quot; omnidirectional shadow mapping&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;program.bind(gl);&#xA;&#xA;  for (i = 0; i &amp;lt; lights.length; i++) {&#xA;    light = lights[i];&#xA;&#xA;    // Updates pointlight's projection matrix&#xA;&#xA;    light.updateProjection();&#xA;&#xA;    // Binds point light's depth framebuffer&#xA;&#xA;    light.depthFramebuffer.bind(gl);&#xA;&#xA;    // Updates point light's framebuffer in order to create it &#xA;    // or if it's resolution changes, it'll be created again.&#xA;&#xA;    light.depthFramebuffer.update(gl);&#xA;&#xA;    // Sets viewport dimensions with depth framebuffer's dimensions&#xA;&#xA;    this.viewport(new Vector2(), light.depthFramebuffer.size);&#xA;&#xA;    if (light instanceof PointLight) {&#xA;&#xA;      up = new Vector3();&#xA;      view = new Matrix4();&#xA;      origin = new Vector3();&#xA;      target = new Vector3();&#xA;&#xA;      for (j = 0; j &amp;lt; 6; j++) {&#xA;&#xA;    // Check in which cubemap's face we are ...&#xA;&#xA;        switch (j) {&#xA;          case Cubemap.POSITIVE_X:&#xA;            target.set(1, 0, 0);&#xA;            up.set(0, -1, 0);&#xA;            break;&#xA;          case Cubemap.NEGATIVE_X:&#xA;            target.set(-1, 0, 0);&#xA;            up.set(0, -1, 0);&#xA;            break;&#xA;          case Cubemap.POSITIVE_Y:&#xA;            target.set(0, 1, 0);&#xA;            up.set(0, 0, 1);&#xA;            break;&#xA;          case Cubemap.NEGATIVE_Y:&#xA;            target.set(0, -1, 0);&#xA;            up.set(0, 0, -1);&#xA;            break;&#xA;          case Cubemap.POSITIVE_Z:&#xA;            target.set(0, 0, 1);&#xA;            up.set(0, -1, 0);&#xA;            break;&#xA;          case Cubemap.NEGATIVE_Z:&#xA;            target.set(0, 0, -1);&#xA;            up.set(0, -1, 0);&#xA;            break;&#xA;        }&#xA;&#xA;    // Creates a view matrix using target and up vectors according to each face of pointlight's&#xA;    // cubemap. Furthermore, I translate it in minus light position in order to place&#xA;    // the point light in the world's origin and render each cubemap's face at this &#xA;    // point of view&#xA;&#xA;        view.lookAt(origin, target, up);&#xA;        view.mul(new EZ3.Matrix4().translate(light.position.clone().negate()));&#xA;&#xA;    // Flips the Y-coordinate of each cubemap face&#xA;    // scaling the projection matrix by (1, -1, 1).&#xA;&#xA;    // This is a perspective projection matrix which has:&#xA;    // 90 degress of FOV.&#xA;    // 1.0 of aspect ratio.&#xA;    // Near clipping plane at 0.01.&#xA;    // Far clipping plane at 2000.0.&#xA;&#xA;        projection = light.projection.clone();&#xA;        projection.scale(new EZ3.Vector3(1, -1, 1));&#xA;&#xA;    // Attaches a cubemap face to current framebuffer in order to record depth values for the face with this line&#xA;    // gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_CUBE_MAP_POSITIVE_X + j, id, 0);&#xA;&#xA;        light.depthFramebuffer.texture.attach(gl, j);&#xA;&#xA;    // Clears current framebuffer's color with these lines:&#xA;    // gl.clearColor(1.0,1.0,1.0,1.0);&#xA;    // gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);&#xA;&#xA;        this.clear(color);&#xA;&#xA;    // Renders shadow caster meshes using the depth program&#xA;&#xA;        for (k = 0; k &amp;lt; shadowCasters.length; k++)&#xA;          this._renderShadowCaster(shadowCasters[k], program, view, projection);&#xA;      }&#xA;    } else {&#xA;       // Directional light &amp;amp; Spotlight case ...&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fourth, this is how I compute Omnidirectional Shadow Mapping using my depth cubemap in my main Vertex Shader &amp;amp; Fragment Shader:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vertex Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;attribute vec3 position;&#xA;&#xA;uniform mat4 uModel;&#xA;uniform mat4 uModelView;&#xA;uniform mat4 uProjection;&#xA;&#xA;varying vec3 vPosition;&#xA;&#xA;void main() {&#xA;  vPosition = vec3(uModel * vec4(position, 1.0));&#xA;&#xA;  gl_Position = uProjection * uModelView * vec4(position, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fragment Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float unpackDepth(in vec4 color) {&#xA;    return dot(color, vec4(1.0 / (256.0 * 256.0 * 256.0), 1.0 / (256.0 * 256.0), 1.0 / 256.0, 1.0 ));&#xA;}&#xA;&#xA;float pointShadow(const in PointLight light, const in samplerCube shadowSampler) {&#xA;    vec3 direction = vPosition - light.position;&#xA;    float vertexDepth = clamp(length(direction), 0.0, 1.0);&#xA;    float shadowMapDepth = unpackDepth(textureCube(shadowSampler, direction));&#xA;&#xA;    return (vertexDepth &amp;gt; shadowMapDepth) ? light.shadowDarkness : 1.0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Finally, this is the result that I'm getting , my scene has a plane, a cube and a sphere. Besides, the red bright sphere is the point light source:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/YM04V.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YM04V.jpg&quot; alt=&quot;Omnidirectional Shadow Mapping Issue&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, I seems like point light depth framebuffer's cubemap it is not doing a good interpolation among their faces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Until now, I've no idea how to solve this.&lt;/p&gt;&#xA;" OwnerUserId="2228" LastEditorUserId="2228" LastEditDate="2015-12-15T02:12:45.093" LastActivityDate="2015-12-21T15:05:18.920" Title="WebGL Omnidirectional Shadow Mapping Issue" Tags="&lt;glsl&gt;&lt;webgl&gt;&lt;shadow-mapping&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1789" PostTypeId="2" ParentId="1784" CreationDate="2015-12-08T22:46:58.063" Score="1" Body="&lt;p&gt;Transformations, and especially rotations, may have different visual effects, depending on the order they are applied to the target object. For example, using a rotation $M_R$ and a translation $M_T$ to the object's vertices $v_i$, $M_R \cdot M_T \cdot v_i$ will usually&lt;sup&gt;1&lt;/sup&gt; result in a different pose than $M_T \cdot M_R \cdot v_i$ does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/lE2zn.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/lE2zn.png&quot; alt=&quot;rotation and translation&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;90 degree rotation and translation along the x axis applied to the box. Left: translation first ($M_R \cdot M_T \cdot v_i$). Right: rotation first ($M_T \cdot M_R \cdot v_i$).&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The key here is to keep in mind to which coordinate system the transformation is applied. Assuming that your to be rotated object initially is in the origin without transformations applied, the first rotation will always yield the desired effect. This is because you're applying the first transformation to the object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But once your object is already transformed (rotated, translated, whatever) by $M_1$, technically you can already apply the next transformation $M_2$ in two places. Namely, &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;$M_1 \cdot M_2 \cdot v_i$ or &lt;/li&gt;&#xA;&lt;li&gt;$M_2 \cdot M_1 \cdot v_i$&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;You describe that &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;so if I rotate the object around the x axis by 180° and then move my mouse left, the object will rotate to the right instead of left (since the y axis is now inverted).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;This is, because the second rotation is applied to the object itself, which is case 1 of the above. But you want to have the second case, so you need to apply the rotations the other way around. That way, the new rotation will be applied to the already rotated object, i.e. the outer coordinate system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ideally, as long as the user holds the mouse button down you adjust that transformation $M_2$. Calculate the distance the mouse moved from the point the user pressed the button, recalculate the whole rotation ($M_2$) and drop whatever you calculated in the time between the mouse-down event and the previous frame. So for the ongoing rotation, pretend that the user moved the mouse instantly from the start to the current position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once the mouse button is released, you can finalize the transformation and multiply it into $M_1$, which is the identity matrix in the beginning. (Assuming you always want to rotate around the world's center.)&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt; There are some special cases where the result is the same, e.g. if the translation direction matches the rotation axis.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-12-09T21:44:33.037" LastActivityDate="2015-12-09T21:44:33.037" CommentCount="3" />
  <row Id="1790" PostTypeId="1" AcceptedAnswerId="1791" CreationDate="2015-12-08T23:04:42.087" Score="8" ViewCount="90" Body="&lt;p&gt;I was reading &lt;a href=&quot;https://research.nvidia.com/sites/default/files/publications/voxel-pipe.pdf&quot;&gt;this paper&lt;/a&gt; about Voxelpipe, a voxelization library from NVIDIA and I found on section 2 Voxelization the terms &lt;strong&gt;&lt;em&gt;6-separating&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;26-separating&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found &lt;a href=&quot;http://labs.cs.sunysb.edu/labs/projects/volume/Papers/Voxel/&quot;&gt;this website&lt;/a&gt; that tries to explain the basic ideas on voxelization but it wasn't very much helpful understanding the terms mentioned.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody explain or point out to some other resource that can help me understand?&lt;/p&gt;&#xA;" OwnerUserId="116" LastActivityDate="2015-12-09T18:19:15.953" Title="What does &quot;6-separating&quot; and &quot;26-separating&quot; voxelization mean?" Tags="&lt;voxelization&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1791" PostTypeId="2" ParentId="1790" CreationDate="2015-12-09T00:20:05.397" Score="12" Body="&lt;p&gt;The terms have to do with the &quot;thickness&quot; of the voxelization. I'll illustrate with the help of a diagram about 2D line rasterization (from &lt;a href=&quot;https://stackoverflow.com/questions/4381269/line-rasterisation-cover-all-pixels-regardless-of-line-gradient&quot;&gt;this unrelated question&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7911d.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7911d.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the right is the typical line rasterization: the algorithm finds the one pixel nearest the line within each row (or column, depending on slope). This produces what we usually think of as a &quot;1-pixel-thick&quot; line. On the left is a conservative rasterization, which finds every pixel whose rectangle is touched by the line, and it produces a thicker line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;6-separating voxelization is like the thin line on the right, and 26-separating is like the thick line on the left, but in 3D. If you imagine the line is actually a triangle viewed on-edge, this is analogous to what the voxelization would look like.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Different types of voxelization may be better depending on what you're going to do with the voxelized data later. If you're using the voxels as a spatial hierarchy to find triangles that intersect a given region, you probably want the thick voxelization, as it's conservative. The thick voxelization might also be preferable for ray-marching, as the thin voxelization could be missed by a diagonal ray. On the other hand, the thin voxelization is a more faithful representation of the original surface, which is probably better for visibility tests, collision detection, fluid simulation, and suchlike.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &quot;n-separating&quot; terminology is a bit unfortunate, but here's what it's getting at. Imagine you're doing a 3D flood-fill in the voxel grid, but in the flood-fill you only look at the 6 direct neighbors of each voxel (±1 step along each axis). Then the &quot;6-separating&quot; (thin) voxelization will stop the flood-fill: it suffices to separate the two sides of the surface, if only 6 neighbors are considered. On the other hand, suppose your flood-fill was allowed to go to diagonal neighbors as well—26 neighbors in all (3×3×3 neighborhood of voxels). Then the 6-separating voxelization wouldn't stop the flood fill, but the 26-separating (thick) one would.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-12-09T18:19:15.953" LastActivityDate="2015-12-09T18:19:15.953" CommentCount="2" />
  <row Id="1792" PostTypeId="2" ParentId="1784" CreationDate="2015-12-09T12:11:36.917" Score="0" Body="&lt;p&gt;Alright, using ArcBall in SharpGL.SceneGraph.Core for my mouse controlled rotations fixed the problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just had to edit the following&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (length &amp;gt; 1.0f)&#xA;        {&#xA;            //  Get normalising factor.&#xA;            float norm = 1.0f / (float)Math.Sqrt(length);&#xA;&#xA;            //  Return normalised vector, a point on the sphere.&#xA;            newVector = new Vertex(scaledX * norm, scaledY * norm, 0);&#xA;        }&#xA;        else&#xA;        {&#xA;            //  Return a vector to a point mapped inside the sphere.&#xA;            newVector = new Vertex(scaledX, scaledY, (float)Math.Sqrt(1.0f - length));&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in ArcBall.MapToSphere so side movements rotate the object around the y axis instead of the z axis. Works as intended now.&lt;/p&gt;&#xA;" OwnerUserId="2219" LastActivityDate="2015-12-09T12:11:36.917" CommentCount="0" />
  <row Id="1793" PostTypeId="1" CreationDate="2015-12-09T17:40:28.850" Score="4" ViewCount="273" Body="&lt;p&gt;A fairly basic, in some ways, question, but one that many people, myself included, don't really know the answer to. GPU manufacturers often cite extremely high numbers, and the spread between polygon counts that various game engines claim to support often spans multiple orders of magnitude, and then still depends heavily on a lot of variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm aware that this is a broad, pretty much open-ended question, and I apologize for that, I just thought that it would be a valuable question to have on here nonetheless.&lt;/p&gt;&#xA;" OwnerUserId="2111" LastActivityDate="2015-12-17T12:49:21.307" Title="How many polygons in a scene can modern hardware reach while maintaining realtime, and how to get there?" Tags="&lt;gpu&gt;&lt;optimisation&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="1794" PostTypeId="1" CreationDate="2015-12-09T18:29:56.273" Score="6" ViewCount="82" Body="&lt;p&gt;I'm new to computer graphics. These days I've been trying to understand how ray tracing using an acceleration data structure works. I came across the term &quot;early ray termination&quot; several times, I looked it over the internet several times too, but I haven't been able to find a satisfactory explanation of it.&#xA;What does it mean to terminate a ray early, and why do we have to do it?&lt;br/&gt;&#xA;Besides, I noticed that the term &quot;front-to-back traversal&quot; is mentioned almost every time there's a mention of &quot;early ray termination&quot;.&#xA;Concretely how does front-to-back traversal work (in the case of a kd-tree for example) ?&lt;/p&gt;&#xA;" OwnerUserId="2233" LastActivityDate="2015-12-10T05:20:02.120" Title="The meaning of early ray termination and front-to-back traversal in ray tracing" Tags="&lt;raytracing&gt;&lt;c++&gt;&lt;optimisation&gt;&lt;data-structure&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="1795" PostTypeId="2" ParentId="1785" CreationDate="2015-12-09T18:40:17.690" Score="1" Body="&lt;p&gt;The endpoint of the line doesn't extend out to the edge of the box because you're using the circle equation with a fixed radius:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;double x2 = x1 + (lenght * cos(radians));&#xA;double y2 = y1 + (lenght * sin(radians));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This makes the endpoint trace out a circle as the angle is changed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want the endpoint to be on the edge of the box, one way is to set up a ray from the center of the box at the desired angle, then calculate the intersection point of the ray with the box. &lt;a href=&quot;http://www.scratchapixel.com/lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/ray-box-intersection&quot; rel=&quot;nofollow&quot;&gt;This article shows how to calculate ray/box intersections&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way is to loop over the points around the sides of the box, rather than looping over angles. You would have four separate loops, one for each side, and holding one of x or y fixed while iterating the other. For example, holding x = 10 and iterating y from -10 to +10 would go up the right side.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the slopes outside [0, 1], you must extend your Bresenham algorithm for it. This can be found in any tutorial or sample of Bresenham, for example &lt;a href=&quot;http://rosettacode.org/wiki/Bitmap/Bresenham&amp;#39;s_line_algorithm#C.2B.2B&quot; rel=&quot;nofollow&quot;&gt;the implementations here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-09T18:40:17.690" CommentCount="3" />
  <row Id="1796" PostTypeId="1" CreationDate="2015-12-09T22:37:12.460" Score="1" ViewCount="40" Body="&lt;p&gt;I am at the moment having some issues with computing an endpoint in a raster grid that is also a valid position in a graph. &#xA;I am trying to draw a line between these two points, but as the point I am computing becomes negative, its position becomes invalid. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I compute the right endpoint value within a 20x20 matrix where my start point is $(10,10)$, and the endpoint has to be 10 steps away from the center at any angle?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is how I compute the angle now: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;coordinate endpoint(double angle, coordinate start, int lenght)&#xA;{&#xA;    double radians = (M_PI/180)*angle;&#xA;&#xA;    double x2 = start.first + (lenght * cos(radians));&#xA;    double y2 = start.second + (lenght * sin(radians));&#xA;&#xA;    return std::make_pair(round(x2),round(y2));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2226" LastEditorUserId="127" LastEditDate="2015-12-10T16:14:28.097" LastActivityDate="2015-12-10T16:14:28.097" Title="Compute angles and thereby endpoints in a raster grid?" Tags="&lt;maths&gt;&lt;raster-image&gt;&lt;triangulation&gt;&lt;debugging&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="1" />
  <row Id="1797" PostTypeId="2" ParentId="1786" CreationDate="2015-12-10T03:11:35.820" Score="2" Body="&lt;p&gt;I think it is fair to say that the reason there are so many niche variations of GLSL/HLSL/Cg/whatnot is because no programming language is nor will ever be a one size fits all tool. Different problems require different tools, so sometimes it is worth the effort of developing a custom built tool if it is going to pay off in the long run.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Stock GLSL is by itself pretty much unusable. It hasn't even acquired much stability across versions, so for a program that targets more than one OpenGL version, some sort of preprocessing is a must. HLSL is a bit more stable across versions, but again, if targeting more than one D3D version, some work will need to done to get good portability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The kinds of things people usually do are pretty much what you said, like a adding support for basic programming features such as modules and uniform syntax across versions, or even portability across different APIs (GL/D3D) without having to rewrite the shader code. More sophisticated things include fully fledged material systems or things like &lt;a href=&quot;http://gameangst.com/?p=441&quot; rel=&quot;nofollow&quot;&gt;generating shader programs on-the-fly&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Shading languages will probably get better and more generic in the future, incorporating things that today are commonly hand-rolled as core features. The new &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_Core_Next&quot; rel=&quot;nofollow&quot;&gt;GCN architecture&lt;/a&gt; is a sign of that. So shading languages will be more usable out-of-the-box a while from now, but custom built solutions will never go away because there's only so much you can generalize.&lt;/p&gt;&#xA;" OwnerUserId="54" LastActivityDate="2015-12-10T03:11:35.820" CommentCount="3" />
  <row Id="1798" PostTypeId="2" ParentId="1794" CreationDate="2015-12-10T05:20:02.120" Score="5" Body="&lt;p&gt;Front-to-back traversal is the idea that when traversing a ray through the acceleration structure, we want to examine nodes that are in front, i.e. closer to the camera, before other nodes. If you only want the first intersection with a surface (as usual in ray tracing), then if you get an intersection in the front node you don't need to traverse any nodes behind it; as any possible intersection there would be later than the one you already found. This saves time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, in a kd-tree, at each node you have a splitting plane dividing space into two child nodes. You would first descend to the child closer to the camera, and only if no intersections are found there do you descend to the other child.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Early ray termination seems to be a term used in the context of volume rendering. When you're traversing a ray through a medium that attenuates light, such as fog, smoke, cloudy liquids, etc., then beyond a certain distance there is no possibility of the ray having any more effect on the image. If you have thick fog with a visibility of 10 meters, then an object 11 meters away cannot be seen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can precalculate the distance at which the attenuation reaches 99% or so (it never quite reaches 100%, but approaches it exponentially). There is no reason to intersect the ray against objects farther than that, so when using the acceleration structure, you can simply discard any nodes beyond that distance. In effect, the ray becomes a line segment terminated at a fixed length. If it doesn't hit anything within that length, there's no point checking further. (Another case where limited-length rays are handy is for shadow rays. You only want to trace them back to the originating light source; any intersections beyond that are irrelevant for the shadow.)&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-10T05:20:02.120" CommentCount="2" />
  <row Id="1799" PostTypeId="1" AcceptedAnswerId="1800" CreationDate="2015-12-10T13:00:04.673" Score="5" ViewCount="148" Body="&lt;p&gt;I developed a ray tracer that use standard phong/blinn phong lighting model. Now I'm modifying it to support physically based rendering, so I'm implementing various BRDF models. At the moment I'm focused on Oren-Nayar and Torrance-Sparrow model. Each one of these is based on spherical coordinates used to express incident wi and outgoing wo light direction. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My question is: which way is the right one convert wi and wo from cartesian coordinate to spherical coordinate?&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm applying the standard formula reported here &lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_coordinate_system#Coordinate_system_conversions&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/Spherical_coordinate_system#Coordinate_system_conversions&lt;/a&gt; but I'm not sure I'm doing the right thing, &lt;strong&gt;because my vector are not with tail at the origin of the cartesian coordinate system, but are centered on the intersection point of the ray with the object.&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here you can find my current implementation:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/chicio/Multispectral-Ray-tracing/tree/brdf/RayTracing/RayTracer/Objects/BRDF&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Multispectral-Ray-tracing/tree/brdf/RayTracing/RayTracer/Objects/BRDF&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/chicio/Multispectral-Ray-tracing/blob/brdf/RayTracing/RayTracer/Math/Vector3D.cpp&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Multispectral-Ray-tracing/blob/brdf/RayTracing/RayTracer/Math/Vector3D.cpp&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me giving an explanation of the correct way to convert the wi and wo vector from cartesian to spherical coordinate?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I copy here the relevant part of code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;spherical coordinate calculation&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float Vector3D::sphericalTheta() const {&#xA;&#xA;    float sphericalTheta = acosf(Utils::clamp(y, -1.f, 1.f));&#xA;&#xA;    return sphericalTheta;&#xA;}&#xA;&#xA;float Vector3D::sphericalPhi() const {&#xA;&#xA;    float phi = atan2f(z, x);&#xA;&#xA;    return (phi &amp;lt; 0.f) ? phi + 2.f * M_PI : phi;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Oren Nayar&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;OrenNayar::OrenNayar(Spectrum&amp;lt;constant::spectrumSamples&amp;gt; reflectanceSpectrum, float degree) : reflectanceSpectrum{reflectanceSpectrum} {&#xA;&#xA;    float sigma = Utils::degreeToRadian(degree);&#xA;    float sigmaPowerTwo = sigma * sigma;&#xA;&#xA;    A = 1.0f - (sigmaPowerTwo / 2.0f * (sigmaPowerTwo + 0.33f));&#xA;    B = 0.45f * sigmaPowerTwo / (sigmaPowerTwo + 0.09f);&#xA;};&#xA;&#xA;Spectrum&amp;lt;constant::spectrumSamples&amp;gt; OrenNayar::f(const Vector3D&amp;amp; wi, const Vector3D&amp;amp; wo, const Intersection* intersection) const {&#xA;&#xA;    float thetaI = wi.sphericalTheta();&#xA;    float phiI = wi.sphericalPhi();&#xA;&#xA;    float thetaO = wo.sphericalTheta();&#xA;    float phiO = wo.sphericalPhi();&#xA;&#xA;    float alpha = std::fmaxf(thetaI, thetaO);&#xA;    float beta = std::fminf(thetaI, thetaO);&#xA;&#xA;    Spectrum&amp;lt;constant::spectrumSamples&amp;gt; orenNayar = reflectanceSpectrum * constant::inversePi * (A + B * std::fmaxf(0, cosf(phiI - phiO) * sinf(alpha) * tanf(beta)));&#xA;&#xA;    return orenNayar;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Torrance-Sparrow&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float TorranceSparrow::G(const Vector3D&amp;amp; wi, const Vector3D&amp;amp; wo, const Vector3D&amp;amp; wh, const Intersection* intersection) const {&#xA;&#xA;    Vector3D normal = intersection-&amp;gt;normal;&#xA;    normal.normalize();&#xA;&#xA;    float normalDotWh = fabsf(normal.dot(wh));&#xA;    float normalDotWo = fabsf(normal.dot(wo));&#xA;    float normalDotWi = fabsf(normal.dot(wi));&#xA;    float woDotWh = fabsf(wo.dot(wh));&#xA;&#xA;    float G = fminf(1.0f, std::fminf((2.0f * normalDotWh * normalDotWo)/woDotWh, (2.0f * normalDotWh * normalDotWi)/woDotWh));&#xA;&#xA;    return G;&#xA;}&#xA;&#xA;float TorranceSparrow::D(const Vector3D&amp;amp; wh, const Intersection* intersection) const {&#xA;&#xA;    Vector3D normal = intersection-&amp;gt;normal;&#xA;    normal.normalize();&#xA;&#xA;    float cosThetaH = fabsf(wh.dot(normal));&#xA;&#xA;    float Dd = (exponent + 2) * constant::inverseTwoPi * powf(cosThetaH, exponent);&#xA;&#xA;    return Dd;&#xA;}&#xA;&#xA;Spectrum&amp;lt;constant::spectrumSamples&amp;gt; TorranceSparrow::f(const Vector3D&amp;amp; wi, const Vector3D&amp;amp; wo, const Intersection* intersection) const {&#xA;&#xA;    Vector3D normal = intersection-&amp;gt;normal;&#xA;    normal.normalize();&#xA;&#xA;    float thetaI = wi.sphericalTheta();&#xA;    float thetaO = wo.sphericalTheta();&#xA;&#xA;    float cosThetaO = fabsf(cosf(thetaO));&#xA;    float cosThetaI = fabsf(cosf(thetaI));&#xA;&#xA;    if(cosThetaI == 0 || cosThetaO == 0) {&#xA;&#xA;        return reflectanceSpectrum * 0.0f;&#xA;    }&#xA;&#xA;    Vector3D wh = (wi + wo);&#xA;    wh.normalize();&#xA;&#xA;    float cosThetaH = wi.dot(wh);&#xA;&#xA;    float F = Fresnel::dieletricFresnel(cosThetaH, refractiveIndex);&#xA;    float g = G(wi, wo, wh, intersection);&#xA;    float d = D(wh, intersection);&#xA;&#xA;    printf(&quot;f %f g %f d %f \n&quot;, F, g, d);&#xA;    printf(&quot;result %f \n&quot;, ((d * g * F) / (4.0f * cosThetaI * cosThetaO)));&#xA;&#xA;    Spectrum&amp;lt;constant::spectrumSamples&amp;gt; torranceSparrow = reflectanceSpectrum * ((d * g * F) / (4.0f * cosThetaI * cosThetaO));&#xA;&#xA;    return torranceSparrow;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE 2&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After some search I found this implementation of Oren-Nayar BRDF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://content.gpwiki.org/index.php/D3DBook:(Lighting)_Oren-Nayar&quot; rel=&quot;nofollow&quot;&gt;http://content.gpwiki.org/index.php/D3DBook:(Lighting)_Oren-Nayar&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the implementation above theta for wi and wo is obtained simply doing arccos(wo.dotProduct(Normal)) and arccos(wi.dotProduct(Normal)). This seems reasonable to me, as we can use the normal of the intersection point as the zenith direction for our spherical coordinate system and do the calculation. The calculation of gamma = cos(phi_wi - phi_wo) do some sort of projection of wi and wo on what it calls &quot;tangent space&quot;. Assuming everything is correct in this implementation, can i just use the formulas |View - Normal x (View.dotProduct(Normal))| and |Light - Normal x (Light.dotProduct(Normal))| to obtain the phi coordinate (instead of using arctan(&quot;something&quot;))? &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;N.B.: I'm also reading &quot;Physically based rendering: from theory to implementation&quot;, that is the book released to pbrt (&lt;a href=&quot;http://www.pbrt.org&quot; rel=&quot;nofollow&quot;&gt;http://www.pbrt.org&lt;/a&gt;). In this implementation there's some sort of change of coordinate system for the point of intersection (using partial derivates and the parametric coordinate of the surface) (I'm reading it now so what I'm saying could not be accurate). I want to find a straight way (maybe the one above in UPDATE 2 is what I'm searching for, if anyone could confirm it).&lt;/p&gt;&#xA;" OwnerUserId="2237" LastEditorUserId="231" LastEditDate="2016-01-19T13:04:54.080" LastActivityDate="2016-01-19T13:04:54.080" Title="BRDF and Spherical coordinate in ray tracing" Tags="&lt;raytracing&gt;&lt;brdf&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1800" PostTypeId="2" ParentId="1799" CreationDate="2015-12-11T06:18:50.267" Score="8" Body="&lt;p&gt;The polar coordinate system commonly used in BRDF definitions is set up relative to the surface being shaded, i.e. in tangent space. The $\theta$ angle measures how far you are from the surface normal while $\phi$ is the azimuth around the plane of the surface relative to some reference direction (which doesn't matter unless the BRDF is anisotropic). So if you want to convert to these coordinates, you have to first get your vector in tangent space (with the origin at the intersection point and two of the axes aligned with the surface), then apply the usual Cartesian-to-spherical transformation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, normally you can and should evaluate BRDFs &lt;em&gt;without&lt;/em&gt; using polar coordinates, trigonometric functions, or angles at all, but just using vector math primitives such as dot products. This is usually more efficient &lt;em&gt;and&lt;/em&gt; it's more robust, as you don't have to deal with angle wraparound, factors of pi, out-of-range arguments to inverse trig functions and so on. For instance, you probably know that the cosine of the angle between vectors can be obtained by just dotting the (normalized) vectors. The sine and tangent can be obtained through trig identities from the cosine (i.e. from the dot product).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fabian Giesen wrote an article on this topic, &lt;a href=&quot;https://fgiesen.wordpress.com/2010/10/21/finish-your-derivations-please/&quot;&gt;Finish your derivations, please&lt;/a&gt;, that refers to the exact Oren-Nayar article you linked and gives an alternate, trig-free form:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Time to brush up your trigonometric identities. A particularly bad offender can be found here – the relevant section from the simplified shader is this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float alpha = max( acos( dot( v, n ) ), acos( dot( l, n ) ) );&#xA;float beta  = min( acos( dot( v, n ) ), acos( dot( l, n ) ) );&#xA;C = sin(alpha) * tan(beta);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;Ouch! If you use some trig identities and the fact that acos is monotonically decreasing over its domain, this reduces to:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float vdotn = dot(v, n);&#xA;float ldotn = dot(l, n);&#xA;C = sqrt((1.0 - vdotn*vdotn) * (1.0 - ldotn*ldotn))&#xA;  / max(vdotn, ldotn);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;  &#xA;  &lt;p&gt;..and suddenly there’s no need to use a lookup texture anymore (and by the way, this has way higher accuracy too).&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And note that this formulation only has one transcendental function (&lt;code&gt;sqrt&lt;/code&gt;) in place of four (two &lt;code&gt;acos&lt;/code&gt;, a &lt;code&gt;sin&lt;/code&gt;, and a &lt;code&gt;tan&lt;/code&gt;).&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2015-12-11T06:27:07.310" LastActivityDate="2015-12-11T06:27:07.310" CommentCount="3" />
  <row Id="1801" PostTypeId="1" AcceptedAnswerId="1803" CreationDate="2015-12-11T11:00:40.783" Score="8" ViewCount="81" Body="&lt;p&gt;Thinking about hybrid raytracing, hence the following question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suppose I have two solid spheres $s_1$ and $s_2$. We know their centres and radii, and we know that they have some overlapping volume in space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We have a typical 3D graphics setup: assume eye is at the origin, and we are projecting the spheres onto a view plane at $z = f$ for some positive $f$. The spheres are beyond the view plane and don't intersect it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $c$ be the circle in space that is points on the surface of both spheres, i.e. the visible (from some angles) 'join' of their overlapping volumes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to calculate if any of $c$ is visible when projected onto our view plane. It might not be, if $s_1$ or $s_2$ get completely in the way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas for approaching this?&lt;/p&gt;&#xA;" OwnerUserId="2244" LastEditorUserId="127" LastEditDate="2015-12-12T11:31:30.270" LastActivityDate="2015-12-12T11:31:30.270" Title="Sphere intersection occlusion (for hybrid raytracing)" Tags="&lt;raytracing&gt;&lt;3d&gt;&lt;occlusion&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1803" PostTypeId="2" ParentId="1801" CreationDate="2015-12-12T11:23:46.880" Score="7" Body="&lt;p&gt;Given that I didn't miss anything, you can probably cut this down to a problem in the 2D space. Viewing onto the plane defined by the center points of the spheres and your camera origin, the scene looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IhbVx.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IhbVx.png&quot; alt=&quot;scene with visible intersection&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The spheres become circles with the center points $C_1$ and $C_2$, and the intersection circle is now only 2 points with only the closer one $P$ being interesting. The camera/eye is arbitrarily set to the point $E$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Calculating if one point on the spheres is visible or not is easy: Simply check whether or not the angles at point $P$ between $E$ and $C_1$ respectively $E$ and $C_2$ are both greater (or equal to) 90 degree&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If $P$ is visible, some part (e.g. at least that point) of the intersection circle is visible. Otherwise the whole intersection circle must be occluded by one of your spheres, namely the one which creates an angle of less than 90 degree.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is how it looks if $P$ is not visible from $E$:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tAnbC.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tAnbC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can clearly see how that point is occluded by the circle around $C_2$ and that the angle between $E$ and $C_2$ in $P$ is less than 90 degree.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;sub&gt;&lt;sup&gt;1&lt;/sup&gt; Having an angle of exactly 90 degree means that the line between $E$ and $P$ just touches the respective circle/sphere in point $P$ as a tangent.&lt;/sub&gt;&lt;/p&gt;&#xA;" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2015-12-12T11:29:06.757" LastActivityDate="2015-12-12T11:29:06.757" CommentCount="1" />
  <row Id="1806" PostTypeId="2" ParentId="1793" CreationDate="2015-12-14T02:15:36.533" Score="4" Body="&lt;p&gt;I think it is commonly accepted that real time is everything that is above interactive. And interactive is defined as &quot;responds to input but is not smooth in the fact that the animation seems jaggy&quot;.&lt;br&gt;&#xA;So real time will depend on the speed of the movements one needs to represent. Cinema projects at 24 FPS and is sufficiently real time for many cases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then how many polygons a machine can deal with is easily verifiable by checking for yourself. Just create a little VBO patch as a simple test and a FPS counter, many DirectX or OpenGL samples will give you the perfect test bed for this benchmark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You'll find if you have a high end graphics card that you can display about 1 million polygons in real time. However, as you said, engines will not claim support so easily because real world scene data will cause a number of performance hogs that are unrelated to polygon count.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You have:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;fill rate&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;texture sampling&lt;/li&gt;&#xA;&lt;li&gt;ROP output&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;draw calls&lt;/li&gt;&#xA;&lt;li&gt;render target switches&lt;/li&gt;&#xA;&lt;li&gt;buffer updates (uniform or other)&lt;/li&gt;&#xA;&lt;li&gt;overdraw&lt;/li&gt;&#xA;&lt;li&gt;shader complexity&lt;/li&gt;&#xA;&lt;li&gt;pipeline complexity (any feedback used? iterative geometry shading? occlusion?)&lt;/li&gt;&#xA;&lt;li&gt;synch points with CPU (pixel readback?)&lt;/li&gt;&#xA;&lt;li&gt;polygon richness&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Depending on the weak and strong points of a particular graphic card, one or another of these points is going to be the bottleneck. It's not like you can say for sure &quot;there, that's the one&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wanted to add that, one cannot use the GFlops spec figure of one specific card and map it linearly to polygon pushing capacity. Because of the fact that polygon treatment has to go through a sequential bottleneck in the graphics pipeline as explained in great detail here:  &lt;a href=&quot;https://fgiesen.wordpress.com/2011/07/03/a-trip-through-the-graphics-pipeline-2011-part-3/&quot; rel=&quot;nofollow&quot;&gt;https://fgiesen.wordpress.com/2011/07/03/a-trip-through-the-graphics-pipeline-2011-part-3/&lt;/a&gt;&lt;br&gt;&#xA;TLDR: the vertices have to fit into a small cache before primitive assembly which is natively a sequential thing (the vertex buffer order matters).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you compare the GeForce 7800 (9yr old?) to this year's 980, it seems the number of operations per second it is capable of has increased one thousand fold. But you can bet that it's not going to push polygons a thousand times faster (which would be around 200 billion a second by this simple metric).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer the question &quot;what can one do to optimize an engine&quot;, as in &quot;not to lose too much efficiency in state switches and other overheads&quot;.&lt;br&gt;&#xA;That is a question as old as engines themselves. And is becoming more complex as history progress.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Indeed in real world situation, typical scene data will contain many materials, many textures, many different shaders, many render targets and passes, and many vertex buffers and so on. One engine I worked with worked with the notion of packets:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One packet is what can be rendered with one draw call.&lt;br&gt;&#xA;It contains identifiers to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;vertex buffer&lt;/li&gt;&#xA;&lt;li&gt;index buffer&lt;/li&gt;&#xA;&lt;li&gt;camera (gives the pass and render target)&lt;/li&gt;&#xA;&lt;li&gt;material id (gives shader, textures and UBO)&lt;/li&gt;&#xA;&lt;li&gt;distance to eye&lt;/li&gt;&#xA;&lt;li&gt;is visible&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So the first step of each frame is to run a quick sort on the packet list using a sort function with an operator that gives priority to visibility, then pass, then material, then geometry and finally distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Drawing close objects gets prirority to maximize early Z culling.&lt;br&gt;&#xA;Passes are fixed steps, so we have no choice but to respect them.&lt;br&gt;&#xA;Material is the most expensive thing to state-switch after render targets.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even in-between different materials IDs, a sub-ordering can been made using a heuristical criterion to diminish the number of shader changes (most expensive within material state-switch operations), and secondly texture binding changes.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;After all this ordering, one can apply mega texturing, virtual texturing, and attribute-less rendering (&lt;a href=&quot;http://renderingpipeline.com/2012/03/are-vertex-shaders-obsolete/&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;) if deemed necessary.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;About engine API also one common thing is to defer the issuing of the state-setting commands required by the client. If a client requests &quot;set camera 0&quot;, it is best to just store this request and if later the client calls &quot;set camera 1&quot; but with no other commands in between, the engine can detect the uselessness of the first command and drop it. This is redundancy elimination, which is possible by using a &quot;fully retained&quot; paradigm. By opposition to &quot;immediate&quot; paradigm, which would be just a wrapper above the native API and issue the commands right as ordered by client code. (&lt;a href=&quot;http://virtrev.git.sourceforge.net/git/gitweb.cgi?p=virtrev/virtrev;a=tree;f=coding/opencl_opengl_sharing/src;hb=HEAD&quot; rel=&quot;nofollow&quot;&gt;example: virtrev&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And finally, with modern hardware, a very expensive (to develop), but potentially highly rewarding step to take is to switch API to metal/mantle/DX12-style and preparing the rendering commands by hand.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;An engine that prepares rendering commands creates a buffer that holds a &quot;command list&quot; that is overwritten at each frame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Usually there is a notion of frame &quot;budget&quot;, a game can afford. You need to do everything in 16 milliseconds, so you clearly partition GPU time &quot;2 ms for lightpre pass&quot;, &quot;4 ms for materials pass&quot;, &quot;6 ms for indirect lighting&quot;, &quot;4 ms for postprocesses&quot;...&lt;/p&gt;&#xA;" OwnerUserId="1614" LastEditorUserId="231" LastEditDate="2015-12-17T12:49:21.307" LastActivityDate="2015-12-17T12:49:21.307" CommentCount="5" />
  <row Id="1807" PostTypeId="2" ParentId="1788" CreationDate="2015-12-15T00:12:41.023" Score="7" Body="&lt;p&gt;&lt;strong&gt;SOLUTION&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After a couple of days I realized that &lt;strong&gt;I was computing my projection matrix using a FOV angle in degrees and It should be in radians&lt;/strong&gt;. I made the conversion and now everything works great. The interpolation among faces of my depth framebuffer's cubemap is now perfect. For this reason, It is important to handle every single trigonometric function's angle in radians.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, I realized that you can compute your view matrix either as I said in the question and in this way:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;view.lookAt(position, target.add(position.clone()), up);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This approach means that your point of view is placed in pointlight's center and you just render in each direction of your cubemap, but which are these directions? well, &lt;strong&gt;these directions are computed adding each target that I've in the switch block (according to each cubemap's face) with your pointlight's position&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Furthermore, &lt;strong&gt;It is not necessary to flip Y-Coordinate of the projection matrix&lt;/strong&gt;, In this case, It is ok dispatch pointlight's perspective projection matrix to your GLSL shader without scaling It by (1, -1, 1) because &lt;strong&gt;I'm working with textures that don't have a flipped Y-Coordinate&lt;/strong&gt;, I think you should flip Y-Coordinate of your pointlight's projection matrix &lt;strong&gt;only if you are working with a flipped texture's Y-Coordinate&lt;/strong&gt;, this in order to have a correct omnidirectional shadow mapping effect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, I'll leave here the final version of my Omnidirectional Shadow Mapping algorithm on CPU/GPU side. On CPU side I'll explain every step that you have to do in order to compute a correct shadow map for each cubemap's face. On the other hand in GPU side, I'll explain my depth program's vertex/fragment shader and omnidirectional shadow mapping function in my main fragment shader, this in order to help somebody who could be learning this technique, or solve future doubts about this algorithm:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  // Disable blending and enable front face culling.&#xA;&#xA;  this.state.disable(gl.BLEND);&#xA;&#xA;  this.state.enable(gl.CULL_FACE);&#xA;  this.state.cullFace(gl.FRONT);&#xA;&#xA;  // Binds depth program&#xA;&#xA;  program.bind(gl);&#xA;&#xA;  // For each pointlight source do&#xA;&#xA;  for (i = 0; i &amp;lt; lights.length; i++) {&#xA;    light = lights[i];&#xA;&#xA;    // Get each pointlight's world position&#xA;&#xA;    position = light.worldPosition();&#xA;&#xA;    // Binds pointlight's depth framebuffer. Besides, in this function,&#xA;    // viewport's dimensions are set according to depth framebuffer's dimension.&#xA;&#xA;    light.depthFramebuffer.bind(gl, this.state);&#xA;&#xA;    // Updates point light's framebuffer in order to create it &#xA;    // or if it's resolution have changed, it'll be created again.&#xA;&#xA;    light.depthFramebuffer.update(gl);&#xA;&#xA;    // Check in which cubemap's face we are ...&#xA;&#xA;    for (j = 0; j &amp;lt; 6; j++) {&#xA;      switch (j) {&#xA;        case Cubemap.POSITIVE_X:&#xA;          target.set(1, 0, 0);&#xA;          up.set(0, -1, 0);&#xA;          break;&#xA;        case Cubemap.NEGATIVE_X:&#xA;          target.set(-1, 0, 0);&#xA;          up.set(0, -1, 0);&#xA;          break;&#xA;        case Cubemap.POSITIVE_Y:&#xA;          target.set(0, 1, 0);&#xA;          up.set(0, 0, 1);&#xA;          break;&#xA;        case Cubemap.NEGATIVE_Y:&#xA;          target.set(0, -1, 0);&#xA;          up.set(0, 0, -1);&#xA;          break;&#xA;        case Cubemap.POSITIVE_Z:&#xA;          target.set(0, 0, 1);&#xA;          up.set(0, -1, 0);&#xA;          break;&#xA;        case Cubemap.NEGATIVE_Z:&#xA;          target.set(0, 0, -1);&#xA;          up.set(0, -1, 0);&#xA;          break;&#xA;      }&#xA;&#xA;      // Creates a view matrix using target and up vectors &#xA;      // according to each face of pointlight's cubemap.&#xA;&#xA;      view.lookAt(position, target.add(position.clone()), up);&#xA;&#xA;      // Attaches cubemap's face to current framebuffer &#xA;      // in order to record depth values in that direction.&#xA;&#xA;      light.depthFramebuffer.texture.attach(gl, j);&#xA;&#xA;      // Clears color &amp;amp; depth buffers of your current framebuffer&#xA;&#xA;      this.clear();&#xA;&#xA;      // Render each shadow caster mesh using your depth program&#xA;&#xA;      for (k = 0; k &amp;lt; meshes.length; k++)&#xA;        this._renderMeshDepth(program, meshes[k], view, light.projection);&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On renderMeshDepth function I've:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  // Computes pointlight's model-view matrix &#xA;&#xA;  modelView.mul(view, mesh.world);&#xA;&#xA;  // Dispatch each matrix to the GLSL depth program&#xA;&#xA;  program.loadUniformMatrix(gl, 'uModelView', modelView);&#xA;  program.loadUniformMatrix(gl, 'uProjection', projection);&#xA;&#xA;  // Renders a mesh using vertex buffer objects (VBO)&#xA;&#xA;  mesh.render(gl, program.attributes, this.state, this.extensions);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depth Program Vertex Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;attribute vec3 position;&#xA;&#xA;uniform mat4 uModelView;&#xA;uniform mat4 uProjection;&#xA;&#xA;void main() {&#xA;  gl_Position = uProjection * uModelView * vec4(position, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Depth Program Fragment Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;// The pack function distributes fragment's depth precision storing &#xA;// it throughout (R,G,B,A) color channels and not just R color channel &#xA;// as usual in shadow mapping algorithms. This is because I'm working&#xA;// with 8-bit textures and one color channel hasn't enough precision &#xA;// to store a depth value.&#xA;&#xA;vec4 pack(const in float depth) {&#xA;  const vec4 bitShift = vec4(255.0 * 255.0 * 255.0, 255.0 * 255.0, 255.0, 1.0);&#xA;  const vec4 bitMask = vec4(0.0, 1.0 / 255.0, 1.0 / 255.0, 1.0 / 255.0);&#xA;&#xA;  vec4 res = fract(depth * bitShift);&#xA;  res -= res.xxyz * bitMask;&#xA;&#xA;  return res;&#xA;}&#xA;&#xA;void main() {&#xA;  // Packs normalized fragment's Z-Coordinate which is in [0,1] interval.&#xA;&#xA;  gl_FragColor = pack(gl_FragCoord.z);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Omnidirectional Shadow Mapping function in my main fragment shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Unpacks fragment's Z-Coordinate which was packed &#xA;// on the depth program's fragment shader.&#xA;&#xA;float unpack(in vec4 color) {&#xA;   const vec4 bitShift = vec4(1.0 / (255.0 * 255.0 * 255.0), 1.0 / (255.0 * 255.0), 1.0 / 255.0, 1.0);&#xA;   return dot(color, bitShift);&#xA;}&#xA;&#xA;// Computes Omnidirectional Shadow Mapping technique using a samplerCube&#xA;// vec3 lightPosition is your pointlight's position in world coordinates.&#xA;// vec3 vPosition is your vertex's position in world coordinates, in code&#xA;// I mean this -&amp;gt; vPosition = vec3(uModel * vec4(position, 1.0));&#xA;// where uModel is your World/Model matrix.&#xA;&#xA;float omnidirectionalShadow(in vec3 lightPosition, in float bias, in float darkness, in samplerCube sampler) {&#xA;    vec3 direction = vPosition - lightPosition;&#xA;    float vertexDepth = clamp(length(direction), 0.0, 1.0);&#xA;    float shadowMapDepth = unpack(textureCube(sampler, direction)) + bias;&#xA;&#xA;    return (vertexDepth &amp;gt; shadowMapDepth) ? darkness : 1.0;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here you have a final render of the algorithm&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/AvMHa.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/AvMHa.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have fun coding beautiful graphics, good luck :) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;C.Z.&lt;/p&gt;&#xA;" OwnerUserId="2228" LastEditorUserId="2228" LastEditDate="2015-12-21T15:05:18.920" LastActivityDate="2015-12-21T15:05:18.920" CommentCount="0" />
  <row Id="1809" PostTypeId="1" AcceptedAnswerId="1811" CreationDate="2015-12-15T18:51:57.800" Score="10" ViewCount="122" Body="&lt;p&gt;I thought I had formed a general understanding of how OpenGL naming conventions and extensions worked, until I stumbled upon a case that confused me.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here's my understanding so far:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;No suffix&lt;/strong&gt; -- e.g. &lt;code&gt;glGenBuffers()&lt;/code&gt;. This function is a part of the core profile. The &lt;a href=&quot;https://www.opengl.org/wiki/GLAPI/glGenBuffers&quot;&gt;wiki page&lt;/a&gt; tells me that this was added to the core profile starting from version 1.5.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;ARB&lt;/strong&gt; -- e.g. &lt;code&gt;glGenBuffersARB()&lt;/code&gt;. This function is part of the standardized &lt;code&gt;GL_ARB_vertex_buffer_object&lt;/code&gt; extension. The &lt;a href=&quot;https://www.opengl.org/registry/specs/ARB/vertex_buffer_object.txt&quot;&gt;spec&lt;/a&gt; of this extension clearly declares &lt;code&gt;GenBuffersARB()&lt;/code&gt; in the &quot;New Procedures and Functions&quot; section. The &quot;Dependencies&quot; section tells me that I can potentially access this from a 1.4+ context, if the hardware supports the extension.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EXT&lt;/strong&gt; -- These are vendor-specific extensions and functions that only some vendors may support. Vertex buffer object doesn't seem to have an EXT extension in the registry.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here's where my understanding breaks down:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;glGenFramebuffers&lt;/code&gt;, as the &lt;a href=&quot;https://www.opengl.org/wiki/GLAPI/glGenFramebuffers&quot;&gt;wiki&lt;/a&gt; shows, was added to the core in 3.0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to access the frame buffer features at a lower core profile version than 3.0. So I want to use it as an extension. The spec registry tells me that there are two available extensions - &lt;a href=&quot;https://www.opengl.org/registry/specs/ARB/framebuffer_object.txt&quot;&gt;ARB&lt;/a&gt; and &lt;a href=&quot;https://www.opengl.org/registry/specs/EXT/framebuffer_object.txt&quot;&gt;EXT&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt; -- If an ARB extension exists, why does an EXT extension exist? Wouldn't you always choose the standardized one over the vendor-specific one?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A look at the ARB spec in the &quot;New procedures and functions&quot; section tells me that the extension defines the &lt;code&gt;GenRenderbuffers()&lt;/code&gt; function. No ARB suffix this time. GLEW doesn't have a function prototype for &lt;code&gt;glGenRenderbuffersARB()&lt;/code&gt; at all. Weird.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The EXT spec does however have a &lt;code&gt;GenRenderbuffersEXT()&lt;/code&gt; function in the new functions section, and GLEW also has&lt;code&gt;glGenRenderbuffersEXT()&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt; -- Why no ARB suffix if there's an EXT suffix? How does this work for ARB, given that the names of the ARB function and the core function are the same? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt; -- I ultimately want Framebuffer features from a 1.4 profile. Which extension and which function-set should I use so that I get maximum hardware compatibility coverage?&lt;/p&gt;&#xA;" OwnerUserId="88" LastEditorUserId="88" LastEditDate="2015-12-15T18:58:07.407" LastActivityDate="2015-12-16T01:03:39.430" Title="OpenGL compatibility, naming conventions and ARB vs EXT" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1811" PostTypeId="2" ParentId="1809" CreationDate="2015-12-16T01:03:39.430" Score="8" Body="&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt; -- Usually the EXT version comes first as a collaboration between two or more vendors. ARB extensions require more discussion among voting members of Khronos and can have changes from the EXT version before getting approved. See the GL_ARB_direct_state_access extension which has many changes compared to GL_EXT_direct_state_access.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt; --&#xA;The Issues section of the GL_ARB_framebuffer_object extension states why the functions do not have ARB suffixes:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;(8) Why don't the new tokens and entry points in this extension have suffixes like other ARB extensions?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   RESOLVED: Unlike most ARB extensions, this is a strict subset of&#xA;   functionality already approved in OpenGL 3.0. This extension&#xA;   exists only to support that functionality on older hardware that&#xA;   cannot implement a full OpenGL 3.0 driver. Since there are no&#xA;   possible behavior changes between the ARB extension and core&#xA;   features, source code compatibility is improved by not using&#xA;   suffixes on the extension.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt; --&#xA;If you want to use framebuffer objects on a context where the GL version is less than 3.0 you need to look at the extension string:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;If GL_ARB_framebuffer_object is supported, use the non ARB functions.&lt;/li&gt;&#xA;&lt;li&gt;If only GL_EXT_framebuffer_object is supported, use the EXT functions.&lt;/li&gt;&#xA;&lt;li&gt;If neither extension are supported, you need to fall back to OS level offscreen rendering like pbuffers.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="40" LastActivityDate="2015-12-16T01:03:39.430" CommentCount="0" />
  <row Id="1815" PostTypeId="1" AcceptedAnswerId="1816" CreationDate="2015-12-18T13:45:43.347" Score="10" ViewCount="109" Body="&lt;p&gt;I computed a Voronoï diagram from a set of points (with &lt;a href=&quot;http://www.boost.org/doc/libs/1_60_0/libs/polygon/doc/voronoi_diagram.htm&quot;&gt;Boost.polygon&lt;/a&gt;). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I try to find a Delaunay triangulation, connecting each cell center for each Voronoï edge, but I miss some edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the following image, the red dots are my initial points, the blue lines are the Voronoï edges (I ignored infinite edges), and the green lines are the triangulation edges (one green edge for each blue edge, connecting two cell origins).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can see that diagonal edges are missing. What am I missing?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JXNcc.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JXNcc.png&quot; alt=&quot;voronoi diagram&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2173" LastEditorUserId="231" LastEditDate="2015-12-18T17:47:13.000" LastActivityDate="2015-12-19T11:31:59.897" Title="How to triangulate from a Voronoï diagram?" Tags="&lt;triangulation&gt;&lt;polygon&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1816" PostTypeId="2" ParentId="1815" CreationDate="2015-12-18T20:44:40.927" Score="14" Body="&lt;p&gt;The centre point in your diagram is a degenerate edge of the Voronoi diagram. If you generate a Voronoi diagram for an irregular point cloud, every vertex will have degree 3. A vertex with degree 4 (or more) can only happen when two (or more) vertices coincide. That means there is a zero-length edge between them. But that edge should still have a corresponding edge in the Delaunay triangulation. The issue is that it's arbitrary which of the two possible edges you pick, because the zero-length edge has no associated direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To visualise what I'm talking about, consider starting with four less regularly spaced points (such that we start off with only degree-3 vertices) and gradually translating them into their regular positions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can do this in two different ways, which both lead to the degenerate case in your diagram. You will see that you start off with two different Delaunay triangulations, which are both valid limits for the degenerate case:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eu33Z.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eu33Z.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/6S6oX.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/6S6oX.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume that your code is missing this degenerate case for one reason or another, but without actually seeing how you compute the Delaunay triangulation from the Voronoi diagram it's impossible to point you any further than this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note that having even higher degeneracies (by more than four points distributed at equal angles around a circle) would probably require additional attention:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5DgNy.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5DgNy.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://i.stack.imgur.com/AddyS.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/AddyS.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These animations also show that (even in a non-degenerate case), the corresponding Voronoi and Delaunay edges do not necessarily actually cross within their finite extent. That might make it harder to see that the 2 (or 3) edges that triangulate the regular polygon at the end actually correspond to several degenerate edges which are all at the centre. Also note that in total there are 5 different triangulations of the pentagon and 14 triangulations of the hexagon (although I don't know whether all 14 can be obtained by deforming a non-degenerate triangulation).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; (by OP)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Voronoï diagrams computed with &lt;a href=&quot;http://www.boost.org/doc/libs/1_60_0/libs/polygon/doc/index.htm&quot;&gt;Boost.polygon&lt;/a&gt; enable to walk through each Voronoï vertix, and each edge linked to those vertices (clockwise or counter-clockwise). In this way, it is possible to create one triangle for each pair of edges (two connected edges will link to 3 cells).&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2015-12-19T11:31:59.897" LastActivityDate="2015-12-19T11:31:59.897" CommentCount="4" />
  <row Id="1818" PostTypeId="1" AcceptedAnswerId="1819" CreationDate="2015-12-21T07:14:25.690" Score="13" ViewCount="783" Body="&lt;p&gt;With OpenGL and such I can render some pretty amazing looking things in &quot;real time&quot; 60 FPS.  However, if I try to make a video of that same scene in let's say Maya, or 3ds Max it would take MUCH MUCH longer for it to render even though it is the same resolution and FPS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why do these two types of rendering take different periods of time for the same result?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: Yes I do realize that 3D animation software can produce highly superior images to what could be done real time. But for this question I am referring to a scene of equal complexity.&lt;/p&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="127" LastEditDate="2015-12-21T10:50:34.440" LastActivityDate="2015-12-23T05:00:45.190" Title="Difference between rendering in OpenGL and 3D animation software" Tags="&lt;rendering&gt;&lt;performance&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="2" />
  <row Id="1819" PostTypeId="2" ParentId="1818" CreationDate="2015-12-21T09:30:23.303" Score="8" Body="&lt;p&gt;The main difference would be that with OpenGL in let's say a video game you will have a process called &lt;em&gt;rasterization&lt;/em&gt; which basically takes care of determining what part of the scene you see.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It needs to be fast so we can experience it as realtime.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore the algorithm does a few simple steps.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;check if a certain part of the scene is in my view frustum&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TqkCF.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TqkCF.jpg&quot; alt=&quot;Frustum Culling&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;check if something is in front of it which may needs to be rendered later using a depth buffer&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/7N5cL.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7N5cL.gif&quot; alt=&quot;Depth Buffer&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;order the objects we found to draw&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;draw them&lt;/em&gt; by projecting them on the screen&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;shade them&lt;/em&gt; based on textures / shaders / lights / ...&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;On the other hand a rendering software (Blender/Max/Maya/...) most likely uses some kind of &lt;em&gt;raytracing&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This involves alot more math to achieve a higher degree of realism.&#xA;It basically works in the same way:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;create a camera and an image plane infront of it&lt;/li&gt;&#xA;&lt;li&gt;shoot one ray (or multiple sample rays) through each pixel&lt;/li&gt;&#xA;&lt;li&gt;check if the ray hits anything in the scene&lt;/li&gt;&#xA;&lt;li&gt;closest hit is the one to be drawn in the pixel finally (like depth buffer)&lt;/li&gt;&#xA;&lt;li&gt;calculate the light for the given point&#xA;&lt;a href=&quot;http://i.stack.imgur.com/jmB0K.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jmB0K.png&quot; alt=&quot;Light Calculation&quot;&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;....&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll stopped listing here since this is the point where raytracing takes off.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead of only checking if a point is hit, most raytracer now begins to calculate: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the amount of light a surface penetrates&lt;/li&gt;&#xA;&lt;li&gt;how much light gets reflected&lt;/li&gt;&#xA;&lt;li&gt;cast new rays from the hitpoint into the scene until it may hit a light source&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;There are a ton of techniques with different degrees of realism which can be used to calculate the light of a certain point in the scene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&#xA;The gist would be that a raytracer mostly tries to be physically accurate when it comes the lighting and therefore has a lot of more calculations to do per pixel (sometimes shoot thousands of rays) and on the other hand games get their speed by drawing bigger chunks of the screen with simpler light calculations and a lot of shader tricks which let it look realistic.&lt;/p&gt;&#xA;" OwnerUserId="239" LastEditorUserId="127" LastEditDate="2015-12-21T10:57:34.817" LastActivityDate="2015-12-21T10:57:34.817" CommentCount="0" />
  <row Id="1820" PostTypeId="1" AcceptedAnswerId="1821" CreationDate="2015-12-21T11:58:38.770" Score="7" ViewCount="189" Body="&lt;p&gt;Im trying to plot the x and y positions of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Archimedean_spiral&quot; rel=&quot;nofollow&quot;&gt;Archimedean spiral&lt;/a&gt; in C++.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/wSmFI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wSmFI.png&quot; alt=&quot;Archimedean spiral&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I've been trying something like this, but no luck:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;int dx = 0;&#xA;int dy = 0;&#xA;int x = 0;&#xA;int y = 0;&#xA;&#xA;for (int i = 0; i &amp;lt; maxPoints; i++)&#xA;{&#xA;    dx = sin(i * PI / 2);&#xA;    dy = cos(-i * PI / 2);&#xA;    x += dx;&#xA;    y += dy;&#xA;&#xA;    plot(x, y);    &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;EDIT: More info&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm developing a 3D game application that demonstrates the use of the &lt;a href=&quot;http://bulletphysics.org/wordpress/&quot; rel=&quot;nofollow&quot;&gt;Bullet&lt;/a&gt; physics engine by simulating dominos. Instead of placing the dominos in the scene manually I want to use some math to do it for me :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For anyone who is interested here it is on &lt;a href=&quot;https://github.com/damorton/bullet-dominos&quot; rel=&quot;nofollow&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2248" LastEditorUserId="2248" LastEditDate="2015-12-22T21:28:53.727" LastActivityDate="2015-12-26T15:50:42.340" Title="Archimedean spiral in C++" Tags="&lt;c++&gt;&lt;geometry&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="1821" PostTypeId="2" ParentId="1820" CreationDate="2015-12-21T12:24:56.043" Score="7" Body="&lt;p&gt;Figured it out :) The dominos are now being placed along the X and Y coordinates generated by the function. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The original code in the question was plotting a wave of points outwards from the centre position or origin and was not what I wanted. What I needed was for each point to follow the &lt;code&gt;Archimedean spiral&lt;/code&gt; with a certain space between the spirals. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially I used &lt;code&gt;integer&lt;/code&gt; values to store the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; coordinates but this was causing a precision error by truncating the &lt;code&gt;floating point&lt;/code&gt; value in order to store it in the &lt;code&gt;integer&lt;/code&gt; data type.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The below example generates points along the spiral continuously, relative to the &lt;code&gt;maxPoints&lt;/code&gt; value. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float x = 0;&#xA;float y = 0;&#xA;float angle = 0.0f;&#xA;&#xA;// Space between the spirals&#xA;int a = 2, b = 2;&#xA;&#xA;for (int i = 0; i &amp;lt; maxPoints; i++)&#xA;{&#xA;    angle = 0.1 * i;&#xA;    x = (a + b * angle) * cos(angle);&#xA;    y = (a + b * angle) * sin(angle);&#xA;&#xA;    plot(x, y);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code for the project is on &lt;a href=&quot;https://github.com/damorton/bullet-dominos/blob/master/CAPhysics/Chapter4.2/BasicDemo.cpp&quot; rel=&quot;nofollow&quot;&gt;GitHub&lt;/a&gt;, you will need &lt;a href=&quot;http://bulletphysics.org/wordpress/&quot; rel=&quot;nofollow&quot;&gt;Bullet&lt;/a&gt; and &lt;a href=&quot;http://freeglut.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;freeglut&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2248" LastEditorUserId="2248" LastEditDate="2015-12-22T21:48:24.973" LastActivityDate="2015-12-22T21:48:24.973" CommentCount="2" />
  <row Id="1822" PostTypeId="2" ParentId="1818" CreationDate="2015-12-21T13:15:35.117" Score="6" Body="&lt;h3&gt;You're comparing apples to oranges&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The game is like the view port in your modelling app. You can use the viewport for render and you will get same 60fps speeds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is no reason why you can not get realtime graphics that are very good out of modelling software like Maya or 3DS Max. Results that are par with many games. They have viewport shaders just like games do. There is also a viewport rendering option that chunks frames to disk as fast as it allows (I've done full HD renders at 30 fps from Maya). All you have to do is stop using the provided software raytracers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are some differences though. The primary difference is that you as a user do not optimize stuff as much as game developers do (optimization is using all the tricks in the book). Second your animation primitives work on the CPU because you need the flexibility. In games one can afford to do optimizations. All in all you pay for not having a programming team next to you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Many of the things may in fact have been precomputed, so they aren't so much faster just better organized. Baking your indirect illumination will beat non baked results every day.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Why are the raytracers slower?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;They aren't*, one just tends to do more work on a ray tracer because its easy. Feature by feature they aren't much slower in computation cycles. For example theres no need for a ray tracer to cast secondary rays (life reflections in that case the ray tracer will cull geometry away, or not even load it, in fact mental ray does just that). Its just usually done because it trivial to do so and that's the clear advantage of ray tracers. You can even configure them to run on the CPU in some cases. They are just optimized for different things:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Emitting data to disk, not just frames but all data. Something which would break most games speediness instantly.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Working on general hardware. The GPU is much faster for certain things once you optimize for the GPU. But it does not work for all loads, in fact a Intel CPU is faster at computing in general than the GPU. The GPU is just massively parallel which the CPU is not. The architecture wins if you can stay in the GPU and minimize transfer and optimize for the GPU architecture.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;So you pay for flexibility and ease of use. But yes ill admit both Maya and Max suffer from extreme old age. So they could be faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; The difference is mostly in optimization (read lots of tricks) and available external resources.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: There is a misconception that this is because it is more physically correct. It certainly can be but the ray tracer is not inherently more physically correct than your average game or any other computation. In fact many games use really good models while quite many modelers do not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* See &lt;a href=&quot;http://www.graphics.cornell.edu/~bjw/mca.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.graphics.cornell.edu/~bjw/mca.pdf&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-12-22T10:05:13.287" LastActivityDate="2015-12-22T10:05:13.287" CommentCount="2" />
  <row Id="1824" PostTypeId="1" AcceptedAnswerId="1825" CreationDate="2015-12-22T02:47:08.040" Score="12" ViewCount="164" Body="&lt;p&gt;I am currently trying to understand a few things about vector graphics rasterization and the different ways it is implemented in various application types.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tested and compared a few programs and noticed that there is a major difference in the way &lt;strong&gt;anti-aliasing&lt;/strong&gt; behaves in the rasterization process. &lt;strong&gt;I am especially interested in the rendering behavior in Illustrator&lt;/strong&gt;. You will see why by reading further.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/J4ocR.png&quot; alt=&quot;Base image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For my testing, I used a really simple composition of triangles organized in an irregular hexagon with different colors.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Vector graphics softwares&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Here are three renders of the same vector graphic in Illustrator, Affinity and Inkscape. (The image produced in Affinity and Inkscape are exactly the same.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/OCJdY.png&quot; alt=&quot;Images rendered in popular graphic editing tools&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see there is an undesired white line on each of the edges in the image rendered with Affinity and Inkscape. The anti-aliasing does not fill this area with a solid color which results in a small gap between adjacent shapes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although there is no gap in the Illustrator render, the edges of the shapes look as smooth as the Affinity render.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an image showing the same area of each image upscaled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/6DBZt.png&quot; alt=&quot;Upscale render showing anti-aliasing&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a very subtle difference between the two images. The Affinity render is a tiny bit smoother but it is almost impossible to see the difference when looking at the images in their original sizes.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Browsers&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;SVG rendering&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Displaying the same graphics exported as SVG in a browser looks very similar to the raster image produced by both Affinity and Inkscape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/wUHjQ.png&quot; alt=&quot;SVG file rendered in browser&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are very tiny differences in edges anti-aliasing (that are not really worth showing here) but SVG rasterization in common browsers behaves pretty much the same.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Decomposed render&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Testing Illustrator rendering a bit further, I tried splitting parts of my graphics and exporting them individually then composing them back together with a raster editing software.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/iXbo2.png&quot; alt=&quot;Image recomposition scheme&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In theory it would result in the same image as having it in a single piece, but the result is slightly different using this method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/qq7Gr.png&quot; alt=&quot;Result of recomposed image&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As shown, when the two parts are composed, there is a tiny gap between them. Though it is more subtle, it is very similar to the graphic rendered in Affinity.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Polygon rendering&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Blender (3D software)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/oOwG9.png&quot; alt=&quot;SVG in Blender viewport&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blender allows you to import SVG files and manipulate them as curve objects. Here is the imported graphic shown in the 3D viewport. (By default material will be affected by lights in the scene. Checking the shadeless property in the material property panel will allow the shapes to be rendered with their original colors.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a render made of the SVG inside Blender.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/2NZfe.png&quot; alt=&quot;Actual render in Blender&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It does not have any gap between the triangles. Other 3D softwares are very likely to work the same way. So Blender behaves just like &lt;strong&gt;Illustrator&lt;/strong&gt;, or does it? Maybe it is the other way around?&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;The real questions&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Which vector drawing library does Illustrator use behind the scenes?&lt;/li&gt;&#xA;&lt;li&gt;Could it be possible that Illustrator makes use of a sort of 3D rendering engine? Is it open source? (probably not?)&lt;/li&gt;&#xA;&lt;li&gt;Can any of the well known vector drawing libraries such as Cairo and Skia achieve the same rendering behavior? (No gap between shapes)&lt;/li&gt;&#xA;&lt;li&gt;Is there any less known vector drawing library out there that has the same behavior?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2319" LastEditorUserId="231" LastEditDate="2015-12-22T12:30:04.157" LastActivityDate="2016-01-06T21:44:45.993" Title="What is Illustrator's vector rasterization process?" Tags="&lt;antialiasing&gt;&lt;vector-graphics&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="1825" PostTypeId="2" ParentId="1824" CreationDate="2015-12-22T08:01:30.097" Score="8" Body="&lt;p&gt;As as far as I can tell Illustrator has 2 or 3 different rasterizer. The onscreen preview is also prone to the same gap artifacting as your show though its tuned to minimize the effect. Your post seems to imply that your interested in the &quot;&lt;em&gt;art optimized&lt;/em&gt;&quot; output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tf7V4.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tf7V4.png&quot; alt=&quot;Art optimized versus screen preview of illustrator&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: The different render modes of illustrator. Art optimized on left and hinted on right. Note that hinted shows slight background through. I dont have the new CC version so i can not show the third.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;How does art optimized work then&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Art optimized render mode is simply a un-antialiased image rendered at higher resolution, that has then been subjected to a &lt;strong&gt;box filtering&lt;/strong&gt;. This is called super sampling which is costly. A box filter is simply the mean value of the samples in the box (pixel). This is indeed a similar technology as what the multi pixel filter used in your blender render uses. You can do the same thing manually and get same results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact the render you get form illustrator is not really good. Its rendered in nonlinear space and not corrected for and you can get better results by swapping to higher order filters and computing in linear color space. All multi sample methods exhibit the same correct rendering (given enough samples even mathematically correct).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EfM61.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EfM61.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Multi pixel filtering, on left illustrator native 'Art Optimized'. On right same image at 16x16 pixels with multi lobed &lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/58162/how-to-draw-smooth-curved-shapes-in-illustrator/58175#58175&quot;&gt;Lanczos filter in linear space&lt;/a&gt;. Make sure your zoom is 1:1.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;The problem&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;You have in fact come at terms with the real problem of current generation of onscreen renderers. They try to avoid multi sampling in favor for coverage calculation. But it simply can not work perfectly! Simply because alpha blend is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$c_0= c_1*\alpha + c_2*(1-\alpha)$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When chained never reaches 0 when chained over semitransparent surfaces so background show trough. Also it does not know how the coverage is distributed so it will do it wrong. There's simply no way around this in a non-overlapping case like this. but if you allow the shapes overlap then the coverage calculation can be solved (See &lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/47342/space-between-2-paths-illustrator&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Other questions&lt;/strong&gt; You can get same result in any other engine by just simply multisampling a non-antialiased image. You can not get the effect in on screen renderings without doing the overlap trick because they optimize for speed. Does illustrator use 3D? New CC might depends on your interpretation of what 3D is. This issue has nothing to do with 3D just about signal reconstruction methods so its not applicable to the question really.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Related post &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/148/how-do-i-accurately-compute-coverage-of-overlapping-analytical-curves/150#150&quot;&gt;how-do-i-accurately-compute-coverage-of-overlapping-analytical-curves&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-12-22T09:11:41.397" LastActivityDate="2015-12-22T09:11:41.397" CommentCount="4" />
  <row Id="1827" PostTypeId="2" ParentId="1818" CreationDate="2015-12-23T00:17:52.060" Score="4" Body="&lt;p&gt;&lt;strong&gt;Real-Time Preview&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Working in the VFX side of the industry, if you're talking about real-time viewport previews and not production rendering, then Maya and 3DS Max typically also use OpenGL (or possibly DirectX -- pretty much the same).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the main conceptual differences between VFX animation software and games is the level of assumptions they can make. For example, in VFX software, it's not uncommon for the artist to load a single, seamless character mesh that spans hundreds of thousands to millions of polygons. Games tend to optimize most for a large scene consisting of a boatload of simple, optimized meshes (thousands of triangles each).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Production Rendering and Path Tracing&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VFX software also places the emphasis not on the real-time preview but on production rendering where light rays are actually simulated one at a time. The real-time preview often is just that, a &quot;preview&quot; of the higher-quality production result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Games are doing a beautiful job of approximating a lot of those effects lately like real-time depth of field, soft shadows, diffuse reflections, etc., but they're in the heavy-duty approximation category (ex: blurry cube maps for diffuse reflections instead of actually simulating light rays).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Content&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coming back to this subject, the content assumptions between a VFX software and game wildly differ. A VFX software's main focus is to allow any possible kind of content to be created (at least that's the ideal, although in practically it's often nowhere close). Games focus on content with a lot more heavier assumptions (all models should be in the range of thousands of triangles, normal maps should be applied to fake details, we shouldn't actually have 13 billion particles, characters aren't actually animated by muscle rigs and tension maps, etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to those assumptions, game engines can often more easily apply acceleration techniques like frustum culling which enable them to maintain a high, interactive frame rate. They can make assumptions that some content is going to be static, baked down in advance. VFX software can't easily make those kinds of assumptions given the much higher degree of flexibility in content creation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Games Do it Better&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This might be kind of a controversial view, but the game industry is a much more lucrative industry than VFX software. Their budgets for a single game can span in the hundreds of millions of dollars, and they can afford to keep releasing next-generation engines every few years. Their R&amp;amp;D efforts are amazing, and there are hundreds upon hundreds of game titles being released all the time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VFX and CAD software, on the other hand, is nowhere near as lucrative. R&amp;amp;D is often outsourced by researchers working in academic settings, with a lot of the industry often implementing techniques published many years before as though it's something new. So VFX software, even coming from companies as large as AutoDesk, often isn't quite as &quot;state-of-the-art&quot; as the latest AAA game engines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They also tend to have a much longer legacy. Maya is a 17-year old product, for example. It's been refurbished a lot, but its core architecture is still the same. This might be analogous to trying to take Quake 2 and keep updating and updating it all the way up until 2015. The efforts can be great but probably won't match Unreal Engine 4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So anyway, that's a little take on that side of the topic. I couldn't make out whether you were talking about real-time previews in viewports or production rendering, so I tried to cover a bit of both.&lt;/p&gt;&#xA;" OwnerUserId="2247" LastEditorUserId="2247" LastEditDate="2015-12-23T05:00:45.190" LastActivityDate="2015-12-23T05:00:45.190" CommentCount="7" />
  <row Id="1828" PostTypeId="2" ParentId="1820" CreationDate="2015-12-23T13:22:21.527" Score="4" Body="&lt;p&gt;This isn't really a direct answer to this question (that already has an answer anyway), but might interest people who want to implement this algorithm in 3D.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I had to try implementing this algorithm to generate 3D spirals in blender using Python (could easily be converted to drawing with PIL or Matplotlib in 2D). So here's the algorithm and result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dcoM7.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dcoM7.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import bpy&#xA;from math import cos, sin&#xA;S = bpy.context.scene&#xA;&#xA;def add_archimedian_spiral( size = 0.1, length = 500, height = 1, name = 'archispiral' ):&#xA;    mesh = bpy.data.meshes.new( name = name )&#xA;&#xA;    o = bpy.data.objects.new(name, mesh)&#xA;    o.location = (0,0,0) # place at object origin&#xA;    S.objects.link( o )&#xA;&#xA;    z     = 0&#xA;    verts = []  &#xA;    for i in range( length ):&#xA;        angle = 0.1 * i&#xA;        x     = ( 2 * size * angle ) * cos( angle )&#xA;        y     = ( 2 * size * angle ) * sin( angle )&#xA;        z    += i / 10000 * height&#xA;        verts.append((x,y,z))&#xA;&#xA;    edges = []&#xA;    for i in range( len( verts ) ):&#xA;        if i == len( verts ) - 1: break&#xA;        edges.append((i, i+1))&#xA;&#xA;    mesh.from_pydata( verts, edges, [] )&#xA;&#xA;add_archimedian_spiral( size = 0.2, length = 500, height = 6 )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2317" LastEditorUserId="127" LastEditDate="2015-12-26T15:50:42.340" LastActivityDate="2015-12-26T15:50:42.340" CommentCount="2" />
  <row Id="1829" PostTypeId="1" CreationDate="2015-12-23T23:52:07.863" Score="3" ViewCount="65" Body="&lt;p&gt;I need to be able to click on this polygon in 2d. This polygons set of vertices aren't in the same space as the screen. (Ie: the edge of the screen may be a value of 20 in one space but 1920 in the other) the scale is different. How can I convert the vertices of the polygon to the screen space so that I can use the mouse coordinates to click on it. (This is only 2d)&lt;/p&gt;&#xA;" OwnerUserId="113" LastEditorUserId="113" LastEditDate="2015-12-26T18:50:11.920" LastActivityDate="2015-12-26T22:21:54.680" Title="How to get polygon coordinates in screen space" Tags="&lt;transformations&gt;&lt;2d&gt;&lt;matrices&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1830" PostTypeId="1" CreationDate="2015-12-24T13:10:00.983" Score="6" ViewCount="73" Body="&lt;p&gt;Suppose i have a 400*300 gray scale image.Now I want to scan each line horizontally and make the average of that line 128,such that the values get scaled as per their difference from mean(128) which will not cause any distortion to image.&lt;/p&gt;&#xA;" OwnerUserId="2335" LastActivityDate="2016-01-01T07:45:53.923" Title="Average intensity of an gray image" Tags="&lt;image-processing&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1831" PostTypeId="2" ParentId="1830" CreationDate="2015-12-24T14:41:00.023" Score="4" Body="&lt;p&gt;Here's an example on how to do this with Python, Numpy and Matplotlib.&#xA;Image on the right has been averaged (row by row), while the left is the original.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/V4wFH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/V4wFH.png&quot; alt=&quot;Before-After-Edited&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITED&lt;/strong&gt;: As suggested by @Nathan Reed, I avoided flipping colors when row average is 0. Rows with an average of 0 now get a value of 0.5 (128). The dark image encountered previously was due to values higher than 1 (255) due to scaling, that were represented as white in matplotlib's image renderer. I fixed this by setting the max value in any pixel to be 1.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;import numpy as np&#xA;from matplotlib import image as mpimg&#xA;from matplotlib import pyplot as plt&#xA;&#xA;def rgb2gray(rgb):&#xA;    return np.dot( rgb[...,:3], [0.299, 0.587, 0.144] )&#xA;&#xA;imgPath = &quot;C:/myImg.jpg&quot;&#xA;&#xA;im = mpimg.imread( imgPath )&#xA;gs = rgb2gray( im )&#xA;&#xA;averaged = gs.copy()&#xA;&#xA;for row in averaged:&#xA;    rowAvg = sum( row ) / len( row )&#xA;    if rowAvg == 0:&#xA;        row = 0.5&#xA;    else:&#xA;        # Scale each row by ratio between 128 and current row avg&#xA;        row *= 0.5 / rowAvg&#xA;&#xA;    # Make sure that no pixel has a higher value than 1&#xA;    if rowAvg &amp;gt; 0: row[row&amp;gt;1] = 1&#xA;&#xA;plt.subplot( 1, 2, 1 ), plt.imshow( gs,       'gray' )&#xA;plt.subplot( 1, 2, 2 ), plt.imshow( averaged, 'gray' )&#xA;&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here's another test with a simple gradient image (left is the original):&#xA;&lt;a href=&quot;http://i.stack.imgur.com/DhczR.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/DhczR.png&quot; alt=&quot;Before-After2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2317" LastEditorUserId="2317" LastEditDate="2015-12-25T22:48:07.470" LastActivityDate="2015-12-25T22:48:07.470" CommentCount="4" />
  <row Id="1832" PostTypeId="1" CreationDate="2015-12-24T23:16:22.657" Score="6" ViewCount="90" Body="&lt;p&gt;I'm not too versed in computer graphics so my question may be vague.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm given a sequence of GPS coordinates which I draw on an iOS map, and they define bus routes. Some of the bus routes happen to share sections of certain streets however, and when multiple routes are being drawn on the map the colored line segments are just being overlapped currently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently looking into computationally separating the lines from each other before drawing, but I'm not sure that will get any success. So my backup is: are there any good techniques to draw the lines in a way that somehow does not hide the lower line segments, and retains at least partially their color?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Again as a disclosure, I'm not very knowledgable in this area. I know from Photoshop there are different color modes on the layers that have different effects when overlapped. Would something of that sort help in this situation? Or maybe there's a way to dash the lines such that it alternates each color, but I imagine that would be fairly difficult.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm just probing around for different techniques for this problem.&lt;/p&gt;&#xA;" OwnerUserId="2339" LastEditorUserId="127" LastEditDate="2015-12-26T18:34:47.877" LastActivityDate="2015-12-26T23:34:56.133" Title="Best technique to draw overlapping colored line segments that follow the same route" Tags="&lt;rendering&gt;&lt;shader&gt;&lt;color&gt;&lt;line-drawing&gt;&lt;computational-geometry&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1833" PostTypeId="1" AcceptedAnswerId="1844" CreationDate="2015-12-25T03:54:51.673" Score="3" ViewCount="74" Body="&lt;p&gt;What is the pixel-level explanation of the fade effect, that was first popularized by Instagram? I want to implement this effect programmatically—that is, I want to understand what this effect is doing to each pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IVdTT.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IVdTT.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, if anyone could point me to resources from which I can learn more about how image filters are implemented, that would be great.&lt;/p&gt;&#xA;" OwnerUserId="2341" LastEditorUserId="231" LastEditDate="2015-12-27T13:35:16.263" LastActivityDate="2015-12-28T00:38:29.163" Title="Instagram's fade effect" Tags="&lt;image-processing&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1834" PostTypeId="2" ParentId="1829" CreationDate="2015-12-25T12:28:38.090" Score="3" Body="&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; This answer is only helpful in 3D&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;If you want to do it geometrically...&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The inverse of the view-projection matrix, $K^{-1}$ is the matrix you want. Where $K = View * Projection$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If $\vec{v}$ is a point in homogeneous screen coordinates, $[x, y, 1]$ where $x$ and $y$ are whatever coordinates your $K$ matrix projects onto. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\vec{d} = K^{-1} * \vec{v}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\vec{d}$ now represents a &lt;strong&gt;line&lt;/strong&gt; in homogeneous coordinates, coming out of your camera origin. Now you'd iterate through the objects in view, and choose the closest one in front of the camera that this line intersects with.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;But there are other ways!&lt;/h1&gt;&#xA;&#xA;&lt;h3&gt;Color Picking&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Here, we render (into a buffer) each object in view with a slightly different color, which corresponds to the object ID. We then read it using &lt;code&gt;glReadPixels(mouse_coordinates...)&lt;/code&gt; and decide which object we clicked. This is pretty easy to do in the new pipeline - you just write a color passthrough shader. Would not recommend pre OpenGL 3.0 (You'll have to do shenanigans to render the scene without interference)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Selection Buffer (Old pipeline)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If you're on the fixed pipeline, you get a nice little toy called the &lt;a href=&quot;http://www.glprogramming.com/red/chapter13.html&quot; rel=&quot;nofollow&quot;&gt;selection buffer&lt;/a&gt; to do the work for you.&lt;/p&gt;&#xA;" OwnerUserId="1988" LastEditorUserId="1988" LastEditDate="2015-12-26T22:21:54.680" LastActivityDate="2015-12-26T22:21:54.680" CommentCount="1" />
  <row Id="1835" PostTypeId="1" CreationDate="2015-12-25T15:40:21.760" Score="2" ViewCount="69" Body="&lt;p&gt;I need to build terrain using one cube with dimensions 1x1x1, the coordinates are provided in a .txt file and there are about 11M triplets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that using my current code I can only draw about 60k of them, then the browser tab is resetted and a prompt to stop an unresponsive script comes out, so I use too much memory and time to generate them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the chunk of code I use to draw them:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-js prettyprint-override&quot;&gt;&lt;code&gt;function generateCubes(data) {&#xA;    var cubeGeometry = new THREE.CubeGeometry(cubeSize, cubeSize, cubeSize);&#xA;    var material = new THREE.MeshLambertMaterial({color: 0x587058});&#xA;    var mesh = new THREE.Mesh(cubeGeometry, material);&#xA;    var mergedGeo = new THREE.Geometry();&#xA;    var instance;&#xA;    var line = data[0].split(';');&#xA;    var translateX = line[0], translateY = line[1], translateZ = line[2];&#xA;    //var group = new THREE.Object3d();&#xA;    for(var i = 0; i &amp;lt; 100000; i++) { // should go to data.length&#xA;        line = data[i].split(';');&#xA;        //instance = mesh.clone();&#xA;        //instance.position.set(line[0] - translateX, line[2] - translateZ, line[1] - translateY);&#xA;        //group.add(instance);&#xA;        mesh.position.x = Number(line[0]) - translateX;&#xA;        mesh.position.y = Math.round(Number(line[2]) - translateZ);&#xA;        mesh.position.z = Number(line[1]) - translateY;&#xA;        mesh.updateMatrix();&#xA;        mergedGeo.merge(instance.geometry, instance.matrix);&#xA;    }&#xA;    group = new THREE.Mesh(mergedGeo, material);&#xA;    scene.add(group);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The function is called from a success in an &lt;code&gt;$.ajax&lt;/code&gt; call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The commented parts are used without merged geometry, that way I can draw around 100k of the data.&lt;/p&gt;&#xA;" OwnerUserId="2343" LastEditorUserId="127" LastEditDate="2015-12-26T16:10:07.643" LastActivityDate="2015-12-26T16:10:07.643" Title="Drawing a cube a million times" Tags="&lt;geometry&gt;&lt;webgl&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="1836" PostTypeId="2" ParentId="1832" CreationDate="2015-12-26T02:28:48.480" Score="2" Body="&lt;p&gt;You could:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Draw them all with different widths and make each transparent (fiddling with alpha blend to make it look nice). Or make the depth proportional to the width, so the widest is always drawn at the bottom of the pyramid. The naive implementation would suffer with many routes.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Not do anything special at intersections, and expect the user to infer what is going on from context.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&quot;PWM&quot; the colors of the lines, one way to do this might be to make a chessboard-like grid texture of 0's and 1's, and depending on whether there is a zero or a one where the road falls, choose the color to draw. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;You could use the above technique for multiple line colors by further subdividing the grid with multiple colors. This would be tedious to do in a shader, but is trivial using a normal CPU image manipulation library.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here is a bus routing app (Transloc Rider) that does this pretty well. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LFHFj.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LFHFj.jpg&quot; alt=&quot;transloc rider&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1988" LastActivityDate="2015-12-26T02:28:48.480" CommentCount="0" />
  <row Id="1837" PostTypeId="2" ParentId="1832" CreationDate="2015-12-26T14:05:08.853" Score="5" Body="&lt;p&gt;This question should most probably be asked on &lt;a href=&quot;http://graphicdesign.stackexchange.com/&quot;&gt;GD.SE&lt;/a&gt; or &lt;a href=&quot;http://ux.stackexchange.com/&quot;&gt;UX.SE&lt;/a&gt;. These sites specialize in how to design the graphics and how to choose the graphics for your purpose. But since you are here basic options are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Routes dont overlap but are parallel like electronic layouts (image 1, A), Works well unless you have many overlapping things.&lt;/li&gt;&#xA;&lt;li&gt;Space interleaved lines, differently colored line regions (image 1, B). Such as dashed lines, however many other interleaved shapes are possible. &lt;/li&gt;&#xA;&lt;li&gt;Spatial solutions, lines on top of each other but laid over in 3d (image 1, C), Works well in isometric drawing.&lt;/li&gt;&#xA;&lt;li&gt;Different line widths (image 1, D)&lt;/li&gt;&#xA;&lt;li&gt;User suggested, overlay (image 1, E).&lt;/li&gt;&#xA;&lt;li&gt;Same color but labels on segments (image 1, F), note not all segments need to be labeled as humans can interpolate values somewhat. This has benefit of being able to be paired with any other effect and works well with selection highlights with info intact. &lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8ZfX0.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8ZfX0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Different overlay methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: None, except the labeling works well when there are very many different lines as humans capability to see different colors in this kind of context as differing data is not very good (ask &lt;a href=&quot;http://ux.stackexchange.com/&quot;&gt;UX.SE&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for what is best depends. None of these are particularity hard to do in code or vector drawing application (but that is out of scope ask &lt;a href=&quot;http://graphicdesign.stackexchange.com/&quot;&gt;GD.SE&lt;/a&gt;). Whichever is best for the user or the design is also not within the scope of this software (ask  &lt;a href=&quot;http://graphicdesign.stackexchange.com/&quot;&gt;GD.SE&lt;/a&gt; or &lt;a href=&quot;http://ux.stackexchange.com/&quot;&gt;UX.SE&lt;/a&gt;).&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-12-26T23:34:56.133" LastActivityDate="2015-12-26T23:34:56.133" CommentCount="0" />
  <row Id="1839" PostTypeId="1" AcceptedAnswerId="1840" CreationDate="2015-12-27T13:29:16.620" Score="2" ViewCount="128" Body="&lt;p&gt;I'm trying to rotate a rectangle so that it faces another rectangle in a spiral.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Im using the Bullet pyhics library and freeglut for the 3d application, for anyone interested &lt;a href=&quot;https://github.com/damorton/bullet-dominos&quot; rel=&quot;nofollow&quot;&gt;https://github.com/damorton/bullet-dominos&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/I300x.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/I300x.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float x = 0;&#xA;float z = 0;&#xA;float angle = 0.0f;&#xA;int a = 2, b = 2;&#xA;float previousX = 0.0f;&#xA;float previousZ = 0.0f;&#xA;&#xA;for (int i = 4; i &amp;lt; maxPoints; i++)&#xA;{&#xA;    angle = 0.1 * i;&#xA;    x = (a + b * angle) * cos(angle);&#xA;    z = (a + b * angle) * sin(angle);&#xA;&#xA;    GameObject* temp = CreateGameObject(x, 0, z);&#xA;&#xA;    float newAngle = atan2(previousZ - z, previousX - x) * 180 / PI;&#xA;    temp-&amp;gt;setRotationYaw(newAngle);&#xA;&#xA;    previousX = x;&#xA;    previousZ = z;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can see from the top view that the rectangles are not pointing towards the previous rectangles position. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sk2W2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sk2W2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: After trying the below code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float x = 0;&#xA;float z = 0;&#xA;float angle = 0.0f;&#xA;int a = 2, b = 2;&#xA;float previousX = 0.0f;&#xA;float previousZ = 0.0f;&#xA;&#xA;for (int i = 0; i &amp;lt; maxPoints; i++)&#xA;{&#xA;    angle = 0.1 * i;&#xA;    x = (a + b * angle) * cos(angle);&#xA;    z = (a + b * angle) * sin(angle);&#xA;&#xA;    GameObject* temp = CreateGameObject(x, 0, z);&#xA;&#xA;    float dirX = -(a + b * angle) * sin(angle) + b * cos(angle);&#xA;    float dirZ = (a + b * angle) * cos(angle) + b * sin(angle);&#xA;    float newAngle = atan2(dirZ, dirX) * 180 / PI;&#xA;    temp-&amp;gt;setRotationYaw(newAngle);&#xA;&#xA;    previousX = x;&#xA;    previousZ = z;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ypk6t.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ypk6t.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Console output:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LFkYQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LFkYQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Solution. Some things to note are that the result of &lt;code&gt;atan2()&lt;/code&gt; was not converted to degrees using &lt;code&gt;atan2() * 180 / PI&lt;/code&gt;. Also that I passed in &lt;code&gt;atan2(X, Z)&lt;/code&gt; compared to &lt;code&gt;atan2(Z, X)&lt;/code&gt; that is sometimes done for hysterical raisins &lt;a href=&quot;http://www2.tcl.tk/10814&quot; rel=&quot;nofollow&quot;&gt;http://www2.tcl.tk/10814&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// create spiral dominos&#xA;float x = 0;&#xA;float z = 0;&#xA;float angle = 0.0f;&#xA;int a = 2, b = 2;&#xA;float previousX = 0.0f;&#xA;float previousZ = 0.0f;&#xA;&#xA;float dominoHeight = 2.0f;&#xA;&#xA;for (int i = 2; i &amp;lt; maxPoints; i++)&#xA;{&#xA;    if (i % 30 == 0) dominoHeight++;&#xA;&#xA;    angle = 0.15 * i;&#xA;    x = (a + b * angle) * cos(angle);&#xA;    z = (a + b * angle) * sin(angle);&#xA;&#xA;    GameObject* temp = CreateGameObject(x, 0, z);&#xA;&#xA;    float dirX = -(a + b * angle) * sin(angle) + (b * cos(angle));&#xA;    float dirZ = (a + b * angle) * cos(angle) + (b * sin(angle));&#xA;    float newAngle = atan2(dirX, dirZ);&#xA;    printf(&quot;dirX: %f - dirZ: %f - angle: %f\n&quot;, dirX, dirZ, newAngle);&#xA;    temp-&amp;gt;setRotationYaw(newAngle);&#xA;&#xA;    previousX = x;&#xA;    previousZ = z;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/KmM1S.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KmM1S.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2248" LastEditorUserId="127" LastEditDate="2015-12-29T23:04:42.653" LastActivityDate="2015-12-29T23:04:42.653" Title="Angle between two points in Cartesian coordinate system C++" Tags="&lt;transformations&gt;&lt;c++&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1840" PostTypeId="2" ParentId="1839" CreationDate="2015-12-27T14:20:42.503" Score="5" Body="&lt;p&gt;The parametric equation for a spiral is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{eqnarray}&#xA;\begin{aligned}&#xA;    x &amp;amp;= &amp;amp;(a + b \theta) \times \cos(\theta)\\&#xA;    z &amp;amp;= &amp;amp;(a + b \theta) \times \sin(\theta)&#xA;\end{aligned}&#xA;\end{eqnarray}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The direction on the spiral point is simply the derivative of the system with respect to $\theta$. Thus the direction is: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{eqnarray}&#xA;\begin{aligned}&#xA; x_{dir} &amp;amp;=&amp;amp;-(a + b \theta) \times \sin(\theta) + b \times \cos(\theta)\\&#xA; z_{dir} &amp;amp;=&amp;amp;(a + b \theta) \times \cos(\theta) + b \times \sin(\theta)&#xA;\end{aligned}&#xA;\end{eqnarray}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally angle is is simply atan2 of the direction equation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/bcKzY.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bcKzY.png&quot; alt=&quot;Spiral&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: This is what I get when i plot squares at $(x, z)$ with direction $(x_{dir}, z_{dir})$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematica &lt;a href=&quot;http://pastebin.com/ukNFUiPc&quot;&gt;source&lt;/a&gt; for image&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2015-12-27T15:16:17.587" LastActivityDate="2015-12-27T15:16:17.587" CommentCount="8" />
  <row Id="1842" PostTypeId="1" CreationDate="2015-12-27T23:25:14.957" Score="4" ViewCount="80" Body="&lt;p&gt;I'm programming on a Mac and I'm learning OpenGL in the library GLFW. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is that my Mac supports OpenGL up to version 4.1, but when I try to compile the shaders with &lt;code&gt;version 410&lt;/code&gt; it says &lt;code&gt;ERROR: 0:1: '' :  version '410' is not supported&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I set the version that GLFW should use?&lt;/p&gt;&#xA;" OwnerUserId="2355" LastEditorUserId="127" LastEditDate="2015-12-29T23:16:44.810" LastActivityDate="2016-01-29T00:15:32.870" Title="How to set the GLFW OpenGL target?" Tags="&lt;opengl&gt;&lt;c++&gt;&lt;glsl&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1843" PostTypeId="2" ParentId="1842" CreationDate="2015-12-28T00:35:18.687" Score="1" Body="&lt;p&gt;I changed the core profile to &lt;code&gt;4.1&lt;/code&gt; by setting the window hints.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3);&#xA;glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 2);&#xA;glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE);&#xA;glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2355" LastActivityDate="2015-12-28T00:35:18.687" CommentCount="2" />
  <row Id="1844" PostTypeId="2" ParentId="1833" CreationDate="2015-12-28T00:38:29.163" Score="5" Body="&lt;p&gt;The effect in the photo is very close to a simple scale-bias per pixel. After a bit of tweaking, I found that applying the transformation: $x' = 0.77x + 38$ to the raw pixel values (as bytes) gives something quite close to your output:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ti9Pv.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ti9Pvm.png&quot; alt=&quot;transformed image&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case the effect of the scale-bias is to reduce the contrast (scale factor &amp;lt; 1) while keeping the overall brightness about the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example Python code (with numpy and pillow) to do this transformation:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;from PIL import Image&#xA;a = np.array(Image.open(&quot;input.png&quot;), dtype=np.float32)&#xA;b = a * 0.77 + 38&#xA;Image.fromarray(np.uint8(np.rint(b))).save(&quot;output.png&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For learning how to implement many basic image processing operations, I've found these &lt;a href=&quot;http://homepages.inf.ed.ac.uk/rbf/HIPR2/wksheets.htm&quot;&gt;Image Processing Operator Worksheets&lt;/a&gt; from the University of Edinburgh helpful.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-28T00:38:29.163" CommentCount="0" />
  <row Id="1845" PostTypeId="1" AcceptedAnswerId="1925" CreationDate="2015-12-28T03:59:31.130" Score="4" ViewCount="86" Body="&lt;p&gt;A friend got an MSI GTX 950 2GD5T graphics card for xmas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I said &quot;yeah, graphics cards are super fast now, that thing probably processes 2 billion triangles per second&quot;. Then I tried to look it up to check if that was even in the right ballpark, but I was unable to find a &quot;triangles per second&quot; stat for that card, and actually for many cards.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is &quot;triangles per second&quot; even a meaningful stat? If so, what is the approximate number for this card? (given otherwise average conditions)&lt;/p&gt;&#xA;" OwnerUserId="2358" LastEditorUserId="231" LastEditDate="2016-01-19T13:04:35.450" LastActivityDate="2016-01-23T03:16:54.280" Title="MSI GTX 950 2GD5T triangles per second" Tags="&lt;efficiency&gt;&lt;hardware&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="1846" PostTypeId="1" AcceptedAnswerId="1847" CreationDate="2015-12-28T15:43:36.270" Score="2" ViewCount="282" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/KbrW7.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KbrW7.png&quot; alt=&quot;ray tracing pseudocode&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I dont understand the if(depth&gt;MAX)return Black part. Is it something to do with shadows because in other algorithms they shoot a shadow ray towards the light source to check for shadows but they dont have it here&lt;/p&gt;&#xA;" OwnerUserId="2359" LastActivityDate="2015-12-28T16:36:12.337" Title="Ray tracing pseudocode shadow" Tags="&lt;raytracing&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1847" PostTypeId="2" ParentId="1846" CreationDate="2015-12-28T16:30:32.037" Score="8" Body="&lt;p&gt;This is a recursive function (it calls itself). Each time it calls itself it does so with &lt;code&gt;depth + 1&lt;/code&gt; instead of &lt;code&gt;depth&lt;/code&gt;. When it is called with &lt;code&gt;depth &amp;gt; MAX&lt;/code&gt;, it simply returns &lt;code&gt;Black&lt;/code&gt; rather than call itself again. This ensures that the function will eventually stop recursing and return a value. It is the colour equivalent of returning zero.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can visualise this as a light ray reflecting from multiple surfaces successively. Each time the ray reflects from a surface the same function is used to calculate the colour of the light in that direction. If you set &lt;code&gt;MAX&lt;/code&gt; to 0, there will be no reflections in your resulting image. If you set &lt;code&gt;MAX&lt;/code&gt; to 2, there will be reflections of reflections which will look more realistic, but take longer to calculate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively, MAX is the number of surfaces the ray will bounce off before it is decided what colour should be displayed.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2015-12-28T16:36:12.337" LastActivityDate="2015-12-28T16:36:12.337" CommentCount="0" />
  <row Id="1848" PostTypeId="1" AcceptedAnswerId="1849" CreationDate="2015-12-28T20:04:35.557" Score="6" ViewCount="97" Body="&lt;p&gt;I am not looking for textbooks and related how-tos, though I'm sure they can color the answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to understand how we handle the rendering of caustic effects — and particularly time-saving models, as different from physical simulation.&lt;/p&gt;&#xA;" OwnerUserId="2360" LastEditorUserId="2360" LastEditDate="2015-12-29T20:08:54.957" LastActivityDate="2015-12-29T20:08:54.957" Title="What is the basis of rendering light caustics?" Tags="&lt;lighting&gt;&lt;photo-realistic&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="1849" PostTypeId="2" ParentId="1848" CreationDate="2015-12-28T22:57:25.847" Score="5" Body="&lt;p&gt;For offline rendering, caustics and dispersion effects (different colors of light separating, as in a prism) are usually calculated using some form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Photon_mapping&quot;&gt;photon mapping&lt;/a&gt;. That's a fairly expensive technique, even by the standards of offline rendering, and it falls more under the &quot;physical simulation&quot; side of things.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you're looking for more approximate techniques suitable for real-time rendering, &lt;a href=&quot;http://cwyman.appspot.com/papers.html&quot;&gt;Chris Wyman's papers&lt;/a&gt; are a good place to look. He wrote several papers on caustics between 2006 and 2009. The one to start with is &lt;a href=&quot;http://cwyman.appspot.com/papers/i3d06_imgSpaceCaustics.pdf&quot;&gt;Interactive Image-Space Techniques for Approximating Caustics&lt;/a&gt; (I3D 2006), #25 on that page. This paper re-uses a GPU refraction rendering technique he developed earlier, #28 on that page, so you'll want to look at that as well for background. Later papers build on these two.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic idea of these papers is to use GPU rasterization to perform a limited, approximate analogue of photon mapping. The scene is rendered from the light's point of view, similar to a shadow map, and each pixel is treated as a photon; its refraction through transparent objects is calculated approximately (usually considering only 1 or 2 layers of refraction), and its final location is stored in a texture. Then, one can render each photon as a small splat into a &quot;caustic map&quot;, which finally gets projected onto the scene, similar to a shadow map.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-28T22:57:25.847" CommentCount="3" />
  <row Id="1850" PostTypeId="1" AcceptedAnswerId="1852" CreationDate="2015-12-29T03:12:26.573" Score="6" ViewCount="63" Body="&lt;p&gt;I have a game that has simple particles (basically dots) moving around the screen leaving a trail.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My ultimate goal&lt;/strong&gt; is to be able to change the opacity of the solid black fading texture each drawing call in a way so that I can keep the length of a particle with trails (using a accumulation buffer) is the same even though the particle movement might move with time-stepping.  The way I render my particles is I render the current frame based on the particles position, lets call this texture A.  Then I take the previous frame and draw a solid black texture over it with set opacity in order to fade this, lets call this texture B.  Then I draw texture A over texture B.  This way the particle leaves a trail based on where it has been.  I am currently time-stepping the movement of the particles so that no-mater how much latency their is it always moves the same speed (considering real-world time).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In essence I want to be able to say say that the particle trial will always be 100 pixels (considering the particle moves 60 pixels in a second) , and have it be that way even if the frame-rate drops.  Currently when the frame rate drops the accumulation buffer gets run less times and thus the trail is longer the more latency their is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The information I need to know to do this&lt;/strong&gt; is how exactly does fading work. Because I need to change the opacity of the black texture in a way that allows it to fade more when the accumulation buffer is called less times per second, and fade less when it is called many times per second.  Right now I really dont understand what happens to a pixel when a black texture with a specific opacity is on it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is some data I collected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Data 1:&lt;/em&gt;&#xA;This is a chart of the particle trails.  The x axis represents the FPS it was running at, and the y axis is how many pixels long the trail was.  On the left it says what opacity the black texture used in the accumulation buffer was.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/iGLTQ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iGLTQ.png&quot; alt=&quot;Graph&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://www.desmos.com/calculator/lbrpdkrpjo&quot;&gt;You can find all the data, and some best fit line formulas at here (on desmos)&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is important to note I stopped counting after V was lower then 20 since the difference between colors starts getting really small.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;some things I noticed about this is that their seems to be either a exponential or division relationship between the numbers.  Also I noticed that their were segments for each frame before it was faded, and that the amount of them that were visible were about the same no-mater the fps when considering the same opacity of the black texture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Data 2:&lt;/em&gt;&#xA;I recreated what I thought was going on with a fading texture with paint.net.  I made a 100x100 picture and colored it red. I then added a layer with one black pixel at an opacity of 25/255 then I duplicated this and added another pixel, basically I created a gradient.  Then I went through and labeled each pixel considering its rounded darkness (in hsv color space).  Then labeled the difference between the darkness values.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/y4YqY.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/y4YqY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/HZ89o.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HZ89o.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;Unfortunately from this I didnt learn much.  Just reaffirmed that it is a non linear relationship. And I also noticed if I did the fade one more time that all the darkness values shifted to the square next to them.  So I could predict the darkness values if I knew how many times the fade had been applied.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;So&lt;/strong&gt; what happens when a semitransparent black texture is blended over another one?&lt;/li&gt;&#xA;&lt;li&gt;If you are willing to put in some extra thought what would a formula&#xA;be that would give me the opacity the black texture needs to be in&#xA;order to make the particle trail be about 'N' pixels long, given the&#xA;dt since last frame?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="2308" LastEditDate="2015-12-29T06:39:30.607" LastActivityDate="2015-12-29T06:39:30.607" Title="Please help me understand what happens as an image is faded to black in order to time-step particle fading" Tags="&lt;opengl&gt;&lt;transparency&gt;&lt;particles&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1852" PostTypeId="2" ParentId="1850" CreationDate="2015-12-29T06:35:14.433" Score="7" Body="&lt;p&gt;The basic equation for alpha blending is as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ c_\text{final} = c_\text{source} \cdot \alpha + c_\text{dest} \cdot (1 - \alpha) $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here, $c_\text{source}$ is the color of the thing being blended, $c_\text{dest}$ is the background onto which you're blending it, and $\alpha$ is between 0 and 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your case, $c_\text{source} = 0$ (black fading texture), so blending black $n$ times over a background reduces to&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ c_\text{final} = c_\text{dest} \cdot (1 - \alpha)^n $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;which is indeed an exponential falloff as you guessed. You can plug this formula in and get numbers pretty similar to those you posted (although when I did it, my results were off by one or two—maybe due to rounding differences; I'm not sure).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Regarding your problem of making the trails appear the same regardless of framerate, a simple approach is to run the accumulation buffer passes multiple times per frame if necessary, so that there's effectively a fixed timestep for the trail regardless of framerate. That will probably give better-looking results than attempting to fiddle with the alpha based on the framerate. It might be too slow, though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A potentially more efficient approach would be to have the particles spawn a new &quot;trail particle&quot; every so often, where each trail particle stays fixed in position, with its alpha going from 100% to 0 over some length of time (and then it gets recycled). You have to re-render all the particles every frame, but it could end up being faster overall if the overhead of the full-screen blending passes for the accumulation buffer get to be too high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I should point out though that all of these approaches actually perform &lt;em&gt;time-based&lt;/em&gt; fading, not distance-based as you asked for (100-pixel trails). If all the particles move at the same speed, you can just tweak the fade time to make the trails the desired length, but if different particles move at different speeds, then you would need to account for that when setting the fade times for their trails. There's also no trivial way to do that with the accumulation buffer approach, since it implicitly uses the same fade time for everything on the screen.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2015-12-29T06:35:14.433" CommentCount="1" />
  <row Id="1854" PostTypeId="1" CreationDate="2015-12-29T20:24:41.790" Score="9" ViewCount="93" Body="&lt;p&gt;Is all &lt;a href=&quot;https://en.wikipedia.org/wiki/Spectral_rendering&quot;&gt;spectral rendering&lt;/a&gt; handled as simulation?  Are there technique more tailored to 'consumer' rendering, such as for real-time or even just 'realistic looking without solving full physical equations'?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to understand how we handle the rendering of spectral effects.  It seems like a photon needs to be described as a range of wavelengths, and incidence with a surface either &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;replaces the original, and resolves multiple new photons across the spectral function, each with their own new vector &lt;/li&gt;&#xA;&lt;li&gt;maintains the original (or marginally modified) photon, given a threshold&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I would prefer to be pointed in the direction of existing work, but appreciate any &lt;em&gt;coloring&lt;/em&gt; of this topic.&lt;/p&gt;&#xA;" OwnerUserId="2360" LastActivityDate="2016-01-06T20:48:37.160" Title="How is spectral rendering handled?" Tags="&lt;lighting&gt;&lt;photo-realistic&gt;&lt;color-science&gt;" AnswerCount="2" CommentCount="2" FavoriteCount="1" />
  <row Id="1857" PostTypeId="1" AcceptedAnswerId="1863" CreationDate="2016-01-01T00:05:16.793" Score="5" ViewCount="113" Body="&lt;p&gt;I'm trying to find the angle it would take for me to rotate a polygon so that a specific side is completely horizontal and on the bottom.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, a shape like this:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/duCe7.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/duCe7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Needs to be rotated so the side with the red square on it is on the bottom and completely horizontal, like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/210Ot.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/210Ot.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I've tried several approaches but all end up having strange edge cases where the angle is incorrect.&lt;/p&gt;&#xA;" OwnerUserId="2381" LastEditorUserId="231" LastEditDate="2016-01-19T13:04:21.223" LastActivityDate="2016-01-19T13:04:21.223" Title="Finding the angle of any side of a polygon" Tags="&lt;transformations&gt;&lt;vector-graphics&gt;&lt;polygon&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="1859" PostTypeId="2" ParentId="1857" CreationDate="2016-01-01T08:31:35.117" Score="3" Body="&lt;p&gt;This algorithm is based on &lt;a href=&quot;http://stackoverflow.com/questions/2827393/angles-between-two-n-dimensional-vectors-in-python&quot;&gt;this answer for finding the angle between vectors&lt;/a&gt;, and &lt;a href=&quot;http://stackoverflow.com/questions/20023209/python-function-for-rotating-2d-objects&quot;&gt;this answer for rotating polygon points&lt;/a&gt;. It's written in Python, and assumes you want to align an edge with the X axis (horizontal axis). If you want to align it with the Y axis, replace the &lt;code&gt;xVec&lt;/code&gt; in the code below with a &lt;code&gt;yVec = [0,1]&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITED&lt;/strong&gt;: Added code for rotating the polygon and displaying the original and rotated polygons. Result of this script is displayed in the image below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/oeSfK.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oeSfK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from PIL import Image, ImageDraw&#xA;from matplotlib import pyplot as plt&#xA;from math import radians, degrees, sqrt, acos, cos, sin&#xA;&#xA;def dotproduct(v1, v2):&#xA;  return sum( (a*b) for a, b in zip(v1, v2) )&#xA;&#xA;def length(v):&#xA;  return sqrt( dotproduct(v, v) )&#xA;&#xA;def angle(v1, v2):&#xA;  return acos( dotproduct(v1, v2) / ( length(v1) * length(v2) ) )&#xA;&#xA;def vec_subtraction( v1, v2 ):&#xA;    return [ e1 - e2 for e1, e2 in zip( v1, v2 ) ]&#xA;&#xA;def calc_edge_vec( poly, edge ):&#xA;    p1 = poly['points'][ edge[0] ]&#xA;    p2 = poly['points'][ edge[1] ]&#xA;&#xA;    return vec_subtraction( p2, p1 )&#xA;&#xA;def rotatePolygon( poly, theta ):&#xA;    ''' Rotates the given polygon which consists of corners represented&#xA;        as (x,y), around the ORIGIN, clock-wise, theta degrees&#xA;    '''&#xA;&#xA;    rotatedPolygon = []&#xA;    for corner in poly:&#xA;        rotatedPolygon.append((&#xA;            corner[0] * cos( theta ) - corner[1] * sin( theta ), # x&#xA;            corner[0] * sin( theta ) + corner[1] * cos( theta )  # y&#xA;        ))&#xA;&#xA;    # Make sure rotated polygon coordinates are within image boundaries&#xA;    minX = min( [ c[0] for c in rotatedPolygon ] ) * -1&#xA;    minX = minX if minX &amp;gt; 0 else 0&#xA;    minY = min( [ c[1] for c in rotatedPolygon ] ) * -1&#xA;    minY = minY if minY &amp;gt; 0 else 0&#xA;&#xA;    rotatedPolygon = [ ( x + minX, y + minY ) for x, y in rotatedPolygon ]&#xA;&#xA;    return rotatedPolygon&#xA;&#xA;def draw_and_plot_polygon( im, poly, color, plotTitle, plotIndex ):&#xA;    draw = ImageDraw.Draw( im )&#xA;&#xA;    for e in poly['edges']:&#xA;        p1 = tuple( poly['points'][ e[0] ] )&#xA;        p2 = tuple( poly['points'][ e[1] ] )&#xA;        draw.line( [p1, p2], fill = color )&#xA;&#xA;    del draw&#xA;&#xA;    plt.subplot( 3, 2, plotIndex ),&#xA;    plt.imshow( im ),&#xA;    plt.title( plotTitle ),&#xA;    plt.xticks([]), plt.yticks([])&#xA;&#xA;    # Draw point names (ABCD)&#xA;    for i, c in enumerate( poly['points'] ):&#xA;        plt.text( c[0], c[1], &quot;ABCD&quot;[i], color = 'white' )&#xA;&#xA;# Define polygon as dictionary of point coordinates and edges&#xA;# each edge is a pair of point indices&#xA;p = {&#xA;    'points' : [ (50,10), (95,30), (80,20), (62,48) ],&#xA;    'edges'  : [ (3,0), (3,1), (1,2), (2,0) ]&#xA;}&#xA;&#xA;xVec = [1,0] # Horizontal axis&#xA;&#xA;# Draw original poly in white&#xA;im = Image.new( 'RGB', (100,100) )&#xA;draw_and_plot_polygon( im, p, (255,255,255), 'Original', 1 )&#xA;&#xA;colors = [ (50,50,255), (255,0,0), (0,255,0), (200, 150, 50) ]&#xA;&#xA;i = 2&#xA;for e, color in zip( p['edges'], colors ):&#xA;    edgeVec = calc_edge_vec( p, e )&#xA;    a       = angle( edgeVec, xVec )&#xA;&#xA;    rotPol = p.copy()&#xA;    rotPol['points'] = rotatePolygon( p['points'], a )&#xA;&#xA;    im = Image.new( 'RGB', (100,100) )&#xA;&#xA;    # Place aligned edge at bottom&#xA;    alingedEdgeY = im.size[1] - rotPol['points'][ e[0] ][ 1 ] # Current edge height&#xA;    rotPol['points'] = [ ( p[0], p[1] + alingedEdgeY ) for p in rotPol['points'] ]&#xA;&#xA;    # Draw rotated polygon&#xA;    angleTitle = &quot;Angle: &quot; + str( round( degrees(a), 2 ) )&#xA;    draw_and_plot_polygon( im, rotPol, color, angleTitle, i )&#xA;&#xA;    i += 1&#xA;&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDITED 2&lt;/strong&gt;: Added some code to position the rotated polygon so that the aligned edge is on the floor (lines 90-91). Here's the result:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/SRPg3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SRPg3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2317" LastEditorUserId="2317" LastEditDate="2016-01-03T05:34:40.750" LastActivityDate="2016-01-03T05:34:40.750" CommentCount="4" />
  <row Id="1860" PostTypeId="1" CreationDate="2016-01-02T08:46:03.247" Score="4" ViewCount="61" Body="&lt;p&gt;I am currently trying to implement &lt;a href=&quot;http://www.crytek.com/download/Light_Propagation_Volumes.pdf&quot; rel=&quot;nofollow&quot;&gt;light propagation volumes&lt;/a&gt; in DirectX 11 and I have already done the RSM part which contains position, normal, depth and flux map generation. But for Injecting Virtual point lights into light propagation volumes, we have to inject lights into an x,y,z grid for which each element consists of 3 vectors for each of red, green and blue channels and each vector consists of four coefficients of a two-band spherical harmonic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To do this, here are my steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare 3 texture arrays. Each slice is a slice of the grid.&lt;/li&gt;&#xA;&lt;li&gt;Inject RSM into the texture array by calculating grid space position.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Can someone briefly explain how this injection can be done and how each virtual point light can be used inside each grid unit?&lt;/p&gt;&#xA;" OwnerUserId="1571" LastEditorUserId="231" LastEditDate="2016-01-02T23:38:25.120" LastActivityDate="2016-01-02T23:38:25.120" Title="Injecting Virtual point lights into 3D Grid" Tags="&lt;lighting&gt;&lt;directx11&gt;&lt;hlsl&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="1861" PostTypeId="1" CreationDate="2016-01-02T13:46:45.817" Score="2" ViewCount="70" Body="&lt;p&gt;I am wondering how professional light visualizer software (like Capture Argo - WYSIWYG - Realizzer - LightConverse) project the lights on the objects (like stage, floor, people), even projecting the various &lt;a href=&quot;https://en.wikipedia.org/wiki/Gobo_%28lighting%29&quot; rel=&quot;nofollow&quot; title=&quot;a physical stencil or template slotted inside, or placed in front of, a lighting source, used to control the shape of emitted light.&quot;&gt;gobos&lt;/a&gt; (shapes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some screenshots:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.diversitronics.de/tl_files/Bilder/Produkte/Capture/renderings/capture1.jpg&quot; rel=&quot;nofollow&quot;&gt;http://www.diversitronics.de/tl_files/Bilder/Produkte/Capture/renderings/capture1.jpg&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.selightingdesign.co.uk/images/portfolio/capture%20hairspray/production%20photos/fullscreen/01.jpg&quot; rel=&quot;nofollow&quot;&gt;http://www.selightingdesign.co.uk/images/portfolio/capture%20hairspray/production%20photos/fullscreen/01.jpg&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do they calculate the various intersections, and project beams for so many light sources in an efficient way? What's the simplest way?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume they are projecting a texture but how?&lt;/p&gt;&#xA;" OwnerUserId="2388" LastEditorUserId="231" LastEditDate="2016-01-02T23:24:23.343" LastActivityDate="2016-01-02T23:24:23.343" Title="Projecting light shapes" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;texture&gt;&lt;performance&gt;&lt;real-time&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="1862" PostTypeId="1" CreationDate="2016-01-02T22:38:07.437" Score="6" ViewCount="294" Body="&lt;p&gt;Without there being black bars I mean. 1080 isn't any multiple of 768 so is there some sort of data loss?&lt;/p&gt;&#xA;" OwnerUserId="2393" LastActivityDate="2016-05-06T01:23:24.967" Title="How does a computer upscale 1024x768 resolution to 1920x1080?" Tags="&lt;pixel-graphics&gt;" AnswerCount="3" CommentCount="4" FavoriteCount="2" />
  <row Id="1863" PostTypeId="2" ParentId="1857" CreationDate="2016-01-03T18:02:07.997" Score="4" Body="&lt;p&gt;Deducing the angle and rotating by that angle works quite well in 2D (describe in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1859/38&quot;&gt;TLousky's post&lt;/a&gt;). This strategy, does not extend very well into three-dimensional realm. I will provide an alternative solution that shows a general strategy that works in a larger set of cases. As a bonus this without needing to think of trigonometry as it can be encoded away.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;First identify the vector that you want to be transformed to orientation in your reference space. Let us call this vector $\vec{a}$, or a along vector.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Next you will need to identify a vector that is perpendicular to this vector. Let us call this vector $\vec{b}$, it is sometimes also called a up vector. In three-dimensions you need 3 vectors but once you identify 2 you can compute the third.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finding out the $\vec{b}$ in 2-D realm is especially easy you could just rotate the vector by $90^{\circ}$ or simply swap the coordinates and make the x coordinate negative (see 1). Alternatively you can use the same procedure as in 3D.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\vec{b} = rotate(90^{\circ})\cdot\vec{a} =&#xA;      \begin{bmatrix} &#xA;         \cos(\pi/2) &amp;amp; -\sin(\pi/2) \\ &#xA;         \sin(\pi/2) &amp;amp; \cos(\pi/2) &#xA;      \end{bmatrix}&#xA;      \begin{bmatrix} &#xA;         x \\ &#xA;         y &#xA;      \end{bmatrix}&#xA;         =       &#xA;      \begin{bmatrix} &#xA;         -y \\ &#xA;         x &#xA;      \end{bmatrix} &#xA;     \tag{1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In 3-D this usually requires a trick. Since each triangle is flat, one can use this to ones advantage. This gives us a perpendicular direction, now we still need a third one and we can get it by cross product of the 2 known vectors. You now have 3 vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BcxhU.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BcxhU.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Vector $\vec{a}$ and derived $\vec{b}$&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Normalize the vectors&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Use these vectors to produce a a rotation matrix (or a affine matrix wich allows you to specify point to be at zero, your choice). This matrix represents the local coordinate of the rotated object. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;All you now need is to invert this matrix and point matrix multiply your polygon corner vectors for a aligned version of the polygon.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;What is especially nifty is the sidestepping of trigonometrical operations. Now i see no point i defining basic matrix operations but i see value in code. So i will use numpy.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;import numpy as np, pylab&#xA;&#xA;pts = np.array([&#xA;        [0.,0.],[10.,1.],[12.,12.],[-2.,11.],[0.,0.]&#xA;      ]).transpose()&#xA;&#xA;f = pylab.figure(facecolor=&quot;1.&quot;)     &#xA;pylab.axis('off')&#xA;pylab.axis('equal')&#xA;pylab.plot(pts[0], pts[1])  &#xA;&#xA;# 1. Get the vector&#xA;vec_a = np.copy(pts[:,2]) - np.copy(pts[:,1])&#xA;&#xA;# 2. up vector&#xA;vec_b = np.copy(vec_a[::-1])&#xA;vec_b[0] *= -1&#xA;&#xA;# 3. normalize&#xA;vec_a /= np.linalg.norm(vec_a)&#xA;vec_b /= np.linalg.norm(vec_b)&#xA;&#xA;# 4. build rotation matrix&#xA;mtx_R = np.concatenate(([vec_a], [vec_b])).transpose()&#xA;mtx_R = np.linalg.inv(mtx_R)&#xA;&#xA;# 5. transform, or set hierarchy transform&#xA;pts2 = np.copy(pts)&#xA;for i in range(len(pts2[0])):&#xA;    pts2[:,i] =  np.dot(mtx_R, pts2[:,i])&#xA;&#xA;pylab.plot(pts2[0],pts2[1])&#xA;pylab.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I admit it cold be have slightly shorter by refactoring the code and eliminate redundant manipulations but its a demo only so... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/xkgLB.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xkgLB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Polygon, in blue axis fixed in green.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Same method works well for straightening, skew and other problems such as perspective correction. You can extend this to affine matrices that way you can encode the translation into the calculation with just 1 more lines. In essence you specify what point is at origin and with 2 more operations one could define around what to rotate. Now it just just rotates about the origin.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-01-04T12:03:57.130" LastActivityDate="2016-01-04T12:03:57.130" CommentCount="0" />
  <row Id="1865" PostTypeId="2" ParentId="1854" CreationDate="2016-01-05T01:14:34.330" Score="1" Body="&lt;p&gt;One hacky method I've seen in real time raytracers / ray marching is to cast a ray per color channel (rgb) and do things Iike have different refraction indices per color channel.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-01-05T01:14:34.330" CommentCount="3" />
  <row Id="1866" PostTypeId="1" AcceptedAnswerId="1867" CreationDate="2016-01-05T15:53:46.143" Score="8" ViewCount="239" Body="&lt;p&gt;I want to find the texture coordinates for point P. I have the vertices of the triangle and their corresponding uv coordinates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The numbers in the little squares in the texture represent color values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the steps of computing the uv coordinates of P?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/cVDiT.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cVDiT.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2214" LastActivityDate="2016-01-06T13:48:06.523" Title="How to map square texture to triangle?" Tags="&lt;uv-mapping&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1867" PostTypeId="2" ParentId="1866" CreationDate="2016-01-05T17:45:59.417" Score="9" Body="&lt;p&gt;This is achieved via &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Barycentric_coordinate_system&quot; rel=&quot;nofollow&quot;&gt;Barycentric&lt;/a&gt; Interpolation&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, we find the barycentric coordinates of $P$. Barycentric coordinates represent how much weight each vertex contributes to the point, and can be used to interpolate any value which is known at the vertices across the face of a triangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the 3 inner triangles $ABP$, $PBC$ and $PCA$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Ob83U.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ob83U.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can say that the barycentric coordinate, or weight of the vertex $A$ on the point $P$ is proportional to the ratio of the area of inner triangle $PBC$ to the area of the whole triangle $ABC$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is intuitively evident if we consider that as $P$ approaches $A$ the triangle $PBC$ grows larger and the other two become smaller. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also intuitively evident should be that the sum of the barycentric coordinates of a point &lt;strong&gt;inside a triangle&lt;/strong&gt; always equals $1$. So, it is enough to find only two of the coordinates to derive the 3rd.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The method for computing the barycentric coordinates is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{aligned}&#xA;{Bary}_A &amp;amp;= \frac{(B_y-C_y)(P_x-C_x) + (C_x-B_x)(P_y-C_y)}{(B_y-C_y)(A_x-C_x) + (C_x-B_x)(A_y-C_y)}\\&#xA;{Bary}_B &amp;amp;= \frac{(C_y-A_y)(P_x-C_x) + (A_x-C_x)(P_y-C_y)}{(B_y-C_y)(A_x-C_x) + (C_x-B_x)(A_y-C_y)}\\&#xA;{Bary}_C &amp;amp;= 1 - {Bary}_A - {Bary}_B&#xA;\end{aligned}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The derivation and reasoning is explained in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Barycentric_coordinate_system&quot; rel=&quot;nofollow&quot;&gt;wikipedia article&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have coordinates, you can determine the texture coordinates of $P$ by interpolating the values at the vertices using the barycentric coordinates as weights:&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;$$P_{uv} = {Bary}_A \cdot A_{uv} + {Bary}_B \cdot B_{uv} + {Bary}_C \cdot C_{uv}$$&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The reasoning is also explained very nicely in &lt;a href=&quot;https://classes.soe.ucsc.edu/cmps160/Fall10/resources/barycentricInterpolation.pdf&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; presentation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also see &lt;a href=&quot;http://gamedev.stackexchange.com/questions/23743/whats-the-most-efficient-way-to-find-barycentric-coordinates&quot;&gt;this question&lt;/a&gt; for efficient methods of computation.&lt;/p&gt;&#xA;" OwnerUserId="457" LastEditorUserId="457" LastEditDate="2016-01-06T13:48:06.523" LastActivityDate="2016-01-06T13:48:06.523" CommentCount="5" />
  <row Id="1869" PostTypeId="2" ParentId="1862" CreationDate="2016-01-06T07:59:53.697" Score="5" Body="&lt;p&gt;There are numerous upscaling and downscaling algorithms available to scale images from any resolution to any other arbitrary resolution. Each algorithm typically involves a trade-off between efficiency, smoothness and sharpness, with varying degrees of each trade-off for different algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Check out &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_scaling#Scaling_methods&quot; rel=&quot;nofollow&quot;&gt;this Wikipedia article&lt;/a&gt; for such algorithms and examples of such algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The most popularly known (and used) algorithm is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bicubic_interpolation&quot; rel=&quot;nofollow&quot;&gt;Bicubic interpolation&lt;/a&gt; algorithm. It interpolates between 2D points on a rectangular grid. Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Cubic_Hermite_spline&quot; rel=&quot;nofollow&quot;&gt;Cubic Splines&lt;/a&gt; (or Cubic Interpolation), It first interpolates on one dimension (finds the interpolated row/column), then interpolates the interpolated row/column in the other dimension.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bilinear interpolation is similar to Bicubic interpolation, except the former interpolates using a linear function and can interpolate only between two values and the latter uses a cubic function and can interpolate between four values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The simple function for Bicubic interpolation is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f(f(p00, p01, p02, p03, y),&#xA;  f(p10, p11, p12, p13, y),&#xA;  f(p20, p21, p22, p23, y),&#xA;  f(p30, p31, p32, p33, y),&#xA;  x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where (x, y) is the interpolated position and p[][] is the 2d array representing the 4 * 4 grid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Check &lt;a href=&quot;http://www.paulinternet.nl/?page=bicubic&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt; for more information and example code, which really helps a lot!&lt;/p&gt;&#xA;" OwnerUserId="1683" LastEditorUserId="174" LastEditDate="2016-05-03T16:13:54.833" LastActivityDate="2016-05-03T16:13:54.833" CommentCount="0" />
  <row Id="1870" PostTypeId="2" ParentId="1862" CreationDate="2016-01-06T18:56:31.283" Score="7" Body="&lt;p&gt;In essence a image is  a group of point samples (read &lt;a href=&quot;http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf&quot; rel=&quot;nofollow&quot;&gt;A pixel is not a little square&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;). When you transform or scale the image you need to resample it. So what you do, theoretically, is take the point samples and convert them into a continuous function. Then you sample that continuous function and reconstruct a signal. So, there are two or three different phases here.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Converting the samples to a continuous function, (function reconstruction).&lt;/li&gt;&#xA;&lt;li&gt;(Transforming)&lt;/li&gt;&#xA;&lt;li&gt;Re-sampling the signal&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that none of these steps have a fixed form. In practice, when optimized, it's impossible to tell that there are steps. The transformation does not really have to be simple it could be mapping the shape into a spiral etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/WWaeC.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/WWaeC.png&quot; alt=&quot;Reconstruction of signal&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: A 1-D signal reconstructed by different filters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice, quite a bit of lore is known about signal reconstruction in the signal processing field. Designing these filters and choosing the right one is a art form on its own. But, in essence, the choice of filter is a tradeoff between blurring and &lt;a href=&quot;https://en.wikipedia.org/wiki/Ringing_artifacts&quot; rel=&quot;nofollow&quot;&gt;ringing&lt;/a&gt;. Of course the algorithm also has other qualities such as how many instructions it takes to implement and how fast and how much memory it needs etc. Which can be very important in realtime or embedded applications.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/XNqUx.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/XNqUx.png&quot; alt=&quot;The process&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Overview of entire process.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="310" LastEditDate="2016-05-05T14:56:32.470" LastActivityDate="2016-05-05T14:56:32.470" CommentCount="0" />
  <row Id="1871" PostTypeId="2" ParentId="1854" CreationDate="2016-01-06T20:48:37.160" Score="1" Body="&lt;p&gt;The most common way I saw is to have photons of several different wavelengths. One then renders with each wavelength and blends the results into the final image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Existing work&quot;: &lt;a href=&quot;http://psychopath.io/spectral-rendering/&quot; rel=&quot;nofollow&quot;&gt;Psychopath Renderer&lt;/a&gt; and &lt;a href=&quot;http://benedikt-bitterli.me/tantalum/&quot; rel=&quot;nofollow&quot;&gt;The Secret Life of Photons&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2016-01-06T20:48:37.160" CommentCount="0" />
  <row Id="1872" PostTypeId="2" ParentId="1824" CreationDate="2016-01-06T21:44:45.993" Score="3" Body="&lt;p&gt;And just to add: it is called &quot;conflation&quot; artifact and this is what AntiGrain Geometry used the &lt;a href=&quot;http://agg.sourceforge.net/antigrain.com/news/release_notes/v24.agdoc.html#toc0007&quot; rel=&quot;nofollow&quot;&gt;compound shapes&lt;/a&gt; rasterizer for, see:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://agg.sourceforge.net/antigrain.com/demo/flash_rasterizer.png&quot; alt=&quot;flash_rasterizer.png&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, this is what NV Path Rendering claims to improve on:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://developer.download.nvidia.com/assets/gamedev/files/An_Introduction_to_NV_path_rendering.pdf&quot; rel=&quot;nofollow&quot;&gt;An Introduction to&#xA;NV_path_rendering&lt;/a&gt; (p. 67) or &lt;a href=&quot;http://developer.download.nvidia.com/assets/gamedev/files/NV_path_rendering_FAQ.pdf&quot; rel=&quot;nofollow&quot;&gt;NV_path_rendering FAQ&lt;/a&gt; (#29).&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2016-01-06T21:44:45.993" CommentCount="0" />
  <row Id="1873" PostTypeId="1" AcceptedAnswerId="1875" CreationDate="2016-01-07T07:42:51.627" Score="8" ViewCount="141" Body="&lt;p&gt;I am taking a Computer Graphics course this quarter.&#xA;One of our lab project is about software rasterization.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I am planing the project proposal and&#xA;thinking about how to make it useful for other people in contemporary game development.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After some brief search,&#xA;I learned a technique called Software Occlusion Culling.&#xA;It does software rasterization on buffers of various resolutions.&#xA;And we can query for occlusion using the hierarchical z buffers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My question&lt;/strong&gt;: What are the usages of software rasterization&#xA;in &lt;em&gt;modern&lt;/em&gt; game engines besides the Software Occlusion Culling?&lt;/p&gt;&#xA;" OwnerUserId="120" LastActivityDate="2016-01-10T01:38:35.283" Title="What are the usages of software rasterization in modern game engines?" Tags="&lt;rendering&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="1874" PostTypeId="2" ParentId="1873" CreationDate="2016-01-07T12:26:55.137" Score="-2" Body="&lt;p&gt;Rasterization is a huge topic with many parts and I'm no engine programmer, but I'll do my best to give some sort of overview (this will be far from a complete list!). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some very basic, lowlevel stuff include:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Frustrum culling:&lt;/strong&gt; throws away everything outside the camera's frustrum (think of it as the viewing area of the camera).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Backface culling:&lt;/strong&gt; throw away every polygon not directing to the camera.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Painter's algorithm:&lt;/strong&gt; draw objects in order of their distance to the camera, start with objects far away, then go toward to the camera.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;More advaned stuff used in games include things like:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tesselation&lt;/strong&gt;: actually pretty much like LOD (see below), but now the latest DirectX Versions introduced tesselation, which provides automatic mesh division or reduction. This gives huge boost in graphics performance, because you can get polys if you need them and get rid of them if you don't want them any more. &lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;LOD&lt;/strong&gt;: A level-of-detail system replaces the mesh of an object based on the distance between object and camera. for example, your character has three meshes: one with 10,000, one whith 5,000 and another with 1,000 polys. if you stay right in front of him, the high resolution mesh (10,000 polys) is shown, if you move away, after 100m the character is replaced by the medium mesh and after another 100m it is replaced by the low resolution mesh. Instead of replacing the mesh you can reduce it, but this would be more complex to code. &lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;binary space partitions&lt;/strong&gt;: the space is divided into two volumes. this will be repeated until you reach your goal, e.g. until the space does contain only polys that should be drawn on screen&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Portals and cells&lt;/strong&gt;: (aka Portal culling) is used in indoor scenes and/or first person shooters. The scene is divided into cells (e.g. rooms of a building) that are connected throw portals (e.g. doors). you can set portals to be open and closed.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Assuming you have a basic understanding of the graphics pipeline, I tried to focus on rasterization. you may also take a look at lighting and shading or other stuff..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These include just a few possible technologies to include into your project. since i have not enough reputation on stackexchange, I cannot provide many links, but you'll find all that stuff in google :)&lt;/p&gt;&#xA;" OwnerUserId="2417" LastActivityDate="2016-01-07T12:26:55.137" CommentCount="1" />
  <row Id="1875" PostTypeId="2" ParentId="1873" CreationDate="2016-01-07T13:09:28.180" Score="4" Body="&lt;p&gt;To my knowledge, &lt;a href=&quot;https://software.intel.com/en-us/articles/software-occlusion-culling&quot; rel=&quot;nofollow&quot;&gt;Software Occlusion Culling&lt;/a&gt; (which you already mentioned), is pretty much the only thing a software rasterizer would still be used for. &lt;a href=&quot;http://procworld.blogspot.fr/2015/08/voxel-occlusion.html&quot; rel=&quot;nofollow&quot;&gt;Procworld&lt;/a&gt; makes use of a similar technique to display its huge voxel environments.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most other culling methods like frustum culling do work on the CPU, but, to stick with the example, the test against the frustum happens on object level, probably with an axis-aligned bounding box (AABB). This intersection test is way simpler than using a full-blown software rasterizer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In certain cases, software rasterization could be used for mouse picking of objects. In game engines, this is often solved using a physics engine and ray-triangle collision with a simplified mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With the CPU being idle while waiting for the GPU in modern interactive 3D applications, one could think that it might be beneficial to use these idle cycles to render on the CPU using a software rasterizer. The problem here, besides the rendering getting horribly complex and convoluted, will often be the bandwidth. Images rendered on the CPU need to be transferred to the GPU before getting displayed, which might cancel out the benefit.&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2016-01-07T13:09:28.180" CommentCount="0" />
  <row Id="1876" PostTypeId="2" ParentId="1873" CreationDate="2016-01-07T13:49:57.700" Score="0" Body="&lt;p&gt;Games with the concept of &quot;fog of war&quot; often have a visibility grid to define the state of the fog of war at each location.  Rasterization is sometimes used to modify the fog of war state for specific shapes on the grid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, using an ability that reveals a circle of the map with a specific radius, or perhaps something that reveals a square.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Not strictly graphics related, but definitely a use of rasterization in modern games, and a technique ive seen used in RTSs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other grid based data in games could use rasterization for similar situations.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-01-10T01:38:35.283" LastActivityDate="2016-01-10T01:38:35.283" CommentCount="0" />
  <row Id="1878" PostTypeId="1" CreationDate="2016-01-08T14:03:31.510" Score="1" ViewCount="57" Body="&lt;p&gt;I have wirtten a fragment shader that works just fine with a single light. Now I am trying to adpat it to work with 8 lights, the implement it in Processing. Clearly I am doing something wrong in the math and I cannot see what it is... I have read other posts about this and try to adapt the answer to my problem, no luck though...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9u1P9.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9u1P9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;////Fragment/////&#xA;&#xA;    #ifdef GL_ES&#xA;precision mediump float;&#xA;precision mediump int;&#xA;#endif&#xA;&#xA;varying vec4 vertColor;&#xA;varying vec3 ecNormal;&#xA;varying vec3 lightDir;&#xA;&#xA;void main() {  &#xA;  vec3 direction = normalize(lightDir);&#xA;  vec3 normal = normalize(ecNormal);&#xA;  float intensity = max(0.0, dot(direction, normal));&#xA;  gl_FragColor = vec4(intensity, intensity, intensity, 1) * vertColor;&#xA;}&#xA;&#xA;////vertex/////&#xA;&#xA;        #define PROCESSING_LIGHT_SHADER&#xA;&#xA;uniform mat4 modelview;&#xA;uniform mat4 transform;&#xA;uniform mat3 normalMatrix;&#xA;&#xA;uniform vec4 lightPosition;&#xA;uniform vec3 lightNormal;&#xA;&#xA;attribute vec4 vertex;&#xA;attribute vec4 color;&#xA;attribute vec3 normal;&#xA;&#xA;varying vec4 vertColor;&#xA;varying vec3 ecNormal;&#xA;varying vec3 lightDir;&#xA;&#xA;void main() {&#xA;  gl_Position = transform * vertex;    &#xA;  vec3 ecVertex = vec3(modelview * vertex);  &#xA;&#xA;  ecNormal = normalize(normalMatrix * normal);&#xA;  lightDir = normalize(lightPosition.xyz - ecVertex);  &#xA;  vertColor = color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2425" LastEditorUserId="2425" LastEditDate="2016-01-08T15:35:27.393" LastActivityDate="2016-01-08T15:35:27.393" Title="Multiple light pixel Shader in GLSL" Tags="&lt;opengl&gt;&lt;shader&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="1879" PostTypeId="2" ParentId="1878" CreationDate="2016-01-08T15:22:36.260" Score="1" Body="&lt;p&gt;From the error message the problem is in line 24 of your fragment shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec3 transformedNormal = normalize(normalMatrix * normal);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;normalMatrix&lt;/code&gt; is a &lt;code&gt;mat4&lt;/code&gt; but &lt;code&gt;normal&lt;/code&gt; is a &lt;code&gt;vec3&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The solution is to make normalMatrix a mat3 instead.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-01-08T15:22:36.260" CommentCount="1" />
  <row Id="1880" PostTypeId="1" CreationDate="2016-01-08T19:44:45.150" Score="5" ViewCount="99" Body="&lt;p&gt;TWO PART QUESTION - We've all seen the movies/TV shows where the police/feds/spies use computer software to take a grainy photo and do a &quot;clean-up&quot; to see a better picture and more details. I assume the concept is executed by some sort of &lt;strong&gt;uber-power&lt;/strong&gt; pixel-smoothing or anti-aliasing type of algorithms to fill in the blanks based on deductive processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PART 1: How &lt;em&gt;real&lt;/em&gt; is this technology in the public/commercial software world? I'm not asking about any &lt;em&gt;speculation&lt;/em&gt; on alleged secret gov software or such, I just want to know where we actually are with this concept today? How much is fully automated vs human-assisted. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;PART 2: Assuming there actually is &lt;em&gt;some&lt;/em&gt; reality with this technology for photographs the second part of this question is how (if at all) has this been applied to videos? Again the issue of fully automated vs human assisted is of interest here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the heart of this post is the ultimate question of how viable is today's software for being able to take an old VHS or DVD recording and process the frames to create a new HD-resolution remaster. Considering that doing this would mean cleaning up tens-of-thousands of frames for even a simple wedding video I am &lt;strong&gt;not&lt;/strong&gt; expecting this technology to be fast of course.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;NOTE: Per in-topic and meta discussions I went ahead and cross-posted this in two other suggested SE forums to acquire their viewpoints and expertise on this matter. So far (it is still a little &quot;early in the day&quot; so to speak) I have received some pretty interesting information in 2 of the 3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When this is all done (when good answers have been selected) I would appreciate a way to merge these &lt;strong&gt;for the benefit of all three SE communities&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Computer Graphics: &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/1880&quot;&gt;What is the state-of-the-art on using computers to &amp;quot;clean-up&amp;quot; images?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Signal Processing: &lt;a href=&quot;http://dsp.stackexchange.com/questions/28168&quot;&gt;http://dsp.stackexchange.com/questions/28168&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Video Production: &lt;a href=&quot;http://video.stackexchange.com/questions/17363&quot;&gt;http://video.stackexchange.com/questions/17363&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2427" LastEditorUserId="2427" LastEditDate="2016-01-09T16:11:12.893" LastActivityDate="2016-01-09T16:11:12.893" Title="What is the state-of-the-art on using computers to &quot;clean-up&quot; images?" Tags="&lt;image-processing&gt;&lt;photo-realistic&gt;&lt;video&gt;" AnswerCount="1" CommentCount="17" FavoriteCount="1" />
  <row Id="1881" PostTypeId="2" ParentId="1880" CreationDate="2016-01-08T22:50:50.167" Score="2" Body="&lt;p&gt;The closest thing I know of to the &quot;&lt;em&gt;computer: enhance!&lt;/em&gt;&quot; trope in real life is the family of &lt;a href=&quot;http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html&quot; rel=&quot;nofollow&quot;&gt;Single Image Super-Resolution&lt;/a&gt; techniques. That page shows a number of examples of the results on various images. You can see that while it improves the visual quality of the enlarged images, it's a long way from what you see on TV where they can read the text on a letter that's reflected in a wine glass, or recognize the face of someone standing in the shadows, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The technique basically works by observing that a single image often contains recurring instances of the same patterns or structures. When you have multiple copies of the same pattern, they often occur at different sub-pixel offsets, i.e. aligned differently to the pixel grid, which means that each copy contains slightly different information about the underlying pattern, and by putting them together you can recover (really, guess at) a higher-resolution version of that pattern. This can then be used to &quot;fill in the missing detail&quot; wherever that pattern occurs. This is a very handwavy explanation, but you can see the paper (linked from the above site) for more details. Most of it is automated, but I'd guess it still requires a fair amount of human parameter-tuning to get good results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper is from 2009, and it looks like there have been a few follow-ups since then, but only incremental improvements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also note that if you have a specific use case such as reading vehicle license plates, I'd guess it's possible to use machine-learning techniques to get much better results than you can in general. In that case, you have a concrete, known set of possible shapes (the characters that can appear on a license plate, in the particular font used for license plates), and rather than &quot;enhancing&quot; an image of literally anything, you're just trying to find the set of characters that best matches the image you're looking at. That's not my area of expertise, though.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-08T22:50:50.167" CommentCount="3" />
  <row Id="1882" PostTypeId="2" ParentId="1765" CreationDate="2016-01-09T12:40:33.980" Score="1" Body="&lt;p&gt;This is a good solution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I just had a problem transforming &lt;code&gt;AxisVector&lt;/code&gt; in screen coordinates, it turns out I must project two points from this vector on the screen, as explained in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1772/2173&quot;&gt;this answer&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2173" LastActivityDate="2016-01-09T12:40:33.980" CommentCount="0" />
  <row Id="1883" PostTypeId="2" ParentId="1765" CreationDate="2016-01-09T12:41:12.300" Score="4" Body="&lt;p&gt;I would not recommend to correlate the extrusion direction with its screen projection because of the asymptotic behaviour in case the axis points toward the screen or the resulting extrusion crosses the focal point of the camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One consistent way of doing it would be computing the extrusion height independently of the axis direction:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Find a virtual construction plane of your operation which is parallel to the screen. For example, in the real world, this plane would be a plane of your computer monitor screen reachable by hand as you sit at your computer.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Usually, you have some kind of central point which serves as a center of screen camera rotation as you navigate your viewport. You can draw your construction plane through this point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way is drawing your construction plane through the intersection of the mouse cursor and your object as you start your extrusion operation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Either way, the construction plane must be parallel to the screen and in front of your camera view for this method to work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you are using an orthogonal projection (camera without a Field of View) then you can draw the construction plane through any point.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;&lt;p&gt;Compute the length of the projection of mouse motion on the construction plane your figured out in point (1). This is your desired extrusion height.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Provide a means of moving the construction plane back and forth along the view axis, if the current viewport has FOV. You don't need to display and the move the construction plane if your viewport uses orthogonal projection.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Using this method your extrusion operation height would be visually consistent with what user expects with mouse motion, and the user would not experience any asymptotic and singularity issues as with the axis projection method.&lt;/p&gt;&#xA;" OwnerUserId="2433" LastActivityDate="2016-01-09T12:41:12.300" CommentCount="3" />
  <row Id="1884" PostTypeId="1" CreationDate="2016-01-09T16:08:47.323" Score="1" ViewCount="51" Body="&lt;p&gt;Why do I need to specify the same name for color input in fragment shader and output color from vertex shader?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//Vertex shader&#xA;out vec3 vertex_color;&#xA;&#xA;void main()&#xA;{&#xA;    vertex_color=vec3(1.0,0.0,0.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//Fragment shader &#xA;in vec3 vertex_color;&#xA;out vec4 frag_color;&#xA;&#xA;void main()&#xA;{&#xA;    frag_color=vec4(vertex_color,1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Since we already hand the color value from vertex shader to fragment shader, why do we need same names?&lt;/p&gt;&#xA;" OwnerUserId="2096" LastEditorUserId="127" LastEditDate="2016-01-09T17:22:14.240" LastActivityDate="2016-01-09T23:16:32.433" Title="OpenGL vertex color" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1885" PostTypeId="2" ParentId="1884" CreationDate="2016-01-09T20:12:41.833" Score="3" Body="&lt;p&gt;Using the same name is exactly how you tell OpenGL that you want the value passed through from vertex to fragment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You say &quot;we already hand the color value from vertex shader to fragment shader&quot;, but that's not correct. Usually, the only value that's passed between shaders &lt;em&gt;automatically&lt;/em&gt; is position, and that's only because it feeds into the GPU's rasterization hardware to draw the triangle on the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any other values such as color, normal, texture coordinates, etc. that you want passed between shader stages have to be explicitly hooked up by the shader author. And the way you do that in GLSL is to create an &lt;code&gt;out&lt;/code&gt; variable in one stage, and an &lt;code&gt;in&lt;/code&gt; variable in the next stage, with the same name.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-09T20:12:41.833" CommentCount="0" />
  <row Id="1886" PostTypeId="2" ParentId="1884" CreationDate="2016-01-09T23:16:32.433" Score="2" Body="&lt;p&gt;Because you may want to pass more than one attribute through to the fragment shader. 2 which are essential are normal vector and the texture coordinates once you start doing lighting and textured meshes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can in newer openGL versions give a numbered location to the attributes you pass through using &lt;code&gt;layout(location=1)&lt;/code&gt;. Then the names don't have to match.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-01-09T23:16:32.433" CommentCount="0" />
  <row Id="1887" PostTypeId="1" CreationDate="2016-01-10T04:54:51.350" Score="6" ViewCount="136" Body="&lt;p&gt;Since long ago I wanted to implement a 2D lightning algorithm based on an &lt;a href=&quot;https://www.youtube.com/watch?v=74uXA7SmDqE&quot; rel=&quot;nofollow&quot;&gt;idea I saw on YouTube&lt;/a&gt;. The video is realtime, but it runs on the CPU and the resolution is pretty low. I'm curious if anyone has an idea how to do this on the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm works the following way:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We have light data maps, these hold the color of the material (e.g. a red glass has 0xFF0000) and how transparent that pixel is.    &lt;/li&gt;&#xA;&lt;li&gt;Work out the light values of each pixel in growing circles from a light source. In other words, first calculate values right next to the light (8 pixels), then the neighbours of these pixels and so on. The color of a pixel is its value from the lightning data map, its alpha equals: (alpha of the pixel next to it closest to the light source - own alpha). So it is some kind of ray casting algorithm that calculates value for each pixel and can handle colored glass and fog/smoke.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So the CPU implementation is pretty trivial. My problem is that I want to use this in a mobile game, but I don't know how this would work as a shader.&#xA;How can this be implemented on the GPU (that would run on mobile, so with OpenGL ES 2.0/3.0, maybe with Metal - if this is not possible then with OpenCL/CUDA)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I am not looking for full implementations, just ideas/shader pseudocode how this would be possible.&lt;/p&gt;&#xA;" OwnerUserId="2437" LastEditorUserId="2437" LastEditDate="2016-01-10T17:59:21.980" LastActivityDate="2016-01-11T17:39:42.400" Title="How to implement a realtime 2D light renderer with fog/colored light on the GPU?" Tags="&lt;raytracing&gt;&lt;algorithm&gt;&lt;shader&gt;&lt;real-time&gt;&lt;opengl-es&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1888" PostTypeId="1" CreationDate="2016-01-10T12:25:09.853" Score="6" ViewCount="128" Body="&lt;p&gt;I'm trying to figure out how to properly implement flat shading for meshes containing non-planar polygons (using OpenGL/GLSL). The aim is to obtain something similar to the result &lt;em&gt;Blender&lt;/em&gt; gives (all polygons below are non-planar):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2Whne.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2Whne.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To provide a bit of context — the non-planar polygons (e.g. quads, pentagons, ...) are either present in the mesh I load or appear as a result of applying a subdivision scheme (such as Catmull-Clark) to the mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case of a triangle mesh, I'm aware of the following approaches to implement flat shading —&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Compute the cross product of &lt;code&gt;dFdx&lt;/code&gt; and &lt;code&gt;dFdy&lt;/code&gt; in the fragment shader&lt;/li&gt;&#xA;&lt;li&gt;Use the geometry shader to compute the triangle normal by taking the cross product of two sides of the triangle&lt;/li&gt;&#xA;&lt;li&gt;When using &lt;code&gt;glDrawElements&lt;/code&gt;, use the &lt;code&gt;flat&lt;/code&gt; interpolation qualifier (which basically uses the normal associated with the provoking vertex for all fragments)&lt;/li&gt;&#xA;&lt;li&gt;When using &lt;code&gt;glDrawArrays&lt;/code&gt;, most vertices (usually all of them) are copied to the GPU multiple times. Therefore, a different normal can be used each time&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For a mesh containing non-planar polygons the first three options don't yield the result I'm after. In my implementation (using the Qt framework) I use &lt;code&gt;GL_TRIANGLE_FAN&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/4ujWL.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/4ujWL.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fourth option could work, but isn't very elegant — I'd like to use either &lt;code&gt;glDrawElements&lt;/code&gt; or &lt;code&gt;glDrawMultiElements&lt;/code&gt;. One other option would be to use &lt;code&gt;glDrawElements&lt;/code&gt; while looping over the faces one by one (instead of invoking it once using &lt;code&gt;glPrimitiveRestartIndex&lt;/code&gt;). In that case, one could upload a &lt;code&gt;uniform&lt;/code&gt; normal for each face. However, this does not seem very efficient. Any other ideas?&lt;/p&gt;&#xA;" OwnerUserId="2440" LastEditorUserId="2440" LastEditDate="2016-01-10T18:16:30.373" LastActivityDate="2016-01-13T14:29:53.797" Title="Flat shading for non-planar polygons" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1889" PostTypeId="2" ParentId="1887" CreationDate="2016-01-10T17:37:36.223" Score="6" Body="&lt;p&gt;If your wall geometry is vector graphics you can simply extrude the segment away from the light position. This means 2 triangles per draw call, all the extrusion offsets can  can be handled in the vertex shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/u80e2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/u80e2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: For each wall generate a shadow volume extrusion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quick and extremely dirty sample implementation of shadow volumes here:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://webglplayground.net/saved/AeN6yoNGih&quot; rel=&quot;nofollow&quot;&gt;http://webglplayground.net/saved/AeN6yoNGih&lt;/a&gt; (please do not save your own version, fork instead)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Now since your shader can color the triangles as they see fit you can apply a shader on the triangles that is colored. Just like you would render transparent surfaces. To do this proper you may need a offscreen buffer for getting a proper color accumulation mode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/o2KTd.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/o2KTd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Coloring the volumes based on distance to light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Certainly the coloring algorithm may be whatever you like. for example the color could become darker the nearer you are to the extrude edge etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here is very quickly done &lt;em&gt;extremely dirty&lt;/em&gt; live example of colored shadow volumes:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://webglplayground.net/saved/9xgxDbtY7f&quot; rel=&quot;nofollow&quot;&gt;http://webglplayground.net/saved/9xgxDbtY7f&lt;/a&gt; (please do not save your own version, fork instead), very bad code style done on the run (and on my phone).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This is just one way hopefully others will elborate. You can also use the exact same methods as you would in 3d. As for nonuniform fog, you could just use a raymarcher.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-01-11T17:39:42.400" LastActivityDate="2016-01-11T17:39:42.400" CommentCount="5" />
  <row Id="1890" PostTypeId="1" CreationDate="2016-01-10T22:15:57.840" Score="5" ViewCount="138" Body="&lt;p&gt;I'm working on implementing &lt;a href=&quot;http://www-ljk.imag.fr/Publications/Basilic/com.lmc.publi.PUBLI_Inproceedings@117681e94b6_1e9c7f4/clouds.pdf&quot;&gt;Bruneton's cloud rendering paper&lt;/a&gt;, which requires one to render a pair of depth maps, similar to how shadow mapping works. In this case, my light source is the sun (or possibly the moon), which I represent in my rendering engine as a normalized vector pointing in the direction of the sun or moon, which is essentially located at infinity from the current camera position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the light source doesn't have a traditional &quot;position,&quot; I'm having a little trouble determining how best to create the viewpoint transformations for the source. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I would also essentially use the same transformation for sun/moon shadowing, it'd kill two birds with one stone.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is thus:&#xA;&lt;strong&gt;How can I generate a view transform for generating depth maps of area light sources located at infinity?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="2408" LastActivityDate="2016-01-10T22:54:46.133" Title="How should I generate the view position for a light at infinity when creating depth maps?" Tags="&lt;c++&gt;&lt;shadow-mapping&gt;&lt;depth-map&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1891" PostTypeId="2" ParentId="1890" CreationDate="2016-01-10T22:54:46.133" Score="5" Body="&lt;p&gt;Instead of using a perspective projection you would use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthographic_projection&quot;&gt;orthographic projection&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then the trick is to position the bounding box to in front of the normal camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An additional option is to add a &lt;a href=&quot;https://en.wikipedia.org/wiki/Shear_mapping&quot;&gt;skew/shear operation&lt;/a&gt; so it maps the light direction to the vertical and keeps the horizontal ground plane horizontal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This avoids leaving a large dead zone underground near the light source.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-01-10T22:54:46.133" CommentCount="0" />
  <row Id="1892" PostTypeId="1" CreationDate="2016-01-11T00:14:00.453" Score="3" ViewCount="100" Body="&lt;p&gt;I've recently rebuild shaders for my program and it stopped &quot;working&quot; ( black screen ) on OS X ( El Capitan ), but &lt;strong&gt;it's ok on Linux on GTX 660&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tested it on another Apple hardware and &lt;strong&gt;it worked on OS X on R9 395&lt;/strong&gt; ( but super slow, because of double ). So I suppose it's problem with my &lt;strong&gt;Intel HD Graphics 5000&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are no shader compilation errors, and here is my shader code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/Marqin/YuriaViewer/blob/4241e384da0f27d26cbf5557518db905a9d40039/vertex.glsl&quot; rel=&quot;nofollow&quot;&gt;https://github.com/Marqin/YuriaViewer/blob/4241e384da0f27d26cbf5557518db905a9d40039/vertex.glsl&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Keep in mind that this software worked on El Capitan with OpenGL 4.1 before shader rewrite. Here are my glfw hints:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 4);&#xA;glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 1);&#xA;glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);&#xA;glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My program also is checking for &lt;code&gt;GL_ARB_gpu_shader_fp64&lt;/code&gt; and it's available on my Macbook ( Macbook Air 2013 mid ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've &quot;debugged&quot; it a little and it looks that &lt;code&gt;i&lt;/code&gt; is always lesser than &lt;code&gt;vis&lt;/code&gt; on OS X, that's why it's black.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also made a simple test - I've typed all uniform values in shader by hand and now it wasn't black, but I've got some gibberish on screen. Then I've changed every &lt;code&gt;dvec3&lt;/code&gt; and &lt;code&gt;dvec2&lt;/code&gt; to float versions and it showed nice fractal. So it looks like &lt;code&gt;double&lt;/code&gt; is not working on OS X. But how can it be? It's saying that &lt;code&gt;GL_ARB_gpu_shader_fp64&lt;/code&gt; is available and it even doesn't complain when I request it in vertex shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I make it work on OS X Macbook?&lt;/p&gt;&#xA;" OwnerUserId="2413" LastEditorUserId="2413" LastEditDate="2016-03-30T22:06:29.490" LastActivityDate="2016-03-30T22:06:29.490" Title="How to make double working in OpenGL 4.1 on OS X ( Intel HD Graphics 5000 )?" Tags="&lt;opengl&gt;&lt;shader&gt;" AnswerCount="0" CommentCount="10" />
  <row Id="1893" PostTypeId="1" CreationDate="2016-01-11T14:52:59.460" Score="1" ViewCount="34" Body="&lt;p&gt;I want to select a pixel within a circle of a certain radius. But the radius is expressed in a decimal form (0.12). What does it mean to have a radius equal to 0.12? And concretely how to pick a pixel within a circle having such a radius?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ksGCK.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ksGCK.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2233" LastActivityDate="2016-01-11T14:52:59.460" Title="Selecting pixels within a circle having a decimal (non integer) radius" Tags="&lt;c++&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="1894" PostTypeId="1" CreationDate="2016-01-11T19:51:14.097" Score="1" ViewCount="42" Body="&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=twUz99ek5do&amp;amp;list=WL&amp;amp;index=5&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=twUz99ek5do&amp;amp;list=WL&amp;amp;index=5&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a process from 1:51, where the guy start making polygons around the bodyparts. I think this is this part is where (with later transformations) you can make a single draw like a 3D animation (with transforming the triangles). What is the name of the process, and is this effect available in other animation softver? &#xA;I'm thinking about mostly maya, spriter or unity.&#xA;Ty &lt;/p&gt;&#xA;" OwnerUserId="2451" LastActivityDate="2016-01-11T22:22:05.730" Title="Spine. What is the name of the process?" Tags="&lt;3d&gt;&lt;animation&gt;&lt;polygon&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1895" PostTypeId="1" AcceptedAnswerId="1899" CreationDate="2016-01-11T20:09:00.557" Score="4" ViewCount="54" Body="&lt;p&gt;I have been working on a graphics library for some time now and have gotten to the point where I have to draw Bezier and line based fonts. Up to this point I am stuck with this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://cdn.discordapp.com/attachments/132932473242910720/136193149239427073/unknown.png&quot; alt=&quot;i&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://cdn.discordapp.com/attachments/132932473242910720/136196291146285056/unknown.png&quot; alt=&quot;a&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The green lines are the Bezier paths, and the white part is what gets rendered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code I use for Beziers is &lt;a href=&quot;http://pastebin.com/s8qDC4NH&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. The one for lines is &lt;a href=&quot;http://pastebin.com/bKv55tch&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. For those who don't know that is Lua. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Path rendering (lines) : 32 - 39&#xA;  The algorithm is as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Iterating from 0 to 1 at certain intervals&lt;/li&gt;&#xA;&lt;li&gt;calculating the x and y with this formula: &lt;code&gt;(1-index)^2*x1+2*(1-index)*index*x2+index^2*x3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Up to this point everything works fine. The green lines are generated using the path method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The white part is rendered in a completely different way:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I get the x coordinates of the Beziers and lines at a particular Y, the put them into a table.&lt;/li&gt;&#xA;&lt;li&gt;I iterate through the table and each time I encounter a point I change the value of state. In the same for loop is also check whether state is on. If it is, I draw a pixel to the screen.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;To find the x values of a y, I use the getX method (line 46 in Bezier and line 31 in Line).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code I use for the drawing itself is this one:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;local xBuffer = {}&#xA;local state = false&#xA;&#xA;for i=0,500 do&#xA;    for k,v in pairs(beziers) do&#xA;        a,b = v.getX(i)&#xA;        if a then&#xA;            xBuffer[round(a)] = 1&#xA;            if b then&#xA;                xBuffer[round(a)] = 1&#xA;            end&#xA;        end&#xA;    end&#xA;    for k,v in pairs(lines) do&#xA;        a = v.getX(i)&#xA;        if a then&#xA;            xBuffer[round(a)] = 1&#xA;        end&#xA;    end&#xA;    state = false&#xA;    for x=0,600 do&#xA;        if xBuffer[x] then&#xA;            state = not state&#xA;        end&#xA;        if state then&#xA;            love.graphics.points(x,i)&#xA;        end&#xA;    end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Quick explanation: for i,v in pairs iterates through the table given as an argument to pairs. love.graphics.points(x,y) sets a point at x,y.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="2447" LastEditorUserId="231" LastEditDate="2016-01-19T13:03:59.860" LastActivityDate="2016-01-19T13:03:59.860" Title="How should I fill a shape consisting of Bezier cures and straight lines?" Tags="&lt;rendering&gt;&lt;algorithm&gt;&lt;geometry&gt;&lt;line-drawing&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1896" PostTypeId="1" CreationDate="2016-01-11T21:47:54.283" Score="2" ViewCount="68" Body="&lt;p&gt;I have managed to implement a garoud shader with specular lighting efects in Processing 3.0 . Now I am trying with a fragment Phong shader but cannot make it work. I can´t find where is the error. It should just implent the phong illumination model, with specular and diffuse components.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eKOTe.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eKOTe.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vertex Shader: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#define PROCESSING_LIGHT_SHADER&#xA;&#xA;uniform mat4 modelview;&#xA;uniform mat4 transform;&#xA;uniform mat3 normalMatrix;&#xA;&#xA;&#xA;&#xA;attribute vec4 vertex;&#xA;attribute vec4 color;&#xA;attribute vec3 normal;&#xA;&#xA;varying vec4 vertColor;&#xA;&#xA;varying vec3 transformedNormal;&#xA;varying vec3 vertexCamera;&#xA;&#xA;void main() {&#xA;  gl_Position = transform * vertex;    &#xA;&#xA;  vertexCamera = vec3(modelview * vertex);  &#xA;  transformedNormal = normalize(normalMatrix * normal);&#xA;  vertColor = color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fragement Shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#ifdef GL_ES&#xA;precision mediump float;&#xA;precision mediump int;&#xA;#endif&#xA;&#xA;uniform vec3 lightPosition;&#xA;uniform vec3 eye_Position;&#xA;&#xA;varying vec4 vertColor;&#xA;varying vec3 lightDir;&#xA;&#xA;varying vec3 transformedNormal; //world pos&#xA;varying vec3 vertexCamera; // world normal&#xA;&#xA;void main() {  &#xA;&#xA; vec3 normalizedPos =  normalize(lightPosition.xyz -vertexCamera);&#xA; vec3 direction = normalize(eye_Position - vertexCamera); &#xA;&#xA;&#xA; float intensity = 0.0;&#xA; float specular = 0.0;&#xA;&#xA;&#xA; float LdotN = max(0, dot(normalizedPos,direction));&#xA; float diffuse = 1 * LdotN;&#xA;&#xA; vec3 R = -normalize(reflect(normalizedPos,transformedNormal));&#xA; specular = pow( max(0, dot( R, direction)), 16);&#xA;&#xA;&#xA;  intensity += (diffuse + specular);&#xA;&#xA;  gl_FragColor = vec4(intensity, intensity, intensity, 1) * vertColor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2425" LastEditorUserId="2425" LastEditDate="2016-01-11T22:15:05.193" LastActivityDate="2016-01-12T16:36:27.080" Title="Shader won't work" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;&lt;fragment-shader&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1897" PostTypeId="2" ParentId="1894" CreationDate="2016-01-11T22:22:05.730" Score="2" Body="&lt;p&gt;This appears to be simply &lt;a href=&quot;https://en.wikipedia.org/wiki/Skeletal_animation&quot; rel=&quot;nofollow&quot;&gt;skeletal animation&lt;/a&gt;, which is a standard technique that is available in all modern animation packages. Whether applied to 3D meshes or (as here) 2D ones, the principle is the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The pseudo-3D effect on the sprites is created by using a mesh where vertices are placed along the contours of the sprite texture, then animated to squash and stretch parts the image in such a way that it creates the illusion of 3D motion, even though the sprite images are authored in 2D. There's &lt;a href=&quot;http://esotericsoftware.com/spine-meshes#Deformation&quot; rel=&quot;nofollow&quot;&gt;a description of the technique in the Spine docs&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-11T22:22:05.730" CommentCount="0" />
  <row Id="1899" PostTypeId="2" ParentId="1895" CreationDate="2016-01-12T12:53:59.170" Score="4" Body="&lt;p&gt;If you are in a hurry to get your renderer working &lt;em&gt;and&lt;/em&gt; you already have the filled polygonal routine &lt;em&gt;functioning correctly&lt;/em&gt;, can I suggest an alternative, possibly easier approach? Though I'm not familiar with Lua, it seems you are solving for the exact intersection of a scan line with the quadratic Bezier which, though admirable, is possibly overkill.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead, tessellate your Beziers into line segments and then throw those into the polygon scan converter. I suggest just using (recursive) binary subdivision: i.e. the quadratic Bezier with control points, $(\overline {A} , \overline {B} , \overline {C})$ can be split into two Beziers, $(\overline {A} , \overline {D} , \overline {E})$ and $(\overline {E} , \overline {F} , \overline {C})$ &#xA;where&#xA;$$&#xA;\begin{align*} &amp;amp; \overline {D}=\dfrac {\overline {A}+\overline {B}} {2}\\ &amp;amp; &#xA;\overline {E} =\dfrac {\overline {A}+2\overline {B}+\overline {C}}{4}\\ &amp;amp;&#xA;\overline {F}=\dfrac {\overline {B}+\overline {C}} {2}&#xA;\end{align*}&#xA;$$&#xA;(which is also great if you only have fixed point maths).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;IIRC, each time you subdivide, the error between the Bezier and just a straight line segment joining the end points goes down by a factor of ~4x, so it doesn't take many subdivisions before it will be indistinguishable from the original. You can also use the bounding box of the control points to decide if you can skip out of the subdivision process early since that will also be a conservative bound on the curve.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-01-12T16:03:03.003" LastActivityDate="2016-01-12T16:03:03.003" CommentCount="3" />
  <row Id="1900" PostTypeId="2" ParentId="1896" CreationDate="2016-01-12T16:36:27.080" Score="2" Body="&lt;p&gt;If you are using Processing the variable &lt;code&gt;lightPosition&lt;/code&gt; must be declared as a &lt;code&gt;vec4&lt;/code&gt;, otherwise it won't be passed in, and will be always 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then your diffuse computation is not correct, the dot product we want it's between the normal and the light direction:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float LdotN = max(0, dot(normalizedPos,transformedNormal));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not sure about the uniform &lt;code&gt;eye_Position&lt;/code&gt;, I believe it's always 0 as nobody is setting it. Anyway, it is not needed. Since the fragment position it's already on camera space, the camera direction is simply: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec3 direction = -normalize(vertexCamera);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The complete fragment shader should be:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#ifdef GL_ES&#xA;precision mediump float;&#xA;precision mediump int;&#xA;#endif&#xA;&#xA;uniform vec4 lightPosition;&#xA;&#xA;varying vec4 vertColor;&#xA;varying vec3 transformedNormal;&#xA;varying vec3 vertexCamera;&#xA;&#xA;void main() {  &#xA;&#xA; vec3 normalizedPos =  normalize(lightPosition.xyz -vertexCamera);&#xA; vec3 direction = -normalize(vertexCamera); &#xA;&#xA; float intensity = 0.0;&#xA; float specular = 0.0;&#xA;&#xA; float LdotN = max(0, dot(normalizedPos,transformedNormal));&#xA; float diffuse = 1 * LdotN;&#xA;&#xA; vec3 R = -normalize(reflect(normalizedPos,transformedNormal));&#xA; specular = pow( max(0, dot( R, direction)), 16);&#xA;&#xA;  intensity += (diffuse + specular);&#xA;&#xA;  gl_FragColor = vec4(intensity, intensity, intensity, 1) * vertColor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2064" LastActivityDate="2016-01-12T16:36:27.080" CommentCount="0" />
  <row Id="1901" PostTypeId="1" AcceptedAnswerId="2012" CreationDate="2016-01-12T22:43:34.653" Score="5" ViewCount="106" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/MZosQ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MZosQ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm working on a shadertoy &quot;snake&quot; game, using the new multi pass rendering abilities to save game state between frames.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using raytracing to render the board (an AABB), and am planning on using spheres to render sections of the snake's body.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The game board is a 16x16 grid and each grid can either have a sphere there (a segment of the snake's body) or not.  Snake body segments don't move, they are just either there on the grid or not.  When the snake moves, a new sphere appears in the front and an old sphere disappears from the back.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem I'm trying to solve is how to render the snake body spheres.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, a naive approach would be to store a 16x16 grid in pixels specifying whether there was a snake body in that grid cell or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would then do a ray vs sphere check for up to 256 different spheres within my pixel shader, which seems like a no go.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another method might be to figure out where the ray begins and ends on the game board (when it's between the high and low height values of where the spheres are) and then use something like bressenham line algorithm to go from the start to the end of the line the ray takes on the board, and check only the grid cells that the ray hits.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem there is that it requires a dynamic loop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe a more practical solution would be to make the camera have a nearly top down view and where the ray enters the playable game world, test any sphere in the cell it hits as well as the 8 neighboring cells.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm betting there are some much better solutions that I'm not thinking of.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know of any interesting techniques or creative solutions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: here is an older version of this game i made, which was CPU / software rendered, to give an idea of what I'm planning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/y7OLJ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/y7OLJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-01-12T22:53:49.307" LastActivityDate="2016-02-08T02:09:01.450" Title="Methods for grid traversal in a glsl pixel shader?" Tags="&lt;raytracing&gt;&lt;real-time&gt;&lt;glsl&gt;&lt;pixel-shader&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="1903" PostTypeId="1" CreationDate="2016-01-13T08:17:56.753" Score="1" ViewCount="46" Body="&lt;p&gt;In R software, I want to plot a graph by using cox regression with the restricted cubic spline method. However, I can't quite get it to work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the code I'm using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;library(rms); require(rms); library(Hmisc)&#xA;d&amp;lt;-curve.men&#xA;dd &amp;lt;- datadist(d)&#xA;options(datadist=&quot;dd&quot;)&#xA;fit &amp;lt;- cph(Surv(time,death) ~ rcs(bmi,4)&#xA;           + systolic +glucose + l_chol + age &#xA;           + edu_g2 + smoke_s + l_chol + drink + activity_l+activity_w,&#xA;             data=d, nk=5)&#xA;plot(Predict(fit,bmi), xlab=&quot;bmi&quot;, ylab=&quot;Relative Risk&quot;, lty=1, lwd=2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which creates the following plot &lt;strong&gt;(A)&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/MoyLw.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MoyLw.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I want to plot the following graph &lt;strong&gt;(B)&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Fx5Qx.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Fx5Qx.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How do I set the reference point? (for example: &lt;code&gt;bmi = 24.21429&lt;/code&gt; as the reference point)&lt;/li&gt;&#xA;&lt;li&gt;I want to add the horizontal line. I tried using &lt;code&gt;abline(h=1)&lt;/code&gt;, but it did not work.&lt;/li&gt;&#xA;&lt;li&gt;All in all, how do I make the graph look like (B)?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2459" LastEditorUserId="231" LastEditDate="2016-01-19T13:07:28.377" LastActivityDate="2016-01-19T13:07:28.377" Title="Problems in using cox regression with restricted cubic spline plot with R software" Tags="&lt;line-drawing&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="1904" PostTypeId="1" AcceptedAnswerId="1907" CreationDate="2016-01-13T09:47:21.997" Score="8" ViewCount="61" Body="&lt;p&gt;I want to extract colors from images like in Adobe Kuler. When I select colorful or muted corresponding colors are shown. But what is the definition for colorful or muted colors? What do the words &quot;colorful&quot;, &quot;deep&quot; and &quot;muted&quot; refer to? &lt;/p&gt;&#xA;" OwnerUserId="2383" LastEditorUserId="231" LastEditDate="2016-01-19T13:03:05.540" LastActivityDate="2016-01-19T13:03:05.540" Title="What does &quot;muted&quot; mean in the context of color?" Tags="&lt;color&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1906" PostTypeId="2" ParentId="1888" CreationDate="2016-01-13T14:29:53.797" Score="6" Body="&lt;p&gt;Without knowing the internals of Blender, I would say it uses shared normals for the quads, splitting the normals only on edges between the quads, not between the triangles. So your idea of uploading a normal per quad is probably close to the result you are looking for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you load the mesh as quads, you are set. If you are using an old version of OpenGL where quads still exist, you may even be able to use solution 3).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you load the mesh as triangles, you will have to figure out which triangles originated in quads. You can likely do this simply by enforcing split normals beyond a fixed angle between adjacent triangles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not sure there is a simple dynamic solution, where you just render any mesh. You may be able to use the geometry shader to look at the adjacent vertices, but the easiest version is to smooth the normals once before upload.&lt;/p&gt;&#xA;" OwnerUserId="2463" LastActivityDate="2016-01-13T14:29:53.797" CommentCount="0" />
  <row Id="1907" PostTypeId="2" ParentId="1904" CreationDate="2016-01-13T22:52:26.323" Score="9" Body="&lt;p&gt;In terms of the HSV color space, &lt;a href=&quot;http://designshack.net/articles/graphics/the-evolution-of-flat-design-muted-colors/&quot;&gt;&quot;muted&quot; colors are those with lower saturation and/or value&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Deep&quot; colors are saturated but not too high in value (e.g. &lt;a href=&quot;https://www.google.com/search?q=deep+red&amp;amp;tbm=isch&quot;&gt;deep red&lt;/a&gt;) while colors with both high saturation and value might be called &quot;bright&quot; (e.g. &lt;a href=&quot;https://www.google.com/search?q=bright+red&amp;amp;tbm=isch&quot;&gt;bright red&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Colorful&quot; is somewhat more vague; it might mean bright colors, but it also might mean having a range of different hues within an image or palette, such as including complementary colors or analogous colors rather than just variations on a single hue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that these are general terms that would be understood by any graphic designer or artist, but the specific algorithms that Adobe Kuler attaches to these words are proprietary and only known to Adobe.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-13T22:52:26.323" CommentCount="1" />
  <row Id="1909" PostTypeId="1" CreationDate="2016-01-14T22:55:11.760" Score="4" ViewCount="122" Body="&lt;p&gt;I am using Unity but this question might not be proper to this engine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have projected the shadow map onto this sphere but there are grazing shadows. Is it possible to avoid it or to correct it as I am using the raw depth data of the texture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sGOqR.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sGOqR.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the pseudo-shader code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;uniform sampler2D _ShadowMap;&#xA;float4 shadowCoord : TEXCOORD1;&#xA;&#xA;// vertex shader&#xA;o.shadowCoord = mul(unity_World2Shadow[0], mul(_Object2World, v.vertex));&#xA;&#xA;// fragment shader&#xA;color.rgb = tex2D(_ShadowMap, i.shadowCoord);&#xA;return color;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="2372" LastEditDate="2016-01-15T07:26:34.417" LastActivityDate="2016-01-18T13:40:31.857" Title="Shadow map projection issue" Tags="&lt;opengl&gt;&lt;algorithm&gt;&lt;shader&gt;&lt;glsl&gt;&lt;shadow-mapping&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="1911" PostTypeId="1" CreationDate="2016-01-15T22:37:17.597" Score="1" ViewCount="45" Body="&lt;p&gt;What advantages do the outcodes offer in &lt;a href=&quot;https://en.wikipedia.org/wiki/Cohen%E2%80%93Sutherland_algorithm&quot; rel=&quot;nofollow&quot;&gt;this algorithm&lt;/a&gt; ? Do they help in simplifying the algorithm ? Do they decrease the number of calculations ?&lt;/p&gt;&#xA;" OwnerUserId="2214" LastEditorUserId="457" LastEditDate="2016-01-16T09:46:49.020" LastActivityDate="2016-01-18T11:58:50.560" Title="Outcodes in Cohen-Sutherland Clipping Algorithm" Tags="&lt;clipping&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="1912" PostTypeId="1" AcceptedAnswerId="1933" CreationDate="2016-01-16T14:05:16.673" Score="8" ViewCount="98" Body="&lt;p&gt;I try to implement a position based cloth simulation using hardware tesselation.&#xA;This means I want to just upload a control quad to the graphics card and then use tesselation and geometry shading to create the nodes in the cloth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This idea follows the paper:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Huynh, David, &quot;Cloth simulation using hardware tessellation&quot; (2011). Thesis. Rochester Institute of Technology&#xA;&lt;a href=&quot;http://scholarworks.rit.edu/theses/265/&quot;&gt;http://scholarworks.rit.edu/theses/265/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know how to use tesselation to create the simulated points.&#xA;What I don't know is how to store the computated information into a framebuffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The geometry and also the tesselation evaluation shaders have informations needed for the per-vertex computations. But can they directly write into the framebuffer?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fragment-shader I know can write to the framebuffer, but my information would be interpolated and I would no longer know what to write at which position.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-01-21T12:20:38.830" Title="Per Vertex Computation in OpenGL Tesselation" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;gpgpu&gt;&lt;simulation&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1914" PostTypeId="2" ParentId="1911" CreationDate="2016-01-18T11:58:50.560" Score="4" Body="&lt;p&gt;The outcodes just represent four boolean flags as a bitfield. Codewise it is easier to move a bitfield around than four booleans. Functionality-wise it becomes very easy to check if &lt;em&gt;any&lt;/em&gt; bits are set (just check if zero), and combining outcodes is as simple as a bitwise OR.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On a side-note: Since bit-operations are fast, it is not a performance issue that the individual bits need to be extracted (e.g. outcode &amp;amp; BOTTOM).&lt;/p&gt;&#xA;" OwnerUserId="2463" LastActivityDate="2016-01-18T11:58:50.560" CommentCount="0" />
  <row Id="1915" PostTypeId="2" ParentId="1909" CreationDate="2016-01-18T13:40:31.857" Score="2" Body="&lt;p&gt;One of the downsides to shadowmapping is that you need to offset, or bias, the point used for shadowmap-lookups. This is done in a variety of ways - typically by a constant (shadowmap-depth) distance or based on the slope of the casting triangle. Both of these tend to be done during the shadowmap-rendering pass.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think what you need to solve your sphere-problem, is to offset the point you are shading a bit along its surface normal, before doing the shadowmap-lookup. This happens during shading, when sampling the shadowmap, not during shadowmap-rendering. Incidentally it is also part of the Unity standard rendering pipeline, so you can steal it from the shader include-files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ref &lt;a href=&quot;http://www.dissidentlogic.com/old/images/NormalOffsetShadows/GDC_Poster_NormalOffset.png&quot; rel=&quot;nofollow&quot;&gt;http://www.dissidentlogic.com/old/images/NormalOffsetShadows/GDC_Poster_NormalOffset.png&lt;/a&gt;&#xA;ref &lt;a href=&quot;http://www.digitalrune.com/Blog/Post/1765/Shadow-Acne&quot; rel=&quot;nofollow&quot;&gt;http://www.digitalrune.com/Blog/Post/1765/Shadow-Acne&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2463" LastActivityDate="2016-01-18T13:40:31.857" CommentCount="0" />
  <row Id="1916" PostTypeId="1" AcceptedAnswerId="1930" CreationDate="2016-01-18T13:43:31.223" Score="1" ViewCount="105" Body="&lt;p&gt;I've stumbled upon this question on a CG exam:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Which of the following techniques/optimizations doesn't apply to ray tracing?&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;A: Back-face culling&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;B: &quot;Shadow-feeler&quot; rays&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;C: Recursive tracing of rays&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;D: Calculation of refraction rays&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;E: None of the above&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;B, C and D are obviously false, but what about A? &lt;a href=&quot;http://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-rendering-a-triangle/single-vs-double-sided-triangle-backface-culling&quot; rel=&quot;nofollow&quot;&gt;Here&lt;/a&gt;, is says that &quot;with ray tracing, this feature is not as useful. Generally with ray tracing, we want geometry in the scene to cast shadows for example, regardless of the orientation of the object's surface with respect to the ray direction however the backface culling option might still be desired for primary rays&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel that the question is a bit ambiguous, since back-face culling can be applied to ray tracing but it's not (correct me if I'm wrong) a &quot;native&quot; feature as the other techniques.&lt;/p&gt;&#xA;" OwnerUserId="2489" LastActivityDate="2016-01-26T14:31:32.670" Title="Is back-face culling considered a technique/optimization of ray tracing?" Tags="&lt;raytracing&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="1919" PostTypeId="1" AcceptedAnswerId="1931" CreationDate="2016-01-18T23:03:14.117" Score="6" ViewCount="42" Body="&lt;p&gt;&lt;a href=&quot;http://renderman.pixar.com/view/implementing-a-skin-bssrdf&quot;&gt;http://renderman.pixar.com/view/implementing-a-skin-bssrdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this paper on subsurface scattering, I'm trying to understand how importance sampling is used to compute single scattering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It says that the outscattered radiance for the single term, $L_o$, can be rewritten as the product of the exponential falloff in $s'_o$ with another function:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;$L_o(x_o,\vec{\omega}_o) = \bigg[&#xA; L_i(x_i,\vec{\omega}_i)\cfrac{\sigma_s(x_o)F_p(\vec{\omega}'_i \cdotp&#xA; \vec{\omega}'_o )}{\sigma_{tc}}e^{-s'_i \sigma_t(x_i)} \bigg]&#xA; e^{-s_o\sigma(x_o)}$&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;By picking samples according to $x \sim \sigma_t e^{-\sigma_t x}$, the&#xA;  integration in $L_o$ can be approximated by a summation by applying&#xA;  importance sampling. Also by choosing $s'_o =&#xA; \cfrac{-\mathbb{log(random())}}{\sigma_{tr}}$, you can sum up all&#xA;  contributions without computing the falloff term in $s_o$.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&#xA;I understand you can approximate an integral using importance sampling, but I didn't follow how it applies to the rendering equation. Please could you describe how this works in greater detail?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$E[f(x)]=\int f(x)p(x) = \int f(x)\cfrac{p(x)}{q(x)}q(x) \approx \cfrac{1}{n}\sum \limits_{i=1}^n  f(x_i)\cfrac{p(x_i)}{q(x_i)}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, where does single scattering fit into the general rendering equation?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$L_{\text{o}}(\mathbf x,\, \omega_{\text{o}}) \,=\, L_e(\mathbf x,\, \omega_{\text{o}}) \ +\, \int_\Omega f_r(\mathbf x,\, \omega_{\text{i}},\, \omega_{\text{o}})\, L_{\text{i}}(\mathbf x,\, \omega_{\text{i}})\, (\omega_{\text{i}}\,\cdot\,\mathbf n)\, \operatorname d \omega_{\text{i}}$&lt;/p&gt;&#xA;" OwnerUserId="2457" LastEditorUserId="231" LastEditDate="2016-01-18T23:26:41.137" LastActivityDate="2016-01-21T07:44:20.010" Title="using importance sampling to reduce the error of outscattered radiance" Tags="&lt;rendering&gt;&lt;importance-sampling&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="1920" PostTypeId="1" AcceptedAnswerId="1921" CreationDate="2016-01-19T07:19:41.533" Score="9" ViewCount="62" Body="&lt;p&gt;The paper &lt;a href=&quot;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&quot;&gt;Microfacet Models for Refraction through Rough Surfaces&lt;/a&gt; (among others) reminds us the following assumptions about the microfacet distribution function D:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Microfacet density is positive valued&lt;/li&gt;&#xA;&lt;li&gt;Total microsurface area is at least as large as the corresponding&#xA;macrosurface’s area&lt;/li&gt;&#xA;&lt;li&gt;The (signed) projected area of the microsurface is the&#xA;same as the projected area of the macrosurface for any&#xA;direction v&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I can see why 1) a distribution density is a positive value, and intuitively believe that 2) means that the total area of sloped microfacets cannot be smaller than their projection.&lt;br&gt;&#xA;However I am not sure to understand the justification for 3). What does the third condition mean mean?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2016-01-19T13:10:44.163" Title="Reasons of the assumptions for the microfacet distribution function?" Tags="&lt;brdf&gt;&lt;distribution&gt;&lt;function&gt;&lt;assumptions&gt;&lt;cook-torrance&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1921" PostTypeId="2" ParentId="1920" CreationDate="2016-01-19T13:10:44.163" Score="10" Body="&lt;p&gt;It's a geometric assumption like the other two. Consider a flat macrosurface. Its projected area in any direction $v$ is just $v\dot\ \hat N$ times its area (where $\hat N$ is the surface normal). In particular, the case where you're looking at it along the normal is simplest: the projected area is equal to the area of the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now split the macrosurface into microfacets. The total area of the microfacets is at least as much (assumption 2), but each 'kink' in the surface bends the normals of the separate microfacets away from the original normal. Whatever the shape of the microfacets, the sum of their projected areas doesn't change. In the case where you're looking along the normal, it's easy to see that the total projected area is the same: the surface would have to get larger or smaller for it to change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For any direction, the microfacet has to cover a portion of the original projected area of the surface. Changing the orientation of the microfacet while still filling that portion doesn't change its projected area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's one tricky case, which is where the microfacets overhang each other. In this case, the total area is greater, because some area is covered by more than one microfacet. But in this case, at least one of the microfacets has to end up pointing away from the view direction, back into the surface. In this case, the dot product is negative, so this cancels out the area covered by more than one microfacet. This is why the text is careful to single out that it's the &lt;strong&gt;signed&lt;/strong&gt; projected area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's one more tricky case, which is where the microfacets extend past the silhouette of the object. This might happen when you're looking from very glancing angles, or where overhanging facets overhang outside the perimeter of the surface. In this case, the projected area of the microfacets will be greater, violating the third assumption. We don't generally consider this case. Intuitively, it matches up with the fact that techniques like bump-mapping don't change the shape of the silhouette of the object.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2016-01-19T13:10:44.163" CommentCount="3" />
  <row Id="1922" PostTypeId="5" CreationDate="2016-01-19T13:12:59.820" Score="0" Body="" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-01-19T13:12:59.820" LastActivityDate="2016-01-19T13:12:59.820" CommentCount="0" />
  <row Id="1923" PostTypeId="4" CreationDate="2016-01-19T13:12:59.820" Score="0" Body="This tag is scheduled for deletion. It is not to be used." OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-01-19T13:12:59.820" LastActivityDate="2016-01-19T13:12:59.820" CommentCount="0" />
  <row Id="1924" PostTypeId="1" CreationDate="2016-01-19T22:34:16.603" Score="5" ViewCount="90" Body="&lt;p&gt;I wrote a ray tracer that implements various BRDF models (Oren Nayar, Lamber, Torrance Sparrow and so on). &#xA;Now I'm trying to implement a BRDF from measured data.&#xA;I choose the Cornell database data available here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.graphics.cornell.edu/online/measurements/reflectance/spraypaints/index.html&quot; rel=&quot;nofollow&quot;&gt;http://www.graphics.cornell.edu/online/measurements/reflectance/spraypaints/index.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use them because there's a representation of the data as spectrum with 31 sample (my ray tracer use spectral data for light calculation and then convert them to CIE XYZ and then RGB values for the final image rendering).&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Which is the correct way to use this data?&lt;/li&gt;&#xA;&lt;li&gt;Which sample technique must be used? &lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Why phi_out has all negative value?&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Can I calculate phi_out using the same approximation as the one I used in Oren Nayar, so ||View - Normal * (View.dot(normal)||&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2237" LastEditorUserId="2237" LastEditDate="2016-01-23T00:14:44.033" LastActivityDate="2016-01-23T00:14:44.033" Title="Ray tracing - BRDF using Cornell measured spectral data" Tags="&lt;raytracing&gt;&lt;brdf&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1925" PostTypeId="2" ParentId="1845" CreationDate="2016-01-20T05:42:51.430" Score="2" Body="&lt;p&gt;Yes, it's a meaningful stat: GPUs have dedicated triangle setup HW and the rate is measured in triangles/GPU clock. According to white papers available on NV's website, the 680 (Kepler) could issue one triangle per SM every other clock - with 8 SMs, this yielded 4 triangles/clock. The Maxwell white paper doesn't indicate a change in this rate per SM - the 980 has 16 SMs so, if there is really is no rate change per SM, it can produce 8 triangles/clock. While the 980 has 2048 CUDA cores, the 950 has 768, implying 6 SMs and 3 triangles/clock. The chip runs around 1 Ghz, so the 950 is probably limited to 3 billion triangles per second.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-01-23T03:16:54.280" LastActivityDate="2016-01-23T03:16:54.280" CommentCount="5" />
  <row Id="1926" PostTypeId="2" ParentId="1924" CreationDate="2016-01-20T06:15:29.523" Score="4" Body="&lt;p&gt;The MATLab data file has 3 arrays&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Input vector - aka, the light source (In Cartesian coordinates)&lt;/li&gt;&#xA;&lt;li&gt;Output vector - aka, the eye (Also in Cartesian coordinates)&lt;/li&gt;&#xA;&lt;li&gt;Spectral data (spectral data is discretized into 31 bins)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;So input[n], with output[n] results in spectral[n]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The ASTM file is a flattened version of the MATLab data file, with one major difference: the input and output vectors are in spherical coordinates, specifically the traditional physics notation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/yQqTA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/yQqTA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data is defined in the following range:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For input vector, $\theta$ (theta) varies from $0$ to $\frac{\pi}{2}$, and $\phi$ (phi) is always a constant $0$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The output vectors are uniformally distributed across the hemisphere. Since $\phi$ is periodic on $2\pi$, mathematically you can define the bounds of $\phi$  in an infinite number of ways. In this data set they chose:&#xA;$$\phi = [-\pi, \pi]$$&#xA;but it would be perfectly fine to choose&#xA;$$\phi = [0, 2\pi]$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, in order to use the data:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Since the input vectors only vary over a quarter circle ($\theta = [0, \frac{\pi}{2}]$ and $\phi = 0$), not the whole hemisphere, you will need to find the matrix transform that transforms your input vector to the quarter circle.&lt;/li&gt;&#xA;&lt;li&gt;Apply the same transform to the output vector.&lt;/li&gt;&#xA;&lt;li&gt;Using the transformed input and output vectors, look up the spectral response of the BRDF.&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I'm not sure of the best way to do this. I defer to others' opinions. I'm sure there is some kind of data structure / algorithm to do the search.&lt;/li&gt;&#xA;&lt;li&gt;The naive way would be to do a O(n) search of all the input and output directions to find the two or three that are closest, then LERP between them.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Use the spectral values as the BRDF in the rendering equation&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-01-20T06:21:05.277" LastActivityDate="2016-01-20T06:21:05.277" CommentCount="2" />
  <row Id="1927" PostTypeId="1" CreationDate="2016-01-20T18:38:14.507" Score="1" ViewCount="54" Body="&lt;p&gt;I'm using NASA WorldWind to place a rectangle on the Earth ('sphere'). When I drag the rectangle on the sphere, the rectangle changes shape and size but the dimensions should be preserved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is needed to preserve the rectangle's original dimensions when it is dragged along the Earth/sphere?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It looks like most of the code in use was copied from this example: &lt;a href=&quot;http://worldwind31.arc.nasa.gov/svn/trunk/WorldWind/src/gov/nasa/worldwind/util/BasicDragger.java&quot; rel=&quot;nofollow&quot;&gt;http://worldwind31.arc.nasa.gov/svn/trunk/WorldWind/src/gov/nasa/worldwind/util/BasicDragger.java&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The line of code in the example that confuses me the most is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;double y = event.getMouseEvent().getComponent().getSize().height -&#xA;                                       this.dragRefObjectPoint.y + dy - 1;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The resulting behavior is the 'rectangle' that is mapped to the globe changes shape and size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also want to point out my experience with computer graphics and NASA worldwind are limited. I'm especially rusty on linear algebra and very weak with geomtery. I'd appreciate suggestions on resources (outside of the obvious NASA WorldWind reference).&lt;/p&gt;&#xA;" OwnerUserId="2506" LastEditorUserId="2506" LastEditDate="2016-01-21T15:39:20.490" LastActivityDate="2016-01-21T15:39:20.490" Title="Slide a rectangle along a sphere" Tags="&lt;rendering&gt;&lt;geometry&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="1928" PostTypeId="1" CreationDate="2016-01-20T22:21:47.147" Score="4" ViewCount="49" Body="&lt;p&gt;I've set up a test project for learning Metal on iOS to do some rendering, but I'm a bit stumped on how to get a prism rotating correctly about its y axis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the prism rendered without depth testing so all sides can be seen, so it looks like this part is at least correct:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Tz8CE.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Tz8CE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are the vertices:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static const float vertexData[] = {&#xA;     0.f,  1.f,  0.5f,&#xA;     1.f, -1.f,  0.f,&#xA;    -1.f, -1.f,  0.f,&#xA;&#xA;     0.f,  1.f,  0.5f,&#xA;     1.f, -1.f,  1.f,&#xA;     1.f, -1.f,  0.f,&#xA;&#xA;     0.f,  1.f,  0.5f,&#xA;    -1.f, -1.f,  1.f,&#xA;     1.f, -1.f,  1.f,&#xA;&#xA;     0.f,  1.f,  0.5f,&#xA;    -1.f, -1.f,  0.f,&#xA;    -1.f, -1.f,  1.f&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, when I turn on rotation, this is what happens:&#xA;&lt;a href=&quot;https://www.dropbox.com/s/esg41j3ibncofox/prism_rotate.mov?dl=0&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/esg41j3ibncofox/prism_rotate.mov?dl=0&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(The video has depth testing turned on). Why is the prism clipping like that (and/or &lt;em&gt;what&lt;/em&gt; is it clipping through)? And it's not rotating about it's centre.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are how the MVP matrices are being calculated:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static simd::float3 viewEye = {0.f, 0.f, -2.f};&#xA;static simd::float3 viewCenter = {0.f, 0.f, 1.f};&#xA;static simd::float3 viewUp = {0.f, 1.f, 0.f};&#xA;static float fov = 45.f;&#xA;&#xA;CGSize size = self.view.bounds.size;&#xA;Uniforms *uniforms = (Uniforms *)[uniformBuffer contents];&#xA;uniforms-&amp;gt;view = AAPL::lookAt(viewEye, viewCenter, viewUp);&#xA;uniforms-&amp;gt;projection = AAPL::perspective_fov(fov, size.width, size.height, 0.1f, 100.f);&#xA;&#xA;uniforms-&amp;gt;model = AAPL::translate(0.f, 0.f, 12.f) * AAPL::rotate(tAngle, 0.f, 1.f, 0.f);&#xA;tAngle += 0.5f;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;transform&lt;/code&gt;, &lt;code&gt;lookAt&lt;/code&gt;, &lt;code&gt;rotate&lt;/code&gt;, and &lt;code&gt;perspective_fov&lt;/code&gt; functions are lifted straight from Apple sample code I used as reference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the shader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;typedef struct {&#xA;    float4 pos [[ position ]];&#xA;    half4 color;&#xA;    float mod;&#xA;} VertexOut;&#xA;&#xA;vertex VertexOut basic_vertex(const device packed_float3* vertex_array [[ buffer(0) ]],&#xA;                              const device packed_float3* colors [[ buffer(1) ]],&#xA;                              constant Uniforms&amp;amp; uniform [[ buffer(2) ]],&#xA;                              uint vid [[ vertex_id ]],&#xA;                              uint iid [[ instance_id ]])&#xA;{&#xA;    float4 v = float4(vertex_array[vid], 1.f);&#xA;    float4x4 mvp_matrix = uniform.projection * uniform.view * uniform.model;&#xA;&#xA;    VertexOut out;&#xA;    out.pos = v * mvp_matrix;&#xA;    uint colorIndex = vid / 3;&#xA;    out.color = half4(half3(colors[colorIndex]), 1.f);&#xA;    return out;&#xA;}&#xA;&#xA;fragment half4 basic_fragment(VertexOut f [[ stage_in ]]) {&#xA;    return f.color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help/tips would be appreciated!&lt;/p&gt;&#xA;" OwnerUserId="2507" LastEditorUserId="127" LastEditDate="2016-01-21T06:22:40.970" LastActivityDate="2016-01-21T06:22:40.970" Title="Why is rotating this prism in iOS Metal not rendering correctly?" Tags="&lt;shader&gt;&lt;3d&gt;&lt;metal&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1929" PostTypeId="2" ParentId="1928" CreationDate="2016-01-21T00:41:04.463" Score="2" Body="&lt;p&gt;The problem was in the shader, and I had the order of multiplication wrong with the MVP matrix. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;out.pos = v * mvp_matrix;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;should be: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;out.pos = mvp_matrix * v;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For whatever reason, I'm used to dealing with row vectors and had myself convinced the problem was due to the matrices themselves and the clip region..&lt;/p&gt;&#xA;" OwnerUserId="2507" LastActivityDate="2016-01-21T00:41:04.463" CommentCount="0" />
  <row Id="1930" PostTypeId="2" ParentId="1916" CreationDate="2016-01-21T06:38:18.427" Score="2" Body="&lt;p&gt;I would exclude (A) for the following reasons:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;In any modern ray-tracer, the acceleration structure (BVH, k-d tree, etc) holding your geometry is the primary source of culling. The final list of primitives to trace for a given ray may include non-visible triangles, but ideally there should be very few with good acceleration structures.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Ray tracers support many more primitives than just triangles. (How do you back-face cull a sphere?)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The following problems with back-face culling in general are also more relevant to ray tracing:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Transparent materials (such as glass) will make certain objects render incorrectly with back-face culling applied.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Non-closed meshes (like plants) need to render the back-facing triangles.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="197" LastActivityDate="2016-01-21T06:38:18.427" CommentCount="0" />
  <row Id="1931" PostTypeId="2" ParentId="1919" CreationDate="2016-01-21T07:44:20.010" Score="3" Body="&lt;p&gt;In general, the idea of importance sampling is to distribute samples in a way that matches the function being integrated (or more practically, matching some factor in it). This reduces the variance of the output values, allowing the Monte Carlo integration to converge faster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In volume scattering, when a medium is homogeneous, we have the &lt;a href=&quot;https://en.wikipedia.org/wiki/Beer%E2%80%93Lambert_law&quot; rel=&quot;nofollow&quot;&gt;Beer–Lambert law&lt;/a&gt;, which states that the transmittance falls off exponentially with depth. That's the source of factors like $e^{-\sigma x}$ in those equations. So when raymarching through a medium, the regions closer to the start of the ray generally contribute more light to the output, while the farther regions have more of their light absorbed along the way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, if you sample points along the ray from an &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_distribution&quot; rel=&quot;nofollow&quot;&gt;exponential distribution&lt;/a&gt;, the samples cluster toward the closer, more important regions. Then you just have to evaluate the scattering at those points and average over them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, importance sampling in the context of &lt;em&gt;surface&lt;/em&gt; scattering is explained pretty well by &lt;a href=&quot;http://computergraphics.stackexchange.com/a/1667/48&quot;&gt;this answer&lt;/a&gt; to another question.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;On your second question, &quot;where does single scattering fit into the general rendering equation?&quot;, well, it doesn't! What people call the &quot;general rendering equation&quot; actually only includes surface scattering, not volume scattering. So it's not really that &quot;general&quot;. A general rendering equation for volume scattering might look something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ \nabla_\omega \, L(x, \omega) = \sigma_e(x) L_e(x, \omega) - \sigma_t(x) L(x, \omega) + \sigma_s(x) \int p(\omega, \omega') \, L(x, \omega') \, d\omega' $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This states that the derivative of radiance as you move along a ray is given by the sum of emitted light from the volume, minus extinction, plus in-scattering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To combine surface and volume scattering, we usually just say that the surface version of the equation applies at points on surfaces, and the volume version applies everywhere else.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-21T07:44:20.010" CommentCount="2" />
  <row Id="1932" PostTypeId="2" ParentId="1927" CreationDate="2016-01-21T12:07:46.430" Score="1" Body="&lt;p&gt;If I understand you correctly, the problem could be defined as the (edge) intersection of a cube with a sphere. In that case, you could shade the edges by calculating the distance to the edges of the cube.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//note: from http://iquilezles.org/www/articles/distfunctions/distfunctions.htm&#xA;float udBox( vec3 p, vec3 b )&#xA;{&#xA;  return length(max(abs(p)-b,0.0));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Aligning the cube poses another &quot;interesting&quot; issue - you probably want to use the sphere-normal and screen-up to generate a basis.&lt;/p&gt;&#xA;" OwnerUserId="2463" LastActivityDate="2016-01-21T12:07:46.430" CommentCount="0" />
  <row Id="1933" PostTypeId="2" ParentId="1912" CreationDate="2016-01-21T12:20:38.830" Score="4" Body="&lt;p&gt;Based on the comment of &quot;ratchet freak&quot; I researched Transform Feedback Buffers and solved my problem that way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I now generate the simulated points on the CPU and put them into a VertexBufferObject.&#xA;I generate a second VBO for the points (along with some others for velocity).&#xA;The connectivity of the cloth is given as an vertex-attribute in ivec4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the transform feedback buffers and a Double-buffering trick using the two VBOs i can always read from the last step (using the connectivity info) and write to another buffer. This is in order to solve problems with concurrency. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The calculations are done in the vertex-shader as GL_POINTS.&#xA;Binding the output of the first shader into another regular shader using additional index-buffer to generate triangles I can without trouble render the cloth any way I want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This idea follows the transform feedback buffer example made in the book &quot;OpenGL Superbible&quot; &lt;a href=&quot;http://www.openglsuperbible.com/&quot; rel=&quot;nofollow&quot;&gt;http://www.openglsuperbible.com/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-01-21T12:20:38.830" CommentCount="0" />
  <row Id="1934" PostTypeId="1" CreationDate="2016-01-21T13:11:14.720" Score="3" ViewCount="39" Body="&lt;p&gt;I'm am currently looking into &lt;a href=&quot;https://github.com/KhronosGroup/glTF/blob/master/specification/README.md&quot; rel=&quot;nofollow&quot;&gt;glTF&lt;/a&gt; to use as a transfer format for my WebGL project. I already have the geometries I wish to render on my WebGL client available as preprocessed vertex and index buffers (and color buffers, but let's ignore them for now).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[       // vertex buffer&#xA;  0,0,0,&#xA;  0,1,0,&#xA;  1,0,0,&#xA;  1,1,0,&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And to draw simple &lt;code&gt;LINES&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[       // index buffer&#xA;  0,2,&#xA;  2,4,&#xA;  4,0,&#xA;  0,1,&#xA;  1,4,&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I wonder how to create glTF files containing these buffers? Is this possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to avoid using COLLADA and &lt;a href=&quot;https://github.com/KhronosGroup/glTF/wiki/converter&quot; rel=&quot;nofollow&quot;&gt;COLLADA2GLTF&lt;/a&gt; as this would only add an additional step and might just bloat the resulting glTF files.&lt;/p&gt;&#xA;" OwnerUserId="361" LastEditorUserId="361" LastEditDate="2016-04-19T09:08:55.813" LastActivityDate="2016-04-19T14:59:49.760" Title="Is it possible to create minimal glTF files with vertex and index buffers?" Tags="&lt;webgl&gt;&lt;vertex-buffer-object&gt;&lt;gltf&gt;&lt;collada&gt;&lt;buffers&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1935" PostTypeId="1" AcceptedAnswerId="1938" CreationDate="2016-01-21T17:47:18.833" Score="5" ViewCount="96" Body="&lt;p&gt;Is it possible that low pass filtering can be applied to Ray tracing ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My guess is that since after the algorithm runs we have an image then low pass filtering helps in order to prevent aliasing from happening. Yet, I am not sure about this. Can someone elaborate on that?&lt;/p&gt;&#xA;" OwnerUserId="2214" LastActivityDate="2016-01-22T14:28:31.440" Title="Low Pass filtering" Tags="&lt;raytracing&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1936" PostTypeId="2" ParentId="1935" CreationDate="2016-01-22T06:19:21.960" Score="5" Body="&lt;p&gt;David Kuri's answer yields a modern approach, but a simple-to-implement solution is to explicitly supersample with jittering. Classic paper: &lt;a href=&quot;http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/cook86_sampling.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/cook86_sampling.pdf&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-01-22T14:28:31.440" LastActivityDate="2016-01-22T14:28:31.440" CommentCount="5" />
  <row Id="1937" PostTypeId="1" CreationDate="2016-01-22T06:50:48.907" Score="3" ViewCount="164" Body="&lt;p&gt;I'm currently working working on a unbiased path tracer. To test how accurate it is. I created a scene in my path tracer and LuxRender then compare the rendered image. I found that the shadow in the image that my path tracer rendered is too dark. Why? Any possible reason?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I have implemented Gamma 2.2 in my renderer. So it won't be the issue of gamma correction. And, recursive path tracing IS enabled. both image are rendered in global illumination.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the rendering result from LuxRender and my own renderer.(Please ignore the back triangle(light source) in Lux. It's how Lux behaves.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;---- Edit ----&lt;br&gt;&#xA;Edit: Here are the trace() function(get radiance)&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c++ prettyprint-override&quot;&gt;&lt;code&gt;float4 Renderer::trace(Ray ray, Scene* scene)&#xA;{&#xA;    const int BOUNCE_DEPTH = getBounceDepth();&#xA;    //Russian roulette: starting at depth 5&#xA;    const int RUSSIAN_ROULETTE_START_DEPTH = 16;&#xA;    const float FACTOR_CUT_OFF = 1e-5;&#xA;&#xA;    float4 factor(1.0f,1.0f,1.0f,1.0f);&#xA;    float4 renderedColor;&#xA;    Ray currentRay = ray;&#xA;    float4 lastNormal = ray.direction;&#xA;    float rrFactor = 1.0f;&#xA;&#xA;    for(int i=0;i&amp;lt;BOUNCE_DEPTH||BOUNCE_DEPTH &amp;lt; 0;i++)//To ignore bounce limit. set bounceDepth to -1&#xA;    {&#xA;        if(i &amp;gt;= RUSSIAN_ROULETTE_START_DEPTH)&#xA;        {&#xA;            const float stopProbability = 0.1f;&#xA;            if(((float)rand()/RAND_MAX) &amp;lt; (1.0-stopProbability))&#xA;                break;&#xA;            vrrFactor *= 1.0f/(1.0f-stopProbability);&#xA;            //cout &amp;lt;&amp;lt; rrFactor &amp;lt;&amp;lt; endl;&#xA;        }&#xA;&#xA;        //stop bouncing if factor is too small.&#xA;        //This is a biased method. Remove this if you want unbiased rendering&#xA;        if(factor.x + factor.y + factor.z &amp;lt;= FACTOR_CUT_OFF)&#xA;            break;&#xA;&#xA;        RayHit hit = acclerator-&amp;gt;raycast(currentRay);&#xA;        if(hit.distance &amp;gt; 0)&#xA;        {&#xA;            int index = hit.index;&#xA;            Triangle emit(scene-&amp;gt;emitColor[scene-&amp;gt;indices[index*3]],&#xA;                scene-&amp;gt;emitColor[scene-&amp;gt;indices[index*3+1]],&#xA;                scene-&amp;gt;emitColor[scene-&amp;gt;indices[index*3+2]]);&#xA;            Triangle triangle(scene-&amp;gt;vertices[scene-&amp;gt;indices[index*3]],&#xA;                scene-&amp;gt;vertices[scene-&amp;gt;indices[index*3+1]],&#xA;                scene-&amp;gt;vertices[scene-&amp;gt;indices[index*3+2]]);&#xA;            Triangle reflectColor(scene-&amp;gt;reflectColor[scene-&amp;gt;indices[index*3]],&#xA;                scene-&amp;gt;reflectColor[scene-&amp;gt;indices[index*3+1]],&#xA;                scene-&amp;gt;reflectColor[scene-&amp;gt;indices[index*3+2]]);&#xA;&#xA;            //if(i &amp;gt; 1)//Only collect indirect lighting&#xA;            renderedColor += factor*emit.interpolation(hit.u,hit.v)*rrFactor;&#xA;            factor *= reflectColor.interpolation(hit.u,hit.v);&#xA;&#xA;            //randomly sample on a hemisphere&#xA;            Ray reflecRay;&#xA;            reflecRay = createRandomReflect(currentRay, triangle,&#xA;                currentRay.origin + normalize(currentRay.direction)*hit.distance);&#xA;&#xA;            float4 triangleNormal = triangle.normal();&#xA;            if(dot(triangleNormal,currentRay.direction) &amp;lt; 0)&#xA;                triangleNormal = -triangleNormal;&#xA;&#xA;            currentRay = reflecRay;&#xA;            lastNormal = triangle.normal();&#xA;            if(dot(currentRay.direction,lastNormal) &amp;lt; 0)&#xA;                lastNormal = -lastNormal;&#xA;            factor *= dot(normalize(currentRay.direction),lastNormal);//Difduse BRDF&#xA;&#xA;    }&#xA;    else&#xA;        break;&#xA;    }&#xA;&#xA;    return renderedColor;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the render() function (generate image)&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-c++ prettyprint-override&quot;&gt;&lt;code&gt;void Renderer::render(Scene* scene)&#xA;{&#xA;    //srand(5000);&#xA;    memset(frameBuffer,0,bufferWidth*bufferHeight*sizeof(float4));&#xA;    int pixelCount = bufferWidth*bufferHeight;&#xA;&#xA;    high_resolution_clock::time_point t1 = high_resolution_clock::now();&#xA;&#xA;    int renderIndex = 0;&#xA;&#xA;    acclerator-&amp;gt;setScene(scene);&#xA;    acclerator-&amp;gt;build();&#xA;&#xA;    const int SAMPLE_NUM = getSampleNum();&#xA;&#xA;    float4 origin = scene-&amp;gt;camera.position&#xA;        + 2.0f*scene-&amp;gt;camera.right/(float)scene-&amp;gt;camera.width&#xA;        + 2.0f*scene-&amp;gt;camera.up/(float)scene-&amp;gt;camera.height;&#xA;&#xA;    #pragma omp parallel for schedule(dynamic)&#xA;    for(int i=0;i&amp;lt;pixelCount;i++)&#xA;    {&#xA;        float4 renderedColor = float4(0.0f,0.0f,0.0f,0.0f);&#xA;        //Create camera ray&#xA;        int x = i % bufferWidth;&#xA;        int y = i / bufferWidth;&#xA;&#xA;        float4 rayVec = (x/(float)scene-&amp;gt;camera.width-0.5f)*scene-&amp;gt;camera.right*scene-&amp;gt;camera.pixelAspectRatio&#xA;            + (0.5f-y/(float)scene-&amp;gt;camera.height)*scene-&amp;gt;camera.up&#xA;            + scene-&amp;gt;camera.direction;&#xA;        for(int j=0;j&amp;lt;SAMPLE_NUM;j++)&#xA;        {&#xA;            Ray ray(origin, rayVec);&#xA;            renderedColor += trace(ray,scene);&#xA;        }&#xA;        renderedColor /= (float)SAMPLE_NUM;&#xA;&#xA;        //simple gamma correction&#xA;        const float gamma = 2.2;&#xA;        const float inversedGamma = 1.0f/gamma;&#xA;        float4 correctedColor = pow(renderedColor,float4(inversedGamma,inversedGamma,inversedGamma,0));&#xA;        ((float4*)frameBuffer)[i] = correctedColor;&#xA;&#xA;&#xA;        #pragma omp atomic&#xA;            renderIndex++;&#xA;&#xA;        #pragma omp critical&#xA;        if(renderIndex%500 == 0)&#xA;            cout &amp;lt;&amp;lt; &quot;Rendering &quot; &amp;lt;&amp;lt; renderIndex &amp;lt;&amp;lt; &quot;/&quot; &amp;lt;&amp;lt; pixelCount &amp;lt;&amp;lt; &quot;. &quot; &amp;lt;&amp;lt; (float)renderIndex*100.0/pixelCount &amp;lt;&amp;lt; &quot;\r&quot;;&#xA;    }&#xA;&#xA;    high_resolution_clock::time_point t2 = high_resolution_clock::now();&#xA;    double elapsed = duration_cast&amp;lt;duration&amp;lt;double&amp;gt;&amp;gt;( t2 - t1 ).count();&#xA;    cout &amp;lt;&amp;lt; endl &amp;lt;&amp;lt; &quot;elapsed: &quot; &amp;lt;&amp;lt; elapsed &amp;lt;&amp;lt; &quot; ,&quot; &amp;lt;&amp;lt; pixelCount*SAMPLE_NUM/elapsed/1000/1000 &amp;lt;&amp;lt; &quot; MSamples/s&quot;&amp;lt;&amp;lt; endl;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;---- End Edit ----&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Scene Rendered by LuxRender (PBRT gives me the same result)&lt;br&gt;&#xA;Renderer: Sampler, Sampler : metropolis, (Other settings are defalut)&#xA;&lt;a href=&quot;http://i.stack.imgur.com/tsd5v.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tsd5v.png&quot; alt=&quot;Scene Rendered by LuxRender&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same scene rendered my by renderer. Note the scene is a bit darker and shadow is way too dark.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/A2knm.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/A2knm.png&quot; alt=&quot;Same Scene rendered my by renderer&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2448" LastEditorUserId="2448" LastEditDate="2016-01-22T15:12:34.297" LastActivityDate="2016-01-28T13:24:04.733" Title="Shadow appears to be too dark in my ray tracer" Tags="&lt;raytracing&gt;&lt;rendering&gt;" AnswerCount="1" CommentCount="7" FavoriteCount="1" />
  <row Id="1938" PostTypeId="2" ParentId="1935" CreationDate="2016-01-22T10:47:42.003" Score="11" Body="&lt;p&gt;Low-pass filtering is a classic tool from signal theory that will effectively remove noise, as you suggested, but will also cancel out desired high-frequency information in the image such as sharp edges. The image will look blurry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Post-filtering Monte Carlo rendering results is an open field of research and many advances have been made over the years, and overview of which can be found in the &lt;em&gt;Denoising Your Monte Carlo Renders: Recent Advances in Image-Space Adaptive Sampling and Reconstruction&lt;/em&gt; SIGGRAPH Course &lt;a href=&quot;http://cgg.unibe.ch/publications/denoising-your-monte-carlo-renders&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2016-01-22T10:47:42.003" CommentCount="0" />
  <row Id="1939" PostTypeId="1" AcceptedAnswerId="1968" CreationDate="2016-01-22T13:10:24.747" Score="4" ViewCount="106" Body="&lt;p&gt;I writing the this video process app which requires the each video frame to be processed such that parts of the video are enlarged. As a reference, one of the snapchat lens filter enlarges the eyes of a person in the video. This is the exact effect I want to achieve. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I achieve this effect?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have attached an example of eye enlargement from the web.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/XESgE.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/XESgE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2520" LastEditorUserId="2520" LastEditDate="2016-01-23T04:52:36.790" LastActivityDate="2016-01-26T15:37:28.180" Title="How enlarge parts of a video like snapchat lens filter" Tags="&lt;image-processing&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="1940" PostTypeId="1" CreationDate="2016-01-22T14:41:41.680" Score="1" ViewCount="23" Body="&lt;p&gt;I'm drawing a contour map representing gravity. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The algorithm is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For each point on the screen (1 pixel per point) I calculate the gravitational pull for each object and combine (using Newtons law of universal gravitation) where M1 is a unit point mass.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next I get the min and max values and create a palette of 10 colors scaled over this range.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then generate a texture from this data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For resolutions such as 1024 * 768 or greater this is expensive (takes &gt; 1sec)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd like to be able to move the masses and see the contour map change in real time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions on how to optimise?&lt;/p&gt;&#xA;" OwnerUserId="2524" LastActivityDate="2016-01-22T14:41:41.680" Title="Optimise drawing contour maps representing gravity" Tags="&lt;optimisation&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="1941" PostTypeId="2" ParentId="96" CreationDate="2016-01-22T14:59:56.743" Score="5" Body="&lt;p&gt;There are several offerings by GPU vendors like AMD's &lt;a href=&quot;http://developer.amd.com/tools-and-sdks/opencl-zone/codexl/&quot; rel=&quot;nofollow&quot;&gt;CodeXL&lt;/a&gt; or NVIDIA's &lt;a href=&quot;https://www.nvidia.com/object/nsight.html&quot; rel=&quot;nofollow&quot;&gt;nSight&lt;/a&gt;/&lt;a href=&quot;https://developer.nvidia.com/linux-graphics-debugger&quot; rel=&quot;nofollow&quot;&gt;Linux GFX Debugger&lt;/a&gt; which allow stepping through shaders but are tied to the respective vendor's hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let me note that, although they are available under Linux, I always had very little success with using them there. I can't comment on the situation under Windows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The option which I have come to use recently, is to modularize my shader code via &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/100/sharing-code-between-multiple-glsl-shaders&quot;&gt;&lt;code&gt;#includes&lt;/code&gt;&lt;/a&gt; and restrict the included code to a common subset of GLSL and C++&amp;amp;&lt;a href=&quot;http://glm.g-truc.net&quot; rel=&quot;nofollow&quot;&gt;glm&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I hit a problem I try to reproduce it on another device to see if the problem is the same which hints at a logic error (instead of a driver problem/undefined behavior). There is also the chance of passing wrong data to the GPU (e.g. by incorrectly bound buffers etc.) which I usually rule out either by output debugging like in &lt;a href=&quot;https://computergraphics.stackexchange.com/a/101&quot;&gt;cifz answer&lt;/a&gt; or by inspecting the data via &lt;a href=&quot;https://apitrace.github.io/&quot; rel=&quot;nofollow&quot;&gt;apitrace&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When it is a logic error I try to rebuild the situation from the GPU on CPU by calling the included code on CPU with the same data.&#xA;Then I can step through it on CPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Building upon the modularity of the code you can also try to write unittest for it and compare the results between a GPU run and a CPU run. However, you have to be aware that there are corner cases where C++ might behave differently than GLSL, thus giving you false positives in these comparisons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, when you can't reproduce the problem on another device, you can only start to dig down where the difference comes from. Unittests might help you to narrow down where that happens but in the end you will probably need to write out additional debug information from the shader like in &lt;a href=&quot;https://computergraphics.stackexchange.com/a/101&quot;&gt;cifz answer&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And to give you an overview here is a flowchart of my debugging process:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/S1jUQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/S1jUQ.png&quot; alt=&quot;Flow chart of the procedure described in the text&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To round this off here is a list of random pros and cons:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;pro&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;step through with usual debugger&lt;/li&gt;&#xA;&lt;li&gt;additional (often better) compiler diagnostics&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;con&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;subtle differences in some mathematic primitives (e.g. &lt;a href=&quot;https://stackoverflow.com/questions/31915601/pow1-inf-nan&quot;&gt;different return values in corner cases&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;additional code to reproduce call environment on CPU&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2521" LastEditorUserId="2521" LastEditDate="2016-01-26T11:23:57.880" LastActivityDate="2016-01-26T11:23:57.880" CommentCount="0" />
  <row Id="1943" PostTypeId="2" ParentId="1939" CreationDate="2016-01-22T16:25:27.713" Score="1" Body="&lt;p&gt;In a post processing step instead of sampling the origin pixel directly from where the target pixel is you instead sample the origin pixel slightly offset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where you get the offset can differ. For example you could provide an extra texture with the offsets encoded in the red and green channels.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-01-22T16:25:27.713" CommentCount="0" />
  <row Id="1945" PostTypeId="1" AcceptedAnswerId="1946" CreationDate="2016-01-22T19:21:10.987" Score="1" ViewCount="66" Body="&lt;p&gt;In a ray tracer, given a point on a sphere (point_of_intersection with a ray) and its normal for that point (point_of_intersection - center_of_sphere) &lt;strong&gt;how do I calculate the tangent space for that point? Do I need other data to calculate the tangent space?&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="2237" LastActivityDate="2016-01-22T20:41:40.293" Title="Ray tracing - tangent space for a point on a sphere" Tags="&lt;raytracing&gt;&lt;maths&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1946" PostTypeId="2" ParentId="1945" CreationDate="2016-01-22T20:15:57.553" Score="3" Body="&lt;p&gt;The tangent space is spanned by the tangent to the point and the bitangent (which is orthogonal to both tangent and normal).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So you need to calculate the tangent which is achieved by calculating the cross-product of the ray-direction and the normal. $T = N \times DIR$&#xA;The resulting vector will be orthogonal to the normal and thereby be the tangent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now calculate the cross-product of the tangent and the normal $BT = T \times N$ to create a vector orthogonal to both. This vector is the bitangent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tangent $T$ and bitangent $BT$ span a plane which is the tangent-space of your intersection-point.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="127" LastEditDate="2016-01-22T20:41:40.293" LastActivityDate="2016-01-22T20:41:40.293" CommentCount="4" />
  <row Id="1947" PostTypeId="1" CreationDate="2016-01-23T14:39:03.033" Score="3" ViewCount="127" Body="&lt;p&gt;For each of major vendor's latest GPU architectures, is there a clear maximum &quot;triangles/second&quot; bottleneck? If so, what is it architecturally and what is the performance?&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-01-26T00:17:15.313" Title="GPU Architecture: What are the theoretical limits on triangle throughput in modern GPUs?" Tags="&lt;gpu&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1948" PostTypeId="2" ParentId="1947" CreationDate="2016-01-23T15:42:58.707" Score="2" Body="&lt;p&gt;Your question is not that easy to answer, as there is no standard definition of what &quot;triangles/second&quot; actually means.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What shading is used? What calculations are done? How are the triangles represented? This and much more has to be taken into account when looking for benchmarks on GPU-performance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To actually compare the performance of GPUs you would have to look at a range of different measurements. GFlop/s and memory-speed as some basic ones, but also look at full benchmark scores (of which there are I am sure many).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are websites that compile these measurements and display them in a comparison mode. A quick search brought me to &lt;a href=&quot;http://gpuboss.com/gpus/GeForce-GTX-760-vs-GeForce-GTX-750&quot; rel=&quot;nofollow&quot; title=&quot;gpuboss.com/gpus/GeForce-GTX-760-vs-GeForce-GTX-750&quot;&gt;this page which seems to have everything needed&lt;/a&gt; (I entered an example search).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This helps to compare the performance of two GPUs but if you want to know if a specific GPU is able to do the specific thing you want it to do, that may be very hard to answer and impossible without much more knowledge about what you want to do.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Calculating the maximum performance is also very hard because I believe the GPU vendors do not share everything they do on their GPUs so it can be hard to estimate the actual performance.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="231" LastEditDate="2016-01-24T12:17:00.900" LastActivityDate="2016-01-24T12:17:00.900" CommentCount="1" />
  <row Id="1950" PostTypeId="1" AcceptedAnswerId="1951" CreationDate="2016-01-23T22:59:21.727" Score="3" ViewCount="60" Body="&lt;p&gt;I am facing a though challange - I need to perform repetitive Image formating. Firstly, I need resize picture A so that it has some fixed width in pixels. Then, I need to place picture A in the middle of picture B. I need to perform this task repetitively for hundreds of images. Is there any way to script this, so that I can just type in for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;perform pictureA.png pictureB.png output.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help is more than welcome!&lt;/p&gt;&#xA;" OwnerUserId="2537" LastActivityDate="2016-01-23T23:59:51.367" Title="Is there a way to script image creation?" Tags="&lt;transformations&gt;&lt;image&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1951" PostTypeId="2" ParentId="1950" CreationDate="2016-01-23T23:59:51.367" Score="8" Body="&lt;p&gt;&lt;a href=&quot;http://www.imagemagick.org&quot;&gt;ImageMagick&lt;/a&gt; is a set of command-line tools that can do the sort of things you describe. For example, this command line will overlay picture B with a centered copy of picture A, resized to 100 pixels wide (keeping aspect ratio):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;convert pictureB.png ( pictureA.png -resize 100 ) -gravity center -composite output.png&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&quot;http://www.imagemagick.org/script/command-line-processing.php&quot;&gt;the ImageMagick docs&lt;/a&gt; for more info about its command lines are structured...it's a powerful piece of software, but it takes a bit of work to get the hang of how to use it.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-23T23:59:51.367" CommentCount="6" />
  <row Id="1952" PostTypeId="1" CreationDate="2016-01-24T13:34:51.840" Score="1" ViewCount="25" Body="&lt;p&gt;Raster scan display forms an image with pixel whereas a Random scan display works with Geometric primitives, but even a geometric primitve( a line) is made up of pixels so what is the difference?&lt;/p&gt;&#xA;" OwnerUserId="2540" LastEditorUserId="2540" LastEditDate="2016-01-24T14:38:09.480" LastActivityDate="2016-01-26T18:38:52.100" Title="How is Raster and Random Scan Display different?" Tags="&lt;raster-image&gt;&lt;display-lists&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1953" PostTypeId="2" ParentId="1952" CreationDate="2016-01-24T13:51:51.840" Score="1" Body="&lt;p&gt;A raster display draws every pixel on the screen in every frame whether there is something to show or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A random scan display activates only those pixel which are occupied by an geometric primitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So yes, they both use pixels, but the difference is in how they draw the pixels onto the screen.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-01-24T13:51:51.840" CommentCount="2" />
  <row Id="1954" PostTypeId="1" AcceptedAnswerId="1955" CreationDate="2016-01-24T21:15:17.677" Score="8" ViewCount="113" Body="&lt;p&gt;Right now I'm trying to implement some sort of depth buffer in software and I have a huge problem when I'm writing to it. Having one mutex is absolute overkill. So I created a number of mutexes equal to the number of threads. I'm locking a mutex based on current pixel (pixel_index % mutexes_number) and this works better, but still very very slow. And I wonder how it's done in a real GPU? Is there a clever algorithm or hardware handles it?&lt;/p&gt;&#xA;" OwnerUserId="386" LastActivityDate="2016-01-25T01:50:49.570" Title="How updating a depth buffer works in GPU?" Tags="&lt;depth-buffer&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1955" PostTypeId="2" ParentId="1954" CreationDate="2016-01-24T22:50:29.833" Score="8" Body="&lt;p&gt;Highly specialized hardware handles it. A typical strategy is for a GPU to tile rasterization and store depth information in compressed formats (e.g. the z-equation when a polygon completely covers a tile). This allows testing across an entire tile at once; other cool HW tricks include depth testing before the pixel shader is run (assuming conditions permit - the shader cannot write a depth value). You might consider something similar in software, such as having each thread &quot;own&quot; a subset of tiles and walk each primitive independently, or mimic multi-gpu strategies such as alternate frames or alternate raster lines.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-01-24T22:50:29.833" CommentCount="0" />
  <row Id="1956" PostTypeId="2" ParentId="1954" CreationDate="2016-01-25T01:50:49.570" Score="7" Body="&lt;p&gt;In a real GPU, instead of having multiple cores trying to read/write the same region of the depth buffer and attempting to synchronize between them, the depth buffer is divided into tiles (such as 16×16 or 32×32), and each tile is assigned to a single core. That core is then responsible for all rasterization in that tile: any triangles that touch that tile will be rasterized (within that tile) by the owning core. Then there is no interference between cores, and no need for them to synchronize when accessing their part of the framebuffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This implies that triangles that touch multiple tiles will need to be rasterized by multiple cores. So, there is a work redistribution step between geometry processing (operations on vertices and triangles) and pixel processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the geometry stage, each core might process a chunk of input primitives; then for each primitive, it can quickly determine which tiles the primitive touches (this is called &quot;coarse rasterization&quot;), and add the primitive to a queue for each core that owns one of the affected tiles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, in the pixel stage, each core can read out the list of primitives in its queue, calculate pixel coverage for the tiles the core owns, and proceed to depth testing, pixel shading and updating the framebuffer, with no need of any further coordination with other cores.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-25T01:50:49.570" CommentCount="0" />
  <row Id="1957" PostTypeId="1" AcceptedAnswerId="1958" CreationDate="2016-01-25T06:33:01.137" Score="4" ViewCount="129" Body="&lt;p&gt;[Please (kindly) let me know if this is not the place for this question]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm hoping to learn about GPU programming, but as a college student, I've a quite limited budget.  I'm wondering whether I should buy a modestly more expensive GPU that uses GDDR5 memory, or if DDR3 memory is sufficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can see some potential pros and cons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GDDR5 is standard in nearly all but the lowest-end GPUs, so I'd be getting experience that would transfer to most popular cards&lt;/li&gt;&#xA;&lt;li&gt;However, is it possible that learning with DDR3 memory will only help me learn to be more efficient, thus my skills would translate (probably with a bit more difficulty) to even better programming abilities on a higher-end GPU&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any advice or words of wisdom?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that I don't want the card for gaming purposes (although I would gladly use it in such a capacity, but that would be a happy by-product of having it around to learn to program on).  I also don't really care too much about being able to run programs faster on the GPU; I mainly want the ability to learn how to program on a GPU.  I'm currently considering only NVIDIA GPUs, as I've learned a smidgen about CUDA programming.&lt;/p&gt;&#xA;" OwnerUserId="1870" LastActivityDate="2016-01-25T07:34:55.527" Title="GDDR5 vs. DDR3 for learning GPU programming on a limited budget?" Tags="&lt;gpu&gt;&lt;memory&gt;&lt;cuda&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="1958" PostTypeId="2" ParentId="1957" CreationDate="2016-01-25T07:34:55.527" Score="3" Body="&lt;p&gt;If your goal is to learn GPU programming, it doesn't matter at all whether you have DDR3 or GDDR5 memory. The way you program it isn't going to change based on how fast the memory is. It will affect performance, but if that's not a primary consideration for you, then you don't need to worry about it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do make sure that you get a GPU that supports the latest graphics APIs (DX12 and OpenGL 4.5). I think that's essentially all GPUs made in the past 5 years or so.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-25T07:34:55.527" CommentCount="3" />
  <row Id="1959" PostTypeId="1" CreationDate="2016-01-25T09:04:13.653" Score="7" ViewCount="61" Body="&lt;p&gt;I'm implementing &lt;a href=&quot;http://mrl.nyu.edu/~perlin/noise/&quot;&gt;improved Perlin noise&lt;/a&gt;. Its key feature for randomisation is the hardcoded permutation table, which gives essentially random but reproducible gradients at the cells of the grid. The permutation table is just a permutation of the integers &lt;code&gt;0..255&lt;/code&gt;, and is usually the following table (copied straight from Perlin's original implementation):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{151, 160, 137, 91, 90, 15, 131, 13, 201, 95, 96, 53, 194, 233, 7,&#xA;225, 140, 36, 103, 30, 69, 142, 8, 99, 37, 240, 21, 10, 23, 190, 6, 148, 247,&#xA;120, 234, 75, 0, 26, 197, 62, 94, 252, 219, 203, 117, 35, 11, 32, 57, 177, 33,&#xA;88, 237, 149, 56, 87, 174, 20, 125, 136, 171, 168, 68, 175, 74, 165, 71, 134,&#xA;139, 48, 27, 166, 77, 146, 158, 231, 83, 111, 229, 122, 60, 211, 133, 230, 220,&#xA;105, 92, 41, 55, 46, 245, 40, 244, 102, 143, 54, 65, 25, 63, 161, 1, 216, 80,&#xA;73, 209, 76, 132, 187, 208, 89, 18, 169, 200, 196, 135, 130, 116, 188, 159, 86,&#xA;164, 100, 109, 198, 173, 186, 3, 64, 52, 217, 226, 250, 124, 123, 5, 202, 38,&#xA;147, 118, 126, 255, 82, 85, 212, 207, 206, 59, 227, 47, 16, 58, 17, 182, 189,&#xA;28, 42, 223, 183, 170, 213, 119, 248, 152, 2, 44, 154, 163, 70, 221, 153, 101,&#xA;155, 167, 43, 172, 9, 129, 22, 39, 253, 19, 98, 108, 110, 79, 113, 224, 232,&#xA;178, 185, 112, 104, 218, 246, 97, 228, 251, 34, 242, 193, 238, 210, 144, 12,&#xA;191, 179, 162, 241, 81, 51, 145, 235, 249, 14, 239, 107, 49, 192, 214, 31, 181,&#xA;199, 106, 157, 184, 84, 204, 176, 115, 121, 50, 45, 127, 4, 150, 254, 138, 236,&#xA;205, 93, 222, 114, 67, 29, 24, 72, 243, 141, 128, 195, 78, 66, 215, 61, 156, 180};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For reference, a small patch drawn from the noise generated by this table looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Gyuqs.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Gyuqs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I would like the code to be a bit more flexible and allow this table to be reshuffled so that I can create a completely new noise field (instead of just sampling it at a different offset). But not all permutations are equally well shuffled. In the unlikely event that the random permutation is just the sorted array from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;255&lt;/code&gt;, the noise would look like this instead:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/3uC8M.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3uC8M.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's kinda bad. Of course, at a chance of $1$ in $256!$, this is not a case I need to be worried about. But surely, this is not the only permutation that yields very noticeable artefacts. Reverse sorted and almost sorted permutations would likely have the same problems. So how many other permutations are unsuitable? Say the code would be used in a popular game to generate a random world up front, it would still be annoying if every 100,000th generated world would look remotely regular.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is, what exactly makes a good (or a bad) permutation table, and how do I assess the quality of a permutation table programmatically, such that I can reshuffle the table once more in the unlikely event that I roll a &quot;bad&quot; table?&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2016-01-25T09:11:09.293" LastActivityDate="2016-01-25T14:06:52.043" Title="What makes a good permutation table?" Tags="&lt;noise&gt;&lt;maths&gt;&lt;procedural-generation&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="1960" PostTypeId="2" ParentId="1959" CreationDate="2016-01-25T11:25:51.673" Score="3" Body="&lt;p&gt;&lt;strong&gt;First of all&lt;/strong&gt; - a number must not occur twice, that is implied since we're talking about permutations. So filling the table with a simple random(255) function won't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Secondly&lt;/strong&gt;, you need to ensure that there are no premature recurrence patterns:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the values 1,2,3,4 - the permutation table 4,3,2,1 is not a very good one because of its short cyclic properties, i.e. 1 -&gt; 4, 4 -&gt; 1. Likewise with 4,2,3,1 or 1,2,3,4. The optimal tables take you all the way through all positions: 3,1,4,2 or 2,4,1,3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This property becomes increasingly  important as you increase the number of dimensions and perform recursive lookups.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However this approach alone may create clusters of too similar values, which may or may not be wanted, which leads me to next point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Thirdly&lt;/strong&gt;, When you generate a table with the non cyclic properties, you need to step through remaining unassigned indices in a random manner. When possible constrain the random step distance here to a certain min and max range, e.g. 5..120 to avoid clustered groups of similar values. These numbers are worth experimenting with. &lt;/p&gt;&#xA;" OwnerUserId="1790" LastEditorUserId="1790" LastEditDate="2016-01-25T11:51:00.367" LastActivityDate="2016-01-25T11:51:00.367" CommentCount="2" />
  <row Id="1961" PostTypeId="2" ParentId="1959" CreationDate="2016-01-25T13:09:10.607" Score="1" Body="&lt;p&gt;One possibility might be to borrow from the cryptographic community and, in particular, the 8-bit to 8-bit substitution used in the AES/Rijndael cipher. &#xA;The table and code to generate it &lt;a href=&quot;https://en.wikipedia.org/wiki/Rijndael_S-box&quot; rel=&quot;nofollow&quot;&gt;can be found on wikipedia.&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd guess that, in order to generate up to 256 additional tables, you could just do something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Func(U8 input, U8 TableNum) = SBox( (TableNum + Sbox(input)) Mod256 )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;(since the SBox function is quite non-linear)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having said that, (and please forgive me if I've got some of the details wrong) in a past life I implemented Perlin noise using a relatively simple RNG/Hash function but found the correlation in X/Y/Z due to my simple mapping of 2 or 3 dimensions to a scalar value was problematic.  I found that a very simple fix was just to use a CRC, eg. &lt;em&gt;something like&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;InputToRNGHash = CRC(Z xor CRC( Y xor CRC(X))). &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Given CRCs intrinsics may be built in to CPU HW, this can be fast approach.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-01-25T14:06:52.043" LastActivityDate="2016-01-25T14:06:52.043" CommentCount="0" />
  <row Id="1962" PostTypeId="1" AcceptedAnswerId="1964" CreationDate="2016-01-25T19:25:52.957" Score="6" ViewCount="46" Body="&lt;p&gt;If I have a &lt;code&gt;std::vector&amp;lt;Foo&amp;gt;&lt;/code&gt; and want to store its size in a &lt;code&gt;GLuint&lt;/code&gt;, my compiler gives a warning that there is possible loss of data.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLuint size = vec.size(); // &quot;conversion from 'size_t' to 'GLuint', possible loss of data&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I correctly do this type of casting?&#xA;Are there more pitfalls of this kind with for example &lt;code&gt;float&lt;/code&gt; to &lt;code&gt;GLfloat&lt;/code&gt;?&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="16" LastEditDate="2016-01-25T19:31:30.200" LastActivityDate="2016-01-25T22:47:38.023" Title="Type conversion to GL-types" Tags="&lt;opengl&gt;&lt;c++&gt;&lt;types&gt;&lt;coding-style&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1963" PostTypeId="1" CreationDate="2016-01-25T19:39:23.373" Score="9" ViewCount="120" Body="&lt;p&gt;Assume there is some value &lt;code&gt;p&lt;/code&gt;, calculated per-frame, that varies continuously over the surface of an object. The value of &lt;code&gt;p&lt;/code&gt; determines the density of some pattern on the surface. For example, in a case with only two possible densities if &lt;code&gt;p &amp;lt; 0.3&lt;/code&gt; it is high density, otherwise it is low.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've thought of a simple high-level solution: create two textures, each of different densities, and based on the value of &lt;code&gt;p&lt;/code&gt; sample from the appropriate one. However, there's a problem with the boundary between high- and low-densities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example to illustrate the problem (note my problem is NOT exclusive to this example pattern of dots. I describe the patterns I'm working with later on):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/krVHY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/krVHY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here is the threshold between low- and high- (displayed on the high density texture but that's not relevant.) If under the line, it implies the high-density texture should be sampled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/6WmhA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/6WmhA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And finally here is the comparison between what is desired and what would actually happen using this method:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/DFsgY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/DFsgY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that when a high-density-only circle crosses the line, it will be ignored when &lt;code&gt;p&lt;/code&gt; indicates the low-density texture to be sampled, resulting in a truncated circle. I don't know how to solve this problem because &lt;code&gt;p&lt;/code&gt; varies every frame, so I can't just 'bake' a boundary between the two densities. It's easy to prevent the reverse problem by creating the high-density texture from the lower one (i.e. if a circle is on the low-density texture ensure it is on the high-density texture.) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm interested if anyone has a way to improve my solution or even has another method entirely. The constraint here is that &lt;code&gt;p&lt;/code&gt; is calculated per-frame in real-time. Another constraint is related to the pattern texture: the pattern is black and white, where black is the pattern and white is the background (like the circles in the example). The pattern may not just be identical shapes repeated, but any arrangement of arbitrary black shapes over a white background. (Maybe pattern is the wrong choice of word.)   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not familiar with the research in this field so I wasn't sure which keywords to search, so I'd even appreciate if anyone could point me in the right direction.&lt;/p&gt;&#xA;" OwnerUserId="31" LastEditorUserId="31" LastEditDate="2016-01-27T04:24:43.543" LastActivityDate="2016-03-14T10:32:47.963" Title="How to create a patterned object with variable pattern density in real-time?" Tags="&lt;texture&gt;&lt;shader&gt;&lt;real-time&gt;" AnswerCount="2" CommentCount="8" FavoriteCount="0" />
  <row Id="1964" PostTypeId="2" ParentId="1962" CreationDate="2016-01-25T21:35:09.860" Score="7" Body="&lt;p&gt;Practically speaking, probably not - try printing the size of each type using sizeof. Probably GLuint is 32 bits and size_t is 64 bits - so 32 bits are thrown out; but float and GLfloat are probably both 32 bit IEEE 754 numbers. &quot;Probably&quot; sprinkled above, because technically speaking the sizes of C ints, floats and the float format are compiler dependent, but GL type sizes (and formats) should be standard (see &lt;a href=&quot;https://www.opengl.org/wiki/OpenGL_Type&quot;&gt;https://www.opengl.org/wiki/OpenGL_Type&lt;/a&gt;). Chances are, GLfloat is just typedef'd as a float.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may be able to turn off this warning in your compiler - I choose to explicitly cast (&quot;GLuint size = GLuint(vec.size())&quot; works) so I am reminded of what's happening.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-01-25T22:47:38.023" LastActivityDate="2016-01-25T22:47:38.023" CommentCount="0" />
  <row Id="1965" PostTypeId="2" ParentId="1947" CreationDate="2016-01-26T00:17:15.313" Score="3" Body="&lt;p&gt;NV Maxwell: According to the NVIDIA GeForce GTX 680 white paper, each Streaming Multiprocessor (called an SMX in Kepler) in the Kepler architecture could process 1 polygon every other clock. The 680 has 8 SMX units, yielding 4 polygons/clock. The NVIDIA GeForce GTX 980 white paper only mentions that geometry performance of 980 should be double that of the 680; the 980 has twice as many Streaming Multiprocessors (called a SMM in Maxwell), implying each Maxwell SMM has the same polygon throughput per clock as a Kepler SMX. The number of SMMs in a particular GPU isn't always easy to find, but the number of cuda cores is usually publicized; since Maxwell has 128 cuda cores per SMM (there are 192 in a Kepler SMX), we can conclude that Maxwell based GPUs can handle 1 polygon/clock for each 256 cuda cores.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-01-26T00:17:15.313" CommentCount="0" />
  <row Id="1966" PostTypeId="2" ParentId="1916" CreationDate="2016-01-26T14:31:32.670" Score="0" Body="&lt;p&gt;While the previous answer gave some reasons, I feel that it missed the core point of why back-face culling is not a well-suited optimization for ray tracing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's take a look at &lt;strong&gt;rasterization&lt;/strong&gt;. For each (potentially) visible triangle, we project it using a matrix multiplication and, if it occupies any pixels, try to paint it while comparing depth values, which need to be interpolated first. By testing the winding order once early on, we can save a lot of operations down the line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, in &lt;strong&gt;ray tracing&lt;/strong&gt;, you'd typically trace rays individually.&lt;sup&gt;1&lt;/sup&gt; That said, when a ray finds a potentially intersecting triangle, the vertex data would need to be loaded from memory (which often is the bottleneck) and only then could backface culling happen. You'll save some cycles for finding the intersection point in case you actually hit a backface, at the cost of a 'useless' test for &lt;strong&gt;every single ray&lt;/strong&gt; hitting a front-face (which, arguably, will be a lot more).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; In fact, this can be called the main difference: in rasterization, &lt;em&gt;triangles don't know about each other&lt;/em&gt;, while in ray tracing, &lt;em&gt;rays don't know about each other&lt;/em&gt;. This makes the global illumination problem (i.e. a problem where different surfaces interact with each other) very hard for rasterization, while interactions between different rays would be hard for ray tracing. Good thing we don't observe a lot of those ;-)&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2016-01-26T14:31:32.670" CommentCount="0" />
  <row Id="1967" PostTypeId="1" AcceptedAnswerId="1971" CreationDate="2016-01-26T14:39:09.130" Score="0" ViewCount="61" Body="&lt;p&gt;I have a video that record the drawing sequence with a pencil by a painter.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/D9UbO.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/D9UbO.png&quot; alt=&quot;video screen&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to reconstruction the sequence from the video.&#xA;And I have google some keywords like &lt;em&gt;drawing sequence reconstruction&lt;/em&gt;, &lt;em&gt;sketch reconstruction&lt;/em&gt;, and have searched like &lt;em&gt;moving object detection&lt;/em&gt; but cannot found what I want to find.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to know how to detect and recognize the drawing between different frames. I have thought the &lt;em&gt;line detect&lt;/em&gt; algorithm may work but fail. And pixel comparison seems to be bad idea. Because that the new drawing may overlap the old ones. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you give me some information that I can read or some keywords to search? Sorry about that because I am not familiar with Computer Image Processing.&lt;/p&gt;&#xA;" OwnerUserId="2551" LastEditorUserId="2041" LastEditDate="2016-01-27T08:58:35.977" LastActivityDate="2016-01-27T08:58:35.977" Title="reconstruction drawing sequence from video" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="1968" PostTypeId="2" ParentId="1939" CreationDate="2016-01-26T15:37:28.180" Score="3" Body="&lt;p&gt;The answer is surprisingly simple: You move the vertices of the face-geometry.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To elaborate, the white wireframe mesh on the left of your image is placed so the geometry aligns with the photo. It is then texture-mapped with the photo, meaning that for each vertex in the mesh, a texture-coordinate is assigned (the point on the photo that should map to the vertex).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With every vertex assigned a texture-coordinate you can deform the mesh, doing ordinary texture-sampling at the interpolated texture-coordinates to have the image stretch - as if it was painted onto the head.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need to do this in realtime, you can use your graphics-card to do this; texture mapping is one of the things GPUs are really good at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, if you want to do this on a video, you need to track the facial features to continuously align the geometry to the video. This is not an easy task, but you can probably find a wealth of libraries to help you out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am a bit uncertain which of the above would be difficult, so please feel free to ask any questions.&lt;/p&gt;&#xA;" OwnerUserId="2463" LastActivityDate="2016-01-26T15:37:28.180" CommentCount="0" />
  <row Id="1969" PostTypeId="2" ParentId="1952" CreationDate="2016-01-26T18:25:00.073" Score="0" Body="&lt;p&gt;True random scan displays aren't used anymore. They were based on cathode-ray tubes where the electron beam wasn't driven in a scanline pattern but as arbitrary line segments, with &quot;direct access&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The early devices were similar to oscilloscopes, requiring frequent refreshing of the trace. Later appeared the memory devices, able to retain the image without refreshing, so that they could render arbitrarily complex drawings (with the drawback that modifying a part of the image wasn't possible).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They indeed didn't really suffer from jaggies, as they were analogic devices, anyway because of digital-to-analog limitations, they had a finite resolution in practice (typically 1K x 1K, a wonder at the time). They allowed monochrome line drawings only.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These dinosaurs have been superseded by digital frame buffers and raster displays. But sometimes the concepts of random scan and display list are mixed up.&lt;/p&gt;&#xA;" OwnerUserId="1703" LastEditorUserId="1703" LastEditDate="2016-01-26T18:38:52.100" LastActivityDate="2016-01-26T18:38:52.100" CommentCount="0" />
  <row Id="1970" PostTypeId="1" CreationDate="2016-01-26T20:35:45.223" Score="0" ViewCount="18" Body="&lt;p&gt;I want to bind information about lightsources in my scene as a uniform buffer with std140 layout.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When setting up the shader which compiles fine I get an error when asking for the indices of the buffer elements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But my shader works as expected. I have access to valid light-sources and everything goes as expected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why do I get an invalid index?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below pasted all code that has to do with the uniform buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;FragmentShader:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 420 core&#xA;layout(binding =0) uniform sampler2D texture;&#xA;struct Light&#xA;{&#xA;    vec3 position;&#xA;    float attenuation;&#xA;    vec3 direction;&#xA;    float coneAngle;&#xA;    vec3 intensity;&#xA;    float ambientCoefficient;&#xA;    float light_type;&#xA;    float padding1;&#xA;    float padding2;&#xA;    float padding3;&#xA;};&#xA;&#xA;layout(std140) uniform Lights{&#xA;    Light lightArray[10];&#xA;    float numLights;&#xA;    float lightsPadding1;&#xA;    float lightsPadding2;&#xA;    float lightsPadding3;&#xA;};&#xA;&#xA;main(){ lightArray[i].intensity // works fine here&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;LightManager:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LightManager::LightManager(CameraNode* pCamera)&#xA;  : m_pCameraNode(pCamera)&#xA;{&#xA;  glGenBuffers(1, &amp;amp;m_uiUniformBuffer);&#xA;  glBindBuffer(GL_UNIFORM_BUFFER, m_uiUniformBuffer);&#xA;&#xA;  GLfloat&#xA; nulldata[164];&#xA;  for (int i = 0; i &amp;lt; 164; i++)&#xA;  {&#xA;    nulldata[i] = 1.0;&#xA;  }&#xA;&#xA;  glBufferData(GL_UNIFORM_BUFFER, 164 * sizeof(float), nulldata, GL_DYNAMIC_DRAW);&#xA;  glBindBufferRange(GL_UNIFORM_BUFFER, 1, m_uiUniformBuffer, 0, 164 * sizeof(float));&#xA;}&#xA;&#xA;    void LightManager::FillUniformBuffer()&#xA;{&#xA;  Light uploadLights[MAX_LIGHTS]; // 16 * 10 = 160 floats; 640 bytes&#xA;&#xA;  glm::mat4 ViewMat = m_pCameraNode-&amp;gt;GetViewMatrix();&#xA;&#xA;  for (int i = 0; i &amp;lt; m_vecLights.size(); ++i)&#xA;  {&#xA;&#xA;&#xA;&#xA;    glm::vec4 lightpos = glm::vec4(m_vecLights[i]-&amp;gt;GetLightPosition(), 1.0);&#xA;    lightpos = ViewMat * lightpos;&#xA;&#xA;    glm::vec4 lightdir = glm::vec4(m_vecLights[i]-&amp;gt;GetLightDirection(), 0.0);&#xA;    lightdir = ViewMat * lightdir;&#xA;&#xA;    Light light;&#xA;    light.position = glm::vec3(lightpos);&#xA;    light.attenuation = m_vecLights[i]-&amp;gt;GetLightAttenuation();&#xA;    light.direction = glm::vec3(lightdir);&#xA;    light.coneAngle = m_vecLights[i]-&amp;gt;GetLightConeAngle();&#xA;    light.intensity = m_vecLights[i]-&amp;gt;GetLightIntensity();&#xA;    light.ambientCoefficient = m_vecLights[i]-&amp;gt;GetLightAmbientCoefficient();&#xA;    light.light_type = m_vecLights[i]-&amp;gt;GetLightType();&#xA;    light.padding1 = 0.0f;&#xA;    light.padding2 = 0.0f;&#xA;    light.padding3 = 0.0f;&#xA;    uploadLights[i] = light;&#xA;  }&#xA;&#xA;  GLfloat buffer[MAX_LIGHTS * 16 + 4]; // 164 floats &#xA;  memcpy(buffer, uploadLights, MAX_LIGHTS * sizeof(Light)); // 640 bytes&#xA;&#xA;  GLfloat numLights[4];&#xA;  numLights[0] = m_vecLights.size();&#xA;  numLights[1] = 1.0f;&#xA;  numLights[2] = 2.0f;&#xA;  numLights[3] = 3.0f;&#xA;&#xA;  memcpy(buffer + MAX_LIGHTS * 16, numLights, 4 * sizeof(GLfloat)); // 16 bytes&#xA;&#xA;  glBindBuffer(GL_UNIFORM_BUFFER, m_uiUniformBuffer);&#xA;  glBufferSubData(GL_UNIFORM_BUFFER,0, MAX_LIGHTS * sizeof(Light) + 4 * sizeof(GLfloat), buffer);&#xA;  glBindBuffer(GL_UNIFORM_BUFFER, 0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Shader UniformBuffer -Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLuint lightBufferIndex = glGetUniformBlockIndex(m_uiProgram, &quot;Lights&quot;);&#xA;  if (lightBufferIndex != -1)&#xA;  {&#xA;&#xA;    const GLchar* uniformNames[2] =&#xA;    {&#xA;      &quot;lightArray&quot;,&#xA;      &quot;numLights&quot;&#xA;    };&#xA;&#xA;    int blockSize;&#xA;    glGetActiveUniformBlockiv(m_uiProgram, lightBufferIndex, GL_UNIFORM_BLOCK_DATA_SIZE, &amp;amp;blockSize);&#xA;&#xA;    tc::inf &amp;lt;&amp;lt; &quot;Shader::LightUniformBlock Size : &quot; &amp;lt;&amp;lt; blockSize &amp;lt;&amp;lt; tc::endl;&#xA;&#xA;    glBindBuffer(GL_UNIFORM_BUFFER, m_uiLightUniformBufferID);&#xA;    glBindBufferBase(GL_UNIFORM_BUFFER, 1, m_uiLightUniformBufferID);&#xA;    glUniformBlockBinding(m_uiProgram, lightBufferIndex, 1);&#xA;&#xA;&#xA;    GLuint indices[2];&#xA;    glGetUniformIndices(m_uiProgram, 2, uniformNames, indices);&#xA;    tc::inf &amp;lt;&amp;lt; &quot;Shader::LightUniformBlock Indices : &quot; &amp;lt;&amp;lt; indices[0] &amp;lt;&amp;lt; &quot; - &quot; &amp;lt;&amp;lt; indices[1] &amp;lt;&amp;lt; tc::endl;&#xA;&#xA;    if (GL_INVALID_INDEX == indices[0])&#xA;    {&#xA;      tc::err &amp;lt;&amp;lt; &quot;BAAAAAAM explosion&quot; &amp;lt;&amp;lt; tc::endl;&#xA;    }&#xA;&#xA;    GLint offsets[2];&#xA;    glGetActiveUniformsiv(m_uiProgram, 2, indices, GL_UNIFORM_OFFSET, offsets);&#xA;    tc::inf &amp;lt;&amp;lt; &quot;Shader::LightUniformBlock Offsets : &quot; &amp;lt;&amp;lt; offsets[0] &amp;lt;&amp;lt; &quot; - &quot; &amp;lt;&amp;lt; offsets[1] &amp;lt;&amp;lt; tc::endl;&#xA;&#xA;&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the error in the Shader UniformBuffer code where I output &quot;BAAAAAM explosion&quot;.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-01-26T20:35:45.223" Title="Invalid-Index on uniform buffer, despite working shading" Tags="&lt;opengl&gt;&lt;glsl&gt;&lt;fragment-shader&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="1971" PostTypeId="2" ParentId="1967" CreationDate="2016-01-27T00:08:04.783" Score="2" Body="&lt;p&gt;This is more a Computer vision question I think.&#xA;I don't know how exactly you can achieve that but some ideas and keywords you may use to find additional resources:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Feature tracking of hand and pen can yield the movement of said things. Assuming you know that the camera is not moving I think that would give you good results. Related is maybe motion tracking. Any information you have about the hand and the camera can in some way be used to make more accurate estimates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Removing the hand from the video could be much harder. I don't remember that part from my lecture. We copied parts of the image onto the part we want to remove and used image metrics to make sure it fits best possible... But since the hand covers a drawing we don't know much about this could be very hard to do in an convincing way.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-01-27T00:08:04.783" CommentCount="3" />
  <row Id="1972" PostTypeId="1" AcceptedAnswerId="1973" CreationDate="2016-01-27T03:28:11.743" Score="3" ViewCount="115" Body="&lt;p&gt;I have been reading this paper by Timothy Wilson &lt;a href=&quot;https://docs.google.com/presentation/d/19x9XDjUvkW_9gsfsMQzt3hZbRNziVsoCEHOn4AercAc/edit#slide=id.p&quot; rel=&quot;nofollow&quot;&gt;Fast Stereo Rendering for VR&lt;/a&gt;&#xA;and it would suit our game engine (DirectX 11) to use this method of stereo rendering. I have managed to get the game rendering Instanced and the screen squished into left and right areas like he mentions but am unable to get the SV_ClipDistance working correctly, when I use his methods or the method the jMonkeyEngine uses I get 4 views of the world! &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is does anyone know if you can write SV_RenderTargetArrayIndex in a pixel shader according to MSDN it says it can be read and written by the pixel shader but in all my experimentation I am unable to get the shader to compile, it complains about too many outputs. If I could get this to work the instancing would work by just setting the SV_RenderTargetArrayIndex to SV_InstanceId. Any help would be much appreciated.&lt;/p&gt;&#xA;" OwnerUserId="288" LastActivityDate="2016-01-27T07:15:21.190" Title="VR stereo rendering with Instancing" Tags="&lt;directx11&gt;&lt;virtual-reality&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1973" PostTypeId="2" ParentId="1972" CreationDate="2016-01-27T07:15:21.190" Score="5" Body="&lt;p&gt;While &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509647%28v=vs.85%29.aspx&quot;&gt;this MSDN page&lt;/a&gt; does claim that SV_RenderTargetArrayIndex can be written in a pixel shader, I believe this is incorrect. Viewport array index and RT array index values are both intended to be output by the geometry shader stage. They can then be read by the pixel shader (and have a constant value per-primitive, based on the GS output). However, it is not possible to set these values from the pixel shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The stereo instancing approach detailed in those slides is interesting precisely because it avoids using slow geometry shaders. However, if you're not using GS, you can't use multiple viewports or render target arrays. That's why the stereo instancing approach uses one large viewport and bakes each eye's viewport transform into its projection matrix, and requires clip planes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Clip planes should work, so if you're having problems with them, you could post another question detailing the specific issues (preferably with screenshots) and we can try to figure that out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(For completeness, note that if you're using OpenGL, the &lt;a href=&quot;https://www.opengl.org/registry/specs/AMD/vertex_shader_viewport_index.txt&quot;&gt;GL_AMD_vertex_shader_viewport_index&lt;/a&gt; extension is available on &lt;a href=&quot;http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_AMD_vertex_shader_viewport_index&quot;&gt;recent GPUs from all three IHVs&lt;/a&gt; and allows setting the viewport index from the vertex shader.)&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-27T07:15:21.190" CommentCount="3" />
  <row Id="1974" PostTypeId="1" CreationDate="2016-01-28T12:31:35.720" Score="10" ViewCount="130" Body="&lt;p&gt;How to reliably find out whether two planar Bezier curves intersect? By &quot;reliably&quot; I mean the test will answer &quot;yes&quot; only when the curves intersect, and &quot;no&quot; only when they don't intersect. I don't need to know what parameters the intersection was found at. I also would like to use floating-point numbers in the implementation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found several answers on StackOverflow which use the curves' bounding-boxes for the test: this is not what I'm after as such test may report intersection even if the curves don't intersect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closest thing I found so far is the &quot;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=49188&quot; rel=&quot;nofollow&quot;&gt;bounding wedge&lt;/a&gt;&quot; by Sederberg and Meyers but it &quot;only&quot; distinguishes between at-most-one and two-or-more intersection, whereas I want to know if there is at-most-zero and one-or-more intersections.&lt;/p&gt;&#xA;" OwnerUserId="141" LastEditorUserId="141" LastEditDate="2016-01-29T09:47:24.390" LastActivityDate="2016-02-11T08:42:57.343" Title="Reliable test for intersection of two Bezier curves" Tags="&lt;computational-geometry&gt;&lt;bezier-curve&gt;&lt;cad&gt;" AnswerCount="2" CommentCount="11" FavoriteCount="0" />
  <row Id="1975" PostTypeId="2" ParentId="1937" CreationDate="2016-01-28T13:15:28.897" Score="3" Body="&lt;p&gt;I don't know whether this is the only cause of your problems, but the Russian roulette is messed up for sure. stopProbability acts as the survivalProbability in your code; therefore, you compensate the estimate with a different factor than you should, which makes the result heavily biased.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would put the code as follows to make it more comprehensible:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;const float survivalProbability = 0.9f;&#xA;if(((float)rand()/RAND_MAX) &amp;gt; survivalProbability)&#xA;    break;&#xA;rrFactor *= 1.0f/survivalProbability;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-01-28T13:24:04.733" LastActivityDate="2016-01-28T13:24:04.733" CommentCount="0" />
  <row Id="1976" PostTypeId="1" AcceptedAnswerId="1977" CreationDate="2016-01-28T18:47:33.133" Score="3" ViewCount="85" Body="&lt;p&gt;I want to ray trace a triangular mesh that I load from a file. The vertices' coordinates are expressed in the reference frame associated with the object  (i.e the mesh) and I don't know the object-to-world matrix. Is there a way to calculate this matrix given these coordinates? If no, how can I ensure that the mesh's triangles are transformed to the world's reference frame? &lt;/p&gt;&#xA;" OwnerUserId="2233" LastActivityDate="2016-01-29T03:35:00.793" Title="How to determine the object-to-world matrix" Tags="&lt;raytracing&gt;&lt;rendering&gt;&lt;transformations&gt;&lt;mesh&gt;&lt;triangulation&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="1977" PostTypeId="2" ParentId="1976" CreationDate="2016-01-28T18:56:25.217" Score="4" Body="&lt;p&gt;No! If you dont have this info and must have a extra frame, just assume the reference frame is a identity matrix*. The point of the extra reference frame is just to be able to move the object but if you have no info about how the object should relate to other objects it does not really matter operator can move it where they want. Everything is relative, if theres no relation then no problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* choosing identity is like choosing a arbitrary constant to be 1, its numerically easy to work with.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-01-28T18:56:25.217" CommentCount="2" />
  <row Id="1978" PostTypeId="2" ParentId="1976" CreationDate="2016-01-28T20:06:48.473" Score="2" Body="&lt;p&gt;The Object-To-World matrix is often called &quot;model-matrix&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This martix tells us how the object relates to some &quot;World&quot;. This world is relative and arbitrary but really convenient because we can place all our objects and even the camera in it and the math works out pretty well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The short answer is, that you imagine that the model is defined in the origin of the world, so that the center of gravity of the object lies exactly in the $(0.0, 0.0, 0.0)$ position of the world.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now if you want to place multiple objects in the same scene, you have to give them a position, and it is really convenient to give all objects a position relative to this world-origin. So you just create a matrix for each object that tells you how you move the object in relation to the world-origin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example translating an object is $\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; x \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; y \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; z \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} $ then your object will have the world-relative position of $(x, y, z)$ (I will omit the useage of homogeneous coordinates which would be better suited for another question).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a similar fashion you can use translation, scaling and rotation (take care of what order you apply these transformations)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Things get a bit complicated if you want to do camera-space computations and projections and stuff like that, but in the simplest version you can just build your model-matrix this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Multiplying each point of your model with this model-matrix gives you the position of the point in world-space (meaning relative to the origin of the world!) and for meshes this moves the whole model just as you would expect.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="2500" LastEditDate="2016-01-29T03:35:00.793" LastActivityDate="2016-01-29T03:35:00.793" CommentCount="3" />
  <row Id="1979" PostTypeId="1" CreationDate="2016-01-29T01:04:29.930" Score="4" ViewCount="95" Body="&lt;p&gt;I'm a Computer Science student who's been getting into bitmaps &amp;amp; pixmaps recently. I clearly understand what pixmaps do. A pixmap, as I understand, defines each pixel individually. So, maybe I could write a pixmap filelike this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;255,0,0 0,255,0 0,0,255&lt;/p&gt;&#xA;&#xA;&lt;p&gt;which would give me a red pixel, a green pixel, and a blue pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Seems pretty straightforward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I fail to understand exactly how bitmaps work. I mean, I understand that a bitmap simply maps a bit to some color at some point on the screen. I watched a YouTube video on how to create a bitmap file, but the guy's explanation of what a bitmap file is went over my head.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I feel I now have a better conceptual grasp of what a bitmap is. Well, maybe not. Each bit in a bitmap corresponds to a certain color that is defined somewhere in the file. But how does the computer know which bit corresponds to which color?&lt;/p&gt;&#xA;" OwnerUserId="2572" LastEditorUserId="2572" LastEditDate="2016-01-29T01:26:15.110" LastActivityDate="2016-02-01T13:48:14.220" Title="How do I create my own bitmap using a text editor?" Tags="&lt;bitmap-graphics&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="1980" PostTypeId="2" ParentId="1979" CreationDate="2016-01-29T02:57:40.227" Score="9" Body="&lt;p&gt;In addition to the pixel values, the width and height of the image are also required, and the colour depth (how many colours are available per pixel). Some formats also specify an alpha value, for transparency, which may be defined per pixel in addition to the red, green and blue components.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many different image file formats but a simple one to experiment with to gain understanding is ppm, one of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Netpbm_format&quot;&gt;Netpbm formats&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Netpbm defines the following file types:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;PBM:&lt;/strong&gt; portable bitmap (pixels are either on or off)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PGM:&lt;/strong&gt; portable graymap (grayscale pixels)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PPM:&lt;/strong&gt; portable pixmap (full colour pixels)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;These have the advantages of being simple to understand, and easy to generate in a text editor for working with simple examples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, a PPM file is simply the characters &quot;P3&quot; to identify it as PPM, followed by the width, height, maximum colour value and the pixel values in English reading order (left to right then top to bottom). Each pixel colour is a red value, a green value, and a blue value. All these values are space separated and human readable. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;P3                         # Indicates an ASCII PPM file&#xA;2 2                        # Width and height of the image&#xA;255                        # Maximum colour value&#xA;255 0 0     0   255 0      # A red pixel (255 0 0) followed by a green pixel&#xA;0   0 255   255 255 255    # A blue pixel followed by a white pixel&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While this makes the format very easy to work with for small examples, it will also make the file size much larger than other formats which use bytes to represent colour components instead of writing out the human readable numbers. Many formats also include compression (either lossless or lossy).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you are accustomed to writing PPM files in ASCII characters, indicated by the file starting with &quot;P3&quot;, you can move on to PPM files using bytes, indicated by the file starting with &quot;P6&quot;. This is a similar format to what many programs use internally, but is no longer human readable in a text editor as not every byte will correspond to a printable ASCII character.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;You also mention that a pixmap can have pixel values that refer to colours stored elsewhere in the file. I have avoided this format as it is not necessary for understanding how a pixmap works. This is known as using a palette (a list of predefined colours so each pixel is represented by an index of a colour in the palette). This is necessary for formats that only allow a small number of distinct colours, but for full colour images there is no need for a palette - each pixel is defined as red, green and blue components without having to refer to anything else.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-01-29T03:33:40.947" LastActivityDate="2016-01-29T03:33:40.947" CommentCount="0" />
  <row Id="1981" PostTypeId="2" ParentId="1979" CreationDate="2016-01-29T09:03:42.017" Score="3" Body="&lt;p&gt;Bitmap is simply means to convert a data range to bits. How that internally works and what the conversion function is, is application dependent. Each format can work differently. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simplest possible bitmaps just store the values directly in the bits themselves. Think of the the simplest form, a pixmap but instead of ASCII values you are using full byte per color. Off course the data does not need to be byte aligned so 3 values may be crammed to 2 bytes for example. Some formats may be using indexed colors (which is what you describe in your edit), this is a form of a compression scheme.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/j130t.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/j130t.gif&quot; alt=&quot;A hypothethical bitmap&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: A hypothetical bitmap with 15 bits per pixel (for 5 bits per color), similar arrangement is used by some bitmap formats (this actual pattern is available in BMP). For a actual format read for example about the &lt;a href=&quot;https://en.wikipedia.org/wiki/BMP_file_format&quot; rel=&quot;nofollow&quot;&gt;BMP format&lt;/a&gt; on wikipedia.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because the lower bit depths have very little space for color values these formats are often accompanied by a lookup table instead. This allows you to store a custom set of colors very tightly packed while allowing for a sensible variance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iurjI.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iurjI.gif&quot; alt=&quot;indexed bitmap&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Indexed bitmaps store a lookup array from where they take the actual color. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that some formats may have pretty sophisticated transforms, or the data is compressed further by some more elaborate compression scheme. But even so a compression of $2/3$ and especially the indexed $1/6$ is pretty good for something that naturally tends to take up space. Off course if we compare with a ASCII pixmap those compression ratios are already quite significantly higher.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many such formats that you could type in with a text editor if you must. However, since your dealing with individual bits its much easier to do this with a &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hex_editor&quot; rel=&quot;nofollow&quot;&gt;hex editor&lt;/a&gt;&lt;/strong&gt; instead as the data is essentially binary. In practice it is easier to write a script to dump full values in the file, except perhaps in the 1 byte is one channel scenario where a hex editor works fine.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-01-29T09:22:36.483" LastActivityDate="2016-01-29T09:22:36.483" CommentCount="0" />
  <row Id="1982" PostTypeId="2" ParentId="1974" CreationDate="2016-01-29T14:04:26.613" Score="6" Body="&lt;p&gt;[Disclaimer: I think the following should work but have not actually coded it myself]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I couldn't think of a &quot;trivial&quot; method of producing a yes/no answer but the following would be a reasonable approach to a practical solution to the question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume our curves are &lt;em&gt;A(s)&lt;/em&gt; and &lt;em&gt;B(t)&lt;/em&gt; with control points {&lt;em&gt;A0, A1..An&lt;/em&gt;} and {&lt;em&gt;B0,..Bm&lt;/em&gt;} respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems to me that, given a pair of 2D Beziers for which we wish to determine do or don't intersect, there are six cases to consider:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Case where we can &quot;trivially&quot; determine they do &lt;em&gt;not&lt;/em&gt; intersect.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Case where they intersect a finite number of times and we can &quot;easily&quot; determine they definitely intersect at least once (but we don't actually care where those intersections occur)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;One of the Beziers is degenerate, i.e. a point (which will occur if all the control points are identical). We can assume we've already handled the case where both are points.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;One or more of the curves are closed, e.g.. A0==An. To make life simpler, we'll subdivide such curves and start again.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;There are an infinite number of points of intersection because each is subset of a &quot;parent&quot; Bezier and they overlap.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;We aren't certain about the above cases and need further investigation&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;For the moment we'll ignore 3 and 4, but come back to them later.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Case 1&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;As you hint in your question, if the respective bounding boxes of the control points of &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;), don't intersect, then the curves can't intersect. Obviously this is a quick reject test but it's overly conservative.&#xA;As you probably know, with a Bezier curve, the convex hull of its control points forms a (tighter) bound on the curve. We can thus use the separating axis technique to decide if the hulls of &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; don't intersect. (e.g. as shown in Wikipedia:)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/u4AxW.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/u4AxW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Case 2&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If the case 1 test fails, you could then check for the &quot;trivial&quot; existence of an intersection. Now there are probably better ways to do this, but the following, relatively cheap, approach occurred to me:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider just curve A:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2An9z.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2An9z.png&quot; alt=&quot;&amp;quot;Fat line&amp;quot; bounds of a Bezier&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We know the curve starts at $A_0$, terminates at $A_n$, and will lie inside the convex hull. For simplicity let us compute the direction of the line segment $\overline{A_0A_n} $ and the compute the bounds on either side (i.e. take dot products of the remaining control points against the perpendicular to $\overline{A_0A_n}$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we do the same with curve B we get the following (possible) case:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/dX6l0.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dX6l0.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we find $A_0$ and $A_n$ are outside opposite bounds of B &lt;em&gt;and&lt;/em&gt; that $B_0$ and $B_m$ are on the outsides of the bounds of A, then, by the continuity of Beziers, there must be at least one intersection.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Case 6&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If we can't immediately show either of the above cases, then split each of the Beziers into two &quot;halves&quot;, i.e. $A^1, A^2, B^1, B^2$. This is relatively straightforward (left as an exercise to the reader) but is &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/1895/how-should-i-fill-a-shape-consisting-of-bezier-cures-and-straight-lines/1899#1899&quot;&gt;particularly trivial for quadratic Beziers&lt;/a&gt;: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Recursively compare the 4 combinations: $(A^1,B^1), (A^2, B^1)...(A^2, B^2)$.&#xA;Clearly if all pass case 1, there is no intersection.&#xA;If any fail 1, then continue with the rest of the tests with that reduced subset.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Case 3 &amp;amp; 5&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;This is where it becomes slightly more tedious.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If &quot;case 3&quot; gets past the &quot;case 1&quot; test, it seems to me that you need to solve for an actual intersection. Given that there is a simple process to map the N control points of a Bezier, A(s), to the N-1 points of the Bezier, A'(s), representing its 1st derivative then (provided care is taken for the relatively rare, so-called &quot;degenerate&quot; situations where the 1st derivative does to zero), then Newton iteration (on one dimension) could be used to find potential solutions.&lt;br&gt;&#xA;Note also that, since the control points of A'(s) are a bound on the derivative values, there is the potential to do early elimination of some cases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Case 5 seems relatively unlikely, so perhaps only if after a few recursions there is no conclusive proof, one could try each end point of A against curve B and vice versa. This would only give a proof of intersection - not a proof of non-intersection.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-02-11T08:42:57.343" LastActivityDate="2016-02-11T08:42:57.343" CommentCount="7" />
  <row Id="1983" PostTypeId="1" CreationDate="2016-01-29T19:14:03.043" Score="9" ViewCount="187" Body="&lt;p&gt;&lt;strong&gt;What I'm asking for&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I stress that I am &lt;strong&gt;not asking for the formula&lt;/strong&gt;---I know the formula, and how to derive it. Several different versions of it are reproduced near the end of the post. In fact, someone else has not only derived it as well, but also nicely presented one of the derivations &lt;a href=&quot;https://goo.gl/FJjkHQ&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I need is a &lt;strong&gt;&lt;em&gt;reputable source&lt;/em&gt;&lt;/strong&gt; for the formula so that, for example, one could put it on Wikipedia without violating its &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:No_original_research&quot; rel=&quot;nofollow&quot;&gt;ban on reporting original research.&lt;/a&gt; [People have &lt;a href=&quot;https://en.wikipedia.org/wiki/Talk:Horizon#About_the_.22curvature_of_the_horizon.22_section&quot; rel=&quot;nofollow&quot;&gt;actually tried&lt;/a&gt;... But the &lt;a href=&quot;https://en.wikipedia.org/wiki/Horizon&quot; rel=&quot;nofollow&quot;&gt;relevant article&lt;/a&gt; has some very conscientious editor who deleted the section on the grounds of it being original research... and, well, unfortunately, the editor is correct, so there's not much point in trying to fight it.]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The reason I'm posting in Computer Graphics stackexchange&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since someone here might have modeled the way the Earth looks form orbit, perhaps he or she might know if this formula (or, more likely, some generalization of it) is published in some book, or journal, or conference proceedings, or class notes, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;I've done the &quot;due googling&quot;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please understand that I'm not asking anyone to go searching for the answer on my behalf. I've done &lt;em&gt;lots&lt;/em&gt; of googling already, and am only posting here as a last resort. My (far-fetched) hope is that someone here will simply &lt;em&gt;know&lt;/em&gt; a reference right off the bat; if not... well, I hope at least you enjoyed the pretty picture below (if I do say so myself, with full awareness I'm talking to people interested in &lt;em&gt;computer graphics&lt;/em&gt; of all things) before you moved on to bigger and better things.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Two sources that come close&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;D. K. Lynch, &quot;Visually discerning the curvature of the Earth,&quot; Applied Optics vol. 47, H39 (2008). It is freely available &lt;a href=&quot;http://thulescientific.com/Lynch%20Curvature%202008.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Unfortunately, instead of doing it the right way (which is not that hard), the author opted for a hack, which (a) I don't completely understand, and (b) which doesn't agree with what I know to be the correct formula.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;R. Hartley and A. Zisserman, &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/0521540518&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Multiple View Geometry in Computer Vision,&lt;/em&gt;&lt;/a&gt; 2nd ed. (Cambridge University Press, Cambridge UK, 2004). In Sec. 8.3, &quot;Action of a projective camera on quadrics,&quot; we &lt;a href=&quot;https://goo.gl/IHvX2d&quot; rel=&quot;nofollow&quot;&gt;read&lt;/a&gt;:&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Suppose the quadric is a sphere, then the cone of rays between the&#xA;  camera centre and quadric is right-circular, i.e. the contour&#xA;  generator is a circle, with the plane of the circle orthogonal to the&#xA;  line joining the camera and sphere centres. This can be seen from the&#xA;  rotational symmetry of the geometry about this line. The image of the&#xA;  sphere is obtained by intersecting the cone with the image plane. It&#xA;  is clear that this is a classical conic section, so that the apparent&#xA;  contour of a sphere is a conic.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In principle, this would be exactly what is needed, if only just a bit more information were included---at least an expression for the eccentricity of the conic as a function of the distance to the sphere and the sphere radius (in the case when the image plane is perpendicular to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Generatrix&quot; rel=&quot;nofollow&quot;&gt;generatrix&lt;/a&gt; of the cone, as is the case when the pinhole camera is directed at a point on the horizon).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Details about the formula for which I need a scholarly reference&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We assume a perfectly spherical, perfectly smooth Earth with no atmosphere. We point an idealized pinhole camera at the horizon, and, using straightforward central projection, compute the shape of the image of the horizon on the back of the camera (i.e. the shape it will have on film---the &quot;film plane&quot;). Here's a graphic (made in &lt;a href=&quot;http://asymptote.sourceforge.net/&quot; rel=&quot;nofollow&quot;&gt;Asymptote&lt;/a&gt;, for those interested) that should make this clearer:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Shaud.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Shaud.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As we saw above, the image of the horizon is a portion of a conic section. Let $\varepsilon$ be the eccentricity of the conic; the derivation &lt;a href=&quot;https://goo.gl/FJjkHQ&quot; rel=&quot;nofollow&quot;&gt;I mentioned above&lt;/a&gt; instead uses a parameter $k$, which is just the inverse eccentricity: $k=1/\varepsilon$. The eccentricity itself is given as $\varepsilon=1/\sqrt{\epsilon(2+\epsilon)}$, where $\epsilon=h/R$ is the ratio of the altitude $h$ of the pinhole above the surface of the Earth and the Earth radius $R$. [Instead of using $\epsilon$, which is the ratio  of the &lt;em&gt;altitude&lt;/em&gt; to $R$, it may be useful to use $\eta$, the ratio of &lt;em&gt;the pinhole's distance to the Earth's center&lt;/em&gt;, $h+R$, to the Earth's radius: $\eta=(R+h)/R=1+\epsilon$. In terms of $\eta$, we have $\varepsilon=1/\sqrt{\eta^{2}-1}$.]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The distance from the pinhole (point $P$ in the graphic) to the film plane is taken to be one unit length.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The $y$-axis in the film plane is chosen to be parallel to the line joining the center of the Earth $C$ (not shown in the image) and the point on the horizon (labeled $V$ in the image) at which the camera is trained. This choice is well-defined because the line $CV$ must be parallel to the film plane. The reason for this is that both $CV$ and the film plane are perpendicular to the line of sight $PV$ (the line joining $P$ and $V$). And &lt;em&gt;that&lt;/em&gt; is because 1. the line $PV$ is tangent to the Earth at $V$, thus perpendicular to $CV$, and 2. $PV$ is perpendicular to the film plane because the camera is trained at $V$. The $x$ axis is of course perpendicular to the $y$ axis and lies in the film plane, and the origin is chosen as the projection of the point $V$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With these definitions out of the way, we are ready to write down a representation of the conic section that is the image of the Earth's horizon. This can be written in many ways, some of which are given below. &lt;strong&gt;&lt;em&gt;What I need is a reputable reference for any one of these formula, or for a formula equivalent to them.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;1. The explicit formula given in the derivation mentioned above&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The derivation &lt;a href=&quot;https://goo.gl/FJjkHQ&quot; rel=&quot;nofollow&quot;&gt;I mentioned above&lt;/a&gt; gives this as the final version:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$[\,y\,(1/\varepsilon-\varepsilon)-1\,]^{2}+x^{2}(1/\varepsilon^{2}-1)=1.$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's represent this in a couple of additional ways.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;2. Expression in terms of the canonical equation of a conic section&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case, the equation takes &lt;a href=&quot;https://goo.gl/wIPjWs&quot; rel=&quot;nofollow&quot;&gt;the following form&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$x^2=2\mu y-(1-\varepsilon^{2}) y^2$,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where, in our case, $\mu=\varepsilon$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The advantage of the canonical form is that it can deal with all conics on an equal footing, including in particular the case of the parabola, $\varepsilon=1$. In the ``standard'' formulation (see below), the case of the parabola can only be dealt with by taking the limit $\varepsilon\to 1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Details: the above formula holds in the case of a right circular cone, whose sides subtend an angle of $2\theta$, being intersected---at a distance $d$ from the vertex of the cone---by a plane at an angle $\omega$ relative to the cone axis. (To clarify: $d$ is the distance from the cone vertex to the point on the ellipse that is the closest to the cone vertex; that point is always one of the ends of the major axis of the ellipse). In this general case, the eccentricity is given as $\varepsilon=\cos \omega/\cos \theta$, while $\mu=d(\varepsilon-\cos|\omega+\theta|)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In terms of the above graphic: $d$ is the distance from $P$ to the film plane (i.e., the distance along the dotted red line); $\theta$ is the angle between the dotted red line and the axis of the cone (which is the line joining $P$ and the center of the Earth---the extension of the black line labeled $h$ in the graphic); the angle $\omega$ is the angle between the axis of the cone and the film plane. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that the film plane is perpendicular to the dotted red line, we have $\omega+\theta=\pi/2$; in addition, we take $d=1$, which then together give that $\mu=\varepsilon$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;3. Expression in terms of the ``standard form' of a conic section&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This form is perhaps the most familiar:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\frac{(x-x_{0})^{2}}{p^2}+\frac{(x-y_{0})^{2}}{q^2}=1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is related to the parameters entering the canonical equation (see 2., above) as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$x_{0}=0$;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$y_{0}=q=\frac{\mu}{1-\varepsilon^{2}}$ (which is $\frac{\varepsilon}{1-\varepsilon^{2}}$ in our case---note that $y_{0}=q$ follows from the fact that the ellipse passes through the origin); and&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$p=q\smash[b]{\sqrt{|1-\varepsilon^{2}|}}=\frac{\mu}{\smash[b]{\sqrt{|1-\varepsilon^{2}|}}}$ (which is $\frac{\varepsilon}{\smash[b]{\sqrt{|1-\varepsilon^{2}|}}}$ in our case).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is obvious that the parabolic case, $\varepsilon=1$, will create problems; as mentioned above, that case must be dealt with through taking the limit $\varepsilon\to 1$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;4. Expression in terms of a parametric curve&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$x=\frac{(\epsilon+1) \cos (\alpha )}{\sin (\alpha )+\epsilon(\epsilon+2)}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$y=\frac{\sqrt{\epsilon (\epsilon+2)} (\sin (\alpha )-1)}{\sin (\alpha )+\epsilon (\epsilon+2)},$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $\alpha$ is the longitude of a point on the horizon, defined so that $\alpha=\pi/2$ corresponds to the point $V$ in the image above (i.e. to the point at which the pinhole camera is trained).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For how one might use these formulas, see &lt;a href=&quot;https://imgur.com/gallery/Bi5cT&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In conclusion...&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone seen the formulas above in some reputable source, possibly in the context of modeling how the Earth looks from space? If so, could you let me know what this source was?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="2574" LastEditorUserId="2574" LastEditDate="2016-02-06T15:11:26.737" LastActivityDate="2016-05-28T07:39:48.077" Title="Need a reputable source for the formula for the shape of Earth's horizon" Tags="&lt;projections&gt;&lt;simulation&gt;" AnswerCount="0" CommentCount="11" />
  <row Id="1984" PostTypeId="1" CreationDate="2016-01-29T20:54:03.493" Score="5" ViewCount="51" Body="&lt;p&gt;I am trying to figure out how to convert a flat representation of a curve into the silhouette of a surface of revolution in a isometric projection. In essence in want the planar cut of the surface edge to be converted to the silhouette.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/NV4V3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/NV4V3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Starting point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The geometry in this case is a Bézier curve although if somebody has solutions for any functions or higher order curves would be fine. So in essence im open to conics and NURBS also. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/gWxcs.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gWxcs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: End result. How to transform the magenta curves?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way I've done this now is by sampling several forms and in this case draw the new curve over the points. But id like to do this automatically and more precise. Please note: I am perfectly aware on how to do this with discrete data, I am looking for a more analytical solution if possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/CsvkZ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CsvkZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: The shape was derived using an old draftsman's trick.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please note: Most applications polygonize NURBS or Bézier surface for rendering. Also trivial root finding does not really work well as result should not be pixels but a curve. Let us to avoid this, as I am perfectly capable of discretisizing the solution and project the edges on the back face culling edge, and doing a secondary fitting, or even fitting on the NURBS underworld and then to 2d*. That is exactly the same solution as I am using now. Seem to me there should be a somewhat analytical transformation possible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2qJA5.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2qJA5.png&quot; alt=&quot;im perfectly capable of solving this with discretionary of the domain&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: I am perfectly capable of doing this with subdivision/discretisation of the domain.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* Though if you know a robust way to do the projection to a curve on surface I'm interested in that too.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-01-30T08:18:57.707" LastActivityDate="2016-01-30T08:18:57.707" Title="Silhouette curve for isometric surface of revolution" Tags="&lt;bezier-curve&gt;&lt;isometric&gt;&lt;silhouette&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="1986" PostTypeId="2" ParentId="1974" CreationDate="2016-01-31T18:20:40.643" Score="7" Body="&lt;p&gt;An alternative way to formulate the problem is to define a function that gives the distance between points on the two curves, as a function of the curves' parameters. Then attempt to find the global minimum of this function. If the curves intersect, the minimum will be zero; otherwise the minimum will be some positive distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be explicit, given a pair of 2D curves defined by $c_1, c_2 : [0, 1] \to \mathbb{R}^2$, define the distance-squared as&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ f(u,v) : [0,1]^2 \to \mathbb{R}_{\geq 0} \equiv \bigl|c_2(v) - c_1(u)\bigr|^2 $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For cubic curves, the function $f$ is then a sixth-degree polynomial in two variables. You can then apply numerical optimization techniques such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method&quot;&gt;simplex method&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_gradient_method&quot;&gt;conjugate gradient descent&lt;/a&gt;. Unfortunately the function can have several local minima (it's not convex), so optimization isn't easy. There may well be more specialized optimization methods available for polynomials, but this isn't an area of expertise for me.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-01-31T18:20:40.643" CommentCount="4" />
  <row Id="1987" PostTypeId="1" AcceptedAnswerId="1989" CreationDate="2016-02-01T05:07:20.643" Score="7" ViewCount="130" Body="&lt;p&gt;A &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6816/pdf/imm6816.pdf&quot; rel=&quot;nofollow&quot;&gt;paper$^1$&lt;/a&gt; I'm reading says fluence measures the incoming radiance from all directions and that fluence is similar to irradiance. It's defined by $\phi(x) = \int_{4 \pi} L(x, \vec{\omega'}) d\omega' $ (The tick mark seems to emphasize $\vec{\omega'}$ varies over the integration, rather than being a fixed direction.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intuitively, radiance is the amount of light given off in a direction but attenuated by how far off the surface is from being perpendicular to the light direction. $L=\frac{d^2\Phi}{d\omega dA \cos \theta}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Irradiance is the area light density of a surface. $E=\frac{d\Phi}{dA}=\int_{2 \pi } L_i(\vec{\omega}) \cos \theta d\omega$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Intensity is the amount of light hitting a surface from a particular direction. $I(\vec{\omega})=\frac{d\Phi}{d\omega}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It also looks like you can write the radiance as irradiance times intensity. &#xA;$L=E\cdot I(\vec{\omega})$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Looking at the formulas, it seems like irradiance sums radiance over the hemisphere over a surface point and attenuated by how much the surface orients toward the light source, while fluence sums up radiance over the whole sphere surrounding a point (to measure inscattering).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Wikipedia says, &quot;radiant fluence is the radiant energy received by a surface per unit area, or equivalently the irradiance of a surface integrated over time of irradiation.&quot; But this definition doesn't seem to match the one given above since there's no time involved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/post/What_is_the_definition_and_difference_between_light_dose_and_light_fluence_rate&quot; rel=&quot;nofollow&quot;&gt;https://www.researchgate.net/post/What_is_the_definition_and_difference_between_light_dose_and_light_fluence_rate&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://omlc.org/education/ece532/class1/irradiance.html&quot; rel=&quot;nofollow&quot;&gt;http://omlc.org/education/ece532/class1/irradiance.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Real-Time Rendering of Translucent Materials with Directional Subsurface Scattering, Alessandro Dal Corso, 2014&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2457" LastEditorUserId="2457" LastEditDate="2016-02-02T05:03:34.853" LastActivityDate="2016-02-02T05:03:34.853" Title="What's the difference between irradiance and fluence/radiant exposure?" Tags="&lt;rendering&gt;&lt;physically-based&gt;&lt;physics&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="1989" PostTypeId="2" ParentId="1987" CreationDate="2016-02-01T21:28:59.743" Score="4" Body="&lt;p&gt;I can only cite what I have learned in my lecture on Global Illumination techniques which was unfortunately some time ago:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Radiant Power : The amount of energy emitted by a light source in unit time. denoted by $\Phi$ and is measured in Watt which equals to Joules per second. (This does not specify any area!)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Irradiance: The irradiance denotes the incident radiant energy on a surface. It is defined as $E=\frac{d\Phi}{dA}$ - This quantity denotes the incoming energy per area and is measure ind $\frac{W}{m^2}$ Note here: radiant energy is measured in Watt which is Joules &lt;strong&gt;per second&lt;/strong&gt;!&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Radiant exitance: This is the reverse direction of Irradiance and is the energy an area emits in unit time and has the same definiton and units as Irradiance only with reverse direction $M = B = \frac{d\Phi}{dA}$. This is also often called radiosity B.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Radiance: Is the power per projected area per solid angle. It is defined as $L=\frac{d²\Phi}{d \omega * dA * cos\theta}=[\frac{W}{sr*m^2}]$ where $\omega$ denotes the solid angle, $\theta$ denotes the angle between emitting surface normal and the direction the energy is traveling to. So this is not attenuated by the distance but by the angles.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In general as I learned the distance between emitting and receiving surface is only important for Radiance because the projected area will be smaller if they are farther apart.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The time you are missing in your definitions seem to come from the conversion of Watt to Joules Per Second.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-02-01T21:28:59.743" CommentCount="0" />
  <row Id="1990" PostTypeId="1" CreationDate="2016-02-02T10:20:07.647" Score="4" ViewCount="186" Body="&lt;p&gt;I installed AMD APP SDK and here my problem. The OpenCL samples do not detect the GPU. HelloWorld give me this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[thomas@Clemence:/opt/AMDAPP/samples/opencl/bin/x86_64]$ ./HelloWorld&#xA;No GPU device available.&#xA;Choose CPU as default device.&#xA;input string:&#xA;GdkknVnqkc&#xA;&#xA;output string:&#xA;HelloWorld&#xA;Passed!&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And here the clinfo output&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[thomas@Clemence:~/Documents/radeontop]$ clinfo&#xA;Number of platforms:                 1&#xA;Platform Profile:                FULL_PROFILE&#xA;Platform Version:                OpenCL 1.2 AMD-APP (1214.3)&#xA;Platform Name:               AMD Accelerated Parallel Processing&#xA;Platform Vendor:                 Advanced Micro Devices, Inc.&#xA;Platform Extensions:                 cl_khr_icd cl_amd_event_callback      cl_amd_offline_devices&#xA;&#xA;&#xA;Platform Name:               AMD Accelerated Parallel Processing&#xA;Number of devices:               1&#xA;Device Type:                     CL_DEVICE_TYPE_CPU&#xA;Device ID:                   4098&#xA;Board name:                  &#xA;Max compute units:               8&#xA;Max work items dimensions:           3&#xA;Max work items[0]:               1024&#xA;Max work items[1]:               1024&#xA;Max work items[2]:               1024&#xA;Max work group size:                 1024&#xA;Preferred vector width char:             16&#xA;Preferred vector width short:            8&#xA;Preferred vector width int:          4&#xA;Preferred vector width long:             2&#xA;Preferred vector width float:            8&#xA;Preferred vector width double:       4&#xA;Native vector width char:            16&#xA;Native vector width short:           8&#xA;Native vector width int:             4&#xA;Native vector width long:            2&#xA;Native vector width float:           8&#xA;Native vector width double:          4&#xA;Max clock frequency:                 3633Mhz&#xA;Address bits:                    64&#xA;Max memory allocation:           4182872064&#xA;Image support:               Yes&#xA;Max number of images read arguments:         128&#xA;Max number of images write arguments:        8&#xA;Max image 2D width:              8192&#xA;Max image 2D height:                 8192&#xA;Max image 3D width:              2048&#xA;Max image 3D height:                 2048&#xA;Max image 3D depth:              2048&#xA;Max samplers within kernel:          16&#xA;Max size of kernel argument:             4096&#xA;Alignment (bits) of base address:        1024&#xA;Minimum alignment (bytes) for any datatype:  128&#xA;Single precision floating point capability&#xA;Denorms:                     Yes&#xA;Quiet NaNs:                  Yes&#xA;Round to nearest even:           Yes&#xA;Round to zero:               Yes&#xA;Round to +ve and infinity:           Yes&#xA;IEEE754-2008 fused multiply-add:         Yes&#xA;Cache type:                  Read/Write&#xA;Cache line size:                 64&#xA;Cache size:                  32768&#xA;Global memory size:              16731488256&#xA;Constant buffer size:                65536&#xA;Max number of constant args:             8&#xA;Local memory type:               Global&#xA;Local memory size:               32768&#xA;Kernel Preferred work group size multiple:   1&#xA;Error correction support:            0&#xA;Unified memory for Host and Device:      1&#xA;Profiling timer resolution:          1&#xA;Device endianess:                Little&#xA;Available:                   Yes&#xA;Compiler available:              Yes&#xA;Execution capabilities:              &#xA;Execute OpenCL kernels:          Yes&#xA;Execute native function:             Yes&#xA;Queue properties:                &#xA;Out-of-Order:                No&#xA;Profiling :                  Yes&#xA;Platform ID:                     0x00007f4ef63f0fc0&#xA;Name:                        Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz&#xA;Vendor:                  GenuineIntel&#xA;Device OpenCL C version:             OpenCL C 1.2 &#xA;Driver version:              1214.3 (sse2,avx)&#xA;Profile:                     FULL_PROFILE&#xA;Version:                     OpenCL 1.2 AMD-APP (1214.3)&#xA;Extensions:                  cl_khr_fp64 cl_amd_fp64     cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_gl_sharing cl_ext_device_fission cl_amd_device_attribute_query cl_amd_vec3 cl_amd_printf cl_amd_media_ops cl_amd_media_ops2 cl_amd_popcnt &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What should I do in order to have access to the GPU? Thanks in advance. I'm working on Ubuntu 14.04.3 LTS Trusty Kernel 3.9&lt;/p&gt;&#xA;&#xA;&lt;p&gt;here my graphics card:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[thomas@elsa:~]$ lspci | grep -i --color 'vga'&#xA;00:02.0 VGA compatible controller: Intel Corporation Xeon E3-1200 v3/4th &#xA;Gen Core Processor Integrated Graphics Controller (rev 06)&#xA;01:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] &#xA;Oland XT [Radeon HD 8670 / R7 250/350]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2592" LastActivityDate="2016-02-02T20:30:54.753" Title="OpenCL doesn't detect GPU" Tags="&lt;gpu&gt;&lt;opencl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1991" PostTypeId="1" AcceptedAnswerId="2050" CreationDate="2016-02-02T12:04:47.037" Score="5" ViewCount="65" Body="&lt;p&gt;I encountered some problems when implementing the cloth simulation algorithm from Baraff &amp;amp; Witkin 98's &lt;a href=&quot;http://www.cs.cmu.edu/~baraff/papers/sig98.pdf&quot; rel=&quot;nofollow&quot;&gt;Large Steps in Cloth Simulation&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Baraff &amp;amp; Witkin 98&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the cloth as a particle system in 3D, which consists of N particles.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Equation of motion&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In each time step, the implicit integration of particle system is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\begin{pmatrix} \Delta \mathbf x \\ \Delta \mathbf v \\ \end{pmatrix} = h\begin{pmatrix} \mathbf v_0 + \Delta \mathbf v \\ \mathbf M^{-1} \mathbf f(\mathbf x_0 + \Delta \mathbf x, \mathbf v_0 + \Delta \mathbf v) \\ \end{pmatrix}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $h$ is the time step (scalar)&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;The partial derivative equation&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Rewrite the aforementioned equality of motion into $\mathbf A \Delta \mathbf v = \mathbf b$ form, (&#xA;Take the 1st order Taylor expansion of $\mathbf f(\mathbf x_0 + \Delta \mathbf x, \mathbf v_0 + \Delta \mathbf v)$, and introduce constraint matrix $\mathbf S$, which make mass matrix from $\mathbf M^{-1}$ to $\mathbf M^{-1}\mathbf S$), we have: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;(\mathbf I - h\mathbf M^{-1} \mathbf S\frac{\partial \mathbf f}{\partial \mathbf v} - h^2\mathbf M^{-1} \mathbf S\frac{\partial \mathbf f}{\partial \mathbf x})\Delta \mathbf v = h\mathbf M^{-1} \mathbf S(\mathbf f_0 + h\frac{\partial \mathbf f}{\partial \mathbf x} \mathbf v_0) + \mathbf z&#xA;$$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where all of the matrices are treated as $N\times N $ block matrices, eack block sizes $3\times 3 $; vectors as $N$ block vector, each sized $3$ (with N number of 3D forces/positions/velocities).&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;$\mathbf I$ is identical matrix&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;$ \mathbf M = diag(\mathbf M_1, \mathbf M_2, ..., \mathbf M_N) $, $\mathbf M_i = diag(m_i, m_i, m_i)$, mass of each particle&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;$ \mathbf S = diag(\mathbf S_1, \mathbf S_2, ..., \mathbf S_N) $, &#xA;$  \mathbf S_i = \begin{cases}  \mathbf I,  &amp;amp; \text{$ndof(i) = 3$} \\ \mathbf I - \mathbf p_i \mathbf p_i^T,  &amp;amp; \text{$ndof(i) = 2$} \\ \mathbf I - \mathbf p_i \mathbf p_i^T - \mathbf q_i \mathbf q_i^T,  &amp;amp; \text{$ndof(i) = 1$} \\ \mathbf 0,  &amp;amp; \text{$ndof(i) = 0$} \end{cases}$, $\mathbf p$ and $\mathbf q$ are two oithogonal constraint direction, $\mathbf S$ is constraint matrix (Baraff98 chapter 5.1)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;$\frac{\partial \mathbf f}{\partial \mathbf v}$ and $\frac{\partial \mathbf f}{\partial \mathbf x}$ are force derivatives, which are symmetrical matrices&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;solve the PDE&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The paper solves the aforementioned PDE by &lt;strong&gt;Modified Preconditioning Conjugate Gradient Method&lt;/strong&gt; (Baraff98 chapter 5.2) for $\Delta \mathbf v$, then update the velocity $\mathbf v$ and position $\mathbf x$ of each particle.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My Questions&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Solve $\mathbf A\mathbf x=\mathbf b$ with PCG method requires the matrix $\mathbf A$ to be symmetrical and positive-definite, where according to the aforementioned PDE, $\mathbf A = (\mathbf I - h\mathbf M^{-1} \mathbf S\frac{\partial \mathbf f}{\partial \mathbf v} - h^2\mathbf M^{-1} \mathbf S\frac{\partial \mathbf f}{\partial \mathbf x})$.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;symmentrical&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In block view: treat $\mathbf A$ as an $N\times N$ block matrix, $\mathbf M^{-1}$ and $\mathbf S$ are both block diagonal matrices, makes $\mathbf A$ a block symmetrical matrix. While in normal view: $\mathbf A$ as $3N\times 3N$ matrix, $\mathbf S$ is symmetrical, but product of $\mathbf M^{-1} \mathbf S$ and $\frac{\partial \mathbf f}{\partial \mathbf v}$ (or $\frac{\partial \mathbf f}{\partial \mathbf x}$) makes $\mathbf A$ not symmetrical. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 1&lt;/strong&gt;: should I treat $\mathbf A\mathbf x=\mathbf b$ as a block matrix rather than normal matrix? If not, how to make $\mathbf A$ symmetrical?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then modified the PDE as $\mathbf A^T\mathbf A\mathbf x=\mathbf A^T\mathbf b$, $\mathbf A^T\mathbf A$ is symmetrical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 2&lt;/strong&gt;: How to make the &lt;code&gt;fliter&lt;/code&gt; procedure (Baraff98 chapter 5.3) compatible with $\mathbf A^T\mathbf A\mathbf x=\mathbf A^T\mathbf b$?&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;positive-definite&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The blocks $\mathbf S_i$ in $\mathbf S$ may become zero block when $ndof(i) = 0$, which makes $\mathbf A^T\mathbf A$ a positive-semidefinite matrix. To apply PCG method, according to Baraff98 chapter 5.3:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The CG method (technically, the preconditioned CG method) takes a symmetric &lt;strong&gt;positive semi-definite&lt;/strong&gt; matrix $\mathbf A$, a &lt;strong&gt;symmetric positive definite&lt;/strong&gt; preconditioning matrix $\mathbf P$ of the same dimension as $\mathbf A$ ,a vector $\mathbf b$ and iteratively solves $\mathbf A \Delta \mathbf v = \mathbf b$.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt;: How to find the symmetrical and positive-definite preconditioning matrix $\mathbf P$, with $\mathbf A^T\mathbf A$ symmetrical and semi PD?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Some docs I also refered:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.cs.ubc.ca/~ascher/papers/ab.pdf&quot; rel=&quot;nofollow&quot;&gt;On the modified conjugate gradient method in cloth simulation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://davidpritchard.org/freecloth/docs/report-single/&quot; rel=&quot;nofollow&quot;&gt;freecloth&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://cg.informatik.uni-freiburg.de/course_notes/sim_03_cloth1.pdf&quot; rel=&quot;nofollow&quot;&gt;Cloth Simulation slides&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2593" LastEditorUserId="2593" LastEditDate="2016-02-13T04:02:09.840" LastActivityDate="2016-02-15T21:30:17.253" Title="Questions about Preconditioning Conjugate Gradient method in Baraff &amp; Witkin 98?" Tags="&lt;real-time&gt;&lt;mesh&gt;&lt;physically-based&gt;&lt;animation&gt;&lt;simulation&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="1992" PostTypeId="1" AcceptedAnswerId="1994" CreationDate="2016-02-02T20:01:54.463" Score="8" ViewCount="58" Body="&lt;p&gt;In traditional rendering, it is typical to do all the calculations with radiometric units, either as full spectral rendering, or component-wise (XYZ, RGB, etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, as modern rendering adds more physically based models, it's convenient for artists to be able to specify values in the more familiar photometric units, for example, intensity of a light in lumens. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/jOg65.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jOg65.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to keep the pipeline all in one type of unit, you can do one of the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Convert photometric units to radiometric units using &lt;a href=&quot;https://en.wikipedia.org/wiki/Luminous_efficacy&quot;&gt;luminous efficacy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Keep the entire render pipeline in photometric units&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sébastian Lagarde presents this idea very well in the course notes of his Siggraph 2014 presentation &lt;a href=&quot;http://www.frostbite.com/2014/11/moving-frostbite-to-pbr/&quot;&gt;Moving Frostbite to PBR&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My questions then are these:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Are there any disadvantages to rendering exclusively in photometric units? &#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;As far as I can tell, photometric units are just radiometric units biased to the human eye. Considering that we will view the final image with our eye, I don't see this as a problem.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Frostbite is a RGB component-wise engine. Would a spectral renderer have any additional disadvantages by rendering exclusively in photometric units?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="310" LastActivityDate="2016-02-03T07:55:00.923" Title="Rendering in radiometric units or photometric?" Tags="&lt;pbr&gt;&lt;render&gt;&lt;photometric&gt;&lt;radiometric&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="3" />
  <row Id="1993" PostTypeId="2" ParentId="1990" CreationDate="2016-02-02T20:30:54.753" Score="3" Body="&lt;p&gt;I should warn you that I don't really know anything about linux or programming or device drivers, but I had your exact problem once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It could be a udev rule problem. Your usergroup might not have permission to write to the gpu device or whatever libOpenCL.so does. Does &lt;code&gt;$ sudo clinfo&lt;/code&gt; find the gpu?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your program might not be using the right opencl library. I think that there is some ubuntu packages that provide libopencl.so. You don't want to use those, they won't know how to talk to your gpu. Could you post:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ ldd /opt/AMDAPP/samples/opencl/bin/x86_64/HelloWorld&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the libOpenCL.so* line (sometimes libcl.so*) doesn't point to a AMD library, you need to find the AMD libOpenCL.so library and make sure that it is found first before whatever you're using at runtime. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would do &lt;code&gt;$ sudo updatedb&lt;/code&gt; then,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ locate libOpenCL.so&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ locate libcl.so&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;depending on which your &lt;code&gt;./HelloWorld&lt;/code&gt; is trying to link to. Then set the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the parent folder of the preferred library.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ export LD_LIBRARY_PATH=/path to lib*cl.so* that was installed by AMD driver/ &#xA;$ clinfo&#xA;$ ./HelloWorld&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2597" LastActivityDate="2016-02-02T20:30:54.753" CommentCount="0" />
  <row Id="1994" PostTypeId="2" ParentId="1992" CreationDate="2016-02-03T07:55:00.923" Score="6" Body="&lt;p&gt;It's fine to use photometric units as an overall scale for setting light brightnesses. However, there's a technical subtlety you should be aware of. I'll quote from &lt;a href=&quot;http://www.reedbeta.com/blog/2014/08/17/radiometry-versus-photometry/&quot;&gt;a blog post I wrote on the subject last year&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;With RGB images, it’s important to recognize that our display devices behave more radiometrically than photometrically. A red pixel value of 255, and a green pixel value of 255, both result in about equal amounts of &lt;em&gt;radiant&lt;/em&gt; flux (watts) being generated by a pixel on a screen—&lt;em&gt;not&lt;/em&gt; equal amounts of luminous flux. By the same token, digital cameras capture pixel values that correspond to radiant flux, not luminous flux.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;That’s why we need to use luma coefficients when converting images to grayscale, or calculating the brightness of a pixel, to get a perceptually accurate result; and it also means that rendering RGB images proceeds more naturally in radiometric units than in photometric ones.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In other words, the wavelength dependence of photometric units is different to what you might be expecting. In ordinary RGB color spaces, white is (1, 1, 1) and has a roughly flat radiometric spectrum; but in a supposed &quot;photometric RGB&quot;, (1, 1, 1) would not be white; it would be a purpleish color, with less energy in the green range and more in the red and blue ranges. A similar problem would afflict spectral renderers trying to measure all their bins in wavelength-dependent photometric units, but even worse, as the radiance needed to generate a given luminance &lt;em&gt;diverges&lt;/em&gt; toward either end of the visible spectrum, where the human luminous efficiency function goes to zero.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, if you wish to use photometric units, IMO it's better to &quot;cheat&quot; a bit and not use &lt;em&gt;true&lt;/em&gt; wavelength-dependent photometric units, but just use some fixed wavelength (such as 555 nm green, which is the peak of the human luminous efficiency function), or perhaps an average over the spectrum, as a reference unit, and apply that single unit to measure all wavelengths. This will get you in less trouble when importing RGB colors and spectra from other sources, and when generating them as outputs.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-03T07:55:00.923" CommentCount="0" />
  <row Id="1995" PostTypeId="1" CreationDate="2016-02-03T12:23:28.790" Score="4" ViewCount="68" Body="&lt;p&gt;I need to do planar reflection mapping in OpenGL. By using a virtual camera (with a reversed camera ray) at the back of reflective plane (a mirror), I rendered the scene to texture, which is then texture mapped to the mirror. The uvs of four corners of the mirror is specified then (using the result of gluProjection). The problem is by default, the uv coordinates are perspective-correctly interpolated, which is not desirable in this case. Is there any way to disable the perspective correction (without using GLSL)?&lt;/p&gt;&#xA;" OwnerUserId="2600" LastActivityDate="2016-02-03T12:23:28.790" Title="How to disable Perspective Correction in Texture Coordinates Interpolation in OpenGL(without using shaders)?" Tags="&lt;opengl&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="1997" PostTypeId="1" CreationDate="2016-02-03T19:38:55.693" Score="2" ViewCount="73" Body="&lt;p&gt;I am implementing foveated rendering. My &lt;a href=&quot;https://en.wikipedia.org/wiki/Framebuffer_Object&quot; rel=&quot;nofollow&quot; title=&quot;Framebuffer Object&quot;&gt;FBO&lt;/a&gt; is divided into 2 layers which are concentric. The first layer covers 1/3 of the Framebuffer whereas the second one covers the rest of the screen. Now I want to avoid rendering meshes in the outer layer which are already in the inner layer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have implemented the following culling algorithm which is only concerned with culling between the layers:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I create rectangles with min and max corners from the radii of the 2 layers. Now these coordinates are in &lt;a href=&quot;https://www.ncl.ucar.edu/Document/Graphics/ndc.shtml&quot; rel=&quot;nofollow&quot; title=&quot;Normalized Device Coordinates&quot;&gt;NDC&lt;/a&gt;. &lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I have the min and max corner of each mesh in the local coordinate system. So I transform them into NDC using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec4 min = mvp * vec4(min_corner, 1.0);&#xA;min = min.xyzw / min.w;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I create a bounding box using min and max corners.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;I check whether 2 rectangles overlap or not.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Now, when I transform into clip space, the x or y coordinates of the min are sometimes greater than the max corner. The same issue occurs when I divide them by w. What's going wrong here?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;Okay. I found the problem. Now I iterate over all the vertices, transform each of them into NDC using &lt;a href=&quot;http://stackoverflow.com/questions/5550620/the-purpose-of-model-view-projection-matrix&quot; title=&quot;Model View Projection matrix&quot;&gt;MVP&lt;/a&gt; and then calculate the min and max corners which works perfect. But this is much more overhead for each frame. Is there any optimization I can do here?&lt;/p&gt;&#xA;" OwnerUserId="2602" LastEditorUserId="231" LastEditDate="2016-02-06T13:14:25.530" LastActivityDate="2016-02-06T21:04:52.903" Title="culling meshes across mulitple layers" Tags="&lt;opengl&gt;&lt;algorithm&gt;&lt;3d&gt;&lt;transformations&gt;&lt;bounding-volume-hierarchy&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="1998" PostTypeId="1" AcceptedAnswerId="2000" CreationDate="2016-02-03T20:13:24.137" Score="8" ViewCount="41" Body="&lt;p&gt;In a webgl pixel shader, all functions are inlined as i understand it, however you can have parameters that are marked as &lt;strong&gt;&lt;em&gt;in&lt;/em&gt;&lt;/strong&gt; versus being &lt;strong&gt;&lt;em&gt;inout&lt;/em&gt;&lt;/strong&gt; meaning that their value can change but the value won't persist outside of the function call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does this mean that the shader must make a copy of the value for the function to work with when it is an &lt;strong&gt;&lt;em&gt;in&lt;/em&gt;&lt;/strong&gt; value?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are shader compilers/optimizers smart enough to know when they don't need to make a copy, or is it best to really just mark up all parameters as &lt;strong&gt;&lt;em&gt;inout&lt;/em&gt;&lt;/strong&gt; and make sure and not modify the ones you don't want modified, if performance is the primary concern?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-02-05T04:32:08.370" Title="Cost of parameter passing in webgl pixel shaders?" Tags="&lt;webgl&gt;&lt;pixel-shader&gt;&lt;efficiency&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="1999" PostTypeId="1" CreationDate="2016-02-04T05:55:44.517" Score="6" ViewCount="56" Body="&lt;p&gt;Geometry shaders appear to have been introduced in 3.2, which makes me wonder how common 3.2-enabled cards are, including support for Geometry shaders within WebGL contexts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Will I be cutting out a large portion of viewers by requiring the use of a geometry shader?&lt;/p&gt;&#xA;" OwnerUserId="71" LastActivityDate="2016-02-04T09:29:22.403" Title="How much should I rely on Geometry shaders in WebGL?" Tags="&lt;webgl&gt;&lt;geometry-shader&gt;&lt;compatibility&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2000" PostTypeId="2" ParentId="1998" CreationDate="2016-02-04T06:25:05.953" Score="7" Body="&lt;p&gt;My experience working with shader compiler stacks a few years back is that they are extremely aggressive, and I doubt you will see any perf difference, but I would suggest testing as much as you can.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would generally recommend providing the compiler (and human readers) with more information where the language allows it, marking parameters according to their usage. Treating an in-only parameter as in/out is more error prone for humans than compilers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some detail: shaders run almost entirely using registers for variables (I worked with architectures that supported up to 256 32 bit registers) - spilling is hugely expensive. Physical registers are shared between shader invocations - think of this as threads in a HyperThread sense sharing a register pool - and if the shader can be complied to use fewer registers there's greater parallelism. The result is shader compilers work very hard to minimize the number of registers used - without spilling, of course. Thus inlining is common since it's desirable to optimize register allocation across the whole shader anyway.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-02-05T04:32:08.370" LastActivityDate="2016-02-05T04:32:08.370" CommentCount="0" />
  <row Id="2001" PostTypeId="2" ParentId="1999" CreationDate="2016-02-04T09:29:22.403" Score="8" Body="&lt;p&gt;WebGL doesn't even currently support geometry shaders, so to directly answer the question: 100% of all users.&lt;/p&gt;&#xA;" OwnerUserId="71" LastActivityDate="2016-02-04T09:29:22.403" CommentCount="0" />
  <row Id="2003" PostTypeId="1" CreationDate="2016-02-04T16:38:11.167" Score="4" ViewCount="36" Body="&lt;p&gt;I am developing the algorithm reported in this article:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf&quot; rel=&quot;nofollow&quot;&gt;Least square conformal mapping&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is presented an algorithm to flat a 3d mesh on the parametric space, but i don't understand the segmentation step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know an alternative segmentation approach simpler than the algorithm presented in that article?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other word, do you know a simple and efficient mesh segmentation algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The requirements are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;charts boundaries should be positioned in such a way that most&#xA;of the discontinuities between the charts will be located in zones&#xA;where they will not cause texture artifacts,&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;boundary curves corresponding to high curvature zones of the model.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thank you for your help in advance.&lt;/p&gt;&#xA;" OwnerUserId="2606" LastActivityDate="2016-02-04T16:38:11.167" Title="3D mesh segmentation simple algorithm" Tags="&lt;3d&gt;&lt;mesh&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="2" />
  <row Id="2004" PostTypeId="2" ParentId="1725" CreationDate="2016-02-05T08:07:12.037" Score="9" Body="&lt;p&gt;For the current state-of-the-art, look for this paper:&#xA;&quot;Maximum Mipmaps for Fast, Accurate, and Scalable Dynamic Height Field&#xA;Rendering&quot;, Tevs et al. 2008&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic idea is to skip a lot of space by having knowledge of the maximum value over large areas of terrain. If the ray stays above that, skip to the next large area. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you look at Figure 8, you'll see a comparison of basic linear stepping vs. maximum mipmaps. Linear stepping results in 200 steps, which can be done real-time on modern gpus but is still actually slow. Max mipmaps do the same in about 10 steps, all in shader.&lt;/p&gt;&#xA;" OwnerUserId="2609" LastActivityDate="2016-02-05T08:07:12.037" CommentCount="1" />
  <row Id="2006" PostTypeId="1" CreationDate="2016-02-06T00:32:49.593" Score="10" ViewCount="123" Body="&lt;p&gt;In Michael Abrash's &lt;a href=&quot;http://www.jagregory.com/abrash-black-book&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;Graphics Programming Black Book, Special Edition&lt;/em&gt;&lt;/a&gt;, Chapter 64 – Quake’s Visible-Surface Determination, &lt;a href=&quot;http://www.jagregory.com/abrash-black-book/#breakthrough&quot; rel=&quot;nofollow&quot;&gt;he writes&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;In the end, John [Carmack] decided that the beam tree was a sort of second-order structure, reflecting information already implicitly contained in the world BSP tree, so he tackled the problem of extracting visibility information directly from the world BSP tree. He spent a week on this, as a byproduct devising a perfect DOOM (2-D) visibility architecture, whereby a single, linear walk of a DOOM BSP tree produces zero-overdraw 2-D visibility. Doing the same in 3-D turned out to be a much more complex problem, though, and by the end of the week John was frustrated by the increasing complexity and persistent glitches in the visibility code. Although the direct-BSP approach was getting closer to working, it was taking more and more tweaking, and a simple, clean design didn’t seem to be falling out. When I left work one Friday, John was preparing to try to get the direct-BSP approach working properly over the weekend.&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;But, with the later chapters being about &lt;em&gt;Quake&lt;/em&gt;'s 3D engine, the 2D method is not mentioned again.  I've been trying to find more information on this &quot;zero-overdraw 2D visibility&quot; BSP renderer, but I've had no luck.&lt;/p&gt;&#xA;" OwnerUserId="2608" LastEditorUserId="2608" LastEditDate="2016-06-10T21:56:41.107" LastActivityDate="2016-06-10T21:56:41.107" Title="Is there a known linear &quot;zero-overdraw 2D visibility&quot; BSP algorithm, without using any intermediate data structures?" Tags="&lt;rendering&gt;&lt;2d&gt;&lt;occlusion&gt;" AnswerCount="0" CommentCount="4" FavoriteCount="1" />
  <row Id="2007" PostTypeId="2" ParentId="1997" CreationDate="2016-02-06T17:36:28.740" Score="1" Body="&lt;p&gt;Regarding optimizations:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could you calculate the min and max corners in modelspace beforehand? Then you could check on the CPU-side the NDCs of these two points and skip the processing of the whole mesh if it is outside of the desired range. This needs only two matrix-vector multiplications on cpu side. And since for rigid bodies these two points do not change over time, you don't have to iterate the meshes in each frame.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="273" LastEditDate="2016-02-06T21:04:52.903" LastActivityDate="2016-02-06T21:04:52.903" CommentCount="2" />
  <row Id="2008" PostTypeId="1" CreationDate="2016-02-07T02:38:23.367" Score="2" ViewCount="123" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8UV6b.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8UV6b.png&quot; alt=&quot;As you can see the back face is lighting as well&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a problem with lighting calculations.&#xA;I am rotating the camera around a cube which was supposed to be illuminated by the light but the resulting lighting is very weird.&#xA;Check out the image above, fragment shader and vertex shader below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//Vertex shader&#xA;&#xA;#version 330 core&#xA;&#xA;layout (location = 0) in vec3 pos;&#xA;layout (location = 1) in vec3 norm;&#xA;&#xA;uniform mat4 model;&#xA;uniform mat4 view;&#xA;uniform mat4 projection;&#xA;uniform vec3 lightPos;&#xA;&#xA;&#xA;out vec3 ourNormal;&#xA;out vec3 fragPos;&#xA;out vec3 ourLightPos;&#xA;&#xA;void main()&#xA;{&#xA;    gl_Position = projection*view*model*vec4(pos,1.0);&#xA;    fragPos = vec3(view * model * vec4(pos, 1.0f));&#xA;    ourNormal = mat3(transpose(inverse(view * model))) * norm;&#xA;    ourLightPos = vec3(view * vec4(lightPos, 1.0));&#xA;}&#xA;&#xA;&#xA;&#xA;//fragment shader&#xA;&#xA;#version 330 core&#xA;&#xA;//Output variables&#xA;out vec4 finalColor;&#xA;&#xA;//Input variables&#xA;in vec3 ourNormal;&#xA;in vec3 fragPos;&#xA;in vec3 ourLightPos;&#xA;&#xA;//Uniform variables&#xA;&#xA;struct Material&#xA;{&#xA;    vec3 ambient;&#xA;    vec3 diffuse;&#xA;    vec3 specular;&#xA;    float shininess;&#xA;};&#xA;&#xA;struct Light&#xA;{&#xA;    vec3 ambient;&#xA;    vec3 diffuse;&#xA;    vec3 specular;&#xA;};&#xA;&#xA;uniform Light light;&#xA;uniform Material material;&#xA;uniform vec3 viewPos;&#xA;&#xA;void main()&#xA;{&#xA;    //Ambient&#xA;    vec3 ambient = light.ambient*material.ambient;&#xA;&#xA;    //Diffuse&#xA;    vec3 normal = normalize(ourNormal);&#xA;    vec3 lightDir = normalize(ourLightPos - fragPos);&#xA;    float diffuseStrength = max(dot(lightDir,normal),0.0);&#xA;    vec3 diffuse = light.diffuse*(diffuseStrength*material.diffuse);&#xA;&#xA;    //Specular&#xA;    vec3 viewDir = normalize(viewPos - fragPos);&#xA;    vec3 reflectDir = reflect(-lightDir,normal);&#xA;    float specularStrength =         pow(max(dot(viewDir,reflectDir),0.0),material.shininess);&#xA;    vec3 specular = light.specular * (specularStrength*material.specular);&#xA;&#xA;    //Last step&#xA;    vec3 result = (ambient+diffuse+specular);&#xA;    finalColor = vec4(result,1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I transformed all the positions, normals and light source position in view space, i.e camera space, but I don't know why it's not working.&lt;/p&gt;&#xA;" OwnerUserId="2096" LastEditorUserId="127" LastEditDate="2016-04-07T12:19:34.177" LastActivityDate="2016-04-07T14:59:33.647" Title="Why does my lighting look incorrect?" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;lighting&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="2009" PostTypeId="1" CreationDate="2016-02-07T17:31:50.120" Score="4" ViewCount="35" Body="&lt;p&gt;I am trying to plug in a 3D game engine as the rendering engine for an existing graphic API system that uses a right-handed coordinate system.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Game engines often uses a left handed coordinate system internally.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I convert the API parameters, 3d vectors, transformation matrices, and quaternions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The right handed versions at the API boundary have to be converted to left handed equivalents before passing to the engine's APIs at a lower layer, and then the output parameters from the engine have to be converted back to the right hand representations for the higher level APIs.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Both coordinate systems I am referring to here are, x pointing to the right, y pointing to the top, and z point away from the eye (left hand) and towards or behinds the eye (right hand), with the eye at the origin or some distance behind the xy plane to allow the origin to be in the camera view.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: For simple 3d-points, the conversion step would be just to flip the sign of the z value.&lt;/p&gt;&#xA;" OwnerUserId="2621" LastEditorUserId="231" LastEditDate="2016-02-07T19:48:24.747" LastActivityDate="2016-02-07T19:48:24.747" Title="implementing right handed coord system APIs on top of a left-handed rendering engine" Tags="&lt;transformations&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2010" PostTypeId="1" CreationDate="2016-02-07T22:50:50.250" Score="6" ViewCount="81" Body="&lt;p&gt;In a modern day desktop or mobile computer, you got the CPU, the RAM, the GPU and it's own video RAM. When a modern OS renderes an image to the screen, where is the actual image data for that screen located? (Talking about a default HDMI output port)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it within the memory of the OS Kernel in RAM, fetching the rendered image of an onscreen window from the GPU when preparing the next frame?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it within the GPUs VRAM, accessed and manipulated by low level CPU instructions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it within the GPUs VRAM, mapped via DMA to the CPUs adress space?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And also, where can I read up on implementing some of these basic access patterns for a custom written science project OS? A friend of mine managed to get the SVGA mode running in assembly, but I guess one can do more.&lt;/p&gt;&#xA;" OwnerUserId="2623" LastActivityDate="2016-04-08T06:34:40.257" Title="Where is the shown display image actually stored and accessed?" Tags="&lt;memory&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2011" PostTypeId="1" CreationDate="2016-02-07T23:01:51.920" Score="1" ViewCount="51" Body="&lt;p&gt;I am using a single VBO to store vertices in the follow format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;v1X, v1Y, v1Z, v1R, v1G, v1B, v2A,&#xA;v2X, ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vertex positioning is fine, shapes show up where expected, however instead of using the colour provided, all shapes show up red.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code given below simply draws two triangles to form one square ground shape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Buffer data preparation method&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public void prepare(float[] data) {&#xA;    glBindBuffer(GL_ARRAY_BUFFER, vboID);&#xA;    FloatBuffer dataBuffer = RenderUtils.fArrayToBuffer(data);&#xA;    if(dataLength != data.length) {&#xA;        glBufferData(GL_ARRAY_BUFFER, dataBuffer, GL_STATIC_DRAW);&#xA;        dataLength = data.length;&#xA;    } else {&#xA;        glBufferSubData(GL_ARRAY_BUFFER, 0, dataBuffer);&#xA;    }&#xA;    glVertexAttribPointer(0, 3, GL_FLOAT, false, 7*4, 0);&#xA;    glVertexAttribPointer(3, 4, GL_FLOAT, false, 7*4, 3*4);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Render code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;floorObj.prepare(new float[]{&#xA;    -5, 0, -5,   1, 0, 0, 1,&#xA;    5, 0, -5,   0, 1, 0, 1,&#xA;    -5, 0, 5,   0, 0, 1, 1,&#xA;&#xA;    5, 0, -5,   1, 0, 0, 1,&#xA;    5, 0, 5,   0, 1, 0, 1,&#xA;    -5, 0, 5,   0, 0, 1, 1,&#xA;});&#xA;glDrawArrays(GL_TRIANGLES, 0, 6);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Vertex shader&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 330 core&#xA;&#xA;in vec3 position;&#xA;in vec4 i_color;&#xA;out vec4 color;&#xA;&#xA;uniform mat4 transform;&#xA;&#xA;void main(){&#xA;    gl_Position = transform * vec4(position, 1);&#xA;    color = i_color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fragment shader&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 330 core&#xA;&#xA;in vec4 color;&#xA;out vec4 f_color;&#xA;&#xA;void main(){&#xA;    f_color = color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As previously stated, vertex positions work fine, however colour does not.&#xA;Just ask if any other code would be useful to determine the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks, - Jasper&lt;/p&gt;&#xA;" OwnerUserId="2622" LastActivityDate="2016-02-07T23:01:51.920" Title="OpenGL - Colours not working properly" Tags="&lt;opengl&gt;&lt;color&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2012" PostTypeId="2" ParentId="1901" CreationDate="2016-02-08T02:09:01.450" Score="2" Body="&lt;p&gt;why not building a bounding box (or spheres) hierarchy ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(but for a shadertoy implementation, the lack of dynamic loop length might spoil the gain ).&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-02-08T02:09:01.450" CommentCount="2" />
  <row Id="2013" PostTypeId="1" CreationDate="2016-02-08T12:46:04.260" Score="1" ViewCount="27" Body="&lt;p&gt;The documentation of &lt;code&gt;glFramebufferRenderbuffer&lt;/code&gt; only says that &lt;code&gt;renderbuffertarget&lt;/code&gt; needs to be &lt;code&gt;GL_RENDERBUFFER&lt;/code&gt;, though it does not specify that the attached renderbuffer, that is also given to the function, needs to be bound to &lt;code&gt;GL_RENDERBUFFER&lt;/code&gt; beforehand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I deem it not having to be so, because why would I have to specify the renderbuffer again if it should be bound beforehand?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Especially since the glNamedFramebufferRenderbuffer was introduced, it should not be nesseccary to bind anything.&lt;/p&gt;&#xA;" OwnerUserId="2623" LastActivityDate="2016-02-08T12:46:04.260" Title="Renderbuffer needs to be bound before glFramebufferRenderbuffer?" Tags="&lt;opengl&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2014" PostTypeId="1" CreationDate="2016-02-08T13:56:57.303" Score="5" ViewCount="91" Body="&lt;p&gt;I am implementing a simple Phong shader in OpenGL GLSL, and the test object is the utah teapot. However on the bottom I get a solid red circle, and on the top there is are sharp sectors that are coloured incorrectly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to fix these issues? What would be the issue in the first place?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/yn0z3.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/yn0z3.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/dlC2c.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dlC2c.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2630" LastActivityDate="2016-02-11T17:05:52.757" Title="Artefacts on top and bottom of utah teapot" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="2016" PostTypeId="1" CreationDate="2016-02-09T07:58:32.120" Score="1" ViewCount="44" Body="&lt;p&gt;I recently started to create a shadow mapping system in OpenTK.&#xA;I ran into a problem: The depthtexture is always completly white -&gt; no depth. Even if the color from the shader is changed (I know it shouldn't effect anything.).&#xA;Here is my shadow mapper class:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class ShadowMapper&#xA;{&#xA;    static int frameBuffer = 0;&#xA;    static int depthTexture;&#xA;    static readonly int shadowMapResolution = 1024;&#xA;    static ShadowShader shader;&#xA;    public static void Init()&#xA;    {&#xA;        shader = new ShadowShader();&#xA;&#xA;        frameBuffer = GL.GenFramebuffer();&#xA;        GL.BindFramebuffer(FramebufferTarget.Framebuffer, frameBuffer);&#xA;        depthTexture = GL.GenTexture();&#xA;        GL.BindTexture(TextureTarget.Texture2D, depthTexture);&#xA;        GL.TexImage2D(TextureTarget.Texture2D, 0, PixelInternalFormat.DepthComponent16, shadowMapResolution, shadowMapResolution, 0, PixelFormat.DepthComponent, PixelType.Float, (IntPtr)null);&#xA;        GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureMagFilter, (int)TextureMagFilter.Nearest);&#xA;        GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureMinFilter, (int)TextureMinFilter.Nearest);&#xA;        GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureWrapS, (int)TextureWrapMode.ClampToEdge);&#xA;        GL.TexParameter(TextureTarget.Texture2D, TextureParameterName.TextureWrapT, (int)TextureWrapMode.ClampToEdge);&#xA;&#xA;        GL.FramebufferTexture(FramebufferTarget.Framebuffer, FramebufferAttachment.DepthAttachment, depthTexture, 0);&#xA;&#xA;        GL.DrawBuffer(DrawBufferMode.None);&#xA;        Console.WriteLine(GL.CheckFramebufferStatus(FramebufferTarget.Framebuffer) == FramebufferErrorCode.FramebufferComplete);&#xA;        GL.BindFramebuffer(FramebufferTarget.Framebuffer, 0);&#xA;    }&#xA;&#xA;    public static void Render(Dictionary&amp;lt;TexturedModel, List&amp;lt;Entity&amp;gt;&amp;gt; entities, Light sun)&#xA;    {&#xA;        Matrix4 depthProjectionMatrix = Matrix4.CreateOrthographic(-10, 10, -10, 10);&#xA;        Matrix4 depthViewMatrix = Matrix4.LookAt(sun.Position, sun.ToLookPosition, new Vector3(0, 1, 0));&#xA;&#xA;        GL.BindTexture(TextureTarget.Texture2D, 0);&#xA;        GL.BindFramebuffer(FramebufferTarget.Framebuffer, frameBuffer);&#xA;        GL.Viewport(0, 0, shadowMapResolution, shadowMapResolution);&#xA;&#xA;        GL.Enable(EnableCap.DepthTest);&#xA;        GL.Clear(ClearBufferMask.DepthBufferBit);&#xA;        shader.Start();&#xA;        foreach (TexturedModel model in entities.Keys)&#xA;        {&#xA;            RawModel rawModel = model.Model;&#xA;            GL.BindVertexArray(rawModel.VaoID);&#xA;            GL.EnableVertexAttribArray(0);&#xA;            foreach (Entity entity in entities[model])&#xA;            {&#xA;                Matrix4 depthModelMatrix = Maths.CreateTransformationMatrix(entity.Position, entity.RotationX, entity.RotationY, entity.RotationZ, entity.Scale);&#xA;                Matrix4 depthMVP = depthProjectionMatrix * depthViewMatrix * depthModelMatrix;&#xA;                shader.LoadMvpMatrix(depthMVP);&#xA;&#xA;                GL.DrawElements(PrimitiveType.Triangles, rawModel.VertexCount, DrawElementsType.UnsignedInt, 0);&#xA;            }&#xA;        }&#xA;&#xA;        GL.DisableVertexAttribArray(0);&#xA;        GL.BindVertexArray(0);&#xA;        GL.Disable(EnableCap.DepthTest);&#xA;        shader.Stop();&#xA;        GL.BindFramebuffer(FramebufferTarget.Framebuffer, 0);&#xA;        GL.Viewport(0, 0, Window.Instance.Width, Window.Instance.Height);&#xA;    }&#xA;&#xA;    public static int DepthTexture&#xA;    {&#xA;        get&#xA;        {&#xA;            return depthTexture;&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And my vertex/fragment shaders:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Vertex Shader:&#xA; #version 330 core&#xA;&#xA;// Input vertex data, different for all executions of this shader.&#xA;in vec3 vertexPosition_modelspace;&#xA;&#xA;// Values that stay constant for the whole mesh.&#xA;&#xA;uniform mat4 depthMVP;&#xA;&#xA;void main(){&#xA;&#xA;gl_Position =  depthMVP * vec4(vertexPosition_modelspace,1);&#xA;&#xA;}&#xA;&#xA;&#xA;Fragment Shader:&#xA; #version 330 core&#xA;&#xA;&#xA;&#xA; // Ouput data&#xA;&#xA; out float fragmentdepth;&#xA; out vec4 out_colour;&#xA;&#xA;&#xA; void main(){&#xA;&#xA; // Not really needed, OpenGL does it anyway&#xA;&#xA; fragmentdepth = gl_FragCoord.z;&#xA; out_colour = vec4(1.0, 0.0, 0.0, 0.0);&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help?&lt;/p&gt;&#xA;" OwnerUserId="2637" LastActivityDate="2016-02-09T07:58:32.120" Title="Shadow Mapping Errors" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;shadow&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="2017" PostTypeId="2" ParentId="2014" CreationDate="2016-02-09T09:34:49.827" Score="4" Body="&lt;p&gt;I'm fairly certain your problem lies in not handling so-called &lt;em&gt;degenerate&lt;/em&gt; Bezier patches correctly, in particular (as &lt;a href=&quot;https://computergraphics.stackexchange.com/users/38/joojaa&quot;&gt;Joojaa&lt;/a&gt; noted), the computation of the surface normal.  I say &lt;em&gt;&quot;so-called degenerate&quot;&lt;/em&gt; because, geometrically, the surfaces are often perfectly well behaved. It's just that some assumptions people frequently make regarding the parametric equations may not hold.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Books such as Gerald Farin's, snappily titled, &lt;em&gt;&quot;Curves and Surfaces for CAGD. A Practical Guide&quot;&lt;/em&gt; will give more details, but I'll try to summarise two simple cases. Now assuming your Bezier is defined as $\bar{B}(u,v)$ the usual two causes of problems are:&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Zero derivatives:&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;To compute the normal at $(a,b)$ one normally (pardon the pun) computes the two derivatives,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\frac{\partial }{\partial v}\bar{B}(a,b)$ and $\frac{\partial }{\partial u}\bar{B}(a,b)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;or scaled versions thereof&lt;/em&gt;, to obtain two tangents and then take the cross product. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;(&lt;em&gt;Implementation note: Since we can use a scaled version of the tangent in the calculation, we really don't have to calculate the actual derivative. For example, especially at the corners, differences of control points can yield a scaled tangent. However for brevity in this discussion we will assume the actual derivatives&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A common occurrence, at least at the corners, is that the 1st partial derivatives at the location can be zero, which leads to an incorrect normal, i.e. a zero vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of the tops and bottoms of the teapot, one whole edge of a number of the (bicubic) Bezier patches has been collapsed to a point, i.e. all 4 control points are the same, and thus, say &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\bar{B}(0,v)==\bar{B}(1,v)$   and   $\frac{\partial }{\partial u}\bar{B}(0,0)=\bar{0}$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The surface, however, is completely well behaved so you can simply choose another derivative that starts at that collapsed point. In this case, say, choosing&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\frac{\partial }{\partial v}\bar{B}(1,0)$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;for the second tangent.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having said this, you &lt;em&gt;still&lt;/em&gt; have to check that your first derivatives are not zero for another reason (e.g 2 or 3 coincident control points), in which case, you can fall back (thanks to L'Hopital's rule) on the second (or if that's zero, even the third!) derivative(s) to obtain valid tangents.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Parallel tangents:&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Another similar problem can arise if your two tangents, are &lt;em&gt;parallel&lt;/em&gt;. - Farin has a good example in his book. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case, I &lt;em&gt;think&lt;/em&gt; you may need to look at using something like $\frac{\partial^2 }{\partial u \partial v}\bar{B}$ or, possibly, just fall back to using a small UV offset to approximate a vector.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-02-11T17:05:52.757" LastActivityDate="2016-02-11T17:05:52.757" CommentCount="0" />
  <row Id="2018" PostTypeId="1" AcceptedAnswerId="2020" CreationDate="2016-02-09T12:07:56.817" Score="11" ViewCount="1539" Body="&lt;p&gt;In recent games I have noticed something called Tessellation, Turning the thing ON destroys my frame rate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have noticed that it when turned on it looks like Anti - Aliasing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone give me further information on what exactly the GPU does.&lt;/p&gt;&#xA;" OwnerUserId="2641" LastActivityDate="2016-02-09T23:53:36.433" Title="What is Tessellation in computer graphics" Tags="&lt;pixel-graphics&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="1" />
  <row Id="2019" PostTypeId="2" ParentId="2018" CreationDate="2016-02-09T12:59:18.023" Score="6" Body="&lt;p&gt;It activate 3 stages in the pipeline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first is the tessellation control shader (hull shader in D3D) which looks at a set of vertices and then outputs how it should be divided up in separate triangles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second is a fixed function stage that will generate the requested triangles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The third stage the tessellation evalutation shader (domain shader in D3D) which is run per generated vertex and will put it in the correct place based on the barycentric coordinates of the generated vertex. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is used for level of detail to generate more triangles when the mesh is closer to the camera.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-02-09T12:59:18.023" CommentCount="0" />
  <row Id="2020" PostTypeId="2" ParentId="2018" CreationDate="2016-02-09T13:14:58.337" Score="21" Body="&lt;p&gt;Tesselation is a technique that allows you to generate primitives (triangles, lines, points and such) on the graphics-card. Specifically, it lets you repeatedly subdivide the current geometry into a finer mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows you to load a relatively coarse mesh on your graphics card, generate more vertices and triangles dynamically and then have a mesh on the screen that looks much smoother.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uT6do.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uT6do.jpg&quot; alt=&quot;Tessellation example&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most of the time this tesselation is done anew in each single frame and this could be the reason that your frame-rate drops once you enable this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tesselation is done in multiple stages, and it is done AFTER the vertex shader. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/31tTU.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/31tTU.png&quot; alt=&quot;Stages&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The terms for each stage varies based on the API. In DirectX, it is the Hull Shader, Hardware Tessellation, and the Domain Shader. In OpenGL, they are called the Tessellation Control Shader, Tesselation Primitive Generation, and the Tessellation Evaluation Shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first and the last stage is programmable, the actual tesselation is done by the hardware in a fixed function stage.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the Tesselation Control shader you set the type and number of subdivisions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then the hardware tessellator divides the geometry according to the Control Shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, the Tessellation Evaluation Shader is called for each newly generated vertex. In this shader, you set the type of primitive you want to generate and how the vertices are spaced and many other things. This shader can also be used to do all sorts of per-vertex calculations just like a vertex shader. It is guaranteed to be called at least once for each generated vertex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to do any further work on the primitives, you can add a Geometry shader.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="273" LastEditDate="2016-02-09T23:53:36.433" LastActivityDate="2016-02-09T23:53:36.433" CommentCount="1" />
  <row Id="2021" PostTypeId="1" CreationDate="2016-02-09T17:21:34.743" Score="6" ViewCount="173" Body="&lt;p&gt;Completely stuck on how to do this. Yes, it is a homework problem, but its the first we've had in the class so not too familiar with graphics. Also, seeing as its homework, please don't just give the answer, just some pointers in the right direction would be great.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried to do it as a triangle fan, and as a polygon but that just ends up in really weird shapes.&lt;/p&gt;&#xA;" OwnerUserId="2645" LastActivityDate="2016-02-10T15:22:58.323" Title="OpenGL with SFML, create an n-pointed star?" Tags="&lt;opengl&gt;&lt;c++&gt;&lt;maths&gt;&lt;computational-geometry&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2022" PostTypeId="1" CreationDate="2016-02-09T18:17:37.490" Score="0" ViewCount="68" Body="&lt;p&gt;Drawing a star using GL_Polygon and a VAO, The outline of the star shows up perfectly but the fill connects the first vertex with the 3rd and 8th vertices so it fills a little outside the star and I'm not sure why.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/znyBv.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/znyBv.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2645" LastActivityDate="2016-02-09T21:14:08.597" Title="OpenGL polygon problem" Tags="&lt;opengl&gt;&lt;polygon&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="2023" PostTypeId="1" AcceptedAnswerId="2028" CreationDate="2016-02-09T18:48:52.057" Score="5" ViewCount="66" Body="&lt;p&gt;I'm attempting to model a simple graphics pipeline - using Matlab at the moment as a modelling tool to get the transformations correct. I appreciate there are software tools that would make this easier - but I'm looking to understand the maths behind it and hence am looking to mostly use simple functions &amp;amp; matrices that I'm learning from a &lt;a href=&quot;http://www.amazon.co.uk/3D-Computer-Graphics-Alan-Watt/dp/0201398559&quot; rel=&quot;nofollow&quot;&gt;book&lt;/a&gt; (very retro!).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've successfully got through the stages of defining simple objects and converting them into a universal world space - but have come unstuck with the mathematics required to convert an object into view space and the back face culling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe my view space transformation is correct because when I plot the composite vectors they appear correct - but - when I do the back-face culling, it seems to fail to remove the correct triangles. Given that it depends only on two things, the line-of-sight vector and face normals, I can't work out what I'm doing wrong.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When defining the triangles in local definition space, I did it so that all the normals pointed outwards. I've put my results into the image below&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/XeAiI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/XeAiI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions are:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Have I gone wrong, or are my expectations incorrect?&lt;/li&gt;&#xA;&lt;li&gt;If so where, and how do I fix?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Progress&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I've plotted the normal's of all the shapes when in view space. They have all been inverted and now point inwards. Is this a property of the transformation and could it be responsible - or should this have no effect because all the polygons are affected the same?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Have changed the code to show this)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;clc; clear all; close all;&#xA;%============Initial verticies &amp;amp; Faces of the shape===========&#xA;[s1_vtx,s1_fcs] = Pyramid();&#xA;[s2_vtx,s2_fcs] = Cube();&#xA;%==============Transform Shape 1 ======================&#xA;Tx = 0; Ty = 0; Tz = 0; %Translation vectors for x,y,z axes&#xA;Sx = 2; Sy = 2; Sz = 2;%Scaling factors in x,y,z dimensions&#xA;Rx = pi/2; Ry = pi/4; Rz = pi/4; %Rotating factors in x,y,z dimensions&#xA;transform = scale(Sx,Sy,Sz)*rotate(Rx,0,0)*translate(Tx,Ty,Tz); %Merge transforms together&#xA;s1_vtx = transform*vertcat(s1_vtx,(ones(1,length(s1_vtx)))); %Add row of ones to end for multiplication&#xA;s1_vtx = s1_vtx(1:3,:); %And remove afterwards&#xA;%==============Transform Shape 2 ======================&#xA;Tx = 0.5; Ty = 0; Tz = 0.5;&#xA;transform = scale(1,2,1)*translate(Tx,Ty,Tz);&#xA;s2_vtx = transform*vertcat(s2_vtx,(ones(1,length(s2_vtx))));&#xA;s2_vtx = s2_vtx(1:3,:);&#xA;%======Create World Space ===========&#xA;ws_vtx = horzcat(s1_vtx(1:3,:), s2_vtx(1:3,:)); %remove homogenous column for patching&#xA;ws_fcs = horzcat(s1_fcs,(s2_fcs+(length(s1_vtx))));&#xA;%======Plot World Space ===========&#xA;grid on; hold on;&#xA;scatter3(ws_vtx(1,:),ws_vtx(2,:),ws_vtx(3,:)) %Plot all the points&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'none');&#xA;for i = 1:length(ws_vtx)&#xA;    str = sprintf('%d',i);&#xA;    text(ws_vtx(1,i),ws_vtx(2,i),ws_vtx(3,i), str,'FontSize',16, 'Color','r', 'FontWeight','b');&#xA;end&#xA;points = zeros(3,3); %Contains 1 triangle&#xA;for i = 1:length(ws_fcs); %For each triangle&#xA;    points(:,1:3) = ws_vtx(:,ws_fcs(1:3,i));&#xA;    U = points(:,2) - points(:,1); %Get two non-parallel vectors&#xA;    V = points(:,3) - points(:,1);&#xA;    average = [0,0,0];&#xA;    for j = 1:length(points)&#xA;        average(j) = (points(j,1) + points(j,2) + points(j,3))/3;&#xA;    end&#xA;    N = cross(U,V)/norm(cross(U,V)); %Normal, normalised to mag 1&#xA;    scatter3(average(1),average(2),average(3));&#xA;    plot3([average(1), average(1)+N(1)],[average(2), average(2)+N(2)],[average(3), average(3)+N(3)]);&#xA;end&#xA;%==================Create view matrix===================&#xA;focus = [1.5,0,1.5]; %The point we're looking at&#xA;Cx = 3; Cy = -3; Cz = 3; %Position of camera&#xA;Vspec = [0;0;1]; %Specified up direction&#xA;T = viewMat(focus, [Cx,Cy,Cz],Vspec); %Create viewspace transform matrix&#xA;p = norm(focus - [Cx,Cy,Cz]);&#xA;U = T(1,1:3); V = T(2,1:3); N = T(3,1:3); %New Up, Right &amp;amp; View direction vectors&#xA;%============Plot the camera vectors=================&#xA;scatter3(Cx,Cy,Cz,'s') %Plot the camera position&#xA;plot3([Cx, Cx+p*N(1)],[Cy, Cy+p*N(2)],[Cz, Cz+p*N(3)]);&#xA;plot3([Cx, Cx+V(1)],[Cy, Cy+V(2)],[Cz, Cz+V(3)]);&#xA;plot3([Cx, Cx+U(1)],[Cy, Cy+U(2)],[Cz, Cz+U(3)]);&#xA;%==================Transform into View Space===================&#xA;ws_vtx = T*vertcat(ws_vtx,(ones(1, length(ws_vtx)))); %Transform matrix&#xA;ws_vtx = ws_vtx(1:3,:); %Remove homogenous dimension&#xA;origin = T*[Cx;Cy;Cz;1]; %Transform origin&#xA;Cx = origin(1); Cy = origin(2); Cz = origin(3); %remove homogenous dimension&#xA;focus = (T*horzcat(focus,1)')';%Transform focus point&#xA;focus = focus(:,1:3);%remove homogenous dimension&#xA;%==================Plot View Space=================&#xA;figure(); hold on; grid on;&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'none');&#xA;scatter3(Cx, Cy, Cz, 's');&#xA;scatter3(focus(1), focus(2), focus(3), 's');&#xA;plot3([Cx, focus(1)],[Cy, focus(2)],[Cz,focus(3)], 'g');&#xA;for i = 1:length(ws_vtx)&#xA;    str = sprintf('%d',i);&#xA;    text(ws_vtx(1,i),ws_vtx(2,i),ws_vtx(3,i), str,'FontSize',16, 'Color','r', 'FontWeight','b');&#xA;end&#xA;%================Plot normals of world space==============&#xA;for i = 1:length(ws_fcs); %For each triangle&#xA;    points(:,1:3) = ws_vtx(:,ws_fcs(1:3,i));&#xA;&#xA;    U = points(:,2) - points(:,1); %Get two non-parallel vectors&#xA;    V = points(:,3) - points(:,1);&#xA;&#xA;    average = [0,0,0];&#xA;    for j = 1:length(points)&#xA;        average(j) = (points(j,1) + points(j,2) + points(j,3))/3;&#xA;    end&#xA;    N = cross(U,V)/norm(cross(U,V)); %Normal, normalised to mag 1&#xA;    scatter3(average(1),average(2),average(3));&#xA;    plot3([average(1), average(1)+N(1)],[average(2), average(2)+N(2)],[average(3), average(3)+N(3)]);&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2646" LastEditorUserId="231" LastEditDate="2016-02-13T01:12:51.580" LastActivityDate="2016-02-13T01:12:51.580" Title="Graphics Pipeline: Viewspace &amp; Back face culling incorrectly" Tags="&lt;transformations&gt;&lt;3d&gt;&lt;matrices&gt;&lt;vectors&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2024" PostTypeId="2" ParentId="2021" CreationDate="2016-02-09T19:57:51.373" Score="2" Body="&lt;p&gt;A simple way would be to think about how you can decompose your star into triangles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then just draw these triangles. Not as a triangle-fan since those may be a bit strange to use.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since triangles are one of the most basic and standard ways to draw all kinds of meshes and shapes (every polygon can be triangulated as far as I know), I suggest du don't use the polygon draw as a beginner. Begin with the basic stuff everyone uses.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-02-09T19:57:51.373" CommentCount="0" />
  <row Id="2026" PostTypeId="2" ParentId="2021" CreationDate="2016-02-09T21:44:18.417" Score="3" Body="&lt;p&gt;Try starting at the origin, then dividing 360 degrees by n to find some useful angles for the points of your star.&#xA;I hope this drawing will help you to find a solution.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/CCGsh.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CCGsh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that you have the angle theta, and the values of alpha and x are chosen by you, you have the tools to calculate all required vertices to make the star using trigonometry.&lt;/p&gt;&#xA;" OwnerUserId="2651" LastEditorUserId="2651" LastEditDate="2016-02-10T15:22:58.323" LastActivityDate="2016-02-10T15:22:58.323" CommentCount="2" />
  <row Id="2028" PostTypeId="2" ParentId="2023" CreationDate="2016-02-10T13:29:37.180" Score="6" Body="&lt;p&gt;I (believe) I've solved this (even if it has taken 2 days). My problem was essentially I wanted to take the dot product of the face normal, and line-of-sight vector like below&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zf6LH.gif&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zf6LH.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And determine the angle to see if the face was looking towards or away from the view point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My erroneous step was that I was doing this AFTER transforming from world-space to view-space - hence the line-of-sight vector I was using was no longer valid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence, to solve this, I simply performed the back-face culling in world-space, prior to the world-view space transformation!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've included functioning code showing the back face culling, but not the view-space transform.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;clear; clc; close all;&#xA;%======Create World Space (hard-coded values for demo) ===========&#xA;ws_vtx = [0,2,0,2,1,0.5,1.5,0.5,1.5,0.5,1.5,0.5,1.5;&#xA;    0,0,0,0,-2,0,0,2,2,0,0,2,2;&#xA;    0,0,2,2,1,0.5,0.5,0.5,0.5,1.5,1.5,1.5,1.5];&#xA;ws_fcs = [1,2,4,3,3,1,6,6,6,6,7,7,9,8,8,8,10,10;&#xA;    2,4,3,1,4,4,9,8,7,11,9,13,8,12,10,6,11,13;&#xA;    5,5,5,5,1,2,7,9,11,10,13,11,13,13,12,10,13,12];&#xA;%==================Create view matrix===================&#xA;focus = [1.5,0,1.5]; %The point we're looking at&#xA;Cx = 3; Cy = -3; Cz = 3; %Position of camera&#xA;Vspec = [0;0;1]; %Specified up direction&#xA;T = viewMat(focus, [Cx,Cy,Cz],Vspec); %Create viewspace transform matrix&#xA;p = norm(focus - [Cx,Cy,Cz]);&#xA;U = T(1,1:3); V = T(2,1:3); N = T(3,1:3); %New Up, Right &amp;amp; View direction vectors&#xA;%============Plot the camera vectors=================&#xA;grid on; hold on; scatter3(Cx,Cy,Cz,'s'); %Plot the camera position&#xA;plot3([Cx, Cx+p*N(1)],[Cy, Cy+p*N(2)],[Cz, Cz+p*N(3)],'g');&#xA;plot3([Cx, Cx+V(1)],[Cy, Cy+V(2)],[Cz, Cz+V(3)],'g');&#xA;plot3([Cx, Cx+U(1)],[Cy, Cy+U(2)],[Cz, Cz+U(3)],'g');&#xA;%===========Get Face Normals============================&#xA;norm_fcs = zeros(3,length(ws_fcs)); &#xA;    for i = 1:length(ws_fcs); %For each triangle&#xA;        points = zeros(3,3); %Contains 1 triangle&#xA;        points(:,1:3) = ws_vtx(:,ws_fcs(1:3,i)); %Get points for triangle&#xA;        U = points(:,2) - points(:,1); %Get two non-parallel vectors&#xA;        V = points(:,3) - points(:,1);&#xA;        norm_fcs(:,i) = cross(U,V); %Normal, normalised to mag 1&#xA;    end&#xA;%=================Back Face Culling======================&#xA;null_vals = 0;&#xA;for i = 1:length(ws_fcs) %Take each triangle &amp;amp; calculate normal&#xA;    if dot(norm_fcs(:,i), N) &amp;gt; 0 %Dot product line of sight &amp;amp; normal of faces&#xA;        ws_fcs(:,i) = [0;0;0]; %If &amp;gt; 0, not visible &amp;amp; remove&#xA;        null_vals = null_vals + 1; %And increment counter&#xA;    end&#xA;end&#xA;ws_fcs_cat = zeros(3, length(ws_fcs) - null_vals); %Create new array&#xA;null_vals = 0;&#xA;for i = 1:length(ws_fcs)&#xA;    if norm(ws_fcs(:,i)) == 0&#xA;        null_vals = null_vals + 1;&#xA;    else&#xA;        ws_fcs_cat(:,i - null_vals) = ws_fcs(:,i);&#xA;    end&#xA;end&#xA;ws_fcs = ws_fcs_cat;&#xA;%======Plot World Space ===========&#xA;scatter3(ws_vtx(1,:),ws_vtx(2,:),ws_vtx(3,:)) %Plot all the points&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'r', 'FaceAlpha', 0.5);&#xA;for i = 1:length(ws_vtx)&#xA;    str = sprintf('%d',i);&#xA;    text(ws_vtx(1,i),ws_vtx(2,i),ws_vtx(3,i), str,'FontSize',16, 'Color','r', 'FontWeight','b');&#xA;end&#xA;points = zeros(3,3); %Contains 1 triangle&#xA;for i = 1:length(ws_fcs); %For each triangle&#xA;    points(:,1:3) = ws_vtx(:,ws_fcs(1:3,i));&#xA;    U = points(:,2) - points(:,1); %Get two non-parallel vectors&#xA;    V = points(:,3) - points(:,1);&#xA;    average = [0,0,0];&#xA;    for j = 1:length(points)&#xA;        average(j) = (points(j,1) + points(j,2) + points(j,3))/3;&#xA;    end&#xA;    N = cross(U,V)/norm(cross(U,V)); %Normal, normalised to mag 1&#xA;    scatter3(average(1),average(2),average(3));&#xA;    plot3([average(1), average(1)+N(1)],[average(2), average(2)+N(2)],[average(3), average(3)+N(3)]);&#xA;end&#xA;xlabel('x'); ylabel('y'); zlabel('z');&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2646" LastActivityDate="2016-02-10T13:29:37.180" CommentCount="0" />
  <row Id="2029" PostTypeId="1" CreationDate="2016-02-11T04:07:50.647" Score="4" ViewCount="43" Body="&lt;p&gt;I'm a beginner in OpenGL and I've been doing some research on the topic of shaders and attributes. Most places I go say that explicit attribute binding (whether it be in the shader itself with &lt;code&gt;layout(location=x)&lt;/code&gt; or with &lt;code&gt;glBindAttribLocation&lt;/code&gt;) is better than letting OpenGL generate some attribute number. However, I didn't completely understand WHY explicit attribute binding is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I guess my question is, what are the pros/cons to explicit/automatic attribute binding, and which one is more conducive to better program design overall?&lt;/p&gt;&#xA;" OwnerUserId="2651" LastActivityDate="2016-02-11T04:07:50.647" Title="Explicit vs Automatic Attribute Binding" Tags="&lt;opengl&gt;&lt;shader&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="2030" PostTypeId="1" CreationDate="2016-02-11T10:06:35.107" Score="5" ViewCount="72" Body="&lt;p&gt;I am implementing a simple ray tracer with OpenGL. I have a shader storage buffer with all the triangles so I can test them for intersections in a compute shader. It works fine up to a certain buffer size. But if I have more than a certain amount, it stops working and I get the following message:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;OpenGL: Buffer usage warning: Discarding a video memory only buffer object. The data store will be reallocated on next usage of the buffer object.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The program does not crash however.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;GL_MAX_SHADER_STORAGE_BLOCK_SIZE&lt;/code&gt; is 2147483647, so 2GB. The size of the buffer when it doesn't work anymore is just a couple of MB, around 6 to 7 MB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I overlooking something? Are there other limits I don't know about?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My specs: Linux Mint 17.2, GTX 980Ti with 6GB of VRAM&lt;/p&gt;&#xA;" OwnerUserId="2666" LastEditorUserId="2666" LastEditDate="2016-03-31T11:34:58.573" LastActivityDate="2016-03-31T18:59:31.903" Title="Cannot use more than a couple of MB in a shader storage buffer" Tags="&lt;opengl&gt;" AnswerCount="0" CommentCount="16" />
  <row Id="2031" PostTypeId="1" AcceptedAnswerId="2032" CreationDate="2016-02-11T12:57:13.237" Score="4" ViewCount="52" Body="&lt;p&gt;What 3D scene reconstruction methods does the term &quot;multi-view stereo&quot; encompass? Is it only used for methods that apply binocular stereo algorithms (taking 2 views as input) in a pairwise manner? Or only methods based on disparity estimation? If not why is the term &quot;stereo&quot; used?&lt;/p&gt;&#xA;" OwnerUserId="2670" LastEditorUserId="231" LastEditDate="2016-02-11T19:48:54.940" LastActivityDate="2016-02-11T19:48:54.940" Title="What is stereo about multi-view stereo?" Tags="&lt;3d&gt;&lt;depth-map&gt;&lt;photometric&gt;&lt;scene-description&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2032" PostTypeId="2" ParentId="2031" CreationDate="2016-02-11T15:08:51.003" Score="5" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Multi-view stereo (MVS) is the general term&#xA;  given to a group of techniques that use stereo correspondence as their&#xA;  main cue and use more than two images.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;A quote from &lt;a href=&quot;http://www.cse.wustl.edu/~furukawa/papers/fnt_mvs.pdf&quot;&gt;'Multi-View Stereo: A Tutorial'&lt;/a&gt; by Yasutaka Furukawa and Carlos Hernández.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So to paraphrase: We have a set of images that is larger than two, and use them in a pairwise manner by applying techniques that finde stereo correspondences to reconstruct the object shown in them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to know more about MVS algorithms that paper I linked seems to be pretty good and at least Furukawa is a name that I know and have read papers from, so he seems to know the topic.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-02-11T15:08:51.003" CommentCount="0" />
  <row Id="2034" PostTypeId="1" AcceptedAnswerId="2038" CreationDate="2016-02-12T08:05:05.823" Score="9" ViewCount="86" Body="&lt;p&gt;Why is it that when you zoom in to some black text you find that it is made up of orange, slightly black and blue pixels like the picture below. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/0gIia.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0gIia.png&quot; alt=&quot;Black text zoomed in&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2673" LastEditorUserId="38" LastEditDate="2016-02-12T20:14:59.897" LastActivityDate="2016-02-12T20:50:25.683" Title="why does black text have orange and blue pixels" Tags="&lt;antialiasing&gt;&lt;font-rendering&gt;&lt;subpixel-rendering&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2035" PostTypeId="1" AcceptedAnswerId="2036" CreationDate="2016-02-12T13:32:56.290" Score="6" ViewCount="74" Body="&lt;p&gt;For a non-computer graphics expert, what happens technically when you do this? Are multiple pixels merged to one to compensate for the lower resolution? And in how far will this affect the image quality?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would a high resolution image look sharper on a low resolution display than if the same image would have the exact same resolution as the screen?&lt;/p&gt;&#xA;" OwnerUserId="2633" LastActivityDate="2016-02-12T13:50:28.900" Title="What happens when you display a high resolution image on a low resolution screen?" Tags="&lt;texture&gt;&lt;image&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2036" PostTypeId="2" ParentId="2035" CreationDate="2016-02-12T13:50:28.900" Score="3" Body="&lt;p&gt;If you scale an image down to a screen that has a lower resolution than yout image, you have to descide what you do, there is no single way of doing this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many different techniques that yield very different results.&#xA;This is very similar to displaying a low resolution image on a high resolution display where do the inverse thing by desciding how to fill the &quot;more&quot; pixels on the screen. There are also many techniques for doing this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/YQKdg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YQKdg.png&quot; alt=&quot;Image scaling comparison. Nearest-neighbor scaling (left) and 2*Sal scaling (right).&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The image shows two upscaling-techniques and shows how different the results can look if different techniques are used. (A nearest-neighbor scaling on the left and a 2*Sal on the right.) Imagine that there are also such differences if you downscale an image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can find a nice overview on different scaling techniques on the wikipedia page for &lt;a href=&quot;https://en.wikipedia.org/wiki/Image_scaling&quot; rel=&quot;nofollow&quot;&gt;Image scaling&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-02-12T13:50:28.900" CommentCount="0" />
  <row Id="2037" PostTypeId="1" AcceptedAnswerId="2047" CreationDate="2016-02-12T18:22:24.593" Score="2" ViewCount="134" Body="&lt;p&gt;I am attempting to model a simple graphics pipeline (i.e. Local-&gt;Word-&gt;View-&gt;Screen-&gt;2D spaces).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been looking at the algorithm required to transform from world to view-space and using the following transform&lt;a href=&quot;http://i.stack.imgur.com/NKmCc.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/NKmCc.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Although I have been using U,V,N,C notation rather than Xc,Yc,Zc,e.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And my matrix transformation at first glance, appears correct: When I plot the vectors onto world space, I get a vector from the camera position to the point we're looking at, and two correctly placed new X and Y vectors (see the left hand image below).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/RavTO.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RavTO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By looking along the line of sight vector (such that it disappears,and the focus &amp;amp; camera points overlap), as in the middle figure, should (I believe?) give an accurate example of what the resultant viewspace transformation should look like, with the green vertical and right-pointing vectors showing the new X &amp;amp; Y vectors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However - when I actually do the view space transform I don't get the middle figure, instead, the resultant transform seems to be looking at the object from a different position possibly aligned with one of the axes? This is shown in the right hand figure - which is actually looking at the object from a slightly lower perspective &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is essentially what have I done wrong, or am I misunderstanding both the transform and thus my results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks very much,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;David&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATES&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After stepping through the code - I discovered that my &lt;a href=&quot;https://books.google.co.uk/books/about/3D_Computer_Graphics.html?id=tQJEAQAAIAAJ&amp;amp;redir_esc=y&quot; rel=&quot;nofollow&quot;&gt;source&lt;/a&gt; book was having me convert the line-of-sight vector to spherical coords and then they were being converted straight back again to create R (viewspace coordination matrix). I've eliminated this redundancy (and potential source of errors) but it hasn't solved the problem...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;clear; clc; close all;&#xA;%======Create World Space (hard-coded values for demo) ===========&#xA;ws_vtx = [0,2,0,2,1,0.5,1.5,0.5,1.5,0.5,1.5,0.5,1.5;&#xA;    0,0,0,0,-2,0,0,2,2,0,0,2,2;&#xA;    0,0,2,2,1,0.5,0.5,0.5,0.5,1.5,1.5,1.5,1.5];&#xA;ws_fcs = [1,2,4,3,3,1,6,6,6,6,7,7,9,8,8,8,10,10;&#xA;    2,4,3,1,4,4,9,8,7,11,9,13,8,12,10,6,11,13;&#xA;    5,5,5,5,1,2,7,9,11,10,13,11,13,13,12,10,13,12];&#xA;%==================Create view matrix===================&#xA;focus = [1.5,0,1.5]; %The point we're looking at&#xA;Cx = 3; Cy = -3; Cz = 3; %Position of camera&#xA;Vspec = [0;0;1]; %Specified up direction for view space (i.e. camera orientation)&#xA;vector = focus - [Cx,Cy,Cz]; %Vector camera to focus point &#xA;p = sqrt(vector(1)^2 + vector(2)^2 + vector(3)^2); %Total magnitude of vector&#xA;N = vector'/p;&#xA;V = Vspec - cross(cross(Vspec,N),N); %Create new &quot;up&quot; direction&#xA;U = cross(N,V); %Create new x-axis&#xA;%Create rotational matrix to view space&#xA;R= [U(1),U(2),U(3),0; % U is direction of camera space X axis&#xA;    V(1),V(2),V(3),0; % V is direction of camera space Y axis&#xA;    N(1),N(2),N(3),0;&#xA;    0   ,   0,   0,1];&#xA;Tr = [1,0,0,-Cx;&#xA;      0,1,0,-Cy;&#xA;      0,0,1,-Cz;&#xA;      0,0,0,  1];&#xA;T = R*Tr; %Total view transform = rotation * translation&#xA;%============Plot the camera vectors &amp;amp; World Space=================&#xA;grid on; hold on; xlabel('x'); ylabel('y'); zlabel('z');&#xA;scatter3(Cx,Cy,Cz,'s'); %Plot the camera position&#xA;plot3([Cx, Cx+p*N(1)],[Cy, Cy+p*N(2)],[Cz, Cz+p*N(3)],'g'); %Plot camera vectors&#xA;plot3([Cx, Cx+V(1)],[Cy, Cy+V(2)],[Cz, Cz+V(3)],'g');&#xA;plot3([Cx, Cx+U(1)],[Cy, Cy+U(2)],[Cz, Cz+U(3)],'g');&#xA;scatter3(ws_vtx(1,:),ws_vtx(2,:),ws_vtx(3,:)) %Plot all the points&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'none');&#xA;for i = 1:length(ws_vtx)&#xA;    str = sprintf('%d',i);&#xA;    text(ws_vtx(1,i),ws_vtx(2,i),ws_vtx(3,i), str,'FontSize',14, 'Color','r');&#xA;end&#xA;%====================Viewspace Transform===========================&#xA;ws_vtx = T*vertcat(ws_vtx,(ones(1,length(ws_vtx)))); %Transform worldspace&#xA;ws_vtx = ws_vtx(1:3,:); %Remove homogenous column&#xA;position = T*[Cx;Cy;Cz;1]; focus = T*horzcat(focus,1)'; %Transform set points&#xA;position = position(1:3,:); focus = focus(1:3,:); %Remove homogenous columns&#xA;%================Plot new viewspace===============================&#xA;figure(); grid on; hold on; xlabel('x'); ylabel('y'); zlabel('z');&#xA;scatter3(position(1),position(2),position(3),'s'); %Plot new camera vectors&#xA;scatter3([position(1); position(1)+focus(1)],...&#xA;    [position(2); position(2)+focus(2)],[position(3); position(3)+focus(3)],'s');&#xA;plot3([position(1), focus(1)],[position(2), focus(2)],[position(3), focus(3)],'g');&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'none');&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2646" LastEditorUserId="2646" LastEditDate="2016-02-13T11:12:12.727" LastActivityDate="2016-02-18T22:21:47.970" Title="Correct view-space transform" Tags="&lt;transformations&gt;&lt;matrices&gt;&lt;matlab&gt;&lt;vectors&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="2038" PostTypeId="2" ParentId="2034" CreationDate="2016-02-12T20:50:25.683" Score="6" Body="&lt;p&gt;This is called subpixel rendering.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The different primary colors in your monitor are not stacked on top of each other. Instead they are arranged near each other. Different monitors have different patterns but most commonly they are aligned so that the colors are side by side. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you know the physical arrangement, then you can prepare the image so that you have calculated a different sample position for each color channel. In essence your boosting the resolution of your image so that you are treating each individual color as a separate pixel. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This offcourse means that you need to know the display orientation and have some data that can be shifted. The image wouldn't be all that useful on other monitors with different pixel alignments. So this is reserved for things that are dynamically generated, most often fonts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It also means that when zoomed or presented on a medium with different or no subpixels, the effect will register as a blur. So zooming in on the pixels of a subpixel rendered image is not very good idea. For this reason it migth be a good idea to disable the effect if you make images for others to use and the subpixel alignment is unknown.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-02-12T20:50:25.683" CommentCount="0" />
  <row Id="2039" PostTypeId="5" CreationDate="2016-02-13T01:11:18.203" Score="0" Body="" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-02-13T01:11:18.203" LastActivityDate="2016-02-13T01:11:18.203" CommentCount="0" />
  <row Id="2040" PostTypeId="4" CreationDate="2016-02-13T01:11:18.203" Score="0" Body="For questions about scalable 2D graphics based on vector operations, as opposed to raster graphics. For example, polygons, Bézier curves and ellipses." OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-02-13T01:11:18.203" LastActivityDate="2016-02-13T01:11:18.203" CommentCount="0" />
  <row Id="2041" PostTypeId="1" AcceptedAnswerId="2045" CreationDate="2016-02-13T21:08:08.733" Score="3" ViewCount="63" Body="&lt;p&gt;Open GL and other graphics APIs support floating point formats smaller than 32 bits (e.g. see &lt;a href=&quot;https://www.opengl.org/wiki/Small_Float_Formats&quot; rel=&quot;nofollow&quot;&gt;https://www.opengl.org/wiki/Small_Float_Formats&lt;/a&gt;). While GPUs seem to handle these formats natively, CPUs have limited support (x86 architecture chips can convert to/from 16 bit floats using the AVX instructions). It is sometimes convenient to pack/unpack these formats on the CPU , but it is not obvious how to do so efficiently.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the fastest techniques to convert between 32 bit floating point and the small float formats on the CPU? Are libraries or sample implementations available?&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-02-15T08:23:19.190" Title="What are the most efficient ways to convert between f32 and smaller float formats on the CPU?" Tags="&lt;gpu&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="2042" PostTypeId="1" CreationDate="2016-02-13T22:37:18.643" Score="6" ViewCount="127" Body="&lt;p&gt;I'm reading Cook's paper &quot;Stochastic Sampling and Distributed Ray Tracing&quot;, I don't understand how the rays are generated. He says:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Determine the focal point by constructing a ray from the eye point&#xA;  (center of the lens) through the screen location of the ray. The focal&#xA;  point is located on this ray so its distance from the eye point is&#xA;  equal to the focal distance.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/mxQ6c.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/mxQ6c.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't parse this image. Where is my screen plane located according to this image? What is the focal point? If you see some optics picture of a thin lens, the focal point is always in the same place, where all the rays converge. If it is always the same point, why do have I to calculate it for each ray?&lt;/p&gt;&#xA;" OwnerUserId="2684" LastEditorUserId="231" LastEditDate="2016-02-14T01:00:16.963" LastActivityDate="2016-02-16T18:38:54.543" Title="Ray tracing with thin lens camera" Tags="&lt;raytracing&gt;&lt;vectors&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="2" />
  <row Id="2043" PostTypeId="1" AcceptedAnswerId="2054" CreationDate="2016-02-14T07:51:07.610" Score="7" ViewCount="94" Body="&lt;p&gt;For mesh representation, what is the benefit of using Half Edge over Winged Edge data structure?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand both mesh representations, the only difference is that half edge uses directional edge and winged edge uses undirectional edge. So far, I cannot think of what is the usefulness of using directional edge, yet it only gives more memory consumption.&lt;/p&gt;&#xA;" OwnerUserId="2687" LastEditorUserId="2687" LastEditDate="2016-02-15T09:16:27.533" LastActivityDate="2016-02-16T19:47:22.990" Title="What is the benefit of using Half Edge over Winged Edge?" Tags="&lt;mesh&gt;&lt;polygon&gt;&lt;data-structure&gt;&lt;scene-graph&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="2044" PostTypeId="2" ParentId="2042" CreationDate="2016-02-14T16:54:51.157" Score="4" Body="&lt;p&gt;Instead of a screen plane in front of the eye, it describes a film plane, where the image is projected, to explicitly model camera optics. You don't need to compute the focal point in doing ray tracing - it's just a way to find the plane of focus for depth of field effects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For depth of field effects I use a standard perspective projection but jitter the position of the eye on a circle parallel to the focal plane -  and I make sure all my rays for a given pixel go through the same spot on the focal plane making it sharp while stuff in front of or behind that plane ends up blurry. It's a simple model of something like an aperture, and gives pretty good results.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-02-15T02:20:39.040" LastActivityDate="2016-02-15T02:20:39.040" CommentCount="0" />
  <row Id="2045" PostTypeId="2" ParentId="2041" CreationDate="2016-02-14T20:02:18.807" Score="5" Body="&lt;p&gt;You definitely want to check out meticulous half to float and float to half implementations by Fabian Giesen. I don't think there is anything faster than those:&#xA;&lt;a href=&quot;https://gist.github.com/rygorous/2144712&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/rygorous/2144712&lt;/a&gt;&#xA;&lt;a href=&quot;https://gist.github.com/rygorous/2156668&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/rygorous/2156668&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For other formats you will find an implementation in DirectXMath (part of WindowsSDK), Mesa and similar libraries. For example, R11G11B10_Float conversions are implemented by DirectXMath in &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/microsoft.directx_sdk.reference.xmfloat3pk(v=vs.85).aspx&quot; rel=&quot;nofollow&quot;&gt;XMFLOAT3PK&lt;/a&gt; and by Mesa in &lt;a href=&quot;https://github.com/laanwj/etna_viv/blob/master/src/minigallium/auxiliary/util/u_format_other.c&quot; rel=&quot;nofollow&quot;&gt;u_format_other.c&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="93" LastEditorUserId="93" LastEditDate="2016-02-15T08:23:19.190" LastActivityDate="2016-02-15T08:23:19.190" CommentCount="3" />
  <row Id="2046" PostTypeId="1" CreationDate="2016-02-15T11:08:29.323" Score="4" ViewCount="45" Body="&lt;p&gt;In the program I need to create, the user should specify the hull geometry of an aircraft wing and fuselage using cubic bezier polycurves. &#xA;But to get to &lt;code&gt;m&lt;/code&gt; bezier patch surfaces the user would need to introduce &lt;code&gt;4 * m&lt;/code&gt; additional control points. It's not as simple a task as outlining sections in Inkscape. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is whether there is a way to create a bicubic bezier surface having only its edge curves (or maybe any additional curves the user could draw)? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it isn't possible with bezier curves, is there any way to create a 3d model by specifying only section curves? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the answer is no, what way would you suggest to create hull geometry (maybe a bit rough) from another 3D model? &lt;/p&gt;&#xA;" OwnerUserId="2694" LastEditorUserId="231" LastEditDate="2016-02-16T01:33:33.340" LastActivityDate="2016-02-16T17:58:09.817" Title="Bicubic bezier surface from 4 bezier curves" Tags="&lt;bezier-curve&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2047" PostTypeId="2" ParentId="2037" CreationDate="2016-02-15T13:39:47.217" Score="2" Body="&lt;p&gt;Your transform looks correct. To transform from world to eye coordinates, I I always use a &quot;lookat&quot; transform, defined by 3 vectors: $\bf{e}$, $\bf{a}$ and $\bf{u}$; in english, the eye position, the point it's looking at, and an up vector, which must not be in the same direction as $\bf{a} - \bf{e}$ (more specifically, not a multiple of it).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The space is defined using $\bf{z} = {{(\bf{e} - \bf{a})}\over{|\bf{e} - \bf{a}}|}$, which means the negative z axis points in the direction of what I'm looking at (this works well for OpenGL); $\bf{x} = {{\bf{u} \times \bf{z}}\over{|\bf{u} \times \bf{z}|}}$ and $\bf{y} = \bf{z} \times \bf{x}$, which, again, for OpenGL, makes $\bf{x}$ rightward on the screen and $\bf{y}$ upward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To transform into eye coordinates is a matter of subtracting they eye's position, then projecting into the space defined by the vectors above:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix} \bf{x}_x &amp;amp; \bf{x}_y &amp;amp; \bf{x}_z &amp;amp; 0 \\ \bf{y}_x &amp;amp; \bf{y}_y &amp;amp; \bf{y}_z &amp;amp; 0 \\ \bf{z}_x &amp;amp; \bf{z}_y &amp;amp; \bf{z}_z &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; -\bf{e}_x \\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; -\bf{e}_y \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; -\bf{e}_z \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which is exactly what you have to begin with.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-02-16T04:09:53.407" LastActivityDate="2016-02-16T04:09:53.407" CommentCount="2" />
  <row Id="2048" PostTypeId="2" ParentId="2046" CreationDate="2016-02-15T17:23:09.787" Score="3" Body="&lt;p&gt;Yes, its pretty standard stuff. Most 3D applications our there can do this. Since the nomenclature of 3d applications is not standardized, the tool has different names in different applications: Loft (Maya, 3DSMax), Blend (Creo, Catia...) and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zeKrH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zeKrH.png&quot; alt=&quot;Blend&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Blend between 3 curves, not closed but same principle applies. Open curves are easier as they clearly define the boundary conditions. (software used is Creo because that's what I had open)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You usually want the first edge be from a surface edge because otherwise you will lose the tangency info. But nothing prevents you from using a natural tangent direction, you just lose control. Also you need to pay attention to how you intend the curves to connect (what point connects to what point).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Personally I prefer drawing the hull or using some of the network based tools. This way allow you to specify control curves in 2 directions, thus giving control of what to connect where and how it should work mid run.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Qm5Bk.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Qm5Bk.png&quot; alt=&quot;Image 2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Curve network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many ways to do this, infinite ways in fact. Also if you want to be compatible with other mainstream CAD apps use NURBS instead of beziers than its easier for you to write a exporter/importer if your data set is rich enough. Beziers are just special cases of NURBS after all so your code itself should not need to change all that much (fitting also gets easier).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What you do is simply fit a curve trough your points in the second direction this will then give you the corresponding hull in the other direction. In your case you could just make a Bezier patch for each segment and then try to deduce a natural tangent direction.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;One way&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Like I said theres a infinite ways of constructing these, you have an artistic freedom of doing whatever you see fit it's a over constrained problem. One way of constructing this would be to have a patch for each consecutive cross section. This is nice and tidy as it generalizes works well with an arbitrary number of cross sections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ES0Mq.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ES0Mq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: Example of deriving one row for 2 consecutive bezier spans.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First recognize there are 3 cases (ill cover 2):&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Free edges,&lt;/li&gt;&#xA;&lt;li&gt;Internal edges,&lt;/li&gt;&#xA;&lt;li&gt;External constraint.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I'll cover 1 and 2. First the Internal edge as its simple you want it to be smooth so the tangent length and direction in both patches must match. If that's not true then its not sufficiently continuous. You could simply take that this is the furthest point from the surrounding patches control points and due to mean value theorem it should have a tangent direction perpendicular to these (Line A-C is parallel with D-E). The tangent length should be about 1/3 of the length of the distance to the adjoining vertices (A-B or A-C, in this case the shorter of the spans. It could be longer or average. though that could pose problems). Obviously if you have more spans just repeat for one span forward and after for more internal spans if needed. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For free edges find the point on the perpendicular line that is as far from the point as half distance between the points (length B-D is equal to length (A-B)/2 and length B-E is equal to length (B-D)/2). The free tangent points towards this point. The length of the tangent is as long as one third of corresponding point difference (length (A-B)/3 and (B-C)/3). Obviously the distances and points could be arbitrary but these are somewhat natural. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Repeat for next point sequence in line and next and next. You end up with 2 spans that means 2 times 16 points.  There are other ways but this is relatively easy to convert into a visual form without going into deeper maths. But this is pretty well known math you can find many ways to form these by searching in literature.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-02-16T17:58:09.817" LastActivityDate="2016-02-16T17:58:09.817" CommentCount="6" />
  <row Id="2049" PostTypeId="1" AcceptedAnswerId="2052" CreationDate="2016-02-15T18:47:24.553" Score="2" ViewCount="62" Body="&lt;p&gt;I'm looking into the graphics pipeline processes and at the moment in particular, perspective projection matrices.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After looking in several different sources, and across the breadth of previous questions such as &lt;a href=&quot;http://stackoverflow.com/questions/25659623/how-does-a-projection-matrix-work&quot;&gt;http://stackoverflow.com/questions/25659623/how-does-a-projection-matrix-work&lt;/a&gt;, I've found that most solutions seem to use the following two matrices;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/adExB.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/adExB.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/EG3Iu.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EG3Iu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I've been expecting these matrices to take the view frustum and convert it into a unit sized cube - however this doesn't seem to be happening.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Taking the left matrix first (which I attempted to use first), I assumed the parameters were as such;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;n/f = &lt;strong&gt;z&lt;/strong&gt; value of near and far planes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;r/l = &lt;strong&gt;x&lt;/strong&gt; values of left and right planes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;t/b = &lt;strong&gt;y&lt;/strong&gt; values of top and bottom planes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the right hand matrix, my book is very vague, and the only parameters that seem to fit are&lt;/p&gt;&#xA;&#xA;&lt;p&gt;d/f - distances to near/far planes (but not necessarily the &lt;strong&gt;z&lt;/strong&gt; values?)&#xA;h - height of the near clip plane.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My initial method of experimentation has been to plot the vertices of the frustum and using the transforms get a resultant that the X, Y values can be used to display the image in 2D and with Z values that can be used to decide what polygons overlay others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, whilst I've been expecting a cube with dimensions of 1 -&gt; -1 along all axes, instead I've been getting just an inverted frustrum.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel I'm either making an elementary mistake or am missing something key - would appreciate any help in clearing up both my uncertainties in what I need to do to convert my shape from the 3D to the 2D, and what I should be expecting. So far, my results are below. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The left hand picture shows the shape in a view frustum - and the right shows the original frustum and transformed frustum in green after applying the perspective transform which I believe should be transformed to a unit sized cube and not just a reflected frustum.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/W4xni.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/W4xni.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've put my code below, and tried to simplify it as much as possible to make it easier to evaluate!. Thanks very much!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Updates&lt;/strong&gt;&#xA;I've updated my coordinates for view space coordinates - using a different method generates view space coordinates that go 'back' towards negative z values. Not sure if that makes it more correct but I hope it helps (but as per the comment).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following the answer below I've added an attempt to perform perspective division. I've added the following code below the main bulk. Any further recommendations would be massively appreciated&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;clc; clear all; close all;&#xA;%======Create View Space (hard-coded values for demo) &amp;amp; Plot  ===========&#xA;ws_fcs = [1,2,4,3,3,1,6,6,6,6,7,7,9,8,8,8,10,10;&#xA;    2,4,3,1,4,4,9,8,7,11,9,13,8,12,10,6,11,13;&#xA;    5,5,5,5,1,2,7,9,11,10,13,11,13,13,12,10,13,12];&#xA;ws_vtx = [-1.3416,0.4472,-1.3416,0.4472,-1.3416,-0.8944,0,0,0.8944,-0.8944,0,0,0.8944;&#xA;          -1.0954,-1.4606,0.7303,0.3651,-1.0954,-0.7303,-0.9129,0,-0.1826,0.1826,0,0.9129,0.7303;&#xA;          -4.899,-4.0825,-4.0825,-3.266,-2.4495,-4.4907,-4.0825,-6.1237,-5.7155,-4.0825,-3.6742,-5.7155,-5.3072];&#xA;&#xA;position = [0;0;0;1]; focus = [0;0;-3.6742]; %Transformed set points&#xA;&#xA;figure(); grid on; hold on; xlabel('x'); ylabel('y'); zlabel('z');&#xA;scatter3(position(1),position(2),position(3),'s');&#xA;scatter3([position(1); position(1)+focus(1)],[position(2); position(2)+focus(2)],...&#xA;    [position(3); position(3)+focus(3)],'s');&#xA;plot3([position(1), focus(1)],[position(2), focus(2)],[position(3), focus(3)],'g');&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx','Facecolor', 'r', 'FaceAlpha', 0.1);&#xA;%====================Create View Volume===========================&#xA;asp_rat = 0.75; %Most displays arent square&#xA;focus = focus/norm(focus); %Normalise focus vector&#xA;nheight = 1; %height of near clip plane&#xA;hoz_angle = 2*pi/3;&#xA;ndistance = 1/tan(hoz_angle/2); %distance of near view plane from origin&#xA;fdistance = 5.5; %Define a far distance&#xA;&#xA;nc = ndistance*focus; %Centre point is focus vector * distance&#xA;ntr = nc + [asp_rat*nheight; nheight; 0]; %Go from centre to TL corner&#xA;nbr = ntr + [0; -2*nheight;0]; %Go down 2* height to bottom&#xA;nbl = nbr + [-2*asp_rat*nheight; 0; 0];&#xA;ntl = nbl + [0; 2*nheight;0];&#xA;nr_plan = [ntr, nbr, nbl, ntl]; %Generate near plane points&#xA;&#xA;fc = fdistance*focus;&#xA;fheight = tan(hoz_angle/2)*fdistance; %Find far plane height by trig&#xA;ftr = fc + [asp_rat*fheight; fheight; 0]; %Go from centre to TL corner&#xA;fbr = ftr + [0; -2*fheight;0]; %Go down 2* height to bottom&#xA;fbl = fbr + [-2*asp_rat*fheight; 0; 0];&#xA;ftl = fbl + [0; 2*fheight;0];&#xA;fr_plan = [ftr, fbr, fbl, ftl]; %Generate near plane points&#xA;&#xA;frust_vtx = horzcat(nr_plan, fr_plan); %Create frustum vertex array&#xA;frust_fcs = [1,8,5,6,4,5;  %Faces defined with normals inwards&#xA;             4,5,1,2,8,8;&#xA;             3,6,2,3,7,4;&#xA;             2,7,6,7,3,1];&#xA;patch('Faces',frust_fcs','Vertices',frust_vtx', 'FaceAlpha', 0.05);&#xA;xlabel('x'); ylabel('y'); zlabel('z');&#xA;%=============Create Perspective Matrix ==============================&#xA;figure(); grid on; hold on; xlabel('x'); ylabel('y'); zlabel('z');&#xA;scatter3(frust_vtx(1,:),frust_vtx(2,:),frust_vtx(3,:)); %Plot orig frustrum &#xA;patch('Faces',frust_fcs','Vertices',frust_vtx', 'FaceAlpha', 0.05);&#xA;&#xA;left = nc(1) - asp_rat*nheight; %Get values for projection matrix&#xA;right = nc(1) + asp_rat*nheight;&#xA;top = nc(2) + nheight;&#xA;bottom = nc(2) -nheight;&#xA;near = ndistance;&#xA;far = fdistance;&#xA;&#xA;proj_mat = [2*near/(right - left),0,(right + left)/(right - left),0;&#xA;    0, 2*near/(top-bottom),(top+bottom)/(top-bottom),0;&#xA;    0,0, -(far+near)/(far-near), -2*far*near/(far-near);&#xA;    0,0,-1,0];&#xA;%====================Perspective Projection Transformation=============&#xA;frust_vtx = proj_mat*vertcat(frust_vtx,(ones(1,length(frust_vtx)))); &#xA;scatter3(frust_vtx(1,:),frust_vtx(2,:),frust_vtx(3,:)); %Plot frustrum points&#xA;patch('Faces',frust_fcs','Vertices',frust_vtx(1:3,:)','FaceColor','g','FaceAlpha', 0.05);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Perspective Division&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;%========================Perspective Division==========================&#xA;for i = 1:length(frust_vtx) &#xA;    frust_vtx(:,i) = frust_vtx(:,i)/frust_vtx(4,i);&#xA;end&#xA;for i = 1:length(ws_vtx) &#xA;    ws_vtx(:,i) = ws_vtx(:,i)/ws_vtx(4,i);&#xA;end&#xA;scatter3(ws_vtx(1,:),ws_vtx(2,:),ws_vtx(3,:)); %Plot shape points&#xA;patch('Faces',ws_fcs','Vertices',ws_vtx(1:3,:)','FaceColor','r','FaceAlpha', 0.05); &#xA;scatter3(frust_vtx(1,:),frust_vtx(2,:),frust_vtx(3,:)); %Plot frustrum points&#xA;patch('Faces',frust_fcs','Vertices',frust_vtx(1:3,:)','FaceColor','g','FaceAlpha', 0.05);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2646" LastEditorUserId="2646" LastEditDate="2016-02-16T10:57:52.897" LastActivityDate="2016-02-16T10:57:52.897" Title="Choosing &amp; Using a Projection Matrix" Tags="&lt;transformations&gt;&lt;3d&gt;&lt;vector-graphics&gt;&lt;matrices&gt;&lt;matlab&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2050" PostTypeId="2" ParentId="1991" CreationDate="2016-02-15T21:30:17.253" Score="4" Body="&lt;p&gt;Baraff and Witkin propose to incorporate constraints by modifying the linear system by the constraints matrices $\mathbf S_i$. As they state in the beginning of Section 5.2, the resulting system is not symmetric anymore. Therefore, the modified linear system must be computed with a linear solver that can treat non-symmetric systems (as - if I got you correctly - you proposed with the normal matrix $\mathbf A^T \mathbf A$). However, Baraff and Witkin introduce the modified conjugate gradient method, which is a variant of the original conjugate gradient method, that computes a solution of the initial linear system with considering the constraints at the same time, without the need to take care of the non-symmetry. In their paper they do not solve the normal equations ($\mathbf A^T \mathbf A$), but rather the original system $\mathbf A \Delta \mathbf v = \mathbf b$, so your questions ask something which is beyond of the paper.&lt;/p&gt;&#xA;" OwnerUserId="2695" LastActivityDate="2016-02-15T21:30:17.253" CommentCount="1" />
  <row Id="2052" PostTypeId="2" ParentId="2049" CreationDate="2016-02-15T23:56:58.973" Score="2" Body="&lt;p&gt;The values will only form a cube after performing the perspective divide, which I don't see happening in your code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is, you take a vector $[x, y, z, 1]$ and transform it by the projection matrix, resulting in a new vector $[x', y', z', w']$. Then divide out the fourth component to get a 3D vector, $[x'/w', y'/w', z'/w']$. This last is the one that should be within a $[-1, 1]$ cube, if the original point was within the view frustum (and if it's an OpenGL-style projection matrix—Direct3D-style ones have $[0,1]$ for the post-projective Z range, instead of $[-1, 1]$).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Without the divide by $w'$, if you just look at $[x', y', z']$, indeed you'll just get a scaled and possibly reflected frustum. Matrix multiplication alone can't transform a frustum into a cube, as it's not a linear or affine transformation; instead, it's a projective transformation, and the divide by $w'$ accomplishes that.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-15T23:56:58.973" CommentCount="3" />
  <row Id="2053" PostTypeId="2" ParentId="2042" CreationDate="2016-02-16T18:38:54.543" Score="2" Body="&lt;p&gt;In a traditional camera, the photons from the scene travel through the lens of the camera, then hit the sensor at the focal length. A consequence of the lens is that the image is upside down and backwards.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/e4pu7.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/e4pu7.gif&quot; alt=&quot;Camera&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In ray tracing, we can optimize this by imagining the focal plane is in front of the camera. You can think that the photons from the scene travel towards the camera hole, and resolve on the image plane. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iOrm3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iOrm3.png&quot; alt=&quot;Photons hit image plane&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another optimization in ray tracing to to trace rays from the camera into the scene, rather than tracing them from light sources and hoping they hit the camera. Therefore, the first rays we shoot out are the ones that go from the eye through each pixel on the virtual screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BW8Vy.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BW8Vy.png&quot; alt=&quot;Shooting a Ray&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to create the ray, we need to know the distances a and b in world units. Therefore, we need to convert pixel units into world units. To do this we need a define a camera. Let's define an example camera as follows:&#xA;$$origin = \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix}$$&#xA;$$coordinateSystem = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; 1 &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}$$&#xA;$$fov_{x} = 90^{\circ}$$&#xA;$$fov_{y} = \frac{fov_{x}}{aspectRatio}$$&#xA;$$focalDist = 1$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The field of view, or fov is an indirect way of specifying the ratio of pixel units to view units. Specifically, it is the viewing angle that is seen by the camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/OyYfj.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/OyYfj.png&quot; alt=&quot;FOV Eye Plane&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The higher the angle, the more of the scene is seen. But remember, changing the fov does not change the size of the screen, it merely squishes more or less of the scene into the same number of pixels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/PBEBi.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/PBEBi.png&quot; alt=&quot;FOV Explaination&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's look at the triangle formed by fovx and the x-axis:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sSUA1.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sSUA1.png&quot; alt=&quot;Triangle&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can use the definition of tangent to calculate the screenWidth in view units&#xA;$$\tan \left (\theta  \right) = \frac{opposite}{adjacent}$$&#xA;$$screenWidth_{view}= 2 \: \cdot \: focalDist \: \cdot \: \tan \left (\frac{fov_{x}}{2}  \right)$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using that, we can calculate the view units of the pixel.&#xA;$$x_{homogenous}= 2 \: \cdot \: \frac{x}{width} \: - \: 1$$&#xA;$$x_{view} = focalDist \: \cdot \: x_{homogenous} \: \cdot \: \tan \left (\frac{fov_{x}}{2}  \right)$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The last thing to do to get the ray is to transform from view space to world space. This boils down to a simple matrix transform. We negate yview because pixel coordinates go from the top left of the screen to the bottom right, but homogeneous coordinates go from (-1, -1) at the bottom left to (1, 1) at the top right.&#xA;$$ray_{world}= \begin{bmatrix} x_{view} &amp;amp; -y_{view} &amp;amp; d\end{bmatrix}\begin{bmatrix} &amp;amp;  &amp;amp; \\ &amp;amp; cameraCoordinateSystem &amp;amp; \\ &amp;amp;  &amp;amp; \end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can see an implementation of this &lt;a href=&quot;https://github.com/RichieSams/lantern/blob/669e6c9ccd33f455961a96c5cfe0a5bc3f2af59c/source/scene/camera.cpp#L86&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; (In the code, I assume the focalDist is 1, so it cancels out)&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2016-02-16T18:38:54.543" CommentCount="1" />
  <row Id="2054" PostTypeId="2" ParentId="2043" CreationDate="2016-02-16T19:42:22.413" Score="5" Body="&lt;p&gt;As far as I can tell, the main advantage of half-edge is that traversal can be a bit simpler due to a guarantee of edges having a consistent orientation within each face.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the problem of iterating over all the vertices or edges of a given face, in counterclockwise order. In the half-edge structure, this can be done by starting with an arbitrary half-edge of that face and simply following the &quot;next&quot; pointers until you get back where you started.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast, doing this in a winged-edge structure is a bit annoying, because the edges are oriented arbitrarily; any given edge that you encounter might point either clockwise or counterclockwise relative to the face you're trying to iterate around, so you have to do an extra conditional check at each step to see if you should traverse the current edge forward or backward.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other kinds of connectivity queries behave similarly: the half-edge version lets you follow links in a consistent sequence while the winged-edge version requires orientation checks at each step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Whether the conditionals are actually a performance problem for winged-edge will probably depend on other factors. For a &quot;naive&quot; implementation with pointers every which way and data scattered across memory, I'd expect cache miss overhead to swamp any effect of the conditionals. On the other hand, if you have a tightly packed data structure with everything hot in the cache, you might see some effects from the conditionals due to incorrect branch predictions, etc. It's hard to say.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Leaving performance alone, I'd prefer half-edge just because it seems easier to reason about and write correct code for, even if it results in a slight memory overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way, there are a couple of other design choices with mesh data structures that often seem to get confused with this one. A commenter mentioned single vs double linking, but naturally you can do either single or double linking with either half-edge or winged-edge. (Although I don't see how single linking with winged-edge would even work, since as mentioned above, you might have to traverse edges either backward or forward in the course of a query.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, there's the question of whether the vertex and face structures store a list of all their edges, or just one (and require you to traverse the edges to find the rest). Having a variable-length list of edges per vertex/face considerably complicates the logic if you want to do it efficiently (i.e. not having a separate heap allocation per vertex/face), but again this is independent of whether the edges are half-edge or winged-edge.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-02-16T19:47:22.990" LastActivityDate="2016-02-16T19:47:22.990" CommentCount="0" />
  <row Id="2055" PostTypeId="1" CreationDate="2016-02-16T20:49:14.753" Score="6" ViewCount="79" Body="&lt;p&gt;I have to use a propriertary graphics-engine for drawing a line. I can rotate the whole drawing by its origin point (P1). What I want, is to rotate it around its center point(M). So basically that it looks like the green line (L_correct instead of L_wrong). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think, it should be possible to correct it, by moving it from P1 to P2. But I cannot figure out what formula could be used, to determine the distance. It must probably involve the angle, with and height... Can anyone give me a hint?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/ovWo0Gg.jpg&quot; alt=&quot;1&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="2701" LastEditorUserId="38" LastEditDate="2016-02-17T09:34:32.137" LastActivityDate="2016-02-17T09:34:32.137" Title="Rotate line around center" Tags="&lt;transformations&gt;&lt;line-drawing&gt;&lt;maths&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2056" PostTypeId="2" ParentId="2055" CreationDate="2016-02-16T21:18:00.447" Score="7" Body="&lt;p&gt;Trick is, to move the entire object so that the point about which you want to rotate is at the center. Then rotate and after that counter move it so that the point is were it was.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact this is not so much of a trick, as such, nearly all graphics engines work this way. It is just abstracted away in many cases. Most often you will see it done in matrix notation like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ T^{-1} R T $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where T is to transformation from world to pivot and R is the rotation matrix. Since it can be easily packaged this way you end up with a function called rotate about the pivot point. This is usually the reason why drawing API's allow for a transformation matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kIH24.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kIH24.png&quot; alt=&quot;Rotate about a pivot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Rotate about a pivot point, &lt;a href=&quot;http://pastebin.com/L2XpvfeG&quot; rel=&quot;nofollow&quot;&gt;image source&lt;/a&gt; available for investigation. Sub images in RTL reading order: Original, inverse transform, inverse and rotate, inverse rotate and move back.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-02-17T09:22:41.763" LastActivityDate="2016-02-17T09:22:41.763" CommentCount="0" />
  <row Id="2057" PostTypeId="1" AcceptedAnswerId="2059" CreationDate="2016-02-16T22:09:03.930" Score="3" ViewCount="72" Body="&lt;p&gt;I am currently reading the awesome paper by Jorge Jimenez about Character rendering : &lt;a href=&quot;http://www.iryoku.com/stare-into-the-future&quot; rel=&quot;nofollow&quot;&gt;Next Generation Character Rendering&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the part about multi-sampled transmittance he uses some Poisson offsets and a jitter texture which contains &lt;em&gt;[cos(x), sin(x)]&lt;/em&gt; and is used to rotate the the Poisson disk samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I am wondering how do you generate such texture ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the part of the ppt which talks about this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/21iCO.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/21iCO.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2372" LastActivityDate="2016-02-17T08:11:53.453" Title="Jitter texture generation" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;texture&gt;&lt;sampling&gt;&lt;hlsl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2058" PostTypeId="2" ParentId="2055" CreationDate="2016-02-16T23:26:31.237" Score="1" Body="&lt;p&gt;I am a bit unsure if my calculations are correct, but on my scribbling paper it seemed to work out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;P1 and P2 lie on a circle around M.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This allows us to measure the distance of the two points by just taking the radius (which is half the line length) of the circle and constructing two rectangular triangles. The accepted answer &lt;a href=&quot;http://math.stackexchange.com/questions/134606/distance-between-any-two-points-on-a-unit-circle&quot;&gt;here&lt;/a&gt; provides a sketch for this construction.&#xA;So given your rotation angle is $\gamma$ and the line is $l$ long it follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$x = 2* sin(\frac{\gamma}{2}) * \frac{l}{2}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The lines M to P1 and M to P2 form a isosceles triangle with the third side being the line segment between P1 and P2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The angle at point M is known to be the rotation angle $\gamma$ so the remaining two angles in this triangle are given by&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\alpha = \frac{180 - \gamma}{2}$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And therefore the angle between the wrong line and the needed translation vector is just &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\beta = \alpha + \gamma$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So now you rotate a unit vector that starts along the wrong line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You rotate it around P1 for $\beta$ and scale it to length x.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This yields the translation vector from P1 to P2. The rotation of the line should indeed be correct.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-02-16T23:26:31.237" CommentCount="1" />
  <row Id="2059" PostTypeId="2" ParentId="2057" CreationDate="2016-02-17T08:11:53.453" Score="7" Body="&lt;p&gt;The texture is probably generated by picking a random angle per pixel, and populating the image with its sine and cosine, remapped into [0, 1]:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\theta \sim [0, 2\pi] \quad \to \quad \begin{bmatrix} \tfrac{1}{2} \cos \theta + \tfrac{1}{2} \\ \tfrac{1}{2} \sin \theta + \tfrac{1}{2} \\ 0 \end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did a quick test and generated something that looks pretty similar (enlarged 4x):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/4ySjS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to be extra fancy, then instead of picking independent random angles for every pixel, you might try stratified sampling or a low-discrepancy sequence. That may give better final results for whatever effect you're using the jitter texture for, due to better local sample distribution.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-17T08:11:53.453" CommentCount="1" />
  <row Id="2060" PostTypeId="1" CreationDate="2016-02-17T14:59:49.933" Score="1" ViewCount="45" Body="&lt;p&gt;I have been playing with voxelization for a little time, I'm still very new to this technique.&#xA;I am trying to use it for ray tracing but I've been stuck with one problem. There are some codes out there in the web that already perform voxelization however, for ray tracing I need to check the intersection between a ray and a triangle.&#xA;Therefore, I've been thinking about what would be a good approach to create a list of triangles for each voxel in order to query the triangles once I know a ray hits a particular voxel? Should I try to modify the code of the voxelizer to benefit from the overlap-test or is it better to create a space-partitioning structure?&lt;/p&gt;&#xA;" OwnerUserId="116" LastActivityDate="2016-02-21T17:39:54.433" Title="How to build a triangle-voxel list from a triangle mesh?" Tags="&lt;space-partitioning&gt;&lt;voxelization&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="2061" PostTypeId="1" AcceptedAnswerId="2062" CreationDate="2016-02-18T01:47:02.473" Score="5" ViewCount="471" Body="&lt;p&gt;Mainly talking about dual-SLI here for consistency. With past DirectX (and OpenGL) APIs, VRAM was mirrored across graphics cards. With dual-SLI, this was possible by rendering one frame with one graphics card and another frame with another one. There was also a rendering option in which one graphics card would render part of the screen. Unfortunately, there doesn't seem to be much technical information on how VRAM stacking would work. What possible techniques could graphics cards or DirectX 12 be using to allow for this?&lt;/p&gt;&#xA;" OwnerUserId="2707" LastActivityDate="2016-02-18T03:27:47.513" Title="How does DirectX 12 SLI VRAM stacking work?" Tags="&lt;directx12&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2062" PostTypeId="2" ParentId="2061" CreationDate="2016-02-18T03:27:47.513" Score="7" Body="&lt;p&gt;In DX12, there is no implicit driver-implemented SLI as there was in DX11. Instead, multiple GPUs are exposed to the application as separate &quot;nodes&quot; within a single DX12 device, and each VRAM resource lives on a single node, specified at creation time. There is no implicit mirroring of resources to both GPUs as in DX11 SLI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, the game engine or application has full control and responsibility over how data and work are distributed between the GPUs. It's up to the app developers to implement patterns like alternating frames between GPUs, or splitting the frame across GPUs, if they wish. For example, to implement alternating frames, the app would have to allocate all buffers, textures, render targets etc. on both GPUs and populate both of them. Then, when rendering a frame, it would generate a command list for the current GPU using its local copies of all resources. Any inter-GPU transfers needed (for temporal effects, for instance) would also be the app's responsibility.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More information on this topic can be found on &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/dn933253.aspx&quot;&gt;this MSDN article on DX12 multi-GPU support&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-18T03:27:47.513" CommentCount="7" />
  <row Id="2063" PostTypeId="1" AcceptedAnswerId="2064" CreationDate="2016-02-18T04:08:25.213" Score="5" ViewCount="129" Body="&lt;p&gt;I'm a newcomer to OpenGL and I was playing around with drawing triangles with different z-coordinates. From what I understand, the z axis point out of the screen, and the -z axis points into the screen. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I draw a square with 3 corners at a 0.0 z-coordinate, and the last corner at, say, 3.0 z-coordinate, I get this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/GL2Mu.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/GL2Mu.png&quot; alt=&quot;wat&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't understand how it's making this shape... I thought it would be something like this, since the 4th vertex is just 'far away'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Dmw2A.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Dmw2A.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone explain?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; This is my vertex data&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// vertex array&#xA;float vertices[] = {&#xA;    -0.5f,  0.5f,  0.0f, 1.0f, 0.0f, 0.0f, // top left, first 3 are location, last 3 are color&#xA;     0.5f,  0.5f,  0.0f, 0.0f, 1.0f, 0.0f, // top right&#xA;    -0.5f, -0.5f,  -2.0f, 0.0f, 0.0f, 1.0f, // bottom left&#xA;     0.5f, -0.5f,  0.0f, 1.0f, 1.0f, 1.0f // bottom right&#xA;};&#xA;&#xA;// element buffer array&#xA;GLuint elements[] = {&#xA;    0, 1, 2,&#xA;    2, 1, 3&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I am calling the draw like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glDrawElements(GL_TRIANGLES, 6,GL_UNSIGNED_INT,0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2651" LastEditorUserId="2651" LastEditDate="2016-02-18T04:24:59.147" LastActivityDate="2016-02-18T04:35:46.963" Title="Confused about z-axis behaviour" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2064" PostTypeId="2" ParentId="2063" CreationDate="2016-02-18T04:35:46.963" Score="6" Body="&lt;p&gt;The post-projective Z range in OpenGL extends between &amp;minus;1 and 1. So, a Z-value of 2 is out of range and will be clipped by the far plane.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason the square appears with a quadrant missing is that it's split into triangles from bottom left to top right, and so each triangle has two vertices with Z = 0 and one vertex with Z = 2. So, interpolating along the triangle, it hits Z = 1 (the far plane) halfway across, and everything beyond that is clipped. This results in half of each triangle disappearing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, you'll need a perspective projection matrix in order for far-away objects to appear smaller on screen. With no projection matrix, you're effectively using a parallel projection.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-18T04:35:46.963" CommentCount="0" />
  <row Id="2067" PostTypeId="1" CreationDate="2016-02-18T17:38:27.070" Score="4" ViewCount="85" Body="&lt;p&gt;So I want to render a buttload of quads by using a single 4 point vertex array and change the position of the quad. What is quicker:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using &lt;code&gt;glBufferSubData&lt;/code&gt; to change the positional data or&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using &lt;code&gt;glMapBufferRange&lt;/code&gt; to DMA the buffer and &lt;code&gt;glUnmapBuffer&lt;/code&gt; to push the data back to the GPU?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using the vertex buffer data, I need to also call &lt;code&gt;glVertexAttribArray&lt;/code&gt; to reassociate the data with the vertexArrayObject.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When using uniform buffers and do it with shader magic I only need to change the uniform buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is quicker?&lt;/p&gt;&#xA;" OwnerUserId="2623" LastActivityDate="2016-02-19T20:06:51.320" Title="Uniform Buffers: What is quicker?" Tags="&lt;opengl&gt;&lt;mapping&gt;&lt;uniform-buffer-object&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2068" PostTypeId="2" ParentId="2037" CreationDate="2016-02-18T22:21:47.970" Score="4" Body="&lt;p&gt;I've done some small changes to how I usually construct my view matrix, here is what I've modified:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// put it after N = ...&#xA;U = cross(N, Vspec);&#xA;U = U / sqrt(dot(U,U)); % normalize&#xA;V = cross(U, N);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I also set N vector as -N to R matrix, like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;R= [U(1),U(2),U(3),0; % U is direction of camera space X axis&#xA;    V(1),V(2),V(3),0; % V is direction of camera space Y axis&#xA;    -N(1),-N(2),-N(3),0;&#xA;    0   ,   0,   0,1];&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.opengl.org/wiki/GluLookAt_code&quot; rel=&quot;nofollow&quot;&gt;as shown e.g. in the code for lookAt matrix for OpenGL&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now you can orient your first graph, as if you look through your camera, this will give you a reference of how it should look on the second graph.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then orient second graph so that you look through -Z axis, Y looks up, and X from left to right. Because your camera view vector is -Z, up is Y and right is X. Once you orient your second graph like this, you'll see that arrow looks identically to the first graph, ray on graph 2 will be drawn as point, because it is directed along -Z (your view direction), and in camera space end and start points will fall on a same pixels (another hint that it is correct).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also modified a bit camera pos, view direction and axes (just to make it easier to observe for myself).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the values, that I've used:&#xA;    focus = [1.2,0,1.5]; %The point we're looking at&#xA;    Cx = 3; Cy = -3; Cz = 3; %Position of camera&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And for the axes:&#xA;    axis([-5,5,-6,6,-7,7]);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/izEmQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/izEmQ.png&quot; alt=&quot;Here is what I&amp;#39;ve got&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can send you whole code if you need.&lt;/p&gt;&#xA;" OwnerUserId="43" LastActivityDate="2016-02-18T22:21:47.970" CommentCount="4" />
  <row Id="2069" PostTypeId="2" ParentId="2067" CreationDate="2016-02-18T23:29:45.807" Score="6" Body="&lt;p&gt;A key rule of thumb for high-performance code in general, and graphics in particular, is to batch large amounts of work together rather than doing it in small chunks. In your case, rather than sending one quad's worth of data at a time and drawing one quad at a time, it will be far more efficient to send many quads' data in a large batch, and draw many quads together in a single draw call.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A good way to accomplish both is &lt;a href=&quot;https://www.opengl.org/wiki/Vertex_Rendering#Instancing&quot; rel=&quot;nofollow&quot;&gt;instancing&lt;/a&gt;. You would have a 4-point vertex buffer for the corners of a quad, and a second vertex buffer for the values that are per-quad (what you're putting in your uniform buffer now). This second buffer would be large enough to hold the data for all the quads at once, not just one quad. To populate the vertex buffer, you can map the whole thing, write all the data, and unmap. Then do a single instanced draw call to render all the quads at once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, if instancing can't be used for some reason (for instance if each quad needs a different texture, and it's not possible or desirable to put them together in an atlas), you can still upload all the data in one batch. Create a uniform buffer large enough to hold an array of all the quads' data; map/write/unmap it all in one big batch as before; then before each draw call, use &lt;a href=&quot;http://docs.gl/gl4/glBindBufferRange&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;glBindBufferRange&lt;/code&gt;&lt;/a&gt; to select one quad's data from that large buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Batching can also be used when you don't know the number of quads in advance. Just pick a &quot;reasonably large&quot; batch size (say, 1024 quads) and size your buffers for that number. Accumulate quads into the buffer until you reach the size limit, then draw them all, and start over (using the &lt;code&gt;GL_MAP_INVALIDATE_BUFFER_BIT&lt;/code&gt; flag with &lt;code&gt;glMapBufferRange&lt;/code&gt; to get a fresh copy of the buffer). You'll also need to do a partial draw at the end of this process to handle any left-over quads that didn't reach the size limit.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-02-19T20:06:51.320" LastActivityDate="2016-02-19T20:06:51.320" CommentCount="5" />
  <row Id="2070" PostTypeId="1" AcceptedAnswerId="2071" CreationDate="2016-02-19T02:32:49.933" Score="3" ViewCount="63" Body="&lt;p&gt;I'm working on a webgl pixel shader which is writing to a 16 bit floating point buffer (each color channel r,g,b,a is a 16 bit floating point number).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to figure out if there are any decent ways to pack the following info into those 16 bits per color channel:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;normal (x,y,z where z is always negative)&lt;/li&gt;&#xA;&lt;li&gt;uv coordinates (x,y between 0 and 1)&lt;/li&gt;&#xA;&lt;li&gt;material index (an integer. 0 to 3 would be plenty)&lt;/li&gt;&#xA;&lt;li&gt;distance (a positive float)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I could live without distance if needed but the others are required.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that I can store just x,y of the normal and calculate z as needed since its always negative (the normal is in screenspace).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That still leaves 5 values to store in four 16 bit floats: normal.xy, uv.xy, material index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone have any good methods for making this happen? It would be nice to have the distance too, otherwise I am limited to directional lighting instead of getting point lights as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is in shadertoy if that context changes any answers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'll be sampling this buffer with nearest neighbor sampling, so you don't have to worry about it being stored in a way that interpolates well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-02-19T05:15:35.553" LastActivityDate="2016-02-19T05:15:35.553" Title="Bitpacking into buffers with webgl (shadertoy)" Tags="&lt;webgl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2071" PostTypeId="2" ParentId="2070" CreationDate="2016-02-19T05:00:16.637" Score="2" Body="&lt;p&gt;Assuming you are not going to filter the buffer using HW texture filtering: since you only need 4 values for the material index, pack it in the sign bits of uv x and y.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-02-19T05:00:16.637" CommentCount="6" />
  <row Id="2072" PostTypeId="1" AcceptedAnswerId="2073" CreationDate="2016-02-19T05:30:18.017" Score="5" ViewCount="100" Body="&lt;p&gt;If you have values on a grid and you want to find the value of a point within one of the cells, you can use techniques like bilinear or bicubic interpolation to get the data at that point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What technique(s) would be used if the data on the grid was a vector (a surface normal specifically)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that quaternions are good for slerping, which I think could be good in this case to do a &quot;bilinear slerp&quot;, but quaternions require basis vectors that represent an orientation, not just a single vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I'm not sure how you'd use quaternions to apply something like bicubic interpolation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What methods would be good in this case, to interpolate vectors on a grid?&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-02-20T20:09:48.967" Title="Interpolating vectors on a grid" Tags="&lt;vectors&gt;&lt;interpolation&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="3" />
  <row Id="2073" PostTypeId="2" ParentId="2072" CreationDate="2016-02-19T18:31:02.727" Score="6" Body="&lt;p&gt;I did some research and found the answer I was looking for.  The three most common ways to interpolate vectors are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Slerp - short for &quot;spherical interpolation&quot;, this is the most correct way, but is also the costliest.  In practice you likely do not need the precision.&lt;/li&gt;&#xA;&lt;li&gt;lerp - short for &quot;linear interpolation&quot;, you just do a regular linear interpolation between the vectors and use that as a result.&lt;/li&gt;&#xA;&lt;li&gt;nlerp - short for &quot;normalized linear interpolation&quot; you just normalize the result of a lerp.  Useful if you need your interpolated vector to be a normalized vector.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In practice, lerp/nlerp are pretty good at getting a pretty close interpolated direction so long as the angle they are interpolating between is sufficiently small (say, 90 degrees, or 120 like Simon mentions in his comment), and nlerp is of course good at keeping the right length, if you need a normalized vector.  If you want to preserve the length while interpolating between non normalized vectors, you could always interpolate the length and direction separately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example of the three interpolations on a large angle.  Dark grey = start vector, light grey = end vector.  Green = slerp, blue = lerp, orange = nlerp.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/LaV6S.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LaV6S.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example of a medium sized angle (~90 degrees) interpolating the same time t between the angles:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/2gjmE.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2gjmE.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, here's a smaller angle (~35 degrees).  You can see that the results of lerp / nlerp are more accurate as the angle between the interpolated vectors gets smaller.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/mrUqf.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/mrUqf.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you do lerp or nlerp, you can definitely do both bilinear as well as bicubic interpolation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using slerp, you can do bilinear interpolation, but I'm not sure how bicubic would translate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I generated these images by taking screenshots from an interactive shadertoy demo I made to demonstrate these differences.  You can see that shadertoy here:&lt;br&gt;&#xA;&lt;a href=&quot;https://www.shadertoy.com/view/4sV3zt&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/4sV3zt&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-02-20T20:09:48.967" LastActivityDate="2016-02-20T20:09:48.967" CommentCount="5" />
  <row Id="2074" PostTypeId="1" CreationDate="2016-02-19T23:18:25.597" Score="3" ViewCount="55" Body="&lt;p&gt;I'm struggling with a problem with image rotation. I'm using the idea of &quot;Rotation by area mapping&quot; in the following link:&#xA;&lt;a href=&quot;http://www.leptonica.com/rotation.html&quot; rel=&quot;nofollow&quot;&gt;http://www.leptonica.com/rotation.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My c# codes are as follows. The idea is first to find the dimension of the rotated image, then find the gray level of each pixel in the rotated image. To find the gray level for each pixel, the pixel coordinates are transformed back to the coordinates in the original image (called original pixel). The original pixel has fractional coordinate values, then the bilinear interpolation method in the link is used to obtain the gray level of the pixel in the rotated image. My codes did most of the job, except there are some clipping at the corners in the rotated image. I put the rotated images of a 16*16 image, we can clearly see the effect: The bottom line in the 45 degree rotated has no gray data; in the 90 degree rotated one, the vertical line in the left has no gray data; and in the 180 degree rotated one, both the vertical line in the left and the horizontal line on the top have no gray. I believe it's caused by the coordinate computation, and I could modify my code by subtracting one of the coordinates by 1 to achieve correct rotation by a certain angle, but it doesn't work for the other case. So I guess there must be something I ignored in my codes. Any suggestions and comments are appreciated!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public void RotateImageClockwise(double degree)&#xA;{&#xA;    Image&amp;lt;double&amp;gt; image = (Image&amp;lt;double&amp;gt;)(PreservedRawImage.Clone());      &#xA;        double sinAngle = Math.Sin(Math.PI * degree / 180.0);&#xA;        double cosAngle = Math.Cos(Math.PI * degree / 180.0);&#xA;        double newHeightRaw, newWidthRaw;&#xA;&#xA;        ////Find the new dimensions of the rotated image.&#xA;        if (Math.Abs(degree) &amp;lt;= 90)&#xA;        {&#xA;            newHeightRaw = image.Width * sinAngle + image.Height * cosAngle;&#xA;            newWidthRaw = image.Width * cosAngle + image.Height * sinAngle;&#xA;        }&#xA;        else if (Math.Abs(degree) &amp;lt;= 180)&#xA;        {&#xA;            newHeightRaw = image.Width * Math.Sin(Math.PI * (180 - degree) / 180.0) + image.Height * Math.Sin(Math.PI * (degree - 90) / 180.0);&#xA;            newWidthRaw = image.Width * Math.Cos(Math.PI * (180 - degree) / 180.0) + image.Height * Math.Cos(Math.PI * (degree - 90) / 180.0);&#xA;        }&#xA;        else&#xA;            throw new Exception(&quot;Rotation angle is not right.&quot;);&#xA;        ////Done with finding the new dimensions&#xA;&#xA;        double newWidthHalf = newWidthRaw / 2;&#xA;        double newHeightHalf = newHeightRaw / 2;&#xA;&#xA;        int newWidth = (int)Math.Round(newWidthRaw);&#xA;        int newHeight = (int)Math.Round(newHeightRaw);&#xA;        Image&amp;lt;double&amp;gt; newImage = new Image&amp;lt;double&amp;gt;(newWidth, newHeight);&#xA;        byte[] bitmap = new byte[newWidth * newHeight];&#xA;        double newMax = double.MinValue, newMin = double.MaxValue;&#xA;        for (int x = 0; x &amp;lt; newHeight; x++)&#xA;        {&#xA;            for (int y = 0; y &amp;lt; newWidth; y++)&#xA;            {&#xA;                ////Transform the coordinates to be centered at the center of the image&#xA;                double xt = x - newHeightHalf;&#xA;                double yt = y - newWidthHalf;&#xA;&#xA;                double originalX = xt * cosAngle - yt * sinAngle + image.Height / 2.0;&#xA;                double originalY = xt * sinAngle + yt * cosAngle + image.Width / 2.0;&#xA;&#xA;                double fractionX = Math.Abs(originalX - (int)Math.Round(originalX));&#xA;                double fractionY = Math.Abs(originalY - (int)Math.Round(originalY));&#xA;&#xA;                ////Find the neighbors of the pixel in the original image&#xA;                IList&amp;lt;Point&amp;lt;int&amp;gt;&amp;gt; neighbors = new List&amp;lt;Point&amp;lt;int&amp;gt;&amp;gt; { new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX), y = (int)Math.Round(originalY), weight = (1 - fractionX) * (1 - fractionY)}, &#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX), y = (int)Math.Round(originalY) + 1, weight = fractionX * (1 - fractionY)},&#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX) + 1, y = (int)Math.Round(originalY), weight = (1 - fractionX) * fractionY},&#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX) + 1, y = (int)Math.Round(originalY) + 1, weight = fractionX * fractionY}&#xA;                                                         };&#xA;                bool hasData = false;&#xA;                double newData = 0;&#xA;                double weight = 0;&#xA;                foreach (var neighbor in neighbors)&#xA;                {&#xA;                    if (neighbor.x &amp;gt;= 0 &amp;amp;&amp;amp; neighbor.x &amp;lt; image.Height&#xA;                        &amp;amp;&amp;amp; neighbor.y &amp;gt;= 0 &amp;amp;&amp;amp; neighbor.y &amp;lt; image.Width&#xA;                       )&#xA;                    {&#xA;                        hasData = true;&#xA;                        newData += neighbor.weight * image.Data[(neighbor.x) * image.Width + neighbor.y];&#xA;                        weight += neighbor.weight;&#xA;                    }&#xA;                }                    &#xA;                if (hasData &amp;amp;&amp;amp; weight != 0)&#xA;                {&#xA;                    newData /= weight;&#xA;                    newImage.Data[x * newWidth + y] = newData;&#xA;                }&#xA;            }                &#xA;        }            &#xA;    } &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dyDw6.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dyDw6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2719" LastActivityDate="2016-02-22T21:48:31.237" Title="Rotate image around its center" Tags="&lt;image-processing&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="2075" PostTypeId="2" ParentId="2074" CreationDate="2016-02-20T07:18:32.540" Score="2" Body="&lt;p&gt;This is more of a long comment, with an image, than an answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Seems to me, at a quick glance, that you consider your pixels sample point to be at integer values. Instead in reality sample is located offset 0.5 pixels of this location.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/0UGXJ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0UGXJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Is the pixel sample located at its integer coordinate? Or is it located offset by 0.5 pixels?&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-02-20T07:24:04.553" LastActivityDate="2016-02-20T07:24:04.553" CommentCount="0" />
  <row Id="2076" PostTypeId="1" CreationDate="2016-02-20T11:38:53.557" Score="4" ViewCount="42" Body="&lt;p&gt;I've mostly succeeded at porting an implementation of Marching Cubes from CPU over to OpenGL compute shaders, but I haven't tackled normals yet and wondering the best way to go about it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My implementation deals specifically with binary valued fields (I'm trying to model 3D fractal functions that don't have a distance estimator yet), so gradient and forward differences methods won't work. I have shared vertices working, and my CPU implementation uses Quilez's method described &lt;a href=&quot;http://www.iquilezles.org/www/articles/normals/normals.htm&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; to accumulate face normals onto each neighbouring vertex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could just port this implementation over to another shader, but the problem I see with this is the massive number of atomics needed. Since we can only use atomics on scalar integer types, and I cant think of a way to pack 3 signed ints into 1 in a summable way, that means 3 axes * 3 vertices = 9 atomic adds per shader invocation. They'll be spread throughout memory of course so it's not like hitting a single atomic counter 9 times, but it still seems like a hell of a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other alternative is to run a shader invocation per-polygon and build the face normal list (i could probably pack to x10y10z10 this way), then a shader per-vertex to accumulate all the normals of neighbouring faces. This would be an enormous memory hog though, the storage space of the face indices would need 12 int per vertex to deal with the worst case. There's also the problem of how to write into this storage without again resorting to atomics to work out how many faces have already been written to a particular vertex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone have any better ideas on how to do this?&lt;/p&gt;&#xA;" OwnerUserId="1937" LastActivityDate="2016-02-20T11:38:53.557" Title="Creating shared vertex normals on GPU" Tags="&lt;opengl&gt;&lt;gpu&gt;&lt;mesh&gt;&lt;compute-shader&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="2077" PostTypeId="1" AcceptedAnswerId="2079" CreationDate="2016-02-20T19:08:41.227" Score="5" ViewCount="369" Body="&lt;p&gt;I've recently made the switch to Visual 2015 and the .fx files have been depreciated. My project contained .fx files for the shaders so I've been switching over.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Only during the switch have I changed to using deferred rendering for lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only shader I've made so far is the Pixel Shader that is used on the 2nd Pass, the shader that does the Per Pixel x Per Light calculations, sums them, and outputs final color.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, whenever I compile (I'm not actually using the shader in runtime, I'm just getting it all setup, the window is still just cleared to a color every frame, no actual rendering) I get a failure that says....&lt;/p&gt;&#xA;&#xA;&lt;p&gt;error X4502: Shader model ps_4_0_level_9_3 doesn't allow reading from position semantics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the only shader I have in the project that is causing this error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I omitted the getGBufferAttributes and calculateLighting methods because I know they are not the issue. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Texture2D NormalTexture                 : register(t0);&#xA;Texture2D DiffuseAlbedoTexture          : register(t1);&#xA;Texture2D SpecularAlbedoTexture         : register(t2);&#xA;Texture2D PositionTexture               : register(t3);&#xA;&#xA;cbuffer LightParams&#xA;{&#xA;    float3 LightPos;&#xA;    float3 LightColor;&#xA;    float3 LightDirection;&#xA;    float2 SpotlightAngles;&#xA;    float4 LightRange;&#xA;};&#xA;&#xA;cbuffer CameraParams&#xA;{&#xA;    float3 CameraPos;&#xA;};&#xA;&#xA;float4 main(in float4 screenPos : SV_Position) : SV_Target0&#xA;{&#xA;    float3 normal;&#xA;    float3 position;&#xA;    float3 diffuseAlbedo;&#xA;    float3 specularAlbedo;&#xA;    float specularPower;&#xA;&#xA;    getGBufferAttributes(screenPos.xy, normal, position, diffuseAlbedo, specularAlbedo, specularPower);&#xA;&#xA;    float3 lighting = calculateLighting(normal, position, diffuseAlbedo, specularAlbedo, specularPower);&#xA;&#xA;    return float4(lighting, 1.0f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code used for this shader is highly based off the code used as the deferred rendering example in the book, Practical Rendering with Direct3D 11 by Jason Zink, Matt Pettineo, and Jack Hoxley.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't understand why they would give source code that doesn't work. Does it have something to do with the new compiler in Visual 2015 and the Runtime?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, since I'm going from only using effects before to now raw HLSL shaders, is there anything I should be aware of that might trip somebody like me up?&lt;/p&gt;&#xA;" OwnerUserId="2726" LastActivityDate="2016-02-20T20:40:35.193" Title="HLSL Deferred Rendering" Tags="&lt;shader&gt;&lt;directx11&gt;&lt;pixel-shader&gt;&lt;hlsl&gt;&lt;deferred-rendering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2078" PostTypeId="1" CreationDate="2016-02-20T19:43:53.280" Score="4" ViewCount="61" Body="&lt;p&gt;I am going throug the topic scan line conversion where the scan line parallel to the x-axis is put through the intersection test with all the edges of the polygon. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would there be any large theoretical differences if we take a small line segment instead, between (x1,y1) and (x2,y2) that is parallel to the x-axis or y-axis and test for intersection with the polygon edges ? The usual case the y = a ( parallel to x-axis) and what I am asking is also parallel to x-axis or y-axis,but constrained between two points instead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to develop a tool-path for 3D printing and looking forward to implement HIlbert curve as a tool-path. Generally the very same scan-line concept is used in 3D printing techniques whenever it comes to tool-path generation where parallel scan line to x-axis is used for scanline and polygon edges interesection. I have already got the typical linear scan line for 3D printing and looking forward to implement another way to maneuever the printing head according to the Hilbert curve. In Hilbert curve you have a line segments that change its orientation in such a calculated manner it never self-intersects and simple it follows the FASS characteristics. Hilbert curve containes several small line segments that are parallel to x-axis or y-axis. I am trying to narrow it down to the one line segment of Hilbert curve where the intersection test is to be conducted between one line segment [ (x1,y1) and (x2,y2) ] and the polygon edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hope that I explained the issue well enough to get some hints to the question asked. Please do ask if anything that I have explained so far is not clear enough.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks    &lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="2712" LastEditDate="2016-02-21T12:06:38.850" LastActivityDate="2016-02-21T12:06:38.850" Title="scan line conversion" Tags="&lt;vector-graphics&gt;" AnswerCount="0" CommentCount="4" FavoriteCount="1" />
  <row Id="2079" PostTypeId="2" ParentId="2077" CreationDate="2016-02-20T20:40:35.193" Score="4" Body="&lt;p&gt;A very novice mistake, I was compiling with the old HLSL compiler. But why would that be the default? Very strange.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For anybody wondering, just right click the HLSL file in the solution explorer and go to properties -&gt; HLSL Compiler -&gt; General, and switch Shader Model to the one you want which was newest one for me (5.0)&lt;/p&gt;&#xA;" OwnerUserId="2726" LastActivityDate="2016-02-20T20:40:35.193" CommentCount="0" />
  <row Id="2080" PostTypeId="1" CreationDate="2016-02-20T23:40:52.130" Score="5" ViewCount="111" Body="&lt;p&gt;I am looking for a explanation about volume rendering in simple words, a step by step kind of, like ray tracing: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;for every pixel in the screen plane, trace a ray starting from the eye&#xA;  point to the screen pixel location, a compute the nearest objet&#xA;  intersection in the scene, calculate the pixel color, and so on.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;something like that&lt;/p&gt;&#xA;" OwnerUserId="2684" LastActivityDate="2016-02-21T09:41:26.677" Title="Volume Rendering in simple words" Tags="&lt;volumetric&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2081" PostTypeId="2" ParentId="2080" CreationDate="2016-02-21T00:44:30.847" Score="7" Body="&lt;p&gt;Volume rendering is not like ray-tracing, it is like &quot;scene rendering&quot;. i.e. there exists several algorithms to render volumes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One close to ray-tracing is ray-marching, and has may variants. The simplest:&#xA;&lt;em&gt;for every pixel in the screen plane, trace a ray starting from the eye point to the screen pixel location, and advance along the ray by constant steps. At each voxel calculate the pixel color and transparency, blend it to the pixel value, and so on.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Blend = &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;$C_{pix} += T_{pix}*C_{vox}*(1-T_{vox})$ &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;$T_{pix} *= T_{vox}$&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;with $C_{vox},T_{vox}$ the color and transparency of the current voxel, and $C_{pix},T_{pix}$ the cumulated color and transparency of the pixel.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2016-02-21T09:41:26.677" LastActivityDate="2016-02-21T09:41:26.677" CommentCount="3" />
  <row Id="2082" PostTypeId="2" ParentId="2060" CreationDate="2016-02-21T17:39:54.433" Score="2" Body="&lt;p&gt;You could do this. Basically what you would do is generate a cube for each voxel (or actually nearby voxels could share the edges). Not exactly sure what benefit you would get form this, unless your intersection algorithm is somehow extremely finely tuned. It could be useful for real time rendering or some sort of sparse structure. While your at it why not dispense of the voxel concept altogether and go all FEM on the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The thing is you want a more uniform sampling across the volumes density function. Because your integrating over a volume functions you want the step size to be somewhat equally spaced so you can control the integration better. Random or pseudo random solution converges often better, but mainly you do not do cube oriented sampling this way as it will make the structure obvious which is not your goal in most cases. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/HaXGH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HaXGH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Different sampling strategies. Images in RTL reading order samples along voxel grids,  Uniforms sampling and jittered ray length sampling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Think of this as solving a ODE you sample the function uniformly and if your value changes too much you might do additional samples. This is probably the reason why you wouldn't really do collision to the voxels.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-02-21T17:39:54.433" CommentCount="0" />
  <row Id="2083" PostTypeId="1" CreationDate="2016-02-21T21:44:14.300" Score="7" ViewCount="103" Body="&lt;p&gt;I know that by now it might be considered as a kind of sad recurrent joke, but by chance, does anyone here has the least information about the colorspace of RGB values in the MERL BRDF measurement database ? &lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-03-29T22:08:33.460" Title="Color space of MERL BRDF database?" Tags="&lt;color-management&gt;&lt;brdf-measurement&gt;&lt;capture&gt;" AnswerCount="3" CommentCount="1" FavoriteCount="1" />
  <row Id="2084" PostTypeId="1" CreationDate="2016-02-22T14:49:23.237" Score="0" ViewCount="24" Body="&lt;p&gt;So, I'm just surfing over this sea and I'm trying to deep-dive into it, so I've this question:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've heard that Procedural Textures are way optimal in terms of hardware performance than Traditional &quot;Bitmap&quot; Textures.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is that true? It depends?&lt;/li&gt;&#xA;&lt;li&gt;If so.. simply put, why they are better (always in terms of hardware performance)?&lt;/li&gt;&#xA;&lt;li&gt;Is there some nice reference documentation that I can read about this?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h3&gt;Thanks&lt;/h3&gt;&#xA;" OwnerUserId="2733" LastActivityDate="2016-02-22T14:49:23.237" Title="Hardware Performance: Procedural Textures vs Traditional Textures" Tags="&lt;texture&gt;&lt;procedural-generation&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2016-02-22T20:22:33.047" />
  <row Id="2085" PostTypeId="2" ParentId="2074" CreationDate="2016-02-22T21:48:31.237" Score="0" Body="&lt;p&gt;Thanks for the replies. According to joojaa's comments, I modified my codes as below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public void RotateImageClockwise(double degree)&#xA;    {&#xA;        Image&amp;lt;double&amp;gt; image = (Image&amp;lt;double&amp;gt;)(PreservedRawImage.Clone());      &#xA;        double sinAngle = Math.Sin(Math.PI * degree / 180.0);&#xA;        double cosAngle = Math.Cos(Math.PI * degree / 180.0);&#xA;        double newHeightRaw, newWidthRaw;&#xA;&#xA;        if (Math.Abs(degree) &amp;lt;= 90)&#xA;        {&#xA;            newHeightRaw = image.Width * Math.Sin(Math.PI * (Math.Abs(degree)) / 180.0) + image.Height * Math.Cos(Math.PI * Math.Abs(degree) / 180.0);&#xA;            newWidthRaw = image.Width * Math.Cos(Math.PI * Math.Abs(degree) / 180.0) + image.Height * Math.Sin(Math.PI * (Math.Abs(degree)) / 180.0);&#xA;        }&#xA;        else if (Math.Abs(degree) &amp;lt;= 180)&#xA;        {&#xA;            newHeightRaw = image.Width * Math.Sin(Math.PI * (180 - Math.Abs(degree)) / 180.0) + image.Height * Math.Sin(Math.PI * (Math.Abs(degree) - 90) / 180.0);&#xA;            newWidthRaw = image.Width * Math.Cos(Math.PI * (180 - Math.Abs(degree)) / 180.0) + image.Height * Math.Cos(Math.PI * (Math.Abs(degree) - 90) / 180.0);&#xA;        }&#xA;&#xA;        int newWidth = (int)Math.Round(newWidthRaw);&#xA;        int newHeight = (int)Math.Round(newHeightRaw);&#xA;        Image&amp;lt;double&amp;gt; newImage = new Image&amp;lt;double&amp;gt;(newWidth, newHeight);&#xA;        for (int x = 0; x &amp;lt; newHeight; x++)&#xA;        {&#xA;            for (int y = 0; y &amp;lt; newWidth; y++)&#xA;            {&#xA;                double xt = x - (newHeightRaw - 1) / 2.0;&#xA;                double yt = y - (newWidthRaw - 1) / 2.0;&#xA;&#xA;                double originalX = xt * cosAngle - yt * sinAngle + (image.Height - 1) / 2.0;&#xA;                double originalY = xt * sinAngle + yt * cosAngle + (image.Width - 1) / 2.0;&#xA;&#xA;                double fractionX = Math.Abs(originalX - (int)Math.Round(originalX));&#xA;                double fractionY = Math.Abs(originalY - (int)Math.Round(originalY));&#xA;&#xA;                IList&amp;lt;Point&amp;lt;int&amp;gt;&amp;gt; neighbors = new List&amp;lt;Point&amp;lt;int&amp;gt;&amp;gt; { new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX), y = (int)Math.Round(originalY), weight = (1 - fractionX) * (1 - fractionY)}, &#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX), y = (int)Math.Round(originalY) + 1, weight = fractionX * (1 - fractionY)},&#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX) + 1, y = (int)Math.Round(originalY), weight = (1 - fractionX) * fractionY},&#xA;                                                           new Point&amp;lt;int&amp;gt;{x = (int)Math.Round(originalX) + 1, y = (int)Math.Round(originalY) + 1, weight = fractionX * fractionY}&#xA;                                                         };&#xA;                bool hasData = false;&#xA;                double newData = 0;&#xA;                double weight = 0;&#xA;                foreach (var neighbor in neighbors)&#xA;                {&#xA;                    if (neighbor.x &amp;gt;= 0 &amp;amp;&amp;amp; neighbor.x &amp;lt; image.Height&#xA;                        &amp;amp;&amp;amp; neighbor.y &amp;gt;= 0 &amp;amp;&amp;amp; neighbor.y &amp;lt; image.Width&#xA;                        &amp;amp;&amp;amp; (neighbor.x) * image.Width + neighbor.y &amp;lt; m_preservedImageValidDataBitmap.Length &amp;amp;&amp;amp; m_preservedImageValidDataBitmap[(neighbor.x) * image.Width + neighbor.y] == 'T')&#xA;                    {&#xA;                        hasData = true;&#xA;                        newData += neighbor.weight * image.Data[(neighbor.x) * image.Width + neighbor.y];&#xA;                        weight += neighbor.weight;&#xA;                    }&#xA;                }                    &#xA;                if (hasData &amp;amp;&amp;amp; weight != 0)&#xA;                {&#xA;                    newData /= weight;&#xA;                    newImage.Data[x * newWidth + y] = newData;                        &#xA;                }&#xA;            }                &#xA;        }&#xA;    } &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now it's working for 90 and 180 degree rotations. But for 45 degree rotation, the bottom line is still blank, which makes me crazy. I'm guessing it's due to the computation inaccuracy when the new image width and height are computed, but I still can't figure it out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/OQB9E.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/OQB9E.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2720" LastActivityDate="2016-02-22T21:48:31.237" CommentCount="0" />
  <row Id="2086" PostTypeId="1" AcceptedAnswerId="2092" CreationDate="2016-02-23T09:43:40.470" Score="3" ViewCount="87" Body="&lt;p&gt;As explained in Holger Dammetz page &lt;a href=&quot;http://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html&quot; rel=&quot;nofollow&quot;&gt;Hammersley Points on the Hemisphere&lt;/a&gt;, the 2D coordinates (u,v) are usually mapped to the spherical coordinates (θ, φ).&#xA;&lt;a href=&quot;http://i.stack.imgur.com/zOAcc.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zOAcc.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;As a result, the coordinate &quot;uniformly distributed&quot; - u in the 2D case - is the polar angle θ. This looks counter-intuitive to me, I would have mapped regularly spaced u values to regularly spaced azimuth φ values. To avoid &quot;empty areas&quot; like in this example:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/R0lK7.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/R0lK7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;which is generated by the mapping of the following 2D set:&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/9DZEr.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9DZEr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;What is the reason not to map (u, v) to (φ, θ) ?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2016-03-10T09:11:45.747" Title="Why map Hammersley 2D set's (u,v) to sphere's (θ, φ) coordinates (and not to (φ, θ) )?" Tags="&lt;2d&gt;&lt;sampling&gt;&lt;mapping&gt;&lt;distribution&gt;&lt;hemisphere&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2087" PostTypeId="1" AcceptedAnswerId="2290" CreationDate="2016-02-23T12:55:57.477" Score="2" ViewCount="50" Body="&lt;p&gt;So, recently I followed this &lt;a href=&quot;http://www.paulsprojects.net/tutorials/smt/smt.html&quot; rel=&quot;nofollow&quot;&gt;tutorial&lt;/a&gt; about shadow mapping, but in the tutorial it doesn't texture the object.. When I try to add a texture, it's distorted by the &lt;code&gt;TexGen&lt;/code&gt;. Also I've tried to use &lt;code&gt;glActiveTexture()&lt;/code&gt; with no success(dunno exactly how &lt;code&gt;glActiveTexture()&lt;/code&gt; work).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So can anyone help me by adding textures into above code base? From there, I can learn how to use &lt;code&gt;glActiveTexture()&lt;/code&gt; and overcome that &lt;code&gt;TexGen&lt;/code&gt; issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.. To note, I'm not using GLSL.&lt;/p&gt;&#xA;" OwnerUserId="2687" LastActivityDate="2016-04-08T12:52:30.253" Title="How can I use multiple texturing with shadow mapping?" Tags="&lt;opengl&gt;&lt;texture&gt;&lt;shadow-mapping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2088" PostTypeId="1" CreationDate="2016-02-23T15:33:31.027" Score="9" ViewCount="134" Body="&lt;p&gt;I have a set of 3D points (which I recover from a library that performs the tessellation of a solid body) that belong to a curve (i.e., an edge of the solid). That means that the curve surely passes by each of these points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nevertheless, the point set is unordered so I need to sort them in order to be able to draw this curve correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any known approach for these type of problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some additional information:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The curves are parametric in general (splines/bezier, circle slices.. ). &lt;/li&gt;&#xA;&lt;li&gt;The points are given as floating point coordinates.&lt;/li&gt;&#xA;&lt;li&gt;The points are packed very densely (but they can be as dense as I want it to be). To give you an idea, for a curve which occupies 19 units in x, 10 units in x and 5 units in z, I quote a sequence of points in a segment of curve: (20.7622, 25.8676, 0) (20.6573, 25.856, 0) (20.5529, 25.8444, 0) (20.4489, 25.8329, 0) (20.3454, 25.8213, 0)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2562" LastEditorUserId="2562" LastEditDate="2016-02-23T16:48:57.977" LastActivityDate="2016-02-23T17:59:07.243" Title="Ordering a set of unorganized points along a curve" Tags="&lt;geometry&gt;&lt;mesh&gt;&lt;curve&gt;" AnswerCount="3" CommentCount="6" FavoriteCount="1" />
  <row Id="2089" PostTypeId="2" ParentId="2088" CreationDate="2016-02-23T16:09:39.853" Score="2" Body="&lt;p&gt;Since you've only got floating-point representations of the points, there is no guarantee that these still lie exactly on the curve, due to rounding errors. So I think the only generic approach is to approximate where on the curve they were, by finding the closest point on the curve to your sample $(X,Y,Z)$. E.g. if your parametric curve is $(x(t), y(t), z(t))$ then you could try to minimise&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;(X-x(t))^2+(Y-y(t))^2+(Z-z(t))^2&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with respect to $t$. If you know what type of curve you're dealing with there might be nice analytic solutions to this, otherwise you'll have to resort to some numerical algorithm. Since your points should be very close to the curve, this method should be reliable (provided the minimisation algorithm is), unless you have a sample exactly where the curve (almost) crosses itself. In that case you're probably out of luck anyway, though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have that $t$ for each of your points, you can simply sort them by $t$. Of course, if you have any control over how you receive the points you might be able to sidestep all of this trouble by returning $t$ along with the point's coordinates right away while generating them.&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2016-02-23T16:09:39.853" CommentCount="0" />
  <row Id="2090" PostTypeId="2" ParentId="2088" CreationDate="2016-02-23T16:48:34.430" Score="3" Body="&lt;p&gt;After some clarifications, there is probably a much better approach that doesn't even require knowing the parametric form of the curve, and also avoids the potentially problematic numeric minimisation step. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the curve does not intersect itself and the points are sufficiently densely packed on the curve (and by that I mean they have to be closer than any two points on the curve that &lt;em&gt;don't&lt;/em&gt; belong to the same segment, e.g. by the curve wrapping around on itself), then you can easily determine the previous and next point to each sample:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Determine the two nearest neighbours to each point. &lt;a href=&quot;https://en.wikipedia.org/wiki/Nearest_neighbor_search#All_nearest_neighbors&quot; rel=&quot;nofollow&quot;&gt;That's an $O(n \log n)$ operation&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;You'll have to do some special treatment for the endpoints. Their two nearest neighbours will be the next &lt;em&gt;two&lt;/em&gt; points along the curve instead of one on each side. You can either detect these heuristically if the ratio  of the distances to the two neighbours differs by more than some threshold (1.5 say, depends on the smoothness of your curve and how densely the points are packed). Or you can treat your nearest-neighbour data as a graph, in which you'll find that the endpoints' two neighbours point at each other (which shouldn't happen anywhere else in the graph).&lt;/li&gt;&#xA;&lt;li&gt;Now you can simply pick one end point, and walk along the nearest neighbours (always choosing the one you didn't arrive from) to traverse the points along the curve.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Especially if you can make the points very dense this should be the most reliable option unless the curve self-intersects. Even in that case, this approach might be salvageable provided the self-intersection is at a sufficiently large angle the curve smooth enough (in that case you can choose the correct neighbours based on some constraint that successive steps cannot make an angle greater than some threshold $\theta$).&lt;/p&gt;&#xA;" OwnerUserId="16" LastActivityDate="2016-02-23T16:48:34.430" CommentCount="0" />
  <row Id="2091" PostTypeId="2" ParentId="2088" CreationDate="2016-02-23T17:59:07.243" Score="5" Body="&lt;p&gt;You have an instance of a problem called &lt;em&gt;curve reconstruction from unorganized points&lt;/em&gt;. Now that you know what to search for you'll find several methods, such as the crust, NN-crust, etc. Here are a few links:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://sarielhp.org/research/CG/applets/Crust/Crust.html&quot;&gt;The Crust Curve Reconstruction Applet&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://web.cse.ohio-state.edu/~tamaldey/curverecon.htm&quot;&gt;Curve Reconstruction&lt;/a&gt; by Tamal Dey&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521863704&quot;&gt;Curve and Surface Reconstruction: Algorithms with Mathematical Analysis&lt;/a&gt;, book by Tamal K. Dey, Cambridge University Press, 2006&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Since you're dealing with curves and the samples are dense, I suggest you compute an Euclidean minimal spanning tree.&lt;/p&gt;&#xA;" OwnerUserId="192" LastActivityDate="2016-02-23T17:59:07.243" CommentCount="0" />
  <row Id="2092" PostTypeId="2" ParentId="2086" CreationDate="2016-02-24T14:37:14.710" Score="5" Body="&lt;p&gt;You can of course, as you suggested, map (u, v) to (φ, θ). Unfortunately, it does not solve the problem for 5 points:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/csMKn.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/csMKn.png&quot; alt=&quot;Switched mapping&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've changed Holger Dammertz' code a bit (switched u and v), and you see that the problem still persists. For a higher number of points, it really doesn't seem to make a (visual) difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Hammersley point set is a way to &lt;strong&gt;very quickly&lt;/strong&gt; determine points on a sphere that have good, but don't need to have a perfect distribution. In the problematic case of very few points, Hammersley might just not be the tool for the job. Pre-calculating and storing points sounds like a better option to me.&lt;/p&gt;&#xA;" OwnerUserId="385" LastActivityDate="2016-02-24T14:37:14.710" CommentCount="0" />
  <row Id="2093" PostTypeId="1" CreationDate="2016-02-25T17:22:44.533" Score="4" ViewCount="69" Body="&lt;p&gt;I have a 3D model, which consists of 160 thousand vertices and 100 thousand triangles. Format is: there's array of vertices' coordinates and array of triangles, which has numbers of vertices (e.g., [1, 2, 5] -- triangle, which consists of first, second and fifth vertex from vertices' array).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I calculate color of each triangle and rotate the model by myself.&#xA;Also I &quot;render&quot; it manually, putting vertices as pixels on &lt;code&gt;PIL&lt;/code&gt; image, but matrix with colors is enough for my purpose (I need only the matrix).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know, whether I will implement robust and correct rendering function, but I cannot find light-weight library as well.&#xA;All I need is to display the set of triangles with specified colors and  background image fast (far less than for a second).&lt;/p&gt;&#xA;" OwnerUserId="2750" LastEditorUserId="2750" LastEditDate="2016-02-25T17:56:00.497" LastActivityDate="2016-03-24T22:51:04.317" Title="Triangulated model rasterization: light-weight Python library" Tags="&lt;3d&gt;&lt;raster-image&gt;&lt;render&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="2094" PostTypeId="1" CreationDate="2016-02-25T22:53:01.660" Score="1" ViewCount="148" Body="&lt;p&gt;&lt;strong&gt;Some possible things that might be the issue and could be explained by someone who is knowledgeable&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Order of my indices, In all I've read, your array of indices for a triangle list is just clockwise order, see below how I ordered it for proof. &lt;strong&gt;EDIT - I have changed my indices according to Jason Allen's comment, no change&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Keep in mind that my code runs fine, no errors, no warnings(that I know of) related to my issue.&lt;/li&gt;&#xA;&lt;li&gt;Can these warnings be an issue?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/UJYGI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/UJYGI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been banging my head over this issue for far too long. I know the answer is going to be ridiculously simple but somehow it has successfully evaded me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been implementing deferred rendering into my engine and my second pass, the one that actually renders to the swap chain buffer, is staying black (or whatever color I clear it to before rendering).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I know the issue has nothing to do with anything associated with the first pass/gBuffer because I am completely bypassing that part, literally commented out the call to it in the game loop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My 2nd Pass Deferred Vertex Shader is as follows... &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float4 main( float4 pos : POSITION ) : SV_POSITION&#xA;{&#xA;     return pos;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My 2nd Pass Pixel Shader is as follows... &#xA;I have omitted the actual lighting code because it is all commented out until I fix the issue. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; float4 main(in float4 screenPos : SV_Position) : SV_Target&#xA; {&#xA;     return float4(1.0f,1.0f,1.0f, 1.0f);&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see I am not doing anything related to lighting or anything, all I'm trying to do is set every pixel to white, I want to see it work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my renderSecondPass function. this is what I call every frame. Irrelevant code has been omitted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; void DirectX::renderSecondPass()&#xA; {&#xA; nullShaders();&#xA;&#xA; d3d11DeviceContext-&amp;gt;VSSetShader(deferredVertexShader, NULL, 0);&#xA; d3d11DeviceContext-&amp;gt;PSSetShader(deferredPixelShader, NULL, 0);&#xA;&#xA; HRESULT hr;&#xA; float clear[] = { 1.0, 0.5, 0.0, 1.0 };&#xA;&#xA; d3d11DeviceContext-&amp;gt;OMSetRenderTargets(1, &amp;amp;d3d11RenderTargetView, NULL);&#xA; d3d11DeviceContext-&amp;gt;ClearRenderTargetView(d3d11RenderTargetView, clear);&#xA;&#xA; d3d11DeviceContext-&amp;gt;IASetInputLayout(InputLayout::Deferred);&#xA; d3d11DeviceContext-&amp;gt;IASetPrimitiveTopology(D3D11_PRIMITIVE_TOPOLOGY_TRIANGLELIST);&#xA;&#xA;&#xA; for (unsigned int i = 0; i &amp;lt; lightBuffer.size(); i++)&#xA; {&#xA;    UINT stride = sizeof(Vertex::Deferred);&#xA;    UINT offset = 0;&#xA;&#xA;    d3d11DeviceContext-&amp;gt;IASetVertexBuffers(0, 1, &amp;amp;deferredQuadVB, &amp;amp;stride, &amp;amp;offset);&#xA;    d3d11DeviceContext-&amp;gt;IASetIndexBuffer(deferredQuadIB, DXGI_FORMAT_R32_UINT, 0);&#xA;&#xA;    d3d11DeviceContext-&amp;gt;DrawIndexed(deferredQuadIndiceCount, 0, 0);&#xA; }&#xA;&#xA; hr = swapChain-&amp;gt;Present(0, 0);&#xA;&#xA; if (FAILED(hr))&#xA; {&#xA;     MessageBox(0, L&quot;Swap Chain Present Error.&quot;, 0, 0);&#xA; }&#xA;&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the Input Layout...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; namespace Vertex&#xA; {&#xA;     struct Deferred&#xA;     {&#xA;         VECTOR&amp;lt;float&amp;gt; Position;&#xA;         float Position_W;&#xA;     };&#xA; }&#xA; // C++ side above &#xA;&#xA; // Shaderside below&#xA; namespace InputLayout&#xA; {&#xA;     static ID3D11InputLayout* Deferred;&#xA; }&#xA;&#xA; const D3D11_INPUT_ELEMENT_DESC InputLayoutDesc::DEFERRED_VS_INPUTLAYOUT_DESC[] =&#xA; {&#xA;     { &quot;POSITION&quot;,  0, DXGI_FORMAT_R32G32B32A32_FLOAT,  0, 0,   D3D11_INPUT_PER_VERTEX_DATA, 0 }&#xA; };&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Creation of the Input Layouts and Loading of the shaders themselves from file. &#xA;Irrelevant code has been omitted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool DirectX::initShaders()&#xA;{&#xA;std::vector&amp;lt;char&amp;gt; shaderBuffer;&#xA;std::string filename;&#xA;HRESULT hr;&#xA;&#xA;// deferredVS&#xA;filename = &quot;Shaders\\deferredVS.cso&quot;;&#xA;if (!shaderFileToMemory(filename, shaderBuffer))&#xA;{&#xA;    MessageBox(0, L&quot;Load defaultVS Shader Into Memory Error.&quot;, 0, 0);&#xA;    return false;&#xA;}&#xA;hr = d3d11Device-&amp;gt;CreateVertexShader(&amp;amp;shaderBuffer[0], shaderBuffer.size(), NULL, &amp;amp;deferredVertexShader);&#xA;hr = d3d11Device-&amp;gt;CreateInputLayout(InputLayoutDesc::DEFERRED_VS_INPUTLAYOUT_DESC, 1, &amp;amp;shaderBuffer[0], shaderBuffer.size(), &amp;amp;InputLayout::Deferred);&#xA;&#xA;// deferredPS&#xA;filename = &quot;Shaders\\deferredPS.cso&quot;;&#xA;if (!shaderFileToMemory(filename, shaderBuffer))&#xA;{&#xA;    MessageBox(0, L&quot;Load deferredPS Shader Into Memory Error.&quot;, 0, 0);&#xA;    return false;&#xA;}&#xA;hr = d3d11Device-&amp;gt;CreatePixelShader(&amp;amp;shaderBuffer[0], shaderBuffer.size(), NULL, &amp;amp;deferredPixelShader);&#xA;&#xA;return true;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Vertex and Index Buffer were created using the following...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; bool DirectX::initDeferredQuad()&#xA; {&#xA;std::vector&amp;lt;Vertex::Deferred&amp;gt; deferredQuadVertices;&#xA;std::vector&amp;lt;unsigned int&amp;gt; deferredQuadIndices;&#xA;&#xA;Vertex::Deferred quadVertices[4];&#xA;Vertex::Deferred quadVertice;&#xA;&#xA;quadVertice.Position_W  = 1.0;&#xA;quadVertice.Position = VECTOR&amp;lt;float&amp;gt;(-1.0, 1.0, 0.0);&#xA;quadVertices[0] = quadVertice;&#xA;quadVertice.Position = VECTOR&amp;lt;float&amp;gt;(1.0, 1.0, 0.0);&#xA;quadVertices[1] = quadVertice;&#xA;quadVertice.Position = VECTOR&amp;lt;float&amp;gt;(-1.0, -1.0, 0.0);&#xA;quadVertices[2] = quadVertice;&#xA;quadVertice.Position = VECTOR&amp;lt;float&amp;gt;(1.0, -1.0, 0.0);&#xA;quadVertices[3] = quadVertice;&#xA;&#xA;D3D11_BUFFER_DESC vbd;&#xA;D3D11_SUBRESOURCE_DATA vinitData;&#xA;HRESULT hr;&#xA;&#xA;vbd.Usage               = D3D11_USAGE_IMMUTABLE;&#xA;vbd.ByteWidth           = sizeof(Vertex::Deferred) * 4;&#xA;vbd.BindFlags           = D3D11_BIND_VERTEX_BUFFER;&#xA;vbd.CPUAccessFlags      = 0;&#xA;vbd.MiscFlags           = 0;&#xA;vbd.StructureByteStride = 0;&#xA;&#xA;vinitData.pSysMem       = &amp;amp;quadVertices;&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateBuffer(&amp;amp;vbd, &amp;amp;vinitData, &amp;amp;deferredQuadVB);&#xA;&#xA;if (FAILED(hr))&#xA;{&#xA;    MessageBox(0, L&quot;Quad Vertex Buffer Creation Error.&quot;, 0, 0);&#xA;    return false;&#xA;}&#xA;&#xA;unsigned int quadIndices[] =&#xA;{   0, 1, 2,&#xA;    2, 1, 3 };&#xA;&#xA;deferredQuadIndiceCount = 6;&#xA;&#xA;D3D11_BUFFER_DESC ibd;&#xA;D3D11_SUBRESOURCE_DATA iinitData;&#xA;&#xA;ibd.Usage               = D3D11_USAGE_IMMUTABLE;&#xA;ibd.ByteWidth           = sizeof(unsigned int) * 6;&#xA;ibd.BindFlags           = D3D11_BIND_INDEX_BUFFER;&#xA;ibd.CPUAccessFlags      = 0;&#xA;ibd.MiscFlags           = 0;&#xA;&#xA;iinitData.pSysMem       = &amp;amp;deferredQuadIndices;&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateBuffer(&amp;amp;ibd, &amp;amp;iinitData, &amp;amp;deferredQuadIB);&#xA;&#xA;if (FAILED(hr))&#xA;{&#xA;    MessageBox(0, L&quot;QuadIndex Buffer Creation Error.&quot;, 0, 0);&#xA;    return false;&#xA;}&#xA;&#xA;return true;&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I thought the issue was here. Since my Vertex Shader is feeding the Rasterizer data directly, the Vertex Shader has to output SV_Position in Clip Space. Wouldn't the coordinates I've made for my vertices be correct for a quad composed of 2 triangles that would take up the entire screen? (I've tried changing the z values, no change)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is resizeSwapChain, I've been bulding/rebuild the viewport in here.&#xA;This function is called at initialization and whenever the window size changes after the window adjustment has been released (WM_EXITSIZEMOVE).&#xA;Irrelevant code has been omitted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; void DirectX::resizeSwapChain()&#xA; {&#xA;const unsigned int clientWidth = uClient-&amp;gt;getAppScreenWidth();&#xA;const unsigned int clientHeight = uClient-&amp;gt;getAppScreenHeight();&#xA;HRESULT hr;&#xA;&#xA;if (!(d3d11RenderTargetView == NULL))&#xA;{&#xA;    d3d11RenderTargetView-&amp;gt;Release();&#xA;    d3d11RenderTargetView = 0;&#xA;}&#xA;&#xA;if (!(d3d11DepthStencilView) == NULL)&#xA;{&#xA;    d3d11DepthStencilView-&amp;gt;Release();&#xA;    d3d11DepthStencilView = 0;&#xA;}&#xA;&#xA;if (!(d3d11StencilBuffer) == NULL)&#xA;{&#xA;    d3d11StencilBuffer-&amp;gt;Release();&#xA;    d3d11StencilBuffer = 0;&#xA;}&#xA;&#xA;hr = swapChain-&amp;gt;ResizeBuffers(1, clientWidth, clientHeight, DXGI_FORMAT_R8G8B8A8_UNORM, 0);&#xA;&#xA;ID3D11Texture2D* backBuffer;&#xA;&#xA;hr = swapChain-&amp;gt;GetBuffer(0, __uuidof(ID3D11Texture2D), reinterpret_cast&amp;lt;void**&amp;gt;(&amp;amp;backBuffer));&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateRenderTargetView(backBuffer, 0, &amp;amp;d3d11RenderTargetView);&#xA;&#xA;backBuffer-&amp;gt;Release();&#xA;backBuffer = 0;&#xA;&#xA;D3D11_TEXTURE2D_DESC depthStencilDesc;&#xA;&#xA;depthStencilDesc.Width = clientWidth;&#xA;depthStencilDesc.Height = clientHeight;&#xA;depthStencilDesc.MipLevels = 1;&#xA;depthStencilDesc.ArraySize = 1;&#xA;depthStencilDesc.Format = DXGI_FORMAT_D24_UNORM_S8_UINT;&#xA;&#xA;if (msaaEnabled)&#xA;{&#xA;    depthStencilDesc.SampleDesc.Count = 4;&#xA;    depthStencilDesc.SampleDesc.Quality = msaaQuality - 1;&#xA;}&#xA;else&#xA;{&#xA;    depthStencilDesc.SampleDesc.Count = 1;&#xA;    depthStencilDesc.SampleDesc.Quality = 0;&#xA;}&#xA;&#xA;depthStencilDesc.Usage = D3D11_USAGE_DEFAULT;&#xA;depthStencilDesc.BindFlags = D3D11_BIND_DEPTH_STENCIL;&#xA;depthStencilDesc.CPUAccessFlags = 0;&#xA;depthStencilDesc.MiscFlags = 0;&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateTexture2D(&amp;amp;depthStencilDesc, 0, &amp;amp;d3d11StencilBuffer);&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateDepthStencilView(d3d11StencilBuffer, 0, &amp;amp;d3d11DepthStencilView);&#xA;&#xA;d3d11ScreenViewport.TopLeftX = 0;&#xA;d3d11ScreenViewport.TopLeftY = 0;&#xA;d3d11ScreenViewport.Width = static_cast&amp;lt;float&amp;gt;(clientWidth);&#xA;d3d11ScreenViewport.Height = static_cast&amp;lt;float&amp;gt;(clientHeight);&#xA;d3d11ScreenViewport.MinDepth = 0.0f;&#xA;d3d11ScreenViewport.MaxDepth = 1.0f;&#xA;&#xA;d3d11DeviceContext-&amp;gt;RSSetViewports(1, &amp;amp;d3d11ScreenViewport);&#xA;&#xA;mainCamera-&amp;gt;setProjectionMatrix(MATRIX4X4&amp;lt;float&amp;gt;::perspectiveFieldOfViewLeftHandMatrix(0.25 * GenericMath::PI, GenericMath::aspectRatio(clientWidth, clientHeight), 0.001, 1000.0));&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is my D3D11 Initialization function, this is where I create my Rasterization states.&#xA;Irrelevant code has been omitted.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; bool DirectX::initDirectX()&#xA; {&#xA;D3D_FEATURE_LEVEL featureLevel;&#xA;&#xA;HRESULT hr = D3D11CreateDevice(NULL, driverType, NULL, NULL, NULL, 0, D3D11_SDK_VERSION, &amp;amp;d3d11Device, &amp;amp;featureLevel, &amp;amp;d3d11DeviceContext);&#xA;&#xA;hr = d3d11Device-&amp;gt;CheckMultisampleQualityLevels( DXGI_FORMAT_R8G8B8A8_UNORM, 4, &amp;amp;msaaQuality);&#xA;&#xA;DXGI_SWAP_CHAIN_DESC swapChainDesc;&#xA;swapChainDesc.BufferDesc.Width = uClient-&amp;gt;getAppScreenWidth();&#xA;swapChainDesc.BufferDesc.Height = uClient-&amp;gt;getAppScreenHeight();&#xA;swapChainDesc.BufferDesc.RefreshRate.Numerator = 60;&#xA;swapChainDesc.BufferDesc.RefreshRate.Denominator = 1;&#xA;swapChainDesc.BufferDesc.Format = DXGI_FORMAT_R8G8B8A8_UNORM;&#xA;swapChainDesc.BufferDesc.ScanlineOrdering = DXGI_MODE_SCANLINE_ORDER_UNSPECIFIED;&#xA;swapChainDesc.BufferDesc.Scaling = DXGI_MODE_SCALING_UNSPECIFIED;&#xA;&#xA;if (msaaEnabled)&#xA;{&#xA;    swapChainDesc.SampleDesc.Count = 4;&#xA;    swapChainDesc.SampleDesc.Quality = msaaQuality - 1;&#xA;}&#xA;else&#xA;{&#xA;    swapChainDesc.SampleDesc.Count = 1;&#xA;    swapChainDesc.SampleDesc.Quality = 0;&#xA;}&#xA;&#xA;swapChainDesc.BufferUsage = DXGI_USAGE_RENDER_TARGET_OUTPUT;&#xA;swapChainDesc.BufferCount = 1;&#xA;swapChainDesc.OutputWindow = *windowsHandle;&#xA;swapChainDesc.Windowed = true;&#xA;swapChainDesc.SwapEffect = DXGI_SWAP_EFFECT_DISCARD;&#xA;swapChainDesc.Flags = 0;&#xA;&#xA;IDXGIDevice* dxgiDevice = 0;&#xA;hr = d3d11Device-&amp;gt;QueryInterface(__uuidof(IDXGIDevice), (void**)&amp;amp;dxgiDevice);&#xA;&#xA;IDXGIAdapter* dxgiAdapter = 0;&#xA;hr = dxgiDevice-&amp;gt;GetParent(__uuidof(IDXGIAdapter), (void**)&amp;amp;dxgiAdapter);&#xA;&#xA;IDXGIFactory* dxgiFactory = 0;&#xA;hr = dxgiAdapter-&amp;gt;GetParent(__uuidof(IDXGIFactory), (void**)&amp;amp;dxgiFactory);&#xA;&#xA;hr = dxgiFactory-&amp;gt;CreateSwapChain(d3d11Device, &amp;amp;swapChainDesc, &amp;amp;swapChain);&#xA;&#xA;dxgiDevice-&amp;gt;Release();&#xA;dxgiDevice = 0;&#xA;&#xA;dxgiAdapter-&amp;gt;Release();&#xA;dxgiAdapter = 0;&#xA;&#xA;dxgiFactory-&amp;gt;Release();&#xA;dxgiFactory = 0;&#xA;&#xA;resizeGBuffers();&#xA;resizeSwapChain();&#xA;&#xA;D3D11_RASTERIZER_DESC rasterizerDesc;&#xA;ZeroMemory(&amp;amp;rasterizerDesc, sizeof(D3D11_RASTERIZER_DESC));&#xA;rasterizerDesc.FillMode = D3D11_FILL_SOLID;&#xA;rasterizerDesc.CullMode = D3D11_CULL_BACK;&#xA;rasterizerDesc.FrontCounterClockwise = false;&#xA;rasterizerDesc.DepthClipEnable = true;&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateRasterizerState( &amp;amp;rasterizerDesc, &amp;amp;d3d11SolidRS);\&#xA;&#xA;ZeroMemory(&amp;amp;rasterizerDesc, sizeof(D3D11_RASTERIZER_DESC));&#xA;&#xA;rasterizerDesc.FillMode = D3D11_FILL_WIREFRAME;&#xA;rasterizerDesc.CullMode = D3D11_CULL_BACK;&#xA;rasterizerDesc.FrontCounterClockwise = false;&#xA;rasterizerDesc.DepthClipEnable = true;&#xA;&#xA;hr = d3d11Device-&amp;gt;CreateRasterizerState( &amp;amp;rasterizerDesc, &amp;amp;d3d11WireframeRS);&#xA;&#xA;d3d11DeviceContext-&amp;gt;RSSetState(d3d11SolidRS);&#xA;&#xA;return true;&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If anyone needs more info, just ask. Thanks&lt;/p&gt;&#xA;" OwnerUserId="2726" LastEditorUserId="2726" LastEditDate="2016-02-26T12:17:57.383" LastActivityDate="2016-02-26T16:05:40.143" Title="Trouble Finding Simple 2D DirecX11/HLSL Issue" Tags="&lt;directx11&gt;&lt;pixel-shader&gt;&lt;hlsl&gt;&lt;deferred-rendering&gt;" AnswerCount="3" CommentCount="4" />
  <row Id="2095" PostTypeId="2" ParentId="2094" CreationDate="2016-02-26T00:06:17.400" Score="1" Body="&lt;p&gt;Your indices are wrong, they should start at 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Correct indices:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;unsigned int quadIndices[] =&#xA;{   0, 1, 2,&#xA;    2, 1, 3 };&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="40" LastActivityDate="2016-02-26T00:06:17.400" CommentCount="1" />
  <row Id="2096" PostTypeId="2" ParentId="2094" CreationDate="2016-02-26T00:18:30.017" Score="4" Body="&lt;p&gt;You have not set a viewport with RSSetViewports. You need to set this to the pixel dimensions of your render target. Without this the viewport will be set to 0,0,0,0 meaning no pixels will be touched.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally you have not set a raster state or blend state. It is good practice to set these and can be helpful early on to set the winding order in the raster state to not cull, this can come in handy if you are working out the vertex positions by hand.&lt;/p&gt;&#xA;" OwnerUserId="2752" LastActivityDate="2016-02-26T00:18:30.017" CommentCount="3" />
  <row Id="2099" PostTypeId="1" AcceptedAnswerId="2104" CreationDate="2016-02-26T13:34:28.383" Score="5" ViewCount="49" Body="&lt;p&gt;I'm beginner in OpenGL and I stumbled into a problem. I'm in OpenGL 2.1 for the time being, if that's of any help. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I set my Frustum as such:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glFrustum(-1366, 1366, -768, 768, 1, 20000);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, when I create a cube from quads like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0.0, 500.0, 0.0&#xA;500.0, 500.0, 0.0&#xA;500.0, 0.0, 0.0&#xA;0.0, 0.0, 0.0&#xA;0.0, 500.0, 500.0&#xA;500.0, 500.0, 500.0&#xA;500.0, 0.0, 500.0&#xA;etc.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is not a cube, as I expected. It's extremely stretched along the z axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a screenshot of how it looks now: &lt;a href=&quot;https://www.dropbox.com/s/diznfumtev7g259/Screenshot%20from%202016-02-26%2017%3A01%3A58.png?dl=0&quot; rel=&quot;nofollow&quot;&gt;https://www.dropbox.com/s/diznfumtev7g259/Screenshot%20from%202016-02-26%2017%3A01%3A58.png?dl=0&lt;/a&gt; (I removed one of the faces temporarily)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The shape starts to resemble a cube a bit when I change the vertices to:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0.0, 500.0, 0.0&#xA;500.0, 500.0, 0.0&#xA;500.0, 0.0, 0.0&#xA;0.0, 0.0, 0.0&#xA;0.0, 500.0, 0.3&#xA;500.0, 500.0, 0.3&#xA;500.0, 0.0, 0.3&#xA;etc.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code related to this (I am translating this from assembly which I am actually using).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Matrices setup on initialization:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glMatrixMode(GL_PROJECTION);&#xA;glLoadIdentity();&#xA;glFrustum(-1366.0, 1366.0, -768.0, 768.0, 1.0, 20000.0);&#xA;&#xA;glMatrixMode(GL_MODELVIEW);&#xA;glLoadIdentity();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Transformations of the cube inside the &quot;render()&quot; function:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glLoadIdentity();&#xA;glTranslated(1000.0, 500.0, 0.0);&#xA;glRotated(degrees, 0.0, 0.0, 1.0); // degrees is +=2 every frame&#xA;glTranslated(-250.0, -250.0, distance); // distance is -=0.001 every frame&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are no other matrix operations in the code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code uses GLUT to initialize a window with double buffer and depth buffer. Depth testing is enabled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It all seems to work as expected, the rotation is ok, movement also. I am only troubled by the fact that when I define a 500.0x500.0x500.0 cube, the actual output is not a cube.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I doing wrong? How to fix this?&lt;/p&gt;&#xA;" OwnerUserId="2508" LastEditorUserId="2508" LastEditDate="2016-02-26T16:00:45.180" LastActivityDate="2016-02-26T22:46:50.040" Title="Frustum - problem with z dimension" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2101" PostTypeId="2" ParentId="2094" CreationDate="2016-02-26T16:05:40.143" Score="2" Body="&lt;p&gt;I can't believe it has taken me this long to find this...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the array I was giving the Index Buffer Desc that contains the data was the wrong one... It was a blank one that I forgot I didn't need, the vector if you look at the code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ruxiT.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ruxiT.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Look at that garbage. Those are supposed to be the indices...&#xA;Well at least I know how to use the debugger now, thanks guys got it working.&lt;/p&gt;&#xA;" OwnerUserId="2726" LastActivityDate="2016-02-26T16:05:40.143" CommentCount="0" />
  <row Id="2102" PostTypeId="1" AcceptedAnswerId="2119" CreationDate="2016-02-26T17:49:05.657" Score="7" ViewCount="75" Body="&lt;p&gt;JFA (the algorithm described here: &lt;a href=&quot;http://www.comp.nus.edu.sg/~tants/jfa/i3d06.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.comp.nus.edu.sg/~tants/jfa/i3d06.pdf&lt;/a&gt;) can be used to get an approximation of a Voronoi diagram, or a distance transform.  It does so in logarithmic time based on the size of the resulting image, not on the number of seeds.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What do you do if your image is not the same size on each axis though?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If they were similar sizes, I'm sure you could just let the shorter axis have extra JFA steps of size 1, while the larger axis finished it work (like having a 512 x 256 sized image).  For vastly different sized axis dimensions this could be a lot more inefficient though - say that you had a volume texture that was 512 x 512 x 4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible to run JFA on each axis separately and still get decent results?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or at that point is a different algorithm more appropriate to use? If so, what algorithm might that be?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my situation ideally I'm looking to support both single point seeds, as well as arbitrary shaped seeds.  Possibly even weighted seeds, where the distance to a seed is adjusted by a multiplier and/or an adder to give it more or less influence.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-02-29T21:47:32.867" LastActivityDate="2016-02-29T22:40:15.907" Title="Is Jump Flood Algorithm Separable?" Tags="&lt;signed-distance-field&gt;&lt;voronoi-diagram&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2103" PostTypeId="2" ParentId="2083" CreationDate="2016-02-26T19:58:08.587" Score="-1" Body="&lt;p&gt;Disney have a tool that analyses the database. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.disneyanimation.com/technology/brdf.html&quot; rel=&quot;nofollow&quot;&gt;http://www.disneyanimation.com/technology/brdf.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is browsing the source code for this an option?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/wdas/brdf&quot; rel=&quot;nofollow&quot;&gt;https://github.com/wdas/brdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could contact the authors perhaps?&lt;/p&gt;&#xA;" OwnerUserId="2748" LastActivityDate="2016-02-26T19:58:08.587" CommentCount="3" />
  <row Id="2104" PostTypeId="2" ParentId="2099" CreationDate="2016-02-26T22:46:50.040" Score="3" Body="&lt;p&gt;Okay. So after 6 hours of tinkering with my code, trying different approaches and googling for solutions, I found this page:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.glprogramming.com/red/chapter03.html&quot; rel=&quot;nofollow&quot;&gt;http://www.glprogramming.com/red/chapter03.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a short C listing which put me on the right track to solve this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I did wrong was that I should have set the frustum to something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLdouble x0 = -(1366/768);&#xA;GLdouble x1 = 1366/768;&#xA;glFrustum(x0, x1, -1, 1, 1, 100);&#xA;// the last two values aren't critical, as long as they are positive&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But before that, I should have set the &lt;strong&gt;glViewport&lt;/strong&gt; properly, before setting up the frustum:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glViewport(0, 0, 1366, 768);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;I didn't do it and that's why my code produced such weird results.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I can create a unit cube with following vertices:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0.0, 1.0, 0.0&#xA;1.0, 1.0, 0.0&#xA;1.0, 0.0, 0.0&#xA;0.0, 0.0, 0.0&#xA;0.0, 1.0, 1.0&#xA;1.0, 1.0, 1.0&#xA;1.0, 0.0, 1.0&#xA;0.0, 0.0, 1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And it will display properly as a cube of size 1 in every direction.&lt;/p&gt;&#xA;" OwnerUserId="2508" LastActivityDate="2016-02-26T22:46:50.040" CommentCount="1" />
  <row Id="2105" PostTypeId="1" CreationDate="2016-02-27T18:39:16.277" Score="5" ViewCount="67" Body="&lt;p&gt;I have a line segment defined by two end points. And I have another randomly chosen third point and I want to find out if the third point is on the line segment or not. By &quot;On the line segment&quot;, I mean that exactly on the line segment, not intersecting the line segment and end up on the other side of the line segment. I hope I explained the issue. Some mathematical explanation along with algorithm to it  is earnestly requested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-02-29T16:23:22.420" Title="Test if a point is on a line segment" Tags="&lt;vectors&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2106" PostTypeId="1" CreationDate="2016-02-27T22:40:53.963" Score="1" ViewCount="45" Body="&lt;p&gt;I have the world point positions of two objects, and would like to test if there is an intersection between the two objects from this information.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What methods can I use to test this, especially at the boundaries. &lt;/p&gt;&#xA;" OwnerUserId="2748" LastActivityDate="2016-02-28T05:45:40.427" Title="How to test if one object A intersects and is therefore partly or wholly inside object B" Tags="&lt;algorithm&gt;&lt;geometry&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2107" PostTypeId="2" ParentId="2083" CreationDate="2016-02-27T22:48:44.553" Score="1" Body="&lt;p&gt;Was interested in this myself and did some digging, I was wondering if they where spectral, but it appears not (although some interesting work on spectral BRDF is here &lt;a href=&quot;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/cliffl/cliffl_spectral_brdf.html&quot; rel=&quot;nofollow&quot;&gt;http://web.cs.wpi.edu/~emmanuel/courses/cs563/write_ups/cliffl/cliffl_spectral_brdf.html&lt;/a&gt; )&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This chat seems to indicate they are camera native linear, probably with sRGB primaries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://groups.google.com/forum/#!msg/pbrt/p9zMvZlaodA/4RD7-We8h4gJ&quot; rel=&quot;nofollow&quot;&gt;https://groups.google.com/forum/#!msg/pbrt/p9zMvZlaodA/4RD7-We8h4gJ&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2748" LastActivityDate="2016-02-27T22:48:44.553" CommentCount="1" />
  <row Id="2108" PostTypeId="2" ParentId="2105" CreationDate="2016-02-28T04:47:55.957" Score="6" Body="&lt;p&gt;Let's say you have your two points that define the line segment: $A$ and $B$, and a point $P$ that you are testing to see if it is on the line segment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Firstly, you get a normalized vector from $A$ to $B$, as well as the distance from $A$ to $B$.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;direction = normalize(B - A);&#xA;length = length(B-A);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next, you use dot product to project the point $P$ onto the line segment defined by $A$ and $B$, by first getting the vector from $A$ to $P$ and then dot producting that against the direction vector.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PRelative = P-A;&#xA;projection = dot(PRelative, direction);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This projection value represents how far down the line segment from $A$ to $B$ that the closest point to $P$ is.  It may be a negative distance, or it might be farther from $A$ than $B$ is, so you next clamp this value to the line segment.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;projection = clamp(projection, 0.0, length);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that in your case, instead of clamping, you could just detect when the projection was out of bounds and return false at this point as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Continuing on, you now have the distance from $A$ to $B$ that the closest point to $P$ is, as the value in $projection$.  You calculate that actual point:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;closestPoint = A + projection * direction;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If $closestPoint$ == $P$, you know that $P$ lies on the line segment defined by $A$ and $B$ because the closest point on that line segment is the point $P$ itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in practice, you might run into some numerical precision problems due to having limited accuracy in computer calculations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To get around this, instead of checking if $closestPoint$ and $P$ are the same, you should probably instead consider them the same if the distance between them is smaller than some threshold value.  This way, if precision issues come up, you'll still get the answers you want in the cases you want them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Too large of a threshold value, and you will get points being considered on the line that you really wouldn't consider on the line (if this happens, make your threshold smaller).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Too small of a threshold value, and some points that you would consider being on the line will not register as being on the line (if this happens, make your threshold larger).&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-02-29T16:23:22.420" LastActivityDate="2016-02-29T16:23:22.420" CommentCount="9" />
  <row Id="2109" PostTypeId="2" ParentId="2106" CreationDate="2016-02-28T05:45:40.427" Score="3" Body="&lt;p&gt;Do you need exact triangle - triangle intersection test that is do you need to know exactly the points of intersection of each object? If you just need a simple yes / no they have intersected then you could get away with creating an AABB ( &lt;a href=&quot;https://en.wikipedia.org/wiki/Bounding_volume&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/Bounding_volume&lt;/a&gt; ) that encompasses each object and do a AABB -&gt; AABB ( good info here &lt;a href=&quot;https://studiofreya.com/3d-math-and-physics/simple-aabb-vs-aabb-collision-detection/&quot; rel=&quot;nofollow&quot;&gt;https://studiofreya.com/3d-math-and-physics/simple-aabb-vs-aabb-collision-detection/&lt;/a&gt; ) intersection test? even if you need a more complex collision check Triangle -&gt; Triangle it is always a good idea to do a simple collision check first to make sure its worth doing the complex Triangle -&gt; Triangle intersection tests. Look here ( &lt;a href=&quot;http://www.realtimerendering.com/intersections.html&quot; rel=&quot;nofollow&quot;&gt;http://www.realtimerendering.com/intersections.html&lt;/a&gt; ) for all the normal object -&gt; object intersection tests you could possibly need &lt;/p&gt;&#xA;" OwnerUserId="288" LastActivityDate="2016-02-28T05:45:40.427" CommentCount="0" />
  <row Id="2110" PostTypeId="1" AcceptedAnswerId="2112" CreationDate="2016-02-28T15:05:52.690" Score="6" ViewCount="73" Body="&lt;p&gt;Single-scattering microfacet-based surface models like the original &lt;a href=&quot;http://www.graphics.cornell.edu/~westin/pubs/TorranceSparrowJOSA1967.pdf&quot;&gt;Torrance-Sparrow BRDF&lt;/a&gt; or derived models like &lt;a href=&quot;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&quot;&gt;the BSDF for rough dielectric surfaces&lt;/a&gt; by Walter et al. neglect inter-reflection of light between microfacets, which results in energy loss causing darkening especially at higher roughness values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem can be easily demonstrated using the furnace test. I present here the results of the furnace test of my implementation of BRDF using the Smith model and GGX microfacet distribution for roughness parameters from 0.2 to 1.0 (the Fresnel coefficient is set to 1 here to make the problem easier to see):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ZmLm3.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZmLm3.jpg&quot; alt=&quot;roughness 0.2&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/lIxSc.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/lIxSc.jpg&quot; alt=&quot;roughness 0.4&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/autib.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/autib.jpg&quot; alt=&quot;roughness 0.6&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/Ij2yS.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Ij2yS.jpg&quot; alt=&quot;roughness 0.8&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/eP5XW.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eP5XW.jpg&quot; alt=&quot;roughness 1.0&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Furnace test of my implementation of rough dielectric (IoR 1.51) BSDF using the Smith model and GGX microfacet distribution for roughness parameters from 0.2 to 1.0:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EZWNf.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EZWNf.jpg&quot; alt=&quot;roughness 0.2&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/wtcLq.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wtcLq.jpg&quot; alt=&quot;roughness 0.4&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/YisLW.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YisLW.jpg&quot; alt=&quot;roughness 0.6&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/SdaAG.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SdaAG.jpg&quot; alt=&quot;roughness 0.8&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/5PdTb.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5PdTb.jpg&quot; alt=&quot;roughness 1.0&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eric Heitz et al. have just recently proposed a &lt;a href=&quot;https://eheitzresearch.wordpress.com/240-2/&quot;&gt;multiple-scattering model&lt;/a&gt; which solves the darkening problem by solving the light interaction completely, but there are performance issues due to stochastic nature of its evaluation routine as metioned by Heitz himself in the &lt;a href=&quot;http://www.luxrender.net/forum/viewtopic.php?f=36&amp;amp;t=12534#p118797&quot;&gt;LuxRender forum&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a known compensation method for recovering the lost energy of single-scattering models? Not necessarily physically correct, but at least not breaking physical plausibility (Helmholtz reciprocity and energy conservation) too much and, ideally, without the necessity of hand tuning parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the &lt;a href=&quot;http://blog.selfshadow.com/publications/s2012-shading-course/burley/s2012_pbs_disney_brdf_notes_v3.pdf&quot;&gt;Disney BSDF&lt;/a&gt;, there is a parametrized component called “sheen” (basically a Fresnel-based glossy lobe) which can be used for compensation of darkening at edges, but as they mention in their &lt;a href=&quot;http://blog.selfshadow.com/publications/s2015-shading-course/burley/s2015_pbs_disney_bsdf_notes.pdf&quot;&gt;Siggraph 2015 course&lt;/a&gt; it is very ad hoc method: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;“...this is very approximate, and doesn't work as well for other&#xA;  roughness values...”&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;http://www.luxrender.net/forum/viewtopic.php?f=36&amp;amp;t=12534#p118797&quot;&gt;aforementioned comment&lt;/a&gt; from Eric Heitz in the LuxRender forum also suggests using some compensation hack but, unfortunately, doesn't go into any details:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;To my knowledge, you can use some simpler hacks to improve energy conservation in the single scattering models (like tweaking albedo). However, if you do that, you cannot get a perfectly energy conserving material (for instance perfect white rough glass) without breaking the reciprocity of the BSDF.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="2479" LastActivityDate="2016-02-28T22:16:11.020" Title="Compensation for energy loss in single-scattering microfacet BSDF models" Tags="&lt;rendering&gt;&lt;brdf&gt;&lt;physically-based&gt;&lt;microfacet&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2111" PostTypeId="2" ParentId="2083" CreationDate="2016-02-28T21:56:21.483" Score="1" Body="&lt;p&gt;Someone points me that &lt;em&gt;&lt;a href=&quot;http://white.stanford.edu/~brian/papers/pdc/ColorBalanceSPIE03.pdf&quot; rel=&quot;nofollow&quot;&gt;this article&lt;/a&gt;  describes the spectral response of the camera the MERL paper used, and even provides colour transform matrices at the end. Unfortunately, using those transforms result in even worse results than assuming sRGB primaries.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-02-28T21:56:21.483" CommentCount="0" />
  <row Id="2112" PostTypeId="2" ParentId="2110" CreationDate="2016-02-28T22:16:11.020" Score="7" Body="&lt;p&gt;To my knowledge, there is no easy and analytic way of recovering the energy lost in single-scattering models. The previous techniques precompute the energy loss and reinject it in the BRDF as a diffuse-like component:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://sirkan.iit.bme.hu/~szirmay/scook.pdf&quot;&gt;http://sirkan.iit.bme.hu/~szirmay/scook.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.cs.cornell.edu/projects/layered-sg14/layered.pdf&quot;&gt;http://www.cs.cornell.edu/projects/layered-sg14/layered.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What they propose is energy conservative and reciprocal, and it is probably the simplest way of fixing the visual darkening due to the energy loss. The main drawback is that since the energy loss does not have an analytic expression, it has to be precomputed and stored in a look-up table. &lt;/p&gt;&#xA;" OwnerUserId="2769" LastActivityDate="2016-02-28T22:16:11.020" CommentCount="0" />
  <row Id="2115" PostTypeId="1" CreationDate="2016-02-29T16:13:22.317" Score="7" ViewCount="88" Body="&lt;p&gt;So I was tasked to create a model of the Cornell Box. I've managed to do everything up until casting shadows, in which case some shadows are cast when there should not be any. Here are pictures of what it looks like now and a larger picture of what it should look like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dSgxY.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dSgxY.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found that for some reason, the box walls are also casting shadows. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A quick run-down of how it works: we have a pinhole camera which casts rays out and finds the nearest collision point, which is everything you're seeing. It saves these points as well as the distance from some start pt (camera in this case) to the collision point in a pre-defined data structure called an Intersection. It then uses this data to calculate the distance away from the light to create the intensities in lighting. The same method that is used to find the nearest collision point is used to find the nearest object for shadows as well using the original collision point as the start and the light source as the direction of the ray vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've narrowed down the problem to raytracing; when checking to see if there are any objects to cast shadows from from say the upper-right region of the back gray wall, the cyan ceiling is picked up at a point from the same side of the light. E.g. say the light source is in the middle and the original point is on our right side of the light. The method somehow picks up a point that is on the same side of the light when the vector should be going straight through the light source and end somewhere on our left side.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my source code for the functions in question:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool ClosestIntersection( vec3 start, vec3 dir, const vector&amp;lt;Triangle&amp;gt;&amp;amp; triangles, Intersection&amp;amp; it ) {&#xA;&#xA;    vec3 least;&#xA;    vec3 e1, e2, b, v0;&#xA;    mat3 A;&#xA;&#xA;    least[0] = m;&#xA;&#xA;    int index = triangles.size()-1;&#xA;&#xA;    for(int i = 0; i &amp;lt; int(triangles.size()); i++) {&#xA;&#xA;        v0 = triangles[i].v0;&#xA;        e1 = triangles[i].v1 - v0;&#xA;        e2 = triangles[i].v2 - v0;&#xA;        b = start - v0;&#xA;&#xA;        A = mat3( -dir, e1, e2);&#xA;&#xA;        if(!getInverse(A,b)) {continue;}&#xA;&#xA;        vec3 x = A * b;&#xA;&#xA;        if(x[0] &amp;lt;= least[0] &amp;amp;&amp;amp; x[1] + x[2] &amp;lt;= 1.f &amp;amp;&amp;amp; x[1] &amp;gt;= 0.f &amp;amp;&amp;amp; x[2] &amp;gt;= 0.f &amp;amp;&amp;amp; x[0] &amp;gt;= 0.00001f) {&#xA;            least = x;&#xA;            index = i;&#xA;        }&#xA;    }&#xA;&#xA;    if (least[0] == m) {&#xA;        return false;&#xA;    }&#xA;    else {&#xA;        it.position = least[0] * dir + start;&#xA;        const vec3 t = it.position-start;&#xA;        it.distance = sqrt(t[0]*t[0] + t[1]*t[1] + t[2]*t[2]);&#xA;        it.triangleIndex = index;&#xA;        return true;&#xA;    }&#xA;}&#xA;&#xA;vec3 DirectLight( const Intersection&amp;amp; i ){&#xA;    const Triangle T = triangles[i.triangleIndex];&#xA;    const vec3 r = lightPos - i.position;&#xA;    const float dist = sqrt(r[0]*r[0] + r[1]*r[1] + r[2]*r[2]);&#xA;&#xA;    Intersection t;&#xA;    t.distance = dist;&#xA;    t.position = i.position;&#xA;    t.triangleIndex = i.triangleIndex; &#xA;&#xA;    //ClosestIntersection( i.position, lightPos, triangles, t);&#xA;&#xA;    /*&#xA;    if(!ClosestIntersection( lightPos, i.position, triangles, t)) {return vec3(1,0,0);}&#xA;    else { return vec3(0,0,0); } */&#xA;&#xA;    if(ClosestIntersection( i.position, lightPos, triangles, t) &amp;amp;&amp;amp; (dist-t.distance &amp;gt; 1.f)) {&#xA;&#xA;        return vec3(0,0,0);&#xA;    } else {&#xA;        const vec3 B = 14.f * T.color * (float(max(dot(r,T.normal),float(0)) / float(4*3.14 * dist* dist)));&#xA;&#xA;        return B; &#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have spent over twelve hours trying to find this problem. Even my lecturer can't figure out the problem. Any help would be much appreciated!&lt;/p&gt;&#xA;" OwnerUserId="2771" LastActivityDate="2016-03-01T15:43:38.867" Title="Raytracing Problem - Casting Shadows" Tags="&lt;raytracing&gt;&lt;shadow&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2117" PostTypeId="1" AcceptedAnswerId="2118" CreationDate="2016-02-29T19:19:41.820" Score="10" ViewCount="81" Body="&lt;p&gt;I've been stuck on how to approach this for a while, so any suggestions would be gratefully appreciated!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to map a texture in the form of a lower right euclidean triangle to a hyperbolic triangle on the Poincare Disk.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the texture (the top left triangle of the texture is transparent and unused). You might recognise this as part of Escher's Circle Limit I&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry, see the comment as I am not allowed to post more than two links it seems! &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And this is what my polygon looks like (it's centred at the origin, which means that two edges are straight lines, however in general all three edges will be circular arcs):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1QB1E.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1QB1E.png&quot; alt=&quot;Wireframe Polygon&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The centre of the polygon is the incentre of the euclidean triangle formed by its vertices and I'm UV mapping the texture using it's incentre, dividing it into the same number of faces as the polygon has and mapping each face onto the corresponding polygon face. However the the result looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/3Jw9I.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3Jw9I.png&quot; alt=&quot;Textured Polygon&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If anybody thinks this is solvable using UV mapping I'd be happy to provide some example code, however I'm beginning to think this might not be possible and I'll have to write my own mapping functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SOLVED with some refinement of @Nathan's answer below since the lines AB, AC, BC may actually be arcs not lines. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Method: pick the longest side, say BC, then subdivide this into an even number of parts. Subdivide the other two side into the same number of parts. Then the lines connecting these (DE in the answer below) must actually also be arcs, not straight lines. Subdivide these new arcs as required, add the new triangles as faces then UV map the lower right triangle of the texture to these new faces. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simple? Not at all. But it worked. &#xA;&lt;a href=&quot;http://i.stack.imgur.com/90UQF.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/90UQF.jpg&quot; alt=&quot;Fish 1&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/bJ08M.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bJ08M.jpg&quot; alt=&quot;Fish 2&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2775" LastEditorUserId="2775" LastEditDate="2016-03-04T17:15:19.837" LastActivityDate="2016-03-04T17:15:19.837" Title="Map a texture onto a hyperbolic triangle" Tags="&lt;texture&gt;&lt;uv-mapping&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2118" PostTypeId="2" ParentId="2117" CreationDate="2016-02-29T20:21:54.260" Score="9" Body="&lt;p&gt;My guess is that to get the texture to look right, you'll have to subdivide the interior of the triangle as well, and approximate the non-linear UV mapping within it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently, it looks like you're subdividing around the edges of the triangle, and forming a fan of smaller triangles between the edge and the incenter. This would be fine if you were just rendering the triangle in a solid color. But when applying a texture, you're getting discontinuities at the boundaries of those subdivisions, because the texture is just stretched linearly across each subdivided triangle and not correctly approximating the hyperbolic mapping along the radial direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You'll need to subdivide along both axes, something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/jhKX2.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and position all the vertices appropriately in screen space and UV space to approximate the hyperbolic coordinates in the interior of the triangle. If you subdivide finely enough, this should produce the illusion of a continuously curving texture mapping.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-02-29T20:21:54.260" CommentCount="2" />
  <row Id="2119" PostTypeId="2" ParentId="2102" CreationDate="2016-02-29T20:29:24.230" Score="4" Body="&lt;h1&gt;Quick answers to your individual questions&lt;/h1&gt;&#xA;&#xA;&lt;h3&gt;What do you do if your image is not the same size on each axis though?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The paper uses square images with side lengths that are a power of 2. This is for ease of explanation, but is not necessary for the algorithm to work. See section 3.1:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Without loss of generality, we can assume n is a power of 2.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;That is, this assumption is not required in order for the algorithm to work.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Is it possible to run JFA on each axis separately and still get decent results?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Running on each axis separately is likely to give more incorrect pixel results, &lt;em&gt;and&lt;/em&gt; take longer to run in most cases. In extreme cases where one of the image side lengths is less than 8 (the number of jump directions), it may be faster as the algorithm treats those 8 directions sequentially, but for any wider image, separating the axes loses the advantage of treating them in parallel.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;In my situation ideally I'm looking to support both single point seeds, as well as arbitrary shaped seeds&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The paper mentions arbitrary shaped seeds in section 6 under the subheading &quot;Generalized Voronoi Diagram&quot;:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;...our algorithms treat such generalized seeds as collections of point seeds and thus expect to inherit the good performance obtained for point seeds.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So provided it suits your purpose to model arbitrary shapes as collections of pixels, you don't need to make any adjustment to the algorithm. Simply feed in a texture that labels all pixels in an arbitrary shaped seed with the same seed number, but different locations.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Possibly even weighted seeds, where the distance to a seed is adjusted by a multiplier and/or an adder to give it more or less influence&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;For &quot;weighting on seeds such as multiplicative and additive&quot;, the paper only mentions the possibility in passing in section 8, as potential future work. However, this should be straightforward to implement provided your desired weighting can be included in the seed data that is passed from pixel to pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The current algorithm passes &lt;code&gt;&amp;lt;s, position(s)&amp;gt;&lt;/code&gt; to specify a seed and its position, and only one seed is stored per pixel at any one time. Extending this to store &lt;code&gt;&amp;lt;s, position(s), weight(s)&amp;gt;&lt;/code&gt; provides all the information required to weight the distance function and calculate whether the new seed being passed to a pixel is closer to it than the one it is currently storing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could even include two weights, one multiplicative and one additive, and just set the multiplicative one to 1 and the additive one to 0 when not required. Then your algorithm would include the possibility of being used for multiplicatively weighted seeds, additively weighted seeds, or even a combination of both at once or some of each. This would just need&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&amp;lt;s, position(s), multiplicative(s), additive(s)&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and the current algorithm would be equivalent to this new algorithm using &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&amp;lt;s, position(s), 1, 0&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;Detailed explanation of why&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;As in the paper, all uses of $\log()$ refer to the base 2 logarithm.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;The algorithm does not need to be adapted for different side lengths&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If the side lengths are not equal, and are not powers of 2, there is no need to adapt the algorithm. It already deals with pixels on the edge of the image for which some of the jump directions lead outside the image. Since the algorithm already omits the seed information for directions that lead outside the image, a width or height that is not a power of 2 will not be a problem. For an image of width W and height H, the maximum jump size required will be&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\large2^{\lceil\log(\max(W,H))\rceil-1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the case of equal width and height N, this reduces to&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\large2^{\lceil\log(N)\rceil-1}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the case of a side length N that is a power of 2, this reduces to&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\large2^{\log(N)-1}=N/2$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;as used in the paper.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In more intuitive terms, round the maximum side length up to the next power of 2, and then halve that to get the maximum jump size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is always enough to cover every pixel in the image from every other pixel in the image, as the offset to any pixel will be in the range 0 to N-1 if the longest side length is N. Combinations of the powers of 2 from 0 to N/2 will cover every integer up to N-1 if N is a power of 2, and if N is not a power of 2 the range covered can only be larger than required, due to taking the ceiling of the logarithm (rounding up to the next power of 2).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Images with sides not a power of 2 will not be drastically less efficient&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The number of jumps depends on the longest side length, say L. If L is a power of 2 then the number of jumps is $\log(L)$. If L is not a power of 2 then the number of jumps is $\lceil\log(L)\rceil$. For a reasonably large image this will not be a large difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, a 1024 by 1024 image will require 10 jump iterations. A 512 by 512 image will require 9 jump iterations. Anything between the two sizes will also require 10 iterations. Even in the worst case of an image only just over a power of 2, like a 513 by 513 image, it will only require 1 additional iteration, which at this scale is approximately 11% more (10 instead of 9).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Non-square images are less efficient per area&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Since the number of iterations required is determined by the longest side length, the time taken for a 1024 by 1024 image will be the same as for a 1024 by 16 image. A square image allows a larger area to be covered in the same number of iterations.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Separating axes is likely to reduce quality&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Section 5 of the paper describes possible errors. Every pixel is reachable by a path from every other pixel, but some paths do not bring the correct nearest seed, due to that seed not being the nearest to a previous pixel in the path. A pixel that does not allow a seed to propagate past it is said to have &quot;killed&quot; that seed. If the nearest seed to a pixel is killed on all paths to that pixel, then the pixel will record some other seed and there will be an incorrect colour in the final image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Only one path needs to exist that does not kill the seed in order for the final result to be correct. Incorrect colours only occur when &lt;em&gt;all&lt;/em&gt; paths from the correct seed to a given pixel are blocked.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If a path involves alternating horizontal and vertical jumps, separating axes will make this path impossible (all horizontal jumps will be taken before all vertical jumps, making alternating impossible). Diagonal jumps will not be possible at all. So any path that alternates or contains diagonal jumps will be excluded. Every pixel will still have a path to every other pixel, but since there are now fewer paths there is more chance of a given pixel being blocked from receiving the correct seed, so the final result will be more error prone.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Separating axes is likely to make the algorithm take longer&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The efficiency would probably be reduced by separating axes, as the flooding would no longer be done in parallel, but would instead be repeated for each axis. For 2D this would likely take approximately twice as long, and for 3D approximately 3 times as long.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may be somewhat mitigated by the lack of diagonal jumps, but I would still expect an overall reduction in efficiency.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-02-29T22:40:15.907" LastActivityDate="2016-02-29T22:40:15.907" CommentCount="5" />
  <row Id="2120" PostTypeId="2" ParentId="2115" CreationDate="2016-03-01T04:28:38.383" Score="4" Body="&lt;p&gt;are you sure you are not re-intersecting the same surface when checking for light occluder ? this is a classical precision issue. There are many way to tackle it:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;First, have a good conditioning of the intersection point, by reintersecting a second time: consider your first hit distance d as approximate, create the point E' = Eye + (d-eps)*ray, recompute the intersection from here. (variant: for the first intersection, just use the bounding box).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Second, for grazing angle, depending on your shape representation, storage and computation precision and scales, tessellation, you might have dangerous imprecision here too, and must still ensure you won't reintersect the same surface. If flat or convex or tesselated, you can tag it an non-intersectable for the shadow-ray test. &#xA;A simpler trick (more approximate) is to displace the start of the shadow ray by epsilon in the normal direction. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2016-03-01T05:24:10.303" LastActivityDate="2016-03-01T05:24:10.303" CommentCount="0" />
  <row Id="2122" PostTypeId="1" CreationDate="2016-03-01T09:25:42.057" Score="7" ViewCount="224" Body="&lt;p&gt;I am quite confused over how GLM library is behaving or I am using it improperly.  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glm::vec2 testVec(6,-4);&#xA;&#xA;float len = testVec.length();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the value &lt;code&gt;2&lt;/code&gt; with the above code snippet. I believe I am trying to get the length of the vector defined by &lt;code&gt;testVec&lt;/code&gt;. You know very well that it is not the correct length of the vector. What am I missing here?&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="16" LastEditDate="2016-03-01T09:49:42.487" LastActivityDate="2016-03-01T12:20:14.087" Title="Get vector length with GLM" Tags="&lt;c++&gt;&lt;vectors&gt;&lt;glm&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2123" PostTypeId="2" ParentId="2122" CreationDate="2016-03-01T09:47:35.483" Score="6" Body="&lt;p&gt;Sorry folks for posting such a trivial issue! The issue is solved. I was using the wrong function. Here goes the correct one:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glm::vec2 testVec(6,-4);&#xA;float len  = glm::length(testVec);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The member function of the same name returns the number of components instead (i.e. &lt;code&gt;vec2::length&lt;/code&gt; will always yield &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;vec3::length&lt;/code&gt; will always yield &lt;code&gt;3&lt;/code&gt;, etc.).&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="16" LastEditDate="2016-03-01T12:20:14.087" LastActivityDate="2016-03-01T12:20:14.087" CommentCount="1" />
  <row Id="2125" PostTypeId="2" ParentId="2115" CreationDate="2016-03-01T15:43:38.867" Score="1" Body="&lt;p&gt;There is a trick that many in fact let you sidestep the issue altogether*. The scene and the set of things to be raytraced does not have to be the same. Most ray tracers allow for objects to not participate in shadow casting, reflections, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your case you could just take the outside walls of from shadow casting and therefore they would not self shadow allowing you to put lights outside the box with no problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway you could add a slight bias to the sampling and consider the objects slightly further than your calculation result gives, for shadow casting this avoids all kinds of problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;* Yes you have a problem you should fix that. But sometimes quick and done is all thats needed&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-03-01T15:43:38.867" CommentCount="0" />
  <row Id="2126" PostTypeId="1" CreationDate="2016-03-02T09:34:14.927" Score="4" ViewCount="52" Body="&lt;p&gt;I am trying to implement svg rendering by opengl using meshes. &#xA;In paths with opacity less than 1, the color of overlapping strokes add up. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/rarVy.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/rarVy.png&quot; alt=&quot;![enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But it should be like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/M9PbX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/M9PbX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the opengl calls are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    glDepthMask(false);&#xA;    glEnable(GL20.GL_BLEND);&#xA;    glBlendFunc(GL20.GL_SRC_ALPHA, GL20.GL_ONE_MINUS_SRC_ALPHA);&#xA;    ....&#xA;    glDrawArrays(....);&#xA;    ....&#xA;    glDepthMask(true);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;is there any opengl calls, shader or other method to have same transparent value around the whole stroke without modifying the geometry of the shape?&lt;/p&gt;&#xA;" OwnerUserId="2784" LastActivityDate="2016-03-02T09:51:44.850" Title="SVG opengl implementation - strokes overlap on transparency" Tags="&lt;opengl&gt;&lt;image-processing&gt;&lt;vector-graphics&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2127" PostTypeId="2" ParentId="2126" CreationDate="2016-03-02T09:51:44.850" Score="4" Body="&lt;p&gt;You can use the depth buffer to ensure that the second time you get on a point the pixels aren't written. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You start with the maximum depth value and each new path (that you want to add to the existing drawn image) you decrement the depth.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-03-02T09:51:44.850" CommentCount="0" />
  <row Id="2128" PostTypeId="1" CreationDate="2016-03-02T11:03:45.360" Score="2" ViewCount="49" Body="&lt;p&gt;I have a line segment defined by two coordinate values and I need to check if the line segment is parallel to the x-axis or y-axis. I believe that it is not that difficult to find. When it comes to test for parallelism against x-axis, we just need to check if the both the end points of the line segments has the equivalent y-value, whereas parallelism test against y-axis can be found if both the coordinate point ends have the same x-value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any other better way do it is most welcome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-03-03T11:41:25.397" Title="vertical or horizontal line test" Tags="&lt;pixel-graphics&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2130" PostTypeId="1" AcceptedAnswerId="2132" CreationDate="2016-03-02T15:28:59.727" Score="15" ViewCount="224" Body="&lt;p&gt;In ray tracing / path tracing, one of the simplest way to anti-alias the image is to supersample the pixel values and average the results. IE. instead of shooting every sample through the center of the pixel, you offset the samples by some amount.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In searching around the internet, I've found two somewhat different methods to do this:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Generate samples however you want and weigh the result with a filter&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;One example is &lt;a href=&quot;https://github.com/mmp/pbrt-v3/blob/master/src/core/film.h#L114&quot; rel=&quot;nofollow&quot;&gt;PBRT&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Generate the samples with a distribution equal to a the shape of a filter&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Two examples are &lt;a href=&quot;http://www.kevinbeason.com/smallpt/&quot; rel=&quot;nofollow&quot;&gt;smallpt&lt;/a&gt; and &lt;a href=&quot;http://computergraphics.stackexchange.com/users/79/benedikt-bitterli&quot;&gt;Benedikt Bitterli&lt;/a&gt;'s &lt;a href=&quot;https://github.com/tunabrain/tungsten/blob/master/src/core/cameras/ReconstructionFilter.hpp&quot; rel=&quot;nofollow&quot;&gt;Tungsten Renderer&lt;/a&gt; &lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Generate and Weigh&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic process is:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create samples however you want (randomly, stratified, low-discrepancy sequences, etc.) &lt;/li&gt;&#xA;&lt;li&gt;Offset the camera ray using two samples (x and y)&lt;/li&gt;&#xA;&lt;li&gt;Render the scene with the ray&lt;/li&gt;&#xA;&lt;li&gt;Calculate a weight using a filter function and the distance of the sample in reference to the pixel center. For example, Box Filter, Tent Filter, Gaussian Filter, etc.)&#xA;&lt;a href=&quot;http://i.stack.imgur.com/gz7Bg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gz7Bg.png&quot; alt=&quot;Filter Shapes&quot;&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Apply the weight to the color from the render&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Generate in the shape of a filter&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The basic premise is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_transform_sampling&quot; rel=&quot;nofollow&quot;&gt;Inverse Transform Sampling&lt;/a&gt; to create samples that are distributed according to the shape of a filter. For example a histogram of a samples distributed in the shape of a Gaussian would be:&lt;br/&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/7pGFS.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/7pGFS.png&quot; alt=&quot;Gaussian Histogram&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This can either be done exactly, or by binning the function into a discrete pdf/cdf. &lt;a href=&quot;http://www.kevinbeason.com/smallpt/&quot; rel=&quot;nofollow&quot;&gt;smallpt&lt;/a&gt; uses the exact inverse cdf of a tent filter. Examples of binning method can be found &lt;a href=&quot;https://gist.github.com/RichieSams/ce2ca2a201227c1de543&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the pros and cons of each method? And why would you use one over the other? I can think of a few things:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generate and Weigh seems to be the most robust, allowing any combination of any sampling method with any filter. However, it requires you to track the weights in the ImageBuffer and then do a final resolve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Generate in the Shape of a Filter can only support positive filter shapes (ie. no Mitchell, Catmull Rom, or Lanczos), since you can not have a negative pdf. But, as mentioned above, it's easier to implement, since you don't need to track any weights.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though, in the end, I guess you can think of method 2 as a simplification of method 1, since it's essentially using an implicit Box Filter weight.&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-04-11T22:29:32.500" LastActivityDate="2016-04-11T22:29:32.500" Title="Anti-aliasing / Filtering in Ray Tracing" Tags="&lt;raytracing&gt;&lt;antialiasing&gt;&lt;pathtracing&gt;&lt;filtering&gt;&lt;supersampling&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="4" />
  <row Id="2131" PostTypeId="1" CreationDate="2016-03-02T22:58:02.173" Score="6" ViewCount="72" Body="&lt;p&gt;I am trying to implement a 2D version of Foster and Fedkiw's paper, &quot;Practical Animation of Liquids&quot; here: &lt;a href=&quot;http://physbam.stanford.edu/~fedkiw/papers/stanford2001-02.pdf&quot; rel=&quot;nofollow&quot;&gt;http://physbam.stanford.edu/~fedkiw/papers/stanford2001-02.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mostly everything works, except for section 8: &quot;Conservation of Mass.&quot; There, we set up a matrix of equations to compute the pressures needed to make the liquid divergent free. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I believe my code matches the paper, however I am getting an unsolvable matrix during the conservation of mass step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are my steps for generating the matrix A:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set the diagonal entries $A_{i,i}$ to the negative of number of adjacent liquid cells to cell i.&lt;/li&gt;&#xA;&lt;li&gt;Set the entries $A_{i,j}$ and $A_{j,i}$ to 1 if both cells i and j have liquid.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that, in my implementation, cell $i$,$j$ in the liquid grid corresponds to row $i + $gridWidth$ * j$ in the matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The paper mentions, &quot;Static object and empty cells don’t disrupt this&#xA;structure. In that case pressure and velocity terms can disappear&#xA;from both sides&quot;, so I delete the columns and rows for cells that have no liquid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question is: Why is my matrix singular? Am I missing some kind of boundary condition in some other place in the paper? Is it the fact that my implementation is 2D?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example matrix from my implementation for a 2x2 grid where the cell at 0,0 has no liquid:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;-1   0   1&#xA;&#xA; 0  -1   1&#xA;&#xA; 1   1  -2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My research has led me to believe that I'm not properly handling the boundary conditions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First of all, at this point I can say that my matrix represents the discrete pressure Poisson equation. It is the discrete analog of applying the Laplacian operator coupling local pressure changes to cell divergence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I can understand, since we're dealing with pressure differences, boundary conditions are needed to &quot;anchor&quot; the pressures to an absolute reference value. Otherwise there may be an infinite number of solutions to the set of equations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://www.math.toronto.edu/mpugh/Teaching/Mat1062/notes2.pdf&quot; rel=&quot;nofollow&quot;&gt;these notes&lt;/a&gt;, 3 different ways are given to apply boundary conditions, to the best of my understanding:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Dirichlet - specifies absolute values at the boundaries.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Neummann - specifies the derivative at the boundaries.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Robin - specifies some kind of linear combination of the absolute value and the derivative at the boundaries.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Foster and Fedki's paper does not mention any of these, but I believe that they enforce Dirichlet boundary conditions, notable because of this statement at the end of 7.1.2, &quot;The pressure in a surface cell is set to atmospheric pressure.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've read the notes I linked a few times and still don't quite understand the math going on. How exactly do we enforce these boundary conditions? Looking at other implementations, there seems to be some kind of notion of a &quot;Ghost&quot; cells that lie at the boundary. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here I've linked to a few sources that may be helpful to others reading this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.math.toronto.edu/mpugh/Teaching/Mat1062/notes2.pdf&quot; rel=&quot;nofollow&quot;&gt;Notes on boundary conditions for Poisson Matrices&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://scicomp.stackexchange.com/questions/5355/writing-the-poisson-equation-finite-difference-matrix-with-neumann-boundary-cond&quot;&gt;Computational Science StackExchange post on Neumann boundary conditions&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://scicomp.stackexchange.com/questions/19485/2d-poisson-solver-for-taylor-green-vortex-problem&quot;&gt;Computational Science StackExchange post on Poisson Solver&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://physbam.stanford.edu/~mlentine/project.html#water&quot; rel=&quot;nofollow&quot;&gt;Water Physbam Implementation&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Here is the code I use to generate the matrix. Note that, instead of explicitly deleting columns and rows, I generate and use a map from liquid cell indices to the final matrix columns/rows.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for (int i = 0; i &amp;lt; cells.length; i++) {&#xA;  for (int j = 0; j &amp;lt; cells[i].length; j++) {&#xA;    FluidGridCell cell = cells[i][j];&#xA;&#xA;    if (!cell.hasLiquid)&#xA;      continue;&#xA;&#xA;    // get indices for the grid and matrix&#xA;    int gridIndex = i + cells.length * j;&#xA;    int matrixIndex = gridIndexToMatrixIndex.get((Integer)gridIndex);&#xA;&#xA;    // count the number of adjacent liquid cells&#xA;    int adjacentLiquidCellCount = 0;&#xA;    if (i != 0) {&#xA;      if (cells[i-1][j].hasLiquid)&#xA;        adjacentLiquidCellCount++;&#xA;    }&#xA;    if (i != cells.length-1) {&#xA;      if (cells[i+1][j].hasLiquid)&#xA;        adjacentLiquidCellCount++;&#xA;    }&#xA;    if (j != 0) {&#xA;      if (cells[i][j-1].hasLiquid)&#xA;      adjacentLiquidCellCount++;&#xA;    }&#xA;    if (j != cells[0].length-1) {&#xA;      if (cells[i][j+1].hasLiquid)&#xA;        adjacentLiquidCellCount++;&#xA;    }&#xA;&#xA;    // the diagonal entries are the negative count of liquid cells&#xA;    liquidMatrix.setEntry(matrixIndex, // column&#xA;                          matrixIndex, // row&#xA;                          -adjacentLiquidCellCount); // value&#xA;&#xA;    // set off-diagonal values of the pressure matrix&#xA;    if (cell.hasLiquid) {&#xA;      if (i != 0) {&#xA;        if (cells[i-1][j].hasLiquid) {&#xA;          int adjacentGridIndex = (i-1) + j * cells.length;&#xA;          int adjacentMatrixIndex = gridIndexToMatrixIndex.get((Integer)adjacentGridIndex);&#xA;          liquidMatrix.setEntry(matrixIndex, // column&#xA;                                adjacentMatrixIndex, // row&#xA;                                1.0); // value&#xA;          liquidMatrix.setEntry(adjacentMatrixIndex, // column&#xA;                                matrixIndex, // row&#xA;                                1.0); // value&#xA;        }&#xA;      }&#xA;      if (i != cells.length-1) {&#xA;        if (cells[i+1][j].hasLiquid) {&#xA;          int adjacentGridIndex = (i+1) + j * cells.length;&#xA;          int adjacentMatrixIndex = gridIndexToMatrixIndex.get((Integer)adjacentGridIndex);&#xA;          liquidMatrix.setEntry(matrixIndex, // column&#xA;                                adjacentMatrixIndex, // row&#xA;                                1.0); // value&#xA;          liquidMatrix.setEntry(adjacentMatrixIndex, // column&#xA;                                matrixIndex, // row&#xA;                                1.0); // value&#xA;        }&#xA;      }&#xA;      if (j != 0) {&#xA;        if (cells[i][j-1].hasLiquid) {&#xA;          int adjacentGridIndex = i + (j-1) * cells.length;&#xA;          int adjacentMatrixIndex = gridIndexToMatrixIndex.get((Integer)adjacentGridIndex);&#xA;          liquidMatrix.setEntry(matrixIndex, // column&#xA;                                adjacentMatrixIndex, // row&#xA;                                1.0); // value&#xA;          liquidMatrix.setEntry(adjacentMatrixIndex, // column&#xA;                                matrixIndex, // row&#xA;                                1.0); // value&#xA;        }&#xA;      }&#xA;      if (j != cells[0].length-1) {&#xA;        if (cells[i][j+1].hasLiquid) {&#xA;          int adjacentGridIndex = i + (j+1) * cells.length;&#xA;          int adjacentMatrixIndex = gridIndexToMatrixIndex.get((Integer)adjacentGridIndex);&#xA;          liquidMatrix.setEntry(matrixIndex, // column&#xA;                                adjacentMatrixIndex, // row&#xA;                                1.0); // value&#xA;          liquidMatrix.setEntry(adjacentMatrixIndex, // column&#xA;                                matrixIndex, // row&#xA;                                1.0); // value&#xA;        }&#xA;      }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2786" LastEditorUserId="2786" LastEditDate="2016-03-04T18:02:17.307" LastActivityDate="2016-03-04T18:02:17.307" Title="Conserving Mass in Liquid Simulation" Tags="&lt;simulation&gt;&lt;fluid-sim&gt;" AnswerCount="0" CommentCount="7" FavoriteCount="1" />
  <row Id="2132" PostTypeId="2" ParentId="2130" CreationDate="2016-03-03T04:59:18.810" Score="6" Body="&lt;p&gt;There is a great paper from 2006 on this topic, &lt;a href=&quot;http://lgdv.cs.fau.de/publications/publication/Pub.2006.tech.IMMD.IMMD9.filter/&quot;&gt;Filter Importance Sampling&lt;/a&gt;. They propose your method 2, study the properties, and come out generally in favor of it. They claim that this method gives smoother rendering results because it weights all samples that contribute to a pixel equally, thereby reducing variance in the final pixel values. This makes some sense, as it's a general maxim in Monte Carlo rendering that importance-sampling will give lower variance than weighted samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Method 2 also has the advantage of being slightly easier to parallelize because each pixel's computations are independent of all other pixels, while in method 1, sample results are shared across neighboring pixels (and therefore have to be synchronized/communicated somehow when pixels are parallelized across multiple processors). For the same reason, it's easier to do adaptive sampling (more samples in high-variance areas of the image) with method 2 than method 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the paper, they also experimented with a Mitchell filter, sampling from abs() of the filter and then weighting each sample with either +1 or &amp;minus;1, like @trichoplax suggested. But this ended up actually increasing the variance and being worse than method 1, so they conclude that method 2 is only usable for positive filters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, the results from this paper may not be universally applicable, and it may be somewhat scene-dependent which sampling method is better. I wrote &lt;a href=&quot;http://www.reedbeta.com/blog/2014/11/15/antialiasing-to-splat-or-not/&quot;&gt;a blog post investigating this question&lt;/a&gt; independently in 2014, using a synthetic &quot;image function&quot; rather than full rendering, and found method 1 to give more visually pleasing results due to smoothing high-contrast edges more nicely. Benedikt Bitterli also commented on that post reporting a similar issue with his renderer (excess high-frequency noise around light sources when using method 2). Beyond that, I found the main difference between the methods was the frequency of the resulting noise: method 2 gives higher-frequency, &quot;pixel-sized&quot; noise, while method 1 gives noise &quot;grains&quot; that are 2-3 pixels across, but the amplitude of noise was similar for both, so which kind of noise looks less bad is probably a matter of personal preference.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-03T04:59:18.810" CommentCount="3" />
  <row Id="2133" PostTypeId="1" AcceptedAnswerId="2142" CreationDate="2016-03-03T09:54:09.387" Score="7" ViewCount="185" Body="&lt;p&gt;I'm aware that most modern GPUs, although designed for floating point, are more or less equivalent in integer performance these days, with a few caveats like the lack of a fused multiply add. I'm not sure how this applies to shift operations though. I'm doing Marching Cubes on GPU, initially writing out a 32-bit packed position for each surface cube then unpacking these in a later pass to the actual vertices in that cube, like this : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ivec3 unpackedPos = ivec3( packedPos &amp;gt;&amp;gt; 20 &amp;amp; 0x3FF,&#xA;                         packedPos &amp;gt;&amp;gt; 10 &amp;amp; 0x3FF,&#xA;                         packedPos &amp;amp; 0x3FF);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It just occurred to me to wonder if shader units have barrel shifters in them these days? Am I doing 2 shifts here or 30?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT&gt;&gt;&#xA;I'm an idiot... Thanks for the answers guys, useful to know, but I've been going about this all wrong. I should just be using the RGB10_A2UI texture format then packing/ unpacking with a single image load/store instruction instead of messing around with bitshifts myself. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;RE_EDIT&gt;&gt; Or not... This method apparently works on red boxes but not on green ones, so it's back to bit-shifts.&lt;/p&gt;&#xA;" OwnerUserId="1937" LastEditorUserId="1937" LastEditDate="2016-03-11T15:10:33.287" LastActivityDate="2016-03-11T15:10:33.287" Title="Do modern GPUs contain barrel shifters?" Tags="&lt;gpu&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2134" PostTypeId="1" CreationDate="2016-03-03T11:11:21.640" Score="2" ViewCount="52" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/I60wq.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/I60wq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use template matching to detect a specific pattern in image.The shift determined is very shaky. Currently I apply it to R,G,B channel separately and average the result to obtain float values.Please, suggest how to obtain subpixel accuracy. I was planning to resize the image and then return the data in original scale, please suggest any other better method &lt;/p&gt;&#xA;" OwnerUserId="2788" LastActivityDate="2016-03-03T11:11:21.640" Title="Template matching subpixel accuracy" Tags="&lt;algorithm&gt;&lt;image-processing&gt;" AnswerCount="0" CommentCount="3" FavoriteCount="0" />
  <row Id="2135" PostTypeId="1" CreationDate="2016-03-03T11:18:33.500" Score="1" ViewCount="84" Body="&lt;p&gt;I want to visualize a scene with simple shapes (e.g. boxes, pyramides, etc.) and I am looking for a tool or framework in C/C++ to do this. Focus of the application is to parameterize the relative coordinates (position+rotation) of the scene objects. Also defined camera view support would be much welcomed. What I actually would want is the VRML World Editor from Simulink. However I need something thats free for company in-house use. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could anybody give suggest me something suitable? &lt;/p&gt;&#xA;" OwnerUserId="2789" LastActivityDate="2016-03-05T17:52:07.293" Title="Free 3D Scene Simulation Framework" Tags="&lt;3d&gt;&lt;c++&gt;&lt;virtual-reality&gt;&lt;scene-description&gt;" AnswerCount="0" CommentCount="5" ClosedDate="2016-03-12T11:39:19.993" />
  <row Id="2136" PostTypeId="2" ParentId="2128" CreationDate="2016-03-03T11:41:25.397" Score="4" Body="&lt;p&gt;It is not generally useful to try to optimise small parts of a program before you have profiled the program to see where the most benefit can be gained by optimisation. Make sure everything is correct and working first, and then profile to decide where to optimise (if optimisation turns out to be necessary).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically, your approach is already correct for all lines. It will correctly identify all lines parallel to either axis, and it will not incorrectly identify any lines that are not parallel to an axis. However, for use in a practical setting, it's worth asking yourself about near-parallel cases and any odd exceptions. Consider all this before even thinking about optimisation, otherwise you may have to repeat all the optimisation later if something you have to change for correctness breaks your initial optimisation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One case to consider is when both the coordinates are identical. That is, the end points have the same x coordinate &lt;em&gt;and&lt;/em&gt; the same y coordinate. This is a  degenerate case that doesn't define a line (it could have any direction). Check your implementation with this case and make sure it gives the output that you want it to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Depending on your purpose, you may want to detect only exactly parallel lines, or you may want to detect lines that are sufficiently near to parallel. In some cases lines near to parallel may satisfy your purpose, and in other cases it may be essential to accept lines near to parallel, if for example there is the possibility of small inaccuracies in the coordinates.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2016-03-03T11:41:25.397" CommentCount="0" />
  <row Id="2137" PostTypeId="1" CreationDate="2016-03-03T11:43:44.337" Score="4" ViewCount="58" Body="&lt;p&gt;I'm wondering what is the best way to achieve an effect like here: &lt;a href=&quot;https://matterport.com/try/&quot; rel=&quot;nofollow&quot;&gt;https://matterport.com/try/&lt;/a&gt; when the camera is traveling between points in space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apart from simple color crossfade there seems to be some sort of perspective reprojection giving the illusion of movement.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea how to do this? All data I have is environment cube maps with color + linear depth at each pixel (so I can reconstruct view space position).&lt;/p&gt;&#xA;" OwnerUserId="304" LastActivityDate="2016-03-03T18:41:14.743" Title="Seamlessly transitioning between nearby environment maps" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;image-processing&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2138" PostTypeId="2" ParentId="2137" CreationDate="2016-03-03T18:41:14.743" Score="4" Body="&lt;p&gt;It looks to me like they have a coarse polygon model of the room, and texture the model by projecting the cubemaps onto it. The model seems to include the rough shapes of the walls and furniture, but not fine details such as table legs, door handles or items on shelves. When the camera moves, the geometry gives you more-or-less-correct parallax, and the crossfade between cubemaps updates the textures.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also see the model clearly when the walkthrough first loads. Note that the camera starts outside of the house, letting you see the whole thing (much as you'd see a game level if you noclip the camera outside it), then it moves the camera into the living room. You can also get back to this view by selecting &quot;Dollhouse&quot; in their toolbar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Their model may have been constructed by an artist, but in your case if you have depth information in the cubemaps, it's probably possible to programmatically extract some geometry from them and stitch the cubemaps together to form a coherent model. I don't know much about that subject, though.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-03T18:41:14.743" CommentCount="0" />
  <row Id="2139" PostTypeId="1" AcceptedAnswerId="2143" CreationDate="2016-03-04T08:49:24.877" Score="6" ViewCount="50" Body="&lt;p&gt;I'm using an affine transformation matrix to transform 2D coordinates from screen (magnitude 10e3) to small parts of fractal sets (magnitude as little as 10e-15).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also need to map the other way round, so I simply invert the matrix. It works when the magnitudes are not too far apart, but it fails when the determinant gets too close to zero to be represented with standard double numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The matrices I use are of the form:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a c e&#xA;b d f&#xA;0 0 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the inversion algorithm I have is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var dt = a * d - b * c;&#xA;return new Matrix(d/dt, -b/dt, -c/dt, a/dt, (c * f - d * e) / dt, -(a * f - b * e) / dt);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there an alternate way of inverting the matrix? I'm not very good at mathematics, so I would need a simple solution, and one I could find the code, or an algorithm, for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An example of such a matrix:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; 3.26091378894248e-9   -1.882689453850103e-9   -0.7172216437740687&#xA;-1.882689453814925e-9  -3.2609137888815494e-9  -0.23371832131832268&#xA; 0                      0                       1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2792" LastEditorUserId="231" LastEditDate="2016-03-05T08:29:31.863" LastActivityDate="2016-03-05T08:29:31.863" Title="How to invert an affine matrix with small values?" Tags="&lt;transformations&gt;&lt;maths&gt;&lt;2d&gt;&lt;affine-transformations&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="2140" PostTypeId="1" CreationDate="2016-03-04T11:53:48.777" Score="2" ViewCount="60" Body="&lt;p&gt;I believe that the issue may already have been discussed here. I want to find if two line segments does intersect and if they do then find and store and intesection points. Now it is already confirmed that one of the line segments will always be parallel to x-axis or y-axis. &#xA;So in that case how should we reformulate the basic algorithm that checks the line segments intersection?&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="16" LastEditDate="2016-03-04T12:39:24.380" LastActivityDate="2016-03-04T21:23:22.713" Title="intersection between line segments - narrowed precondition" Tags="&lt;geometry&gt;&lt;maths&gt;&lt;vectors&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="2141" PostTypeId="2" ParentId="2140" CreationDate="2016-03-04T16:28:17.557" Score="1" Body="&lt;p&gt;The arbitrary line can be expressed as &lt;code&gt;y = a*x+b&lt;/code&gt; (assuming it's not parallel to the y axis). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the other line is parallel to the y axis then you can simply fill in the x coordinate of any point on the line into the formula of the first line. Then you can check whether they intersect by ensuring the result lies between the endpoints of the second line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the second line parallel to the x axis you just flip the x and y coordinates.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-03-04T16:28:17.557" CommentCount="0" />
  <row Id="2142" PostTypeId="2" ParentId="2133" CreationDate="2016-03-04T20:43:42.453" Score="7" Body="&lt;p&gt;Yes ( with 50% of the FMA 32b throughput on nVIDIA Maxwell).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &#xA;&lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions&quot;&gt;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2016-03-04T21:03:29.910" LastActivityDate="2016-03-04T21:03:29.910" CommentCount="0" />
  <row Id="2143" PostTypeId="2" ParentId="2139" CreationDate="2016-03-04T21:10:52.300" Score="7" Body="&lt;p&gt;I found a solution to my specific problem. Instead of computing the determinant and hitting the precision wall, I use the Gauss-Jordan method step by step.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my specific case of affine transformation matrices and the range of values I use, I don't hit any precision problem this way.&lt;/p&gt;&#xA;" OwnerUserId="2792" LastActivityDate="2016-03-04T21:10:52.300" CommentCount="0" />
  <row Id="2144" PostTypeId="2" ParentId="2140" CreationDate="2016-03-04T21:23:22.713" Score="1" Body="&lt;p&gt;the arbitrary line can be represented as $P = (x,y) = P_0 + \lambda. \vec{dir}$ (works in n dimensions, no special case).&#xA;If your other line is $x=x_1$ simply inject this in to solve for $\lambda$ and get $y$: $y=y_0+(x_1-x_0).\frac{{dir}_y}{{dir}_x}$.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-03-04T21:23:22.713" CommentCount="2" />
  <row Id="2146" PostTypeId="1" CreationDate="2016-03-06T11:51:59.727" Score="1" ViewCount="143" Body="&lt;p&gt;Problem Description:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Reference:&lt;br&gt;&#xA;&lt;a href=&quot;http://www.terathon.com/lengyel/Lengyel-Oblique.pdf&quot; rel=&quot;nofollow&quot;&gt;Oblique View Frustum Depth Projection and Clipping&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&quot;Mathematics for 3D Game Programming and Computer Graphics&quot; 3rd edition, chapter 5.6.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;M is the original projection matrix. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Plane C is the modified near plane, and C'=transpose(inverse(M))*C is the projected plane by M.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Why in NDC the corner Q' of the view frustum lying opposite the plane C' is given by Q'=(Cx, Cy, 1, 1)?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2190" LastEditorUserId="2190" LastEditDate="2016-03-07T02:58:54.323" LastActivityDate="2016-03-07T02:58:54.323" Title="A question about derivation in oblique view frustum" Tags="&lt;transformations&gt;&lt;projections&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2147" PostTypeId="1" AcceptedAnswerId="2148" CreationDate="2016-03-07T07:52:07.727" Score="7" ViewCount="230" Body="&lt;p&gt;So let's assume that i have some convex smooth and unclosed surface.&#xA;I'm moving each point of it in a normal direction in this point by some constant factor (This factor is same for all points on surface).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I replace this operation by Uniform or Non-uniform Scaling + Translate? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Will resulting surfaces be mathematically identical in these cases?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example i want to transform this surface (side view).&#xA;&lt;a href=&quot;http://i.stack.imgur.com/JuAlC.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JuAlC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;?&lt;/p&gt;&#xA;" OwnerUserId="2644" LastEditorUserId="2644" LastEditDate="2016-03-07T08:39:03.620" LastActivityDate="2016-03-07T10:48:23.870" Title="Moving each point of a surface in direction of corresponding normal" Tags="&lt;transformations&gt;&lt;maths&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2148" PostTypeId="2" ParentId="2147" CreationDate="2016-03-07T08:23:15.437" Score="7" Body="&lt;p&gt;No this cannot be modelled by (non-uniform) scaling. It's fairly easy to construct a counterexample:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/CWujH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CWujH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The issue is that the amount a section of the curve/surface grows depends on its curvature, not its orientation in space. Notice here that the circular arc grows uniformly in all directions (by a factor of $3/2$) whereas the length of the horizontal segments remains unchanged at a length of $2$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, if your surface is not only convex but also has constant curvature, then it's just a circular arc, and for circles your transformation is equivalent to uniform scaling. You can probably also construct curves of varying curvature where your transformation happens to correspond to non-uniform scaling, but for general convex surfaces, that's not the case.&lt;/p&gt;&#xA;" OwnerUserId="16" LastEditorUserId="16" LastEditDate="2016-03-07T10:48:23.870" LastActivityDate="2016-03-07T10:48:23.870" CommentCount="0" />
  <row Id="2149" PostTypeId="1" CreationDate="2016-03-07T11:09:43.583" Score="1" ViewCount="37" Body="&lt;p&gt;There is an web-interface that load geometric models and show it inside the bounding volume. WebGL is used to generate it. So far no scene graph technique is used to organize the elements in the scene. Is there any javascript api that does support the scene graph technique ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there is, I am also required to know more if there is any mechanism to transfer the scene graph related information from the javascript to C++ based API. As of right now the scene that is developed without scenegraph technique stores that scene as an .stl file and C++ based API read the .stl file and process further. I believe that it is not the optimal way to deal with it. It would have much faster and optimal if we have the scene graph system support in javascript to organize the scene and the direct scene related data transfer from javascript to C++ for further processing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let me know if I have not explained the issue well enough. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-03-09T16:14:24.720" Title="Get data from javascript" Tags="&lt;javascript&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2150" PostTypeId="1" CreationDate="2016-03-07T12:04:51.657" Score="2" ViewCount="32" Body="&lt;p&gt;I have bunch of surfaces, like in these two images:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/0Bu6o.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0Bu6o.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EY43g.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EY43g.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently to make single surface with common parametrization I sample points uniformly from all source surfaces, and fit single NURBS surface from these points. And in most cases it works, but in some rare cases (When surfaces are more curved) this single surface fails to maintain acceptable accuracy at source regions. I tried to increase number of samples, and number of control points for NURBS, but it doesn't help. And btw, solution for images above will be simple planes, because source surfaces are trimmed planes too, but in general case source surfaces will be curvy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additional info:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Source surfaces are trimmed NURBS surfaces, but I can convert them to any other discrete format and work with it.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I don't care about free space between surfaces, values of a resulting surface in this space can be linear interpolation between nearby source surfaces, or even some constant value.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Source surfaces cannot overlap or coincide.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Resulting surface must contain all source regions inside (With some arbitrary tolerance), with common parametrization, also it must not have self-intersections.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Is there any other possible solution to this problem? Maybe some algorithm to order such surfaces in parameter space, or at least name of this problem to google it easily?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#xA;" OwnerUserId="2644" LastActivityDate="2016-03-07T12:04:51.657" Title="Making surface with common parametrization from bunch of other surfaces" Tags="&lt;algorithm&gt;&lt;maths&gt;&lt;nurbs&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="2151" PostTypeId="2" ParentId="306" CreationDate="2016-03-07T12:10:32.883" Score="39" Body="&lt;p&gt;&lt;strong&gt;EDIT: Please see my other answer with a concrete solution.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have actually solved this exact problem over a year ago for my master's thesis. In the Valve paper, they show that you can AND two distance fields to achieve this, which works as long as you only have one convex corner. For concave corners, you also need the OR operation. &lt;a href=&quot;https://lambdacube3d.wordpress.com/2014/11/12/playing-around-with-font-rendering/&quot;&gt;This guy&lt;/a&gt; actually developed some obscure system to switch between the two operations using four texture channels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, there is a much simpler operation that can facilitate both AND and OR depending on the situation, and this is the principal idea of my thesis: &lt;strong&gt;the median of three&lt;/strong&gt;. So basically, you use exactly three channels (ideal for RGB), which are completely interchangeable, and combine them using the median operation (choose the middle value out of the three).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To accomodate anti-aliasing, we don't work with just booleans, but floating point values, and the AND operation becomes the minimum, and the OR becomes the maximum of two values. The median of three can indeed do both: if &lt;em&gt;a&lt;/em&gt; &amp;lt; &lt;em&gt;b&lt;/em&gt;, for (&lt;em&gt;a&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;), the median is the minimum, and for (&lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt;), it is the maximum.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The rendering process is still extremely simple. The entire fragment shader including anti-aliasing can look something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;int main() {&#xA;    // Bilinear sampling of the distance field&#xA;    vec3 s = texture2D(sdf, p).rgb;&#xA;    // Acquire the signed distance&#xA;    float d = median(s.r, s.g, s.b) - 0.5;&#xA;    // Weight between inside and outside (anti-aliasing)&#xA;    float w = clamp(d/fwidth(d) + 0.5, 0.0, 1.0);&#xA;    // Combining the background and foreground color&#xA;    gl_FragColor = mix(outsideColor, insideColor, w);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the only difference from the original method is computing the median right after sampling the texture. You will have to implement the median function though, &lt;a href=&quot;http://stackoverflow.com/a/19045659/1189488&quot;&gt;which can be done with just 4 min/max operations&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now of course, the question is, &lt;strong&gt;how do I build such a three-channel distance field?&lt;/strong&gt; And this is the tricky part. The most obvious approach that I took in the beginning was to perform a decomposition of the input shape/glyph into three components, and then generate a conventional distance field out of each. The rules for this decomposition aren't that complicated. Firstly , the area with at least 2 out of 3 channels on is the inside. Then, if you imagine this as the RGB color channels, convex corners must be made of a secondary color, and its two primary components continue outward. Concave corners are the inverse: Two secondary colors enclose their common primary color, and the wedge between where both edges continue inward is white. I also found that some padding is necessary where two primary or two secondary colors would otherwise touch to avoid artifacts (for example, in the middle stroke of the &quot;N&quot; in the picture).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following image is an example decomposition generated by the program from my thesis:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/2fVah.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2fVah.png&quot; alt=&quot;Multi-channel decomposition of glyphs&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This approach however has some drawbacks. One of them is that the special effects, such as outlines and shadows will no longer work correctly. Fortunatelly, I also came up with a second, much more elegant method, which generates the distance fields directly, and even supports all of the graphical effects. It is also included in my thesis and so is also over a year old. I am not going to give any more details right now, because I am currently writing a paper that describes this second technique in detail, but I will post it here as soon as it's finished.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, here is an example of the difference in quality. The texture resolution is the same in each image, but the left one uses a regular texture, the middle one uses an ordinary distance field, and the right one uses my three-channel distance field. The performance overhead is only the difference between sampling an RGB texture versus a monochrome one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/aG1NR.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/aG1NR.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2811" LastEditorUserId="2811" LastEditDate="2016-04-25T07:59:45.870" LastActivityDate="2016-04-25T07:59:45.870" CommentCount="9" />
  <row Id="2152" PostTypeId="2" ParentId="2086" CreationDate="2016-03-08T13:10:25.380" Score="1" Body="&lt;p&gt;&lt;strike&gt;The &quot;Uniform Mapping&quot; here is incorrect.  It does not transform to a uniform distribution on the sphere.&lt;/strike&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Very very bad me.  I misread the equation AND I didn't even consult my own reference [1], if I had I would have seen my mistake.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Still the method from [2] is interesting and is described below.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Additional note:&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;To map a uniform point set from the unit square to the sphere or half sphere requires an area-preserving map.  That will maintain the uniform property.  For pseudo-random that's sufficient (points are independent), however low-discrepancy point set will have its discrepancy measure effected by distortions in area introduced by the map.  I'm not aware of any references with respect to this point.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;The method from [2] starts by mapping unit square to the unit disk.  One way we can do this is by using a rejection method.  Pseudo-code looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;d = 0;&#xA;&#xA;do {&#xA;  p = uniform2d(...);  // [0,1) x [0,1)   given to be uniform&#xA;  p = 2p;              // [0,2) x [0,2)   scaling (area-preserving)&#xA;  p = p-1;             // (-1,1) x (-1,1) translating (area-preserving)&#xA;  d = dot(p,p);        // is p in disk?&#xA;} while(d &amp;gt;= 1);       //   if not rinse-and-repeat&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;the area of the transformed square is $4$ and the unit disk is $\pi$, so the acceptance rate is $\frac{\pi }{4}$ (~.785) so on average requires $\frac{4}{\pi}$ (~1.273) iterations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also non-rejection based methods exist, for example [3] which requires one square root and one trig operation. Note that the rejection method does not distort areas so the discrepancy measure remains the same.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Once we have $ p = \left(u_0, u_1 \right) $ uniformly distributed in the unit disk, we have:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ u_0, u_1 \in \left(-1,1\right) $$&#xA;$$ d = p\cdot p = u_0^2+u_1^2,\; d &amp;lt; 1 $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;then the transform to the sphere is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ s=2\sqrt{1-d} $$&#xA;$$ p = \left\{s\thinspace u_0,\thinspace s\thinspace u_1, 1-2d\right\} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;or if you wanted a half-sphere, say positive Z, (just to state the obvious):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$p = \left\{s\thinspace u_0,\thinspace s\thinspace u_1, d\right\} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a Stackexchange: Mathematics thread that discusses this method ([4],[5]).&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Also if you want to procedurally generate a LDS the additive recurrence formulation of the Weyl sequence is very cheap.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;References:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://mathworld.wolfram.com/SpherePointPicking.html&quot; rel=&quot;nofollow&quot;&gt;Wolfram MathWorld: Sphere Point Picking&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&quot;Choosing a Point from the Surface of a Sphere.&quot;, George Marsaglia, 1972. (&lt;a href=&quot;http://projecteuclid.org/euclid.aoms/1177692644;&quot; rel=&quot;nofollow&quot;&gt;PDF&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://mathworld.wolfram.com/DiskPointPicking.html&quot; rel=&quot;nofollow&quot;&gt;Wolfram MathWorld: Disk Point Picking&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/838326/3-random-numbers-to-describe-point-on-a-sphere/838336#838336&quot;&gt;SE Mathematics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://math.stackexchange.com/questions/838326/3-random-numbers-to-describe-point-on-a-sphere/838882#838882&quot;&gt;SE Mathematics&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2831" LastEditorUserId="2831" LastEditDate="2016-03-10T09:11:45.747" LastActivityDate="2016-03-10T09:11:45.747" CommentCount="3" />
  <row Id="2153" PostTypeId="1" AcceptedAnswerId="2154" CreationDate="2016-03-08T15:42:51.960" Score="8" ViewCount="125" Body="&lt;p&gt;I am wondering what's the best way to integrate a dynamic loop function in a shader?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, it seems that dynamic arrays are not possible. So, is it better to create a maximum size array and only fill a part of it or define arrays with predefined sizes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, what's the best way to iterate over this array?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it better to use an unrolled loop or a dynamic loop for something between 4 to 128 iterations?&#xA;I've also seen that it's possible to unroll it to a maximum predefined number of iterations then stop it with a condition such as &lt;code&gt;if (i == myCurrentMaximumIterationNumber)&lt;/code&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="127" LastEditDate="2016-03-08T21:40:08.187" LastActivityDate="2016-03-10T11:49:50.090" Title="Loop performance in a shader" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;shader&gt;&lt;glsl&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2154" PostTypeId="2" ParentId="2153" CreationDate="2016-03-08T17:43:45.363" Score="5" Body="&lt;p&gt;Shader compilers are extremely aggressive about unrolling since early HW often didn't have flow control and the cost on more recent HW can vary. If you have a benchmark you are actively testing against and a range of relevant hardware, then try things to see what happens. Your dynamic loop is more amenable to developer intervention than a static loop - but leaving it to the compiler is still good advice unless you have a benchmark available. With a benchmark, exploration is worthwhile (and fun).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW, the biggest loss with a dynamic loop on a GPU is that individual &quot;threads&quot; in a wavefront/warp will finish at different times. The threads that stop later force all the ones that finish early to execute NOPs. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Nested loops should be carefully thought through: I implemented a block based entropy decoder that encoded runs of zeros (for JPEG like compression). The natural implementation was to decode the runs in a tight inner loop - which meant often only one thread was making progress; by flattening the loop and explicitly testing in each thread if it was currently decoding a run or not, I kept all threads active through the fixed length loop (the decoded blocks were all the same size). If the threads were like CPU threads, the change would have been terrible, but on the GPU I was running on, I got a 6 fold increase in performance (which was still terrible - there weren't enough blocks to keep the GPU busy - but it was a proof of concept).&lt;/p&gt;&#xA;" OwnerUserId="2500" LastEditorUserId="2500" LastEditDate="2016-03-10T11:49:50.090" LastActivityDate="2016-03-10T11:49:50.090" CommentCount="0" />
  <row Id="2155" PostTypeId="1" AcceptedAnswerId="2156" CreationDate="2016-03-08T20:27:51.557" Score="5" ViewCount="62" Body="&lt;p&gt;I've never written shaders before, and now I'm trying to implement SSAO with SceneKit on OS X and iOS. I'm trying different SSAO shaders that I find, e.g. from Three.js, &lt;a href=&quot;http://theorangeduck.com/page/pure-depth-ssao&quot;&gt;this one&lt;/a&gt;, and &lt;a href=&quot;https://www.shadertoy.com/view/Ms33WB&quot;&gt;this one&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On OS X they all work as they should, but on iPhone &lt;em&gt;all&lt;/em&gt; of them giving me the same artefacts (images show SSAO pass only). The lines move when I move the camera. So it must be because of some differences of OpenGL ES. Anyone knows what's wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/C1NhL.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/C1NhL.png&quot; alt=&quot;img1&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://i.stack.imgur.com/3FQYE.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3FQYE.png&quot; alt=&quot;img2&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="2840" LastActivityDate="2016-03-09T21:19:12.457" Title="SSAO artefacts on iPhone (OpenGL ES)" Tags="&lt;shader&gt;&lt;opengl-es&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2156" PostTypeId="2" ParentId="2155" CreationDate="2016-03-08T23:20:07.050" Score="7" Body="&lt;p&gt;Resolved by adding &lt;code&gt;precision highp sampler2D&lt;/code&gt; to my shaders.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;About default precision settings on ES - &lt;a href=&quot;https://www.khronos.org/files/opengles_shading_language.pdf&quot;&gt;https://www.khronos.org/files/opengles_shading_language.pdf&lt;/a&gt; page 36.&lt;/p&gt;&#xA;" OwnerUserId="2840" LastEditorUserId="2840" LastEditDate="2016-03-09T21:19:12.457" LastActivityDate="2016-03-09T21:19:12.457" CommentCount="4" />
  <row Id="2158" PostTypeId="1" CreationDate="2016-03-09T14:17:54.727" Score="2" ViewCount="42" Body="&lt;p&gt;Ok guys, tricky one!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to modify &lt;a href=&quot;http://wiki.unity3d.com/index.php?title=MirrorReflection2&quot; rel=&quot;nofollow&quot;&gt;this&lt;/a&gt; Unity script + shader to include normal mapping. The tl;dr is, it uses a single texture rather than a cubemap as it is only for reflecting in a planar surface. So it doesn't use the standard ray bounce onto an axis aligned cube. Instead it :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;creates a reflection matrix from the plane (pos and normal)&lt;/li&gt;&#xA;&lt;li&gt;reflects the camera position and view matrix through the plane&lt;/li&gt;&#xA;&lt;li&gt;renders to texture from the reflected camera (with oblique near clipping plane set to cull everything behind the mirror)&lt;/li&gt;&#xA;&lt;li&gt;creates a projection matrix and sends it as uniform to the shader (basically just a standard MVP matrix but with scale cancelled, and biased from [-1, 1] to [0, 1]&lt;/li&gt;&#xA;&lt;li&gt;multiplies the object space coordinate by the projection matrix to get a texcoord and then looks up the mirror texture with this&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So there are no normals actually used in the shader at all at present. I know how to construct a TBN matrix and use this to warp the per-pixel normals, but I have no idea how this translates into a texture offset in this case. I have a feeling I might need screen-space normals like in the question &lt;a href=&quot;http://stackoverflow.com/questions/14317350/transform-world-space-normals-to-screen-space-normals&quot;&gt;here&lt;/a&gt; but pretty stumped beyond that. Any ideas?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT -- Here's the script&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;using UnityEngine;&#xA;using System.Collections;&#xA;&#xA;[ExecuteInEditMode()]&#xA;public class PlanarRealtimeReflection : MonoBehaviour&#xA;{&#xA;    public bool m_DisablePixelLights = true;&#xA;    public int m_TextureResolution = 1024;&#xA;    public float m_clipPlaneOffset = 0.07f;&#xA;    private float m_finalClipPlaneOffset = 0.0f;&#xA;    public bool m_NormalsFromMesh = false;&#xA;    public bool m_BaseClipOffsetFromMesh = false;&#xA;    public bool m_BaseClipOffsetFromMeshInverted = false;&#xA;    private Vector3 m_calculatedNormal = Vector3.zero;&#xA;    private Vector3 m_calculatedTangent = Vector4.zero;&#xA;    public LayerMask m_ReflectLayers = -1;&#xA;&#xA;    private Hashtable m_ReflectionCameras = new Hashtable(); //Camera -&amp;gt; Camera table&#xA;&#xA;    private RenderTexture m_ReflectionTexture = null;&#xA;    private int m_OldReflectionTextureSize = 0;&#xA;&#xA;    private static bool s_InsideRendering = false;&#xA;&#xA;    //This is called when it's known that the object will be rendered by some&#xA;    //camera. We render reflections and do other updates here.&#xA;    //Because the script executes in edit mode, reflections for the scene view&#xA;    //camera will just work!&#xA;    public void OnWillRenderObject()&#xA;    {&#xA;        if(!enabled || !renderer || !renderer.sharedMaterial || !renderer.enabled)&#xA;            return;&#xA;&#xA;        Camera cam = Camera.current;&#xA;        if(!cam)&#xA;            return;&#xA;&#xA;        if(m_NormalsFromMesh &amp;amp;&amp;amp; GetComponent&amp;lt;MeshFilter&amp;gt;() != null)&#xA;            m_calculatedNormal = transform.TransformDirection(GetComponent&amp;lt;MeshFilter&amp;gt;().sharedMesh.normals[0]);&#xA;            m_calculatedTangent = transform.TransformDirection(GetComponent&amp;lt;MeshFilter&amp;gt;().sharedMesh.tangents[0]);&#xA;&#xA;        if(m_BaseClipOffsetFromMesh &amp;amp;&amp;amp; GetComponent&amp;lt;MeshFilter&amp;gt;() != null)&#xA;            m_finalClipPlaneOffset = (transform.position - transform.TransformPoint(GetComponent&amp;lt;MeshFilter&amp;gt;().sharedMesh.vertices[0])).magnitude + m_clipPlaneOffset;&#xA;        else if(m_BaseClipOffsetFromMeshInverted &amp;amp;&amp;amp; GetComponent&amp;lt;MeshFilter&amp;gt;() != null)&#xA;            m_finalClipPlaneOffset = -(transform.position - transform.TransformPoint(GetComponent&amp;lt;MeshFilter&amp;gt;().sharedMesh.vertices[0])).magnitude + m_clipPlaneOffset;&#xA;        else&#xA;            m_finalClipPlaneOffset = m_clipPlaneOffset;&#xA;&#xA;        //Safeguard from recursive reflections.        &#xA;        if(s_InsideRendering)&#xA;            return;&#xA;        s_InsideRendering = true;&#xA;&#xA;        Camera reflectionCamera;&#xA;        CreateSurfaceObjects(cam, out reflectionCamera);&#xA;&#xA;        //Find out the reflection plane: position and normal in world space&#xA;        Vector3 pos = transform.position;&#xA;        Vector3 normal = m_NormalsFromMesh &amp;amp;&amp;amp; GetComponent&amp;lt;MeshFilter&amp;gt;() != null ? m_calculatedNormal : transform.up;&#xA;        Vector3 tangent = m_NormalsFromMesh &amp;amp;&amp;amp; GetComponent&amp;lt;MeshFilter&amp;gt;() != null ? m_calculatedTangent : transform.right;&#xA;        Vector3 bitangent = Vector3.Cross(normal, tangent);&#xA;&#xA;        //Optionally disable pixel lights for reflection&#xA;        int oldPixelLightCount = QualitySettings.pixelLightCount;&#xA;        if(m_DisablePixelLights)&#xA;            QualitySettings.pixelLightCount = 0;&#xA;&#xA;        UpdateCameraModes(cam, reflectionCamera);&#xA;&#xA;        //Render reflection&#xA;        //Reflect camera around reflection plane&#xA;        float d = -Vector3.Dot (normal, pos) - m_finalClipPlaneOffset;&#xA;        Vector4 reflectionPlane = new Vector4 (normal.x, normal.y, normal.z, d);&#xA;        Matrix4x4 reflection = Matrix4x4.zero;&#xA;        CalculateReflectionMatrix (ref reflection, reflectionPlane);&#xA;        Vector3 oldpos = cam.transform.position;&#xA;        Vector3 newpos = reflection.MultiplyPoint(oldpos);&#xA;        reflectionCamera.worldToCameraMatrix = cam.worldToCameraMatrix * reflection;&#xA;&#xA;        //Setup oblique projection matrix so that near plane is our reflection plane.&#xA;        //This way we clip everything below/above it for free.&#xA;        Vector4 clipPlane = CameraSpacePlane(reflectionCamera, pos, normal, 1.0f);&#xA;        Matrix4x4 projection = cam.projectionMatrix;&#xA;        CalculateObliqueMatrix (ref projection, clipPlane);&#xA;        reflectionCamera.projectionMatrix = projection;&#xA;&#xA;        reflectionCamera.cullingMask = ~(1&amp;lt;&amp;lt;4) &amp;amp; m_ReflectLayers.value; //never render water layer&#xA;        reflectionCamera.targetTexture = m_ReflectionTexture;&#xA;        GL.SetRevertBackfacing (true);&#xA;        reflectionCamera.transform.position = newpos;&#xA;        //Vector3 euler = cam.transform.eulerAngles;&#xA;        //reflectionCamera.transform.eulerAngles = euler; // new Vector3(0, euler.y, euler.z);&#xA;        reflectionCamera.Render();&#xA;        reflectionCamera.transform.position = oldpos;&#xA;        GL.SetRevertBackfacing (false);&#xA;        Material[] materials = renderer.sharedMaterials;&#xA;        foreach(Material mat in materials)&#xA;        {&#xA;            if (mat.HasProperty(&quot;_ReflectionTex&quot;))  &#xA;                mat.SetTexture(&quot;_ReflectionTex&quot;, m_ReflectionTexture);&#xA;            mat.SetVector(&quot;_Normal&quot;, new Vector4(normal.x,normal.y,normal.z,0));&#xA;            mat.SetVector(&quot;_Tangent&quot;, new Vector4(tangent.x, tangent.y, tangent.z));&#xA;            mat.SetVector(&quot;_Bitangent&quot;, new Vector4(bitangent.x, bitangent.y, bitangent.z));&#xA;        }&#xA;&#xA;        //Set matrix on the shader that transforms UVs from object space into screen&#xA;        //space. We want to just project reflection texture on screen.&#xA;        Matrix4x4 scaleOffset = Matrix4x4.TRS(&#xA;            new Vector3(0.5f,0.5f,0.5f), Quaternion.identity, new Vector3(0.5f,0.5f,0.5f));&#xA;        Vector3 scale = transform.localScale;//.lossyScale;&#xA;        Matrix4x4 mtx = transform.localToWorldMatrix * Matrix4x4.Scale(new Vector3(1.0f/scale.x, 1.0f/scale.y, 1.0f/scale.z));&#xA;        mtx = scaleOffset *  cam.projectionMatrix * cam.worldToCameraMatrix * mtx;&#xA;        foreach(Material mat in materials)&#xA;            mat.SetMatrix(&quot;_ProjMatrix&quot;, mtx);&#xA;        //Restore pixel light count&#xA;        if(m_DisablePixelLights)&#xA;            QualitySettings.pixelLightCount = oldPixelLightCount;&#xA;&#xA;        s_InsideRendering = false;&#xA;    }&#xA;&#xA;&#xA;    //Cleanup all the objects we possibly have created&#xA;    void OnDisable()&#xA;    {&#xA;        if(m_ReflectionTexture)&#xA;        {&#xA;            DestroyImmediate(m_ReflectionTexture);&#xA;            m_ReflectionTexture = null;&#xA;        }&#xA;        foreach(DictionaryEntry kvp in m_ReflectionCameras)&#xA;            DestroyImmediate(((Camera)kvp.Value).gameObject);&#xA;        m_ReflectionCameras.Clear();&#xA;    }&#xA;&#xA;&#xA;    private void UpdateCameraModes(Camera src, Camera dest)&#xA;    {&#xA;        if(dest == null)&#xA;            return;&#xA;        //set camera to clear the same way as current camera&#xA;        dest.clearFlags = src.clearFlags;&#xA;        dest.backgroundColor = src.backgroundColor;        &#xA;        if(src.clearFlags == CameraClearFlags.Skybox)&#xA;        {&#xA;            Skybox sky = src.GetComponent(typeof(Skybox)) as Skybox;&#xA;            Skybox mysky = dest.GetComponent(typeof(Skybox)) as Skybox;&#xA;            if(!sky || !sky.material)&#xA;            {&#xA;                mysky.enabled = false;&#xA;            }&#xA;            else&#xA;            {&#xA;                mysky.enabled = true;&#xA;                mysky.material = sky.material;&#xA;            }&#xA;        }&#xA;        //update other values to match current camera.&#xA;        //even if we are supplying custom camera&amp;amp;projection matrices,&#xA;        //some of values are used elsewhere (e.g. skybox uses far plane)&#xA;        dest.farClipPlane = src.farClipPlane;&#xA;        dest.nearClipPlane = src.nearClipPlane;&#xA;        dest.orthographic = src.orthographic;&#xA;        dest.fieldOfView = src.fieldOfView;&#xA;        dest.aspect = src.aspect;&#xA;        dest.orthographicSize = src.orthographicSize;&#xA;    }&#xA;&#xA;    //On-demand create any objects we need&#xA;    private void CreateSurfaceObjects(Camera currentCamera, out Camera reflectionCamera)&#xA;    {&#xA;        reflectionCamera = null;&#xA;&#xA;        //Reflection render texture&#xA;        if(!m_ReflectionTexture || m_OldReflectionTextureSize != m_TextureResolution)&#xA;        {&#xA;            if(m_ReflectionTexture)&#xA;                DestroyImmediate(m_ReflectionTexture);&#xA;            m_ReflectionTexture = new RenderTexture(m_TextureResolution, m_TextureResolution, 16);&#xA;            m_ReflectionTexture.name = &quot;__SurfaceReflection&quot; + GetInstanceID();&#xA;            m_ReflectionTexture.isPowerOfTwo = true;&#xA;            m_ReflectionTexture.hideFlags = HideFlags.DontSave;&#xA;            m_OldReflectionTextureSize = m_TextureResolution;&#xA;        }&#xA;&#xA;        //Camera for reflection&#xA;        reflectionCamera = m_ReflectionCameras[currentCamera] as Camera;&#xA;        if(!reflectionCamera) //catch both not-in-dictionary and in-dictionary-but-deleted-GO&#xA;        {&#xA;            GameObject go = new GameObject(&quot;Surface Refl Camera id&quot; + GetInstanceID() + &quot; for &quot; + currentCamera.GetInstanceID(), typeof(Camera), typeof(Skybox));&#xA;            reflectionCamera = go.camera;&#xA;            reflectionCamera.enabled = false;&#xA;            reflectionCamera.transform.position = transform.position;&#xA;            reflectionCamera.transform.rotation = transform.rotation;&#xA;            reflectionCamera.gameObject.AddComponent(&quot;FlareLayer&quot;);&#xA;            go.hideFlags = HideFlags.HideAndDontSave;&#xA;            m_ReflectionCameras[currentCamera] = reflectionCamera;&#xA;        }        &#xA;    }&#xA;&#xA;    //Extended sign: returns -1, 0 or 1 based on sign of a&#xA;    private static float sgn(float a)&#xA;    {&#xA;        if (a &amp;gt; 0.0f) return 1.0f;&#xA;        if (a &amp;lt; 0.0f) return -1.0f;&#xA;        return 0.0f;&#xA;    }&#xA;&#xA;    //Given position/normal of the plane, calculates plane in camera space.&#xA;    private Vector4 CameraSpacePlane (Camera cam, Vector3 pos, Vector3 normal, float sideSign)&#xA;    {&#xA;        Vector3 offsetPos = pos + normal * m_finalClipPlaneOffset;&#xA;        Matrix4x4 m = cam.worldToCameraMatrix;&#xA;        Vector3 cpos = m.MultiplyPoint(offsetPos);&#xA;        Vector3 cnormal = m.MultiplyVector(normal).normalized * sideSign;&#xA;        return new Vector4(cnormal.x, cnormal.y, cnormal.z, -Vector3.Dot(cpos,cnormal));&#xA;    }&#xA;&#xA;    //Adjusts the given projection matrix so that near plane is the given clipPlane&#xA;    //clipPlane is given in camera space. See article in Game Programming Gems 5 and&#xA;    //http://aras-p.info/texts/obliqueortho.html&#xA;    private static void CalculateObliqueMatrix (ref Matrix4x4 projection, Vector4 clipPlane)&#xA;    {&#xA;        Vector4 q = projection.inverse * new Vector4(&#xA;            sgn(clipPlane.x),&#xA;            sgn(clipPlane.y),&#xA;            1.0f,&#xA;            1.0f&#xA;            );&#xA;        Vector4 c = clipPlane * (2.0F / (Vector4.Dot (clipPlane, q)));&#xA;        //third row = clip plane - fourth row&#xA;        projection[2] = c.x - projection[3];&#xA;        projection[6] = c.y - projection[7];&#xA;        projection[10] = c.z - projection[11];&#xA;        projection[14] = c.w - projection[15];&#xA;    }&#xA;&#xA;    //Calculates reflection matrix around the given plane&#xA;    private static void CalculateReflectionMatrix (ref Matrix4x4 reflectionMat, Vector4 plane)&#xA;    {&#xA;        reflectionMat.m00 = (1F - 2F*plane[0]*plane[0]);&#xA;        reflectionMat.m01 = (  - 2F*plane[0]*plane[1]);&#xA;        reflectionMat.m02 = (  - 2F*plane[0]*plane[2]);&#xA;        reflectionMat.m03 = (  - 2F*plane[3]*plane[0]);&#xA;&#xA;        reflectionMat.m10 = (  - 2F*plane[1]*plane[0]);&#xA;        reflectionMat.m11 = (1F - 2F*plane[1]*plane[1]);&#xA;        reflectionMat.m12 = (  - 2F*plane[1]*plane[2]);&#xA;        reflectionMat.m13 = (  - 2F*plane[3]*plane[1]);&#xA;&#xA;        reflectionMat.m20 = (  - 2F*plane[2]*plane[0]);&#xA;        reflectionMat.m21 = (  - 2F*plane[2]*plane[1]);&#xA;        reflectionMat.m22 = (1F - 2F*plane[2]*plane[2]);&#xA;        reflectionMat.m23 = (  - 2F*plane[3]*plane[2]);&#xA;&#xA;        reflectionMat.m30 = 0F;&#xA;        reflectionMat.m31 = 0F;&#xA;        reflectionMat.m32 = 0F;&#xA;        reflectionMat.m33 = 1F;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the shader so far&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Shader &quot;Realtime Reflections/New Shader&quot;&#xA;{&#xA;    Properties {        &#xA;        _MainAlpha(&quot;MainAlpha&quot;, Range(0, 1)) = 1&#xA;        _MapAmount(&quot;Distortion Amount&quot;, Range(0, 0.05)) = 0&#xA;        _MainTex (&quot;Base (RGB) Gloss (A)&quot;, 2D) = &quot;white&quot; {}&#xA;        _ReflectionTex (&quot;Reflection&quot;, 2D) = &quot;white&quot; {}&#xA;        _NormalMap (&quot;Normal Map&quot;, 2D) = &quot;white&quot; {}&#xA;        _RefColor(&quot;Color&quot;,Color) = (1,1,1,1)&#xA;    }&#xA;    SubShader {&#xA;        Tags {&#xA;            &quot;RenderType&quot;=&quot;Opaque&quot;}&#xA;        LOD 100&#xA;        Pass {&#xA;            CGPROGRAM&#xA;            #pragma vertex vert&#xA;            #pragma fragment frag&#xA;            #include &quot;UnityCG.cginc&quot;&#xA;&#xA;            uniform float4x4    _ProjMatrix;&#xA;            uniform float       _RefType;&#xA;            uniform float4      _Normal;&#xA;            uniform float4      _Tangent;&#xA;            uniform float4      _Bitangent;&#xA;            sampler2D           _ReflectionTex;&#xA;            sampler2D           _MainTex;&#xA;            sampler2D           _NormalMap;&#xA;            float4              _MainTex_ST;&#xA;            float4              _NormalMap_ST;       &#xA;            float4              _RefColor;&#xA;            float               _MainAlpha;&#xA;            float               _MapAmount;&#xA;&#xA;            struct outvertex {&#xA;                float4 pos :        SV_POSITION;&#xA;                float2 colorUV :    TEXCOORD0;&#xA;                float2 bumpUV :     TEXCOORD1;&#xA;                float4 varyingPos : TEXCOORD2;&#xA;            };&#xA;&#xA;            outvertex vert(appdata_full v) {&#xA;&#xA;                outvertex o;&#xA;                o.pos = mul (UNITY_MATRIX_MVP,v.vertex);&#xA;                o.colorUV = TRANSFORM_TEX(v.texcoord.xy,_MainTex);&#xA;                o.bumpUV = TRANSFORM_TEX(v.texcoord1.xy,_NormalMap);&#xA;                o.varyingPos = v.vertex;    &#xA;                o. color = fixed4(mul(float3x3(UNITY_MATRIX_IT_MV),v.normal),1);&#xA;                return o;&#xA;            }&#xA;&#xA;            float4 frag(outvertex i) : COLOR {          &#xA;                float4 posProj = mul(_ProjMatrix, i.varyingPos);      &#xA;                float2 normPosProj = posProj.xy / posProj.w;&#xA;                float3 bump = (float3(tex2D(_NormalMap, i.bumpUV));&#xA;                //normPosProj += ?????????&#xA;                half4 flecol = tex2D(_ReflectionTex, normPosProj);  &#xA;                half4 maincol =  tex2D(_MainTex, i.colorUV);                                    &#xA;                half4 outcolor = half4(1,1,1,1);                &#xA;                outcolor = maincol*_MainAlpha+flecol*(1-_MainAlpha);&#xA;                return outcolor*_RefColor;&#xA;            }&#xA;            ENDCG&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I had the idea to get the normal in screen space (so z is always toward camera) then simply find the delta between the regular normal and the warped one in x,y and use this as the texture offset, but I have no idea if this is feasible or how to do it.&lt;/p&gt;&#xA;" OwnerUserId="1937" LastEditorUserId="16" LastEditDate="2016-03-09T14:42:48.583" LastActivityDate="2016-03-09T14:42:48.583" Title="Normal mapping for planar reflections" Tags="&lt;reflection&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2159" PostTypeId="2" ParentId="2149" CreationDate="2016-03-09T16:14:24.720" Score="3" Body="&lt;p&gt;There does not seem to be a single answer for this question, but I will try to give some directions in possible solutions. First of all I presume that &quot;the scene graph technique&quot; means storing the object data in some kind of data structure (I know what a scenegraph is, but there are many different possibilities for indoor/outdoor scenes etc). There seem to be some Javascript libraries, but they do much more then simply providing the datastructure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For defining the scenegraph structure you could look at glTF(&lt;a href=&quot;https://github.com/KhronosGroup/glTF&quot; rel=&quot;nofollow&quot;&gt;https://github.com/KhronosGroup/glTF&lt;/a&gt;) defined by Khronos themselves. This really depends on what kind of features your scene graph requires.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the C++ library is required for the processing you can look into providing Javascript bindings for it. You can either try to compile the library to javascript using emscripten (&lt;a href=&quot;https://github.com/kripken/emscripten&quot; rel=&quot;nofollow&quot;&gt;https://github.com/kripken/emscripten&lt;/a&gt;). Otherwise provide a C-interface and call it directly using ccall(). This way you can directly share this datastructure via parameters.&lt;/p&gt;&#xA;" OwnerUserId="64" LastActivityDate="2016-03-09T16:14:24.720" CommentCount="2" />
  <row Id="2160" PostTypeId="1" AcceptedAnswerId="2162" CreationDate="2016-03-09T18:34:38.510" Score="1" ViewCount="58" Body="&lt;p&gt;Does anyone out there know of any software for developing real time visualisations and analysis of real time sound, say using a mike? I've found a few but none allow the user to build up their own visualisations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any ideas, Jonathan&lt;/p&gt;&#xA;" OwnerUserId="2848" LastActivityDate="2016-03-22T13:34:43.987" Title="live music visualisation" Tags="&lt;real-time&gt;" AnswerCount="2" CommentCount="1" ClosedDate="2016-03-22T17:20:14.490" />
  <row Id="2161" PostTypeId="1" AcceptedAnswerId="2278" CreationDate="2016-03-09T20:58:55.873" Score="4" ViewCount="81" Body="&lt;p&gt;Down the road, I'm planning to implement one or another GPU based ambient occlusion technique. My goal will be to approximate a physically based offline renderer, so I was pretty happy when Wikipedia led me to &lt;a href=&quot;http://people.mpi-inf.mpg.de/~ritschel/Papers/SSDO.pdf&quot; rel=&quot;nofollow&quot;&gt;Approximating Dynamic Global Illumination in Image Space&lt;/a&gt;, which describes SSDO. Based on the references, it looks to be from the 2008 era. Are there more modern approaches with similar accuracy goals?&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-04-07T02:45:58.850" Title="Modern Screen Space Ambient Occlusion Techniques" Tags="&lt;gpu&gt;&lt;global-illumination&gt;&lt;ssao&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2162" PostTypeId="2" ParentId="2160" CreationDate="2016-03-09T22:28:11.673" Score="1" Body="&lt;p&gt;One tool I've seen people use for this sort of thing is &lt;a href=&quot;https://processing.org/&quot; rel=&quot;nofollow&quot;&gt;Processing&lt;/a&gt;, a Java-based language for real-time artsy stuff that supports graphics and audio. I haven't ever worked with it myself, but apparently it's pretty easy to use it for programming visualizations. &lt;a href=&quot;https://www.youtube.com/watch?v=a-Ht7d-tECY&quot; rel=&quot;nofollow&quot;&gt;Here's an example&lt;/a&gt; of a simple visualization someone made; look in the video description for a link to their code&amp;mdash;it's quite short. Their example loads an mp3 file, but Processing also supports real-time input from a mic.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-09T22:28:11.673" CommentCount="1" />
  <row Id="2163" PostTypeId="1" CreationDate="2016-03-10T03:43:14.837" Score="0" ViewCount="73" Body="&lt;p&gt;I've been searching in the internet for quite a while and i couldn't find any explanation or at least a how to create a vector graphic application (specifically in windows) where i can create a 3D vector shape , say a box. I know there is OpenGL and DirectX for that and the new Vulkan API but i don't wan't to use those API and library's. basically what i am asking is where can i find examples or at least tutorials for creating custom made API graphics library for the C++ language. If there is not a way in C++ language then any other way to do it. By example masm or nasm Assembly language.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If my question is too broad please specifie any additional information you need to be able to answer my question .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;thanks&lt;/p&gt;&#xA;" OwnerUserId="2853" LastEditorUserId="2853" LastEditDate="2016-03-10T03:52:12.440" LastActivityDate="2016-03-12T20:11:20.327" Title="creating vector shapes using only c++" Tags="&lt;c++&gt;&lt;vectors&gt;" AnswerCount="1" CommentCount="10" ClosedDate="2016-03-14T15:53:57.133" />
  <row Id="2164" PostTypeId="1" AcceptedAnswerId="2165" CreationDate="2016-03-10T12:44:09.337" Score="2" ViewCount="55" Body="&lt;p&gt;I'm new to graphics and want to implement algorithms like dda,scan-line, clipping,etc. I'm confused which library to use. OpenGl, graphics in turbo c,or any other. Please suggest.&lt;/p&gt;&#xA;" OwnerUserId="2855" LastActivityDate="2016-03-10T13:05:29.447" Title="library / tool for implementing algorithms" Tags="&lt;opengl&gt;&lt;pixel-graphics&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2165" PostTypeId="2" ParentId="2164" CreationDate="2016-03-10T13:05:29.447" Score="5" Body="&lt;p&gt;If you're completely new to graphics programming I would start with Processing. It's based on Java and JOGL but comes with a whole bunch of drawing commands built in, and its own basic IDE. It's very good for prototyping algorithms and getting quick visual feedback without having to write a ton of boilerplate code, and it insulates you from the underlying API (OpenGL in this case). Presumably you're not going for high performance, since the algorithms you mention are efficiently implemented in hardware, and you just want to familiarize yourself with how they work? If so it's a great place to start.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you've got the hang of Processing you can move over to OpenFrameworks, which is very similar but in C++, with the option to 'drill down' and call the underlying OpenGL commands directly. From there, the sky's the limit. :-)&lt;/p&gt;&#xA;" OwnerUserId="1937" LastActivityDate="2016-03-10T13:05:29.447" CommentCount="0" />
  <row Id="2166" PostTypeId="1" AcceptedAnswerId="2296" CreationDate="2016-03-10T14:09:49.317" Score="3" ViewCount="129" Body="&lt;p&gt;I know that if you turn of vsync, it synchronizes rendering with the vertical redraw cycle to prevent tearing, and that doing so caps your rendering rate (FPS) at the monitor refresh rate, which is commonly 60hz / 60 fps, although other rates exist as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when you are not running at a full 60fps how does vsync affect your frame rate?  I've heard people say that you will be limited to a multiple of 60fps (well, ~16ms to be precise), but from observation, fps can fluctuate wildly.&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-04-10T02:55:44.603" Title="How does vsync affect fps exactly when not at full vsync fps?" Tags="&lt;real-time&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="2167" PostTypeId="1" CreationDate="2016-03-10T14:17:37.640" Score="7" ViewCount="41" Body="&lt;p&gt;A signed distance field texture is a technique where you store the distance from a pixel to the surface of a shape within the color information for that pixel, allowing almost vector graphics quality rendering using textures (&lt;a href=&quot;http://blog.demofox.org/2014/06/30/distance-field-textures/&quot;&gt;http://blog.demofox.org/2014/06/30/distance-field-textures/&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When reading the texture data in a shader, you get values between 0 and 1, which is meant to map between -1 and 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The distance data is essentially &quot;normalized&quot; when creating the shader, which means that instead of storing a true positive or negative distance value in the texture, you instead multiply (divide) the real distance by some constant value, clamp it to be between 0 and 1 and then store that distance value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That constant value essentially controls the width in pixels of the &quot;gradient&quot; band, where the surface of the shape goes from 0 to 1 (-1 to 1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an optimal value for this based on the output texture resolution or anything else?  I always use trial and error until it looks right but feel there must be a better way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, is there a way to choose an optimal output texture size?  Or should you just always use a texture size smaller than the size you intend to render it at?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-03-10T14:17:37.640" Title="Choosing width of data for signed distance field texture" Tags="&lt;texture&gt;&lt;2d&gt;&lt;signed-distance-field&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2168" PostTypeId="2" ParentId="2166" CreationDate="2016-03-10T14:17:41.987" Score="7" Body="&lt;p&gt;It depends on how a missed frame is handled by the driver.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One option is to just wait until the next vsync, causing a hitch of 32 ms and if the application is just at the limit of 16 ms can cause fluctuations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The next option is to queue the frame for display next frame but don't wait on it. This will still cause a visual hitch but the application can then immediately start on the next frame instead of being forced to wait 16 ms. If the next frame is done faster then the late frame may even never be shown. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The final option is to not block and push the late frame to the display immediately which can cause tearing.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-03-10T14:17:41.987" CommentCount="2" />
  <row Id="2170" PostTypeId="1" CreationDate="2016-03-11T17:56:09.000" Score="6" ViewCount="84" Body="&lt;p&gt;I have a convoluted polyline composed of vertices and straight segments and a separate point that is independent of the polyline. I am trying draw a straight new line from the point at a given angle and determine if the new line intersects the polyline and get the location of that intersection. If there are multiple intersections I want the location of the closest intersection. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently doing this by drawing a line an arbitrary (long) distance from the point at the desired angle and then intersecting it with the polyline using a python gis toolkit (shapely). If there are multiple intersections I sort them by distance and select the closest one. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This method works but is somewhat clumsy. Is there a more efficient method of calculating the closest intersection? It almost seems like ray tracing (of which I have minimal knowledge) but I can't find much online that applies to my case. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other method that I contemplated was to iterate over every line segment in the polyline and use y = mx+b to see if the segment intersects the new line from the point. The polyline is potentially quite long with many segments, but I've also seen some sorting tricks online to make this method more efficient. Is there a method that I'm missing?&lt;/p&gt;&#xA;" OwnerUserId="2863" LastEditorUserId="2863" LastEditDate="2016-03-11T18:30:49.033" LastActivityDate="2016-03-13T08:09:05.523" Title="Calculating intersection of polyline and line" Tags="&lt;geometry&gt;&lt;2d&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="2172" PostTypeId="1" CreationDate="2016-03-11T21:16:34.320" Score="1" ViewCount="69" Body="&lt;p&gt;Can an algorithm be derived to classify digital images according to how they were generated, i.e. the classes would be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Real-world photograph captured with an actual camera.&lt;/li&gt;&#xA;&lt;li&gt;A Computer-Generated realistic scene.&lt;/li&gt;&#xA;&lt;li&gt;A hand-drawn image or a painting.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Could perhaps a Machine Learning approach, geared towards analyzing statistics of the signal provide sufficient insight on the problem ?&#xA;In that case, is there any known implementation of such algorithm publicly available ?&lt;/p&gt;&#xA;" OwnerUserId="2865" LastEditorUserId="2828" LastEditDate="2016-03-14T05:19:00.283" LastActivityDate="2016-03-14T05:19:00.283" Title="Separating photos from other images algorithmically?" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="4" ClosedDate="2016-03-12T11:28:35.130" />
  <row Id="2173" PostTypeId="2" ParentId="376" CreationDate="2016-03-12T00:10:18.600" Score="4" Body="&lt;p&gt;The motivation behind ambient occlusion (AO) in general is to approximate the way crevices and corners are often shadowed, because less indirect light bounces into them. An example from a photo of my office&amp;mdash;note the darkening along the edges where walls and ceiling meet. The room is lit only by the light coming in through the window and bouncing around.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1JZDi.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1JZDi.jpg&quot; alt=&quot;photo demonstrating AO-like effect in the corner of a room&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To accurately simulate this phenomenon, offline renderers use techniques like path tracing and photon mapping. For real-time purposes, we either precalculate it offline, or we approximate it somehow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Screen-space ambient occlusion (SSAO) is based on the observation that you can detect corners and crevices by looking at the depth buffer (and possibly also the normal vectors) of a rendered image, and so you can calculate approximate AO as a post-pass. The depth buffer is a coarse representation of the geometry in the scene, so by sampling depth buffer values in the neighborhood of a target pixel, you can get an idea of the shape of the surrounding geometry, and make a guess how darkened by AO it should be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qFOoX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qFOoX.png&quot; alt=&quot;diagram of how depth buffer represents geometry, from Bavoil and Sainz (2008)&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This diagram, from &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot; rel=&quot;nofollow&quot;&gt;Bavoil and Sainz (2008)&lt;/a&gt;, shows how depth buffer values, interpreted as a heightfield of sorts, represent a discretized version of some geometry. In calculating SSAO for the center pixel, you'd look at the depth values of the surrounding pixels and plug them into some formula, designed to produce a darker value when the geometry is more concave (like that in the diagram), and a lighter value when the geometry is flat or convex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The formula that the depth values go into is called the &quot;kernel&quot; by analogy with &lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_%28image_processing%29&quot; rel=&quot;nofollow&quot;&gt;filter kernels&lt;/a&gt; used for blurs, edge detection and suchlike. However, SSAO is more complicated than just a linear convolution of the depth values. The devil is in the details. The distribution of samples, and the formula processing them to generate the occlusion value, has been the subject of much research over the last decade, trying to improve the realism and reduce artifacts while maintaining good performance.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-12T00:10:18.600" CommentCount="0" />
  <row Id="2174" PostTypeId="2" ParentId="2172" CreationDate="2016-03-12T00:13:16.267" Score="0" Body="&lt;p&gt;Difficulty depends on what you really want to classify, i.e. what exactly your data base of images is made ok.&#xA;For simple hand-drawn or computer art, I would think of just looking the color histogram (but for complex art with gradients; but still these gradients are likely to induce a very smooth colormap).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the other extreme, distinguishing realistic computer graphics image from reality is probably impossible.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-03-12T00:13:16.267" CommentCount="1" />
  <row Id="2175" PostTypeId="2" ParentId="1963" CreationDate="2016-03-12T02:40:28.083" Score="7" Body="&lt;p&gt;In order to ensure that the pattern shapes are always either wholly present or absent, never cut off, it's necessary to ensure that the same &lt;code&gt;p&lt;/code&gt; value is used for all texels within the shape. In your example of circles, all the texels in a given circle need to agree on &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume that you have some way of evaluating &lt;code&gt;p&lt;/code&gt; at a given point on the surface (whether it is looked up from a texture or calculated from some function). Then one way to ensure a group of texels all get the same &lt;code&gt;p&lt;/code&gt; value is to ensure they all look it up from the same point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The UVs of this evaluation point could be stored in extra channels of the pattern texture. For instance, you could have the red and green channels store the UV coordinates at which to evaluate &lt;code&gt;p&lt;/code&gt;, the blue channel store the threshold at which to turn on that pattern element, and the alpha store the antialiased gray level of the pattern to display. The UV+threshold data could also be in a separate, secondary texture if desired.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To generate this UV+threshold texture, starting from an input pattern texture, you could programmatically find connected components (e.g. by searching for black pixels and flood-filling). Set the evaluation point for all texels in each component to the UV of the component's center, and generate a random threshold for it. Then, when rendering, use a pixel shader that first samples this texture, then looks up &lt;code&gt;p&lt;/code&gt; at the given evaluation point and compares it to the given threshold.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That way, each pattern shape will see a uniform &lt;code&gt;p&lt;/code&gt; value and threshold, and will either turn on or turn off fully. As &lt;code&gt;p&lt;/code&gt; increases, more of the shapes will pass their threshold and appear, giving the impression of a continuously varying density.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-12T02:40:28.083" CommentCount="1" />
  <row Id="2176" PostTypeId="2" ParentId="2163" CreationDate="2016-03-12T20:11:20.327" Score="3" Body="&lt;p&gt;What you are looking for is a way to get access to individual pixels in an image, in a way that you can modify those pixels with CPU code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have that you'll want to find software rendering / software rasterization methods.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A great place to start would be reading up in the various bresenham algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is a lot of info out there on that. Here is one article for instance:&#xA;&lt;a href=&quot;http://blog.demofox.org/2015/01/17/bresenhams-drawing-algorithms/&quot; rel=&quot;nofollow&quot;&gt;http://blog.demofox.org/2015/01/17/bresenhams-drawing-algorithms/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-03-12T20:11:20.327" CommentCount="2" />
  <row Id="2177" PostTypeId="2" ParentId="2170" CreationDate="2016-03-13T08:09:05.523" Score="4" Body="&lt;p&gt;Given that you want to test for intersection against rays with many different starting points and directions, it's worth investigating raytracing-style acceleration structures, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bounding_volume_hierarchy&quot; rel=&quot;nofollow&quot;&gt;bounding volume hierarchy (BVH)&lt;/a&gt;. In 2D, this would look like a tree of axis-aligned bounding boxes that divide up the space. Each leaf node of this tree would store a reference to a small list of nearby segments of the polyline, as well as a bounding box that contains them. Higher nodes would store a bounding box that contains all their children.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's a ton of material on BVH construction and traversal out there, so I'd recommend doing a few searches and reading up. It's not too difficult to implement a top-down construction method, by starting with the bounding box of the whole polyline and recursive splitting. Then, to use it for acceleration, when you do a ray query you can find which BVH nodes the ray touches and test only the segments in those nodes for intersections.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, all that being said, I'd like to propose a different approach to your original problem of working with contour lines. It seems to me that this is a place where &lt;a href=&quot;https://en.wikipedia.org/wiki/Delaunay_triangulation&quot; rel=&quot;nofollow&quot;&gt;Delaunay triangulation&lt;/a&gt; can probably be very helpful. It basically constructs a triangle mesh that connects a set of input vertices and edges. For example, here is a contour map I found on the internet:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/nEET2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/nEET2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And now here it is with a constrained Delaunay triangulation, calculating using &lt;a href=&quot;https://www.cs.cmu.edu/~quake/triangle.html&quot; rel=&quot;nofollow&quot;&gt;Jonathan Shewchuk's &quot;Triangle&quot; program&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/EElJb.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EElJb.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, the triangulation gives you a lot of information about neighboring regions in the contour map, so I suspect that you could find a way to get what you want from it somehow. For instance, you could find all the triangles that touch the two contour lines you're interested in, and look at their edges, midpoints, and so on.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-13T08:09:05.523" CommentCount="2" />
  <row Id="2178" PostTypeId="1" CreationDate="2016-03-13T11:40:41.090" Score="3" ViewCount="56" Body="&lt;p&gt;I was trying to draw a plane and cubes in a framebuffer but nothing is drawing in that framebuffer besides clearing it with a color.&#xA;Here's the code...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//C++ Headers&#xA;#include&amp;lt;cassert&amp;gt;&#xA;&#xA;//Third party Headers&#xA;#include&amp;lt;GL/glew.h&amp;gt;&#xA;#include&amp;lt;GLFW/glfw3.h&amp;gt;&#xA;#include&amp;lt;SOIL.h&amp;gt;&#xA;&#xA;//My Headers&#xA;#include&quot;SLMath/SLMath.hpp&quot;&#xA;#include&quot;GLOBALS.h&quot;&#xA;&#xA;&#xA;//Author Headers&#xA;#include&quot;Shader.h&quot;&#xA;#include&quot;Texture.h&quot;&#xA;&#xA;&#xA;//linking libraries&#xA;&#xA;#pragma comment(lib,&quot;OpenGL32.lib&quot;)&#xA;#pragma comment(lib,&quot;glfw3.lib&quot;)&#xA;#pragma comment(lib,&quot;glew32.lib&quot;)&#xA;#pragma comment(lib,&quot;SOIL.lib&quot;)&#xA;&#xA;&#xA;void main()&#xA;{&#xA;glfwInit();&#xA;glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3);&#xA;glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3);&#xA;glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE);&#xA;glfwWindowHint(GLFW_SAMPLES,4);&#xA;&#xA;GLFWwindow *window = glfwCreateWindow(WIDTH, HEIGHT, &quot;fUNKY CUBE&quot;, 0, 0);&#xA;glfwMakeContextCurrent(window);&#xA;&#xA;glewExperimental = GL_TRUE;&#xA;glewInit();&#xA;&#xA;glfwSetKeyCallback(window, keyCallback);&#xA;&#xA;assert(glGetError()!=GL_NO_ERROR);&#xA;&#xA;glGenVertexArrays(1,&amp;amp;VAOplane);&#xA;glGenVertexArrays(1,&amp;amp;VAOcube);&#xA;glGenVertexArrays(1,&amp;amp;VAOquad);&#xA;glGenBuffers(1,&amp;amp;VBOplane);&#xA;glGenBuffers(1,&amp;amp;VBOcube);&#xA;glGenBuffers(1, &amp;amp;VBOquad);&#xA;&#xA;glBindBuffer(GL_ARRAY_BUFFER,VBOplane);&#xA;glBufferData(GL_ARRAY_BUFFER,sizeof(plane),plane,GL_STATIC_DRAW);&#xA;&#xA;glBindBuffer(GL_ARRAY_BUFFER,VBOcube);&#xA;glBufferData(GL_ARRAY_BUFFER,sizeof(cube),cube,GL_STATIC_DRAW);&#xA;&#xA;glBindBuffer(GL_ARRAY_BUFFER,VBOquad);&#xA;glBufferData(GL_ARRAY_BUFFER,sizeof(quad),quad,GL_STATIC_DRAW);&#xA;&#xA;glBindVertexArray(VAOplane);&#xA;{&#xA;    glBindBuffer(GL_ARRAY_BUFFER,VBOplane);&#xA;&#xA;    glEnableVertexAttribArray(0);&#xA;    glVertexAttribPointer(0,3,GL_FLOAT,GL_FALSE,sizeof(GLfloat)*5,(GLvoid*)(0));&#xA;&#xA;    glEnableVertexAttribArray(1);&#xA;    glVertexAttribPointer(1,2,GL_FLOAT,GL_FALSE,sizeof(GLfloat)*5,(GLvoid*)(sizeof(GLfloat)*3));&#xA;}&#xA;glBindVertexArray(0);&#xA;&#xA;&#xA;glBindVertexArray(VAOcube);&#xA;{&#xA;    glBindBuffer(GL_ARRAY_BUFFER,VBOcube);&#xA;&#xA;    glEnableVertexAttribArray(0);&#xA;    glVertexAttribPointer(0,3,GL_FLOAT,GL_FALSE,sizeof(GLfloat)*8,(GLvoid*)(0));&#xA;&#xA;    glEnableVertexAttribArray(1);&#xA;    glVertexAttribPointer(1,2,GL_FLOAT,GL_FALSE,sizeof(GLfloat)*8,(GLvoid*)(sizeof(GLfloat)*6));&#xA;}&#xA;glBindVertexArray(0);&#xA;&#xA;glBindVertexArray(VAOquad);&#xA;{&#xA;    glBindBuffer(GL_ARRAY_BUFFER, VBOquad);&#xA;&#xA;    glEnableVertexAttribArray(0);&#xA;    glVertexAttribPointer(0, 2, GL_FLOAT, GL_FALSE, sizeof(GLfloat) * 4, (GLvoid*)(0));&#xA;&#xA;    glEnableVertexAttribArray(1);&#xA;    glVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, sizeof(GLfloat) * 4, (GLvoid*)(sizeof(GLfloat) * 2));&#xA;}&#xA;glBindVertexArray(0);&#xA;&#xA;&#xA;&#xA;&#xA;/*Setting up framebuffer*/&#xA;&#xA;GLuint fbo;&#xA;glGenFramebuffers(1,&amp;amp;fbo);&#xA;glBindFramebuffer(GL_FRAMEBUFFER,fbo);&#xA;&#xA;//Our color attachement&#xA;GLuint texColorBuffer;&#xA;glGenTextures(1,&amp;amp;texColorBuffer);&#xA;glBindTexture(GL_TEXTURE_2D,texColorBuffer);&#xA;glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER,GL_LINEAR);&#xA;glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);&#xA;glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, WIDTH, HEIGHT, 0, GL_RGB, GL_UNSIGNED_BYTE, 0);&#xA;glBindTexture(GL_TEXTURE_2D,0);&#xA;&#xA;&#xA;glFramebufferTexture2D(GL_FRAMEBUFFER,GL_COLOR_ATTACHMENT0,GL_TEXTURE_2D,texColorBuffer,0);&#xA;&#xA;//our depth attachment&#xA;GLuint depthRenderbuffer;&#xA;glGenRenderbuffers(1,&amp;amp;depthRenderbuffer);&#xA;glBindRenderbuffer(GL_RENDERBUFFER,depthRenderbuffer);&#xA;glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, WIDTH, HEIGHT);&#xA;glBindRenderbuffer(GL_RENDERBUFFER,0);&#xA;&#xA;&#xA;glFramebufferRenderbuffer(GL_FRAMEBUFFER,GL_DEPTH_STENCIL_ATTACHMENT,GL_RENDERBUFFER,depthRenderbuffer);&#xA;&#xA;if (glCheckFramebufferStatus(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE)&#xA;    assert(0);&#xA;&#xA;&#xA;glBindFramebuffer(GL_FRAMEBUFFER,0);&#xA;&#xA;glViewport(0,0,WIDTH,HEIGHT);&#xA;&#xA;Texture floorTex(&quot;textures/floortexture1.jpg&quot;);&#xA;Texture cubeTex(&quot;textures/cubetexture1.jpg&quot;);&#xA;&#xA;Shader shader(&quot;vs.txt&quot;,&quot;fs.txt&quot;);&#xA;Shader shader2(&quot;vs2.txt&quot;,&quot;fs2.txt&quot;);&#xA;&#xA;//glPolygonMode(GL_FRONT_AND_BACK,GL_LINE);&#xA;&#xA;while(!glfwWindowShouldClose(window))&#xA;{&#xA;    glfwPollEvents();&#xA;&#xA;    glBindFramebuffer(GL_FRAMEBUFFER, fbo);&#xA;&#xA;&#xA;    glClearColor(0.1,0.0,0.0,0.0);&#xA;    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);&#xA;    glEnable(GL_DEPTH_TEST);&#xA;    //float radius = 5.0f;&#xA;    //cameraPos.z = cos(SLMath::radians(glfwGetTime()*30.0f))*radius;&#xA;    //cameraPos.x = sin(SLMath::radians(glfwGetTime()*30.0f))*radius;&#xA;&#xA;    SLMath::Mat4 projection;&#xA;    SLMath::Mat4 view;&#xA;    SLMath::Mat4 model;&#xA;&#xA;    GLint projLoc = glGetUniformLocation(shader.program,&quot;projection&quot;);&#xA;    GLint viewLoc = glGetUniformLocation(shader.program,&quot;view&quot;);&#xA;    GLint modelLoc = glGetUniformLocation(shader.program,&quot;model&quot;);&#xA;&#xA;    SLMath::perspective(projection,45.0f,(float)WIDTH/(float)HEIGHT,0.1f,100.0f);&#xA;    SLMath::lookAt(view,cameraPos,cameraDir,cameraUp);&#xA;&#xA;    glUniformMatrix4fv(projLoc,1,GL_FALSE,projection.valuePtr());&#xA;    glUniformMatrix4fv(viewLoc,1,GL_FALSE,view.valuePtr());&#xA;    glUniformMatrix4fv(modelLoc,1,GL_FALSE,model.valuePtr());&#xA;&#xA;&#xA;    floorTex.bind();&#xA;    shader.Use();&#xA;    glBindVertexArray(VAOplane);&#xA;    glDrawArrays(GL_TRIANGLES,0,6);&#xA;    glBindVertexArray(0);&#xA;&#xA;    cubeTex.bind();&#xA;&#xA;    for(int i=0;i&amp;lt;3;i++)&#xA;    {&#xA;        model.reset();&#xA;        SLMath::translate(model,cubePositions[i]);&#xA;        glUniformMatrix4fv(modelLoc,1,GL_FALSE,model.valuePtr());&#xA;&#xA;        glBindVertexArray(VAOcube);&#xA;        glDrawArrays(GL_TRIANGLES,0,36);&#xA;        glBindVertexArray(0);&#xA;&#xA;    }&#xA;&#xA;    glBindFramebuffer(GL_FRAMEBUFFER,0);&#xA;&#xA;    glClearColor(1.0f,1.0f,1.0f,1.0);&#xA;    glClear(GL_COLOR_BUFFER_BIT);&#xA;    glDisable(GL_DEPTH_TEST);&#xA;&#xA;    shader2.Use();&#xA;    glBindTexture(GL_TEXTURE_2D, texColorBuffer);&#xA;    glBindVertexArray(VAOquad);&#xA;    glDrawArrays(GL_TRIANGLES,0,6);&#xA;    glBindVertexArray(0);&#xA;&#xA;&#xA;&#xA;&#xA;    glfwSwapBuffers(window);&#xA;}&#xA;&#xA;&#xA;glDeleteBuffers(1, &amp;amp;VBOquad);&#xA;glDeleteBuffers(1, &amp;amp;VBOcube);&#xA;glDeleteBuffers(1,&amp;amp;VBOplane);&#xA;glDeleteVertexArrays(1, &amp;amp;VAOquad);&#xA;glDeleteVertexArrays(1, &amp;amp;VAOcube);&#xA;glDeleteVertexArrays(1,&amp;amp;VAOplane);&#xA;glfwDestroyWindow(window);&#xA;glfwTerminate();&#xA;exit(EXIT_SUCCESS);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting this...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/cAx72.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cAx72.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;instead of this...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/H3p6i.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/H3p6i.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have checked my code several times....what is wrong??&lt;/p&gt;&#xA;" OwnerUserId="2096" LastEditorUserId="2096" LastEditDate="2016-03-13T11:58:24.033" LastActivityDate="2016-03-16T16:01:20.923" Title="Clearing but not drawing objects" Tags="&lt;opengl&gt;&lt;glsl&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="2180" PostTypeId="1" AcceptedAnswerId="2181" CreationDate="2016-03-13T22:52:45.323" Score="5" ViewCount="73" Body="&lt;p&gt;I am a beginner in digital image processing and computer graphics. I would like to program a similar behavior than the Shadermap 3 normal editor (displacement layer more specifically).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As shown in this &lt;a href=&quot;https://youtu.be/NIHpBNnYYU8?t=34s&quot;&gt;Shadermap 3 demonstration video&lt;/a&gt;, the displacement brush allows the user to paint a &lt;strong&gt;displacement map (grayscale)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/sliKgEI.png&quot; alt=&quot;Shadermap 3&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The brush strokes produce shapes with a smooth value transitioning from the middle of the shape to its edge, in this case giving the shape a slightly curved profile. As it is also shown, the shape's profile can be fine-tuned with the brush curve editor.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What combination of filters and/or algorithms could Shadermap 3 possibly use to convert a pixel blob or shape to displacement map with a given width and curve?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/C7o1bbM.png&quot; alt=&quot;Combination&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="2319" LastActivityDate="2016-03-14T02:55:34.097" Title="Converting raster shape/blob into displacement map" Tags="&lt;algorithm&gt;&lt;image-processing&gt;&lt;curve&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2181" PostTypeId="2" ParentId="2180" CreationDate="2016-03-14T02:55:34.097" Score="6" Body="&lt;p&gt;One way I can think of is to make a &quot;signed distance transform&quot; of the image where there is information for each pixel about how far the pixel is to the closest surface of the shape.  Since it's signed, youll be able to know if the pixel is inside or outside he shape, and by how much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using this knowledge, you could easily make a new image, where the pixel is black if the distance is greater than -10, else it is white.  That would make the shape shrink by 10 pixels.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can use smoothstep or other curves to transition between black and white over a number of pixels, to soften it anti alias the edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can make the distance transform by brute force, but another method would be to make a voronoi diagram first and then turn that into a distance transform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There is lots of info out there on both voronoi diagrams as well as (signed) distance transforms but these two links will probably be helpful to get you started:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://blog.demofox.org/2016/02/29/fast-voronoi-diagrams-and-distance-dield-textures-on-the-gpu-with-the-jump-flooding-algorithm/&quot;&gt;http://blog.demofox.org/2016/02/29/fast-voronoi-diagrams-and-distance-dield-textures-on-the-gpu-with-the-jump-flooding-algorithm/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://blog.demofox.org/2016/03/02/actually-making-signed-distance-field-textures-with-jfa/&quot;&gt;http://blog.demofox.org/2016/03/02/actually-making-signed-distance-field-textures-with-jfa/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="56" LastActivityDate="2016-03-14T02:55:34.097" CommentCount="10" />
  <row Id="2182" PostTypeId="2" ParentId="1963" CreationDate="2016-03-14T10:32:47.963" Score="0" Body="&lt;p&gt;Another thought is along these lines.  If you have a uniform point set defined on the plane and a mapping function from the plane to the target surface then the density function on the surface is how areas map from the plane surface to the target.  So an area preserving map would result in a uniform points on the target.&lt;/p&gt;&#xA;" OwnerUserId="2831" LastActivityDate="2016-03-14T10:32:47.963" CommentCount="0" />
  <row Id="2183" PostTypeId="2" ParentId="2178" CreationDate="2016-03-14T18:22:33.197" Score="2" Body="&lt;p&gt;You would want to provide us with the implementation of the shaders you use, or at the very least the result of shader compilation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A simple typo in a shader obviously prevents compilation and therefore successful rasterization of the object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In short, check your shader !&lt;/p&gt;&#xA;" OwnerUserId="2828" LastActivityDate="2016-03-14T18:22:33.197" CommentCount="2" />
  <row Id="2184" PostTypeId="1" CreationDate="2016-03-14T18:38:52.403" Score="2" ViewCount="113" Body="&lt;p&gt;I'm trying to achieve the same effect in OpenGL/GLSL as what Blender does when you add a texture, set to &quot;Reflection&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This ultimately is a way to fake specularity, or &quot;metallicness&quot; if I get it right.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I followed this tutorial (Mid-page, &quot;Sphere Mapping&quot; section):&#xA;&lt;a href=&quot;http://www.ozone3d.net/tutorials/glsl_texturing_p04.php&quot; rel=&quot;nofollow&quot;&gt;http://www.ozone3d.net/tutorials/glsl_texturing_p04.php&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's the problem(some serious artifacts):&#xA;&lt;a href=&quot;https://www.youtube.com/watch?v=hx66_xWhVu4&amp;amp;feature=youtu.be&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=hx66_xWhVu4&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Everything's happening at the bottom of the Vertex Sahder:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version 330 core&#xA;&#xA;layout(location = 0) in vec3 vertexPosition_modelspace;&#xA;layout(location = 1) in vec2 vertexUV;&#xA;layout(location = 2) in vec3 vertexNormal_modelspace;&#xA;layout(location = 3) in vec3 vertexTangent_modelspace;&#xA;&#xA;out vec2 uv;&#xA;out vec3 toLightVector[4];&#xA;out vec3 toCameraVector;&#xA;out float fog_Visibility;&#xA;&#xA;uniform mat4 FTM;&#xA;uniform mat4 P;&#xA;uniform mat4 V;&#xA;uniform vec3 LP[4];&#xA;&#xA;const float fog_Density = 0.2;&#xA;const float fog_Gradient = 0.9;&#xA;&#xA;void main(){&#xA;&#xA;    vec4 worldPosition = FTM * vec4(vertexPosition_modelspace, 1.0);&#xA;    uv = vertexUV;&#xA;&#xA;    vec3 surfaceNormal = (FTM * vec4(vertexNormal_modelspace, 0.0)).xyz;&#xA;&#xA;    vec3 norm = normalize(surfaceNormal);&#xA;    vec3 tang = normalize((FTM * vec4(vertexTangent_modelspace, 0.0)).xyz);&#xA;    vec3 bitang = normalize(cross(norm, tang));&#xA;&#xA;    mat3 toTangentSpace = mat3(tang.x, bitang.x, norm.x, tang.y, bitang.y, norm.y, tang.z, bitang.z, norm.z);&#xA;&#xA;    for (int i = 0; i &amp;lt; 4; i++) {&#xA;        toLightVector[i] = toTangentSpace * (LP[i] - worldPosition.xyz);&#xA;    }&#xA;&#xA;    toCameraVector = toTangentSpace * ((inverse(V) * vec4(0.0, 0.0, 0.0, 1.0)).xyz - worldPosition.xyz);&#xA;&#xA;    vec4 positionRelativeToCam = V * worldPosition;&#xA;    gl_Position = P * positionRelativeToCam;&#xA;&#xA;    // Fog&#xA;    float fog_Distance = length(positionRelativeToCam.xyz);&#xA;    fog_Visibility = exp(-pow((fog_Distance * fog_Density), fog_Gradient));&#xA;    fog_Visibility = clamp(fog_Visibility, 0.0, 1.0);&#xA;&#xA;    // Reflection Map&#xA;    vec3 u = normalize(toCameraVector);&#xA;    vec3 r = reflect(u, norm);&#xA;    float m = 2.0 * sqrt(r.x*r.x + r.y*r.y + (r.z + 1.0)*(r.z + 1.0));&#xA;&#xA;    gl_TexCoord[0].s = r.x / m + 0.5;&#xA;    gl_TexCoord[0].t = r.y / m + 0.5;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And in the Fragment Shader, all I do is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;out_color = texture(reflectionSampler, gl_TexCoord[0].st);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The reflection map seems to do it's thing similarly to the way it's seen in Blender, but not quite.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to be able to do this without calculating lights. Thats how Blender does it, it works Shadeless mode so all it should take is the camera's vector...&lt;/p&gt;&#xA;" OwnerUserId="2879" LastActivityDate="2016-04-14T10:33:20.317" Title="Guidance with Sphere-Map calculation in GLSL" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;glsl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2185" PostTypeId="2" ParentId="2184" CreationDate="2016-03-14T21:29:39.643" Score="2" Body="&lt;p&gt;You probably need to calculate the reflection vector per pixel, not per vertex. The calculation is non-linear, so doing it per vertex and interpolating will not give good results unless the model is finely tesselated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Send &lt;code&gt;norm&lt;/code&gt; and &lt;code&gt;toCameraVector&lt;/code&gt; down to the pixel shader, normalize them both and move all your calculations under &lt;code&gt;// Reflection Map&lt;/code&gt; to the pixel shader. That should give you some better results.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-14T21:29:39.643" CommentCount="0" />
  <row Id="2187" PostTypeId="1" CreationDate="2016-03-15T05:20:10.700" Score="5" ViewCount="43" Body="&lt;p&gt;Color spaces issues are such a pain, especially when you create color data at the middle of the chain, and a pile of libs and apps separate this to the display... Here I only worry about gamma transform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://shadertoy.com&quot;&gt;Shadertoy&lt;/a&gt; addicts are coding in this web site webGLSL shaders, which in my case are rendered through google chrome on linux ubuntu and displayed on a calibrated monitor. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is one layer already doing the gamma transform, or should I do pow(color,1./2.2) at the end of my rendering ?&#xA;For me (linux,etc), it seems I must do it. Is it also true on windows, mac OS, whether Angle or Native OpenGL is used (windows), whatever the browser ?&#xA;The point is that often shaders look ok on windows and very dark on linux, for instance, both people being sure of their settings and qualibration.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-03-28T08:47:03.953" Title="gamma transform in webGLSL: when already done or yet to be done?" Tags="&lt;glsl&gt;&lt;webgl&gt;&lt;gamma&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2188" PostTypeId="1" CreationDate="2016-03-15T10:18:23.130" Score="2" ViewCount="46" Body="&lt;p&gt;I am trying to model an organic object using OpenSCAD. However this will need to be ported over to older software. This restricts the primitives I can use (only cone, cylinder, torus, sphere and cubes) and the operations I can perform (difference, union, intersection, scaling and transformation). However modeling with these restrictions gives very unnatural looking objects. I need to somehow implement by hand, using only these primitives/operations the hull and minkowski operations to give my models a more organic look. Does anybody know how this could be done?&lt;/p&gt;&#xA;" OwnerUserId="2858" LastActivityDate="2016-03-15T10:18:23.130" Title="OpenSCAD Hull/Minkowski function by hand?" Tags="&lt;transformations&gt;&lt;3d&gt;&lt;model&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="2189" PostTypeId="1" CreationDate="2016-03-15T13:40:03.177" Score="3" ViewCount="100" Body="&lt;p&gt;I am doing exercises from the OpenGL SuperBible 6th Ed. I have managed to set up rendering into a framebuffer texture and then rendering from that texture to a cube.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But for some reason, &lt;strong&gt;when the texture gets applied to the cube, it is rendered incorrectly. I was expecting to see an entire rotating cube on a green background on each face of the main cube, but instead the faces have a texture applied as if split in half.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's a screenshot of my current situation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/KCtjJ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KCtjJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've spent a lot of time trying to fix this. I mean, the last 5 hours or so. I rewrote most of the code once and it fixed some of my previous erros, but now I'm stuck with this last part. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I used the SB sources as reference (&lt;a href=&quot;https://github.com/openglsuperbible/sb6code/blob/master/src/basicfbo/basicfbo.cpp&quot; rel=&quot;nofollow&quot;&gt;https://github.com/openglsuperbible/sb6code/blob/master/src/basicfbo/basicfbo.cpp&lt;/a&gt;), but I can't find the reason why it looks like this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also read this &lt;a href=&quot;https://www.opengl.org/wiki/Framebuffer_Object_Examples&quot; rel=&quot;nofollow&quot;&gt;https://www.opengl.org/wiki/Framebuffer_Object_Examples&lt;/a&gt; but can't find a solution there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What did I miss?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my current code (the most important App class which does most of the work):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &quot;config.h&quot;&#xA;&#xA;#include &amp;lt;GL/glew.h&amp;gt;&#xA;#include &amp;lt;GL/freeglut.h&amp;gt;&#xA;#include &amp;lt;glm/glm.hpp&amp;gt;&#xA;#include &amp;lt;glm/gtc/matrix_transform.hpp&amp;gt;&#xA;#include &amp;lt;glm/gtc/type_ptr.hpp&amp;gt;&#xA;&#xA;#include &quot;App.h&quot;&#xA;#include &quot;GLSLProgram.h&quot;&#xA;&#xA;void App::renderToFramebuffer() {&#xA;    float clearColor[] = { 0.0f, 0.5f, 0.0f, 1.0f };&#xA;    float one = 1.0f;&#xA;&#xA;    m_rotation += 1.0f;&#xA;&#xA;    glBindFramebuffer(GL_FRAMEBUFFER, m_FBO);&#xA;&#xA;    glViewport(0, 0, 512, 512);&#xA;    glClearBufferfv(GL_COLOR, 0, clearColor);&#xA;    glClearBufferfv(GL_DEPTH, 0, &amp;amp;one);&#xA;&#xA;    glm::mat4 modelView = glm::translate(glm::mat4(), glm::vec3(-1.0f, -0.0f, -5.0f));&#xA;    glm::vec3 rotationY = glm::vec3(0.0f, 1.0f, 0.0f);&#xA;    modelView = glm::rotate(modelView, glm::radians(45.0f), rotationY);&#xA;    glm::vec3 rotationX = glm::vec3(1.0f, 0.0f, 0.0f);&#xA;    modelView = glm::rotate(modelView, glm::radians(m_rotation), rotationX);&#xA;&#xA;    glm::mat4 projectionMatrix = glm::perspective(50.0f, 1.0f, 1.0f, 1000.0f);&#xA;&#xA;    m_shaderNotTextured-&amp;gt;start();&#xA;    m_shaderNotTextured-&amp;gt;setMatrix(projectionMatrix, GL_PROJECTION_MATRIX);&#xA;    m_shaderNotTextured-&amp;gt;setMatrix(modelView, GL_MODELVIEW_MATRIX);&#xA;&#xA;    glDrawArrays(GL_TRIANGLES, 0, 36);&#xA;&#xA;    m_shaderNotTextured-&amp;gt;stop();&#xA;&#xA;    glBindFramebuffer(GL_FRAMEBUFFER, 0);&#xA;}&#xA;&#xA;void App::renderToScreen() {&#xA;    glBindVertexArray(m_VAO);&#xA;&#xA;    glViewport(0, 0, SCREEN_X, SCREEN_Y);&#xA;    glClearBufferfv(GL_COLOR, 0, glm::value_ptr(m_clearColor));&#xA;    glClearBufferfv(GL_DEPTH, 0, &amp;amp;m_depthBufferDefault);&#xA;&#xA;    glBindTexture(GL_TEXTURE_2D, m_TBO);&#xA;&#xA;    glm::mat4 modelView = glm::translate(glm::mat4(), glm::vec3(0.0f, 0.0f, -5.0f));&#xA;    glm::vec3 rotation = glm::vec3(0.0f, 1.0f, 0.0f);&#xA;    modelView = glm::rotate(modelView, glm::radians(45.0f), rotation);&#xA;    glm::vec3 rotationX = glm::vec3(1.0f, 0.0f, 0.0f);&#xA;    modelView = glm::rotate(modelView, glm::radians(45.0f), rotationX);&#xA;&#xA;    m_shaderTextured-&amp;gt;start();&#xA;    m_shaderTextured-&amp;gt;setMatrix(m_projectionMatrix, GL_PROJECTION_MATRIX);&#xA;    m_shaderTextured-&amp;gt;setMatrix(modelView, GL_MODELVIEW_MATRIX);&#xA;&#xA;    glDrawArrays(GL_TRIANGLES, 0, 36);&#xA;&#xA;    glBindTexture(GL_TEXTURE_2D, 0);&#xA;&#xA;    m_shaderTextured-&amp;gt;stop();&#xA;}&#xA;&#xA;void App::render() {&#xA;    renderToFramebuffer();&#xA;    renderToScreen();&#xA;    glutSwapBuffers();&#xA;}&#xA;&#xA;App::App() {&#xA;    glEnable(GL_TEXTURE_2D);&#xA;&#xA;    glEnable(GL_CULL_FACE);&#xA;    glEnable(GL_DEPTH_TEST);&#xA;    glDepthFunc(GL_LEQUAL);&#xA;&#xA;    m_clearColor = glm::vec4(0.0f, 0.0f, 0.0f, 1.0f);&#xA;    m_depthBufferDefault = 1.0f;&#xA;    m_projectionMatrix = glm::perspective(50.0f, (float) SCREEN_X / (float) SCREEN_Y, 1.0f, 1000.0f);&#xA;&#xA;    m_shaderNotTextured = new GLSLProgram();&#xA;    m_shaderNotTextured-&amp;gt;addVertexShader(&quot;shaders/notex.vs.glsl&quot;);&#xA;    m_shaderNotTextured-&amp;gt;addFragmentShader(&quot;shaders/notex.fs.glsl&quot;);&#xA;    m_shaderNotTextured-&amp;gt;compile();&#xA;&#xA;    m_shaderTextured = new GLSLProgram();&#xA;    m_shaderTextured-&amp;gt;addVertexShader(&quot;shaders/tex.vs.glsl&quot;);&#xA;    m_shaderTextured-&amp;gt;addFragmentShader(&quot;shaders/tex.fs.glsl&quot;);&#xA;    m_shaderTextured-&amp;gt;compile();&#xA;&#xA;    glGenVertexArrays(1, &amp;amp;m_VAO);&#xA;    glGenBuffers(NUM_BUFFERS, m_buffers);&#xA;&#xA;    glGenTextures(1, &amp;amp;m_TBO);&#xA;    glGenTextures(1, &amp;amp;m_TDBO);&#xA;&#xA;    glGenFramebuffers(1, &amp;amp;m_FBO);&#xA;&#xA;    glBindVertexArray(m_VAO);&#xA;&#xA;    glBindBuffer(GL_ARRAY_BUFFER, m_buffers[VBO]);&#xA;        static const GLfloat vertex_data[] =&#xA;        {&#xA;             // Position                 Tex Coord&#xA;            -0.25f, -0.25f,  0.25f,      0.0f, 1.0f,&#xA;            -0.25f, -0.25f, -0.25f,      0.0f, 0.0f,&#xA;             0.25f, -0.25f, -0.25f,      1.0f, 0.0f,&#xA;&#xA;             0.25f, -0.25f, -0.25f,      1.0f, 0.0f,&#xA;             0.25f, -0.25f,  0.25f,      1.0f, 1.0f,&#xA;            -0.25f, -0.25f,  0.25f,      0.0f, 1.0f,&#xA;&#xA;             0.25f, -0.25f, -0.25f,      0.0f, 0.0f,&#xA;             0.25f,  0.25f, -0.25f,      1.0f, 0.0f,&#xA;             0.25f, -0.25f,  0.25f,      0.0f, 1.0f,&#xA;&#xA;             0.25f,  0.25f, -0.25f,      1.0f, 0.0f,&#xA;             0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;             0.25f, -0.25f,  0.25f,      0.0f, 1.0f,&#xA;&#xA;             0.25f,  0.25f, -0.25f,      1.0f, 0.0f,&#xA;            -0.25f,  0.25f, -0.25f,      0.0f, 0.0f,&#xA;             0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;&#xA;            -0.25f,  0.25f, -0.25f,      0.0f, 0.0f,&#xA;            -0.25f,  0.25f,  0.25f,      0.0f, 1.0f,&#xA;             0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;&#xA;            -0.25f,  0.25f, -0.25f,      1.0f, 0.0f,&#xA;            -0.25f, -0.25f, -0.25f,      0.0f, 0.0f,&#xA;            -0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;&#xA;            -0.25f, -0.25f, -0.25f,      0.0f, 0.0f,&#xA;            -0.25f, -0.25f,  0.25f,      0.0f, 1.0f,&#xA;            -0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;&#xA;            -0.25f,  0.25f, -0.25f,      0.0f, 1.0f,&#xA;             0.25f,  0.25f, -0.25f,      1.0f, 1.0f,&#xA;             0.25f, -0.25f, -0.25f,      1.0f, 0.0f,&#xA;&#xA;             0.25f, -0.25f, -0.25f,      1.0f, 0.0f,&#xA;            -0.25f, -0.25f, -0.25f,      0.0f, 0.0f,&#xA;            -0.25f,  0.25f, -0.25f,      0.0f, 1.0f,&#xA;&#xA;            -0.25f, -0.25f,  0.25f,      0.0f, 0.0f,&#xA;             0.25f, -0.25f,  0.25f,      1.0f, 0.0f,&#xA;             0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;&#xA;             0.25f,  0.25f,  0.25f,      1.0f, 1.0f,&#xA;            -0.25f,  0.25f,  0.25f,      0.0f, 1.0f,&#xA;            -0.25f, -0.25f,  0.25f,      0.0f, 0.0f,&#xA;        };&#xA;&#xA;        glBufferData(GL_ARRAY_BUFFER, sizeof(vertex_data), vertex_data, GL_STATIC_DRAW);&#xA;&#xA;        glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 5 * sizeof(GLfloat), 0);&#xA;        glEnableVertexAttribArray(0);&#xA;&#xA;        glVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, 5 * sizeof(GLfloat), (GLvoid *) (2 * sizeof(GLfloat)));&#xA;        glEnableVertexAttribArray(1);&#xA;&#xA;    glBindBuffer(GL_ARRAY_BUFFER, 0);&#xA;&#xA;    glBindFramebuffer(GL_FRAMEBUFFER, m_FBO);&#xA;&#xA;        glBindTexture(GL_TEXTURE_2D, m_TBO);&#xA;            glTexStorage2D(GL_TEXTURE_2D, 9, GL_RGBA8, 512, 512);&#xA;&#xA;            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);&#xA;            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);&#xA;&#xA;        glBindTexture(GL_TEXTURE_2D, m_TDBO);&#xA;            glTexStorage2D(GL_TEXTURE_2D, 9, GL_DEPTH_COMPONENT32F, 512, 512);&#xA;&#xA;        glFramebufferTexture(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, m_TBO, 0);&#xA;        glFramebufferTexture(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, m_TDBO, 0);&#xA;&#xA;        GLenum drawBuffers[] = { GL_COLOR_ATTACHMENT0 };&#xA;        glDrawBuffers(1, drawBuffers);&#xA;    glBindFramebuffer(GL_FRAMEBUFFER, 0);&#xA;&#xA;    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_buffers[IBO]);&#xA;&#xA;        static const GLushort vertex_indices[] =&#xA;        {&#xA;            0, 1, 2,&#xA;            2, 1, 3,&#xA;            2, 3, 4,&#xA;            4, 3, 5,&#xA;            4, 5, 6,&#xA;            6, 5, 7,&#xA;            6, 7, 0,&#xA;            0, 7, 1,&#xA;            6, 0, 2,&#xA;            2, 4, 6,&#xA;            7, 5, 3,&#xA;            7, 3, 1&#xA;        };&#xA;&#xA;        glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(vertex_indices), vertex_indices, GL_STATIC_DRAW);&#xA;&#xA;    glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0);&#xA;&#xA;}&#xA;&#xA;App::~App() {&#xA;    delete m_shaderNotTextured;&#xA;    delete m_shaderTextured;&#xA;    glDeleteBuffers(NUM_BUFFERS, m_buffers);&#xA;}&#xA;&#xA;&#xA;void App::update() {&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, I have no idea why in SB6 code they are creating an array of indices and then not using it. I copied the part of the code with vertex data, initialized everything and then realized the indices aren't used. But I didn't remove them yet.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: and here is the framebuffer cube colored to show more of how it is being displayed (you may notice how on most faces the texture is applied in halves):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BvEAn.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BvEAn.png&quot; alt=&quot;the cube when colored&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit2: and this is what I get when I don't bind the framebuffer and instead draw to screen after changing cubes to quads&lt;/strong&gt; - so the image texture is being drawn correctly to the FB:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/SVXWI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SVXWI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am thinking - maybe this has something to do with the size of the texture or the size of viewport when rendering? I'm still completely baffled by this...&lt;/p&gt;&#xA;" OwnerUserId="2508" LastEditorUserId="2508" LastEditDate="2016-03-19T14:53:34.067" LastActivityDate="2016-03-19T14:53:34.067" Title="Drawing to framebuffer texture - texture then split in half" Tags="&lt;opengl&gt;&lt;texture&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="2190" PostTypeId="1" AcceptedAnswerId="2194" CreationDate="2016-03-16T04:00:38.943" Score="5" ViewCount="59" Body="&lt;p&gt;Different BRDFs are usually used to compute diffuse and specular reflection.&lt;br&gt;&#xA;Some of the most often used include for example the Lambert BRDF for diffuse reflection and the Cook-Torrance BRDF for specular reflection.&lt;br&gt;&#xA;Cook-Torrance BRDFs are parametrized by the surface roughness: the rougher the surface, the &quot;blurrier&quot; the reflection.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From a conceptual point of view, should we consider that for an extremely rough surface, the specular reflection must look like a diffuse reflection? Can there be a continuity between both components of the reflection?&lt;/p&gt;&#xA;" OwnerUserId="110" LastEditorUserId="127" LastEditDate="2016-03-18T15:01:56.223" LastActivityDate="2016-03-19T04:42:47.883" Title="Should not specular reflection on a rough surface be similar to diffuse reflection?" Tags="&lt;reflection&gt;&lt;specular&gt;&lt;diffuse&gt;&lt;continuity&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="2191" PostTypeId="2" ParentId="2178" CreationDate="2016-03-16T16:01:20.923" Score="1" Body="&lt;p&gt;Answering my own question!&#xA;The reason for this behaviour is that I forgot to activate shader program before passing matrices to it!&lt;/p&gt;&#xA;" OwnerUserId="2096" LastActivityDate="2016-03-16T16:01:20.923" CommentCount="0" />
  <row Id="2192" PostTypeId="1" CreationDate="2016-03-16T16:05:17.363" Score="5" ViewCount="126" Body="&lt;p&gt;I know how shadow mapping works but I am not getting the cause of shadow acne!&#xA;Can anyone tell me cause of shadow acne in a simple way and how is it related to depth map resolution? &lt;/p&gt;&#xA;" OwnerUserId="2096" LastActivityDate="2016-03-17T13:44:31.807" Title="Cause of shadow acne" Tags="&lt;opengl&gt;&lt;raytracing&gt;&lt;rendering&gt;&lt;shader&gt;&lt;transformations&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2193" PostTypeId="2" ParentId="2192" CreationDate="2016-03-16T18:49:36.300" Score="6" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/o7MUP.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/o7MUP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: A bad case of shadow acne. (Synthetic and a bit exaggerated)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Shadow acne is caused by the discrete nature of the shadow map. A shadow map is composed of samples, a surface is continuous. Thus, there can be a spot on the surface where the discrete surface is further than the sample. The problem does persist even if you multi sample, but you can sample smarter in ways that can nearly eliminate this at significant cost.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5HSkc.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5HSkc.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2:&lt;/strong&gt; A side cutaway of a shadow function and its discrete samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The canonical way to solve this is to offset the shadow map slightly so the object no longer self shadows itself. This offset is called a bias. One can use more smart offsets than just a fixed value but a fixed value works quite well and has minimal overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/SyZdy.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/SyZdy.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 3:&lt;/strong&gt; Shadow function biased (offset) forward.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-03-16T20:05:24.677" LastActivityDate="2016-03-16T20:05:24.677" CommentCount="8" />
  <row Id="2194" PostTypeId="2" ParentId="2190" CreationDate="2016-03-17T08:34:35.613" Score="8" Body="&lt;p&gt;No, because the underlying physics is not the same, nor the lobe shape - not to speak of their parameters such as color and Fresnel term.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specular is really true surface interaction with the interface material/air, so it has Fresnel modulation and the internal medium has no influence on colors. But the surface condition strongly influence the reflectance, of course. Think of it as the reflections on ocean surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Diffuse is due to the subsurface scattering, light entering the medium, and thus gains the color characteristic and loose directionality. Think of it as the green color of the water turbidity. If depth of penetration is less than pixel size CG people don't call it &quot;subsurface&quot; but the physics is the same. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course diffuse input is what specular let pass, which is angle (and polarization) dependant. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Beside, there exist transparent medium with internal interfaces (e.g. thin shells), so case exists were light goes inside but still acts specularely (or even wavely) and not diffusely. Oppositely, most plastics and paints/varnishs and biological medium are intrinsically transparent, but contain pigments (often based on a metal atom) that cause the opacity (diffusion or specular on these &quot;objects in the object&quot;). Think of them as the fishes in clear water :-) .&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2016-03-19T04:42:47.883" LastActivityDate="2016-03-19T04:42:47.883" CommentCount="4" />
  <row Id="2197" PostTypeId="2" ParentId="2192" CreationDate="2016-03-17T13:44:31.807" Score="4" Body="&lt;p&gt;As an addition to the answer of joojaa:&#xA;Using a bias to offset the shadow function does indeed solve the problem with shadow acne, but it can introduce an additional problem: Peter Panning&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/mdoX0.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/mdoX0.png&quot; alt=&quot;Sample image that has Peter Panning&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you see in the picture on the left the shadow is disconnected from the shadow casting wall. This gives the impression that the geometry hovers over the ground (just like Peter Pan can hover, hence the name Peter Panning).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To solve this problem you have to use &quot;thick&quot; geometry that has a volume and then render the shadow map using the back-faces. If the offset is smaller than the thickness of the geometry there will be no Peter Panning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TwFxZ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TwFxZ.png&quot; alt=&quot;Scene with thick geometry and thereby no Peter Panning&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Both images are taken from &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot; rel=&quot;nofollow&quot;&gt;this tutorial&lt;/a&gt; where you can also learn more about how shadow mapping works, how shadow acne is created and solved and what Peter Panning is.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-03-17T13:44:31.807" CommentCount="3" />
  <row Id="2198" PostTypeId="1" CreationDate="2016-03-17T16:15:53.137" Score="3" ViewCount="39" Body="&lt;p&gt;I have a set of closed loops on the surface of a non-self-intersecting triangle mesh, and I need to create inset versions of them on the surface of the mesh. In other words, I want to generate a &lt;a href=&quot;https://en.wikipedia.org/wiki/Straight_skeleton&quot; rel=&quot;nofollow&quot;&gt;straight skeleton&lt;/a&gt; from them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Within each single triangle of the mesh, this algorithm should behave just like a 2D straight skeleton.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found several tools for offsetting loops of edges in a mesh, or generating loops equidistant from a point on the mesh, but I need to be able to input arbitrary loops of my own and offset them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I ran through various corner cases by hand on some graph paper, and it seems to me the algorithm described by &lt;a href=&quot;http://www.dma.fi.upm.es/personal/mabellanas/tfcs/skeleton/html/documentacion/Straight%20Skeletons%20Implementation.pdf&quot; rel=&quot;nofollow&quot;&gt;Felkel and Obdrzalek&lt;/a&gt; should be perfectly applicable, except the wavefront may at times have to switch between contracting and expanding when it encounters geometries that give it more space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This algorithm seems like it would be useful in several CAM applications, does anyone know of an existing implementation?&lt;/p&gt;&#xA;" OwnerUserId="2896" LastActivityDate="2016-03-30T18:15:13.220" Title="Can I run polygon insetting on the surface of a mesh?" Tags="&lt;geometry&gt;&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2201" PostTypeId="1" AcceptedAnswerId="2204" CreationDate="2016-03-18T09:08:55.750" Score="1" ViewCount="79" Body="&lt;p&gt;Given a 3D object in Computer graphics, whose surface is represented as a 3D triangular mesh (mesh of 3D triangle objects), I need to find the maximum continual Convex patches on the surface of the given 3D object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using OpenGl to render the graphics within a C++ program. What kind of methods or algorithms should I use to find the convex patches.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have to apply different colors to the different convex patches on the object to signify the selection. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say I have a sphere then the whole sphere is one maximal convex patch. Any portion of the sphere surface will be a convex patch, by maximal I mean the maximum continuous convex patches that can be found. Well in the rendering, depending on the viewing angles, the maximal convex patches visible to the viewer will have to colored. I have to report all the convex patches, each patch being the maximal in that area.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using &quot;maximal&quot; to mean a convex surface which is not a subset of a larger convex surface, rather than to mean the largest convex surface that exists in the triangle mesh. The patches need to be strictly convex (never flat).&lt;/p&gt;&#xA;" OwnerUserId="2898" LastEditorUserId="2898" LastEditDate="2016-04-03T08:45:59.447" LastActivityDate="2016-04-03T08:45:59.447" Title="maximal convex patching in Computer graphics" Tags="&lt;3d&gt;&lt;mesh&gt;&lt;computational-geometry&gt;&lt;triangulation&gt;" AnswerCount="1" CommentCount="10" />
  <row Id="2204" PostTypeId="2" ParentId="2201" CreationDate="2016-03-20T03:54:01.847" Score="1" Body="&lt;p&gt;Start from any triangle. Traverse it's edge's and check that the angle between the two triangles is less than 180deg. If it is add it to the current selection and continue expanding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The check is actually really simple if you use vector geometry. Say A - B is the common edge with C on the selected side and D on the other. Then just check if &lt;code&gt;dot((D-B), cross((A-B), (C-B)) &amp;lt; 0&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Single triangle patches should be ruled out, as for us to determine convexity, it has to span across several triangles.&#xA;We have to keep sampling until we've tested everything.  When a convex patch continues further, it's size keeps on increasing by including the neighbouring triangles that satisfy convexity, until the patch reaches the maximum size it can attain.&lt;/p&gt;&#xA;" OwnerUserId="2898" LastEditorUserId="2898" LastEditDate="2016-04-01T09:31:26.370" LastActivityDate="2016-04-01T09:31:26.370" CommentCount="7" />
  <row Id="2206" PostTypeId="1" CreationDate="2016-03-20T21:24:10.253" Score="1" ViewCount="95" Body="&lt;p&gt;I implemented a spectral path tracing using physically base BRDF models such as Oren-Nayar,Specular Reflection and Transmission, Lambertian. All calculation in the path tracer uses standard illuminant and macbeth color checker SPD, spectral power distribution. The result of the path tracer for each pixel is the SPD obtained as a sum of SPD obtained from each sample calculated by the pat tracer. This SPD is then converted to CIE XYZ color and to RGB. The result scene obtained is the following one (in this example taking 500 samples per pixel):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8CdNZ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8CdNZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, everything seems fine, except for the brightness/luminance of the scene. Every object in the scene is darker than it have to be. The floor and the front wall of the cornell box in the scene must be white and neutral8 (from macbeth color checker), but they are dark gray.&#xA;The following method is the one that trace the samples for a pixel of the path tracer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vector3D PathTracer::getPixelColor(const Ray&amp;amp; ray, int bounce) {&#xA;&#xA;    Spectrum&amp;lt;constant::spectrumSamples&amp;gt; L(0.0f);&#xA;    int numberOfSamples = 500;&#xA;    float sampleWeight = 1.0f/(float)numberOfSamples;&#xA;&#xA;    for (int i = 0; i &amp;lt; numberOfSamples; i++) {&#xA;&#xA;        Spectrum&amp;lt;constant::spectrumSamples&amp;gt; spectrumSample = trace(ray, bounce);&#xA;        L = L + spectrumSample * sampleWeight;&#xA;    }&#xA;&#xA;    Spectrum&amp;lt;constant::spectrumSamples&amp;gt; Li = scene-&amp;gt;light-&amp;gt;spectrum;&#xA;&#xA;    ColorMatchingFunction* colorMatchingFunction = new Standard2ObserverColorMatchingFunction();&#xA;&#xA;    //Get tristimulus values.&#xA;    Vector3D tristimulus = CIE1931XYZ::tristimulusValues(L, Li, colorMatchingFunction);&#xA;&#xA;    //Convert tristimulus to sRGB.&#xA;    Vector3D color = CIE1931XYZ::tristimulusTosRGB(tristimulus);&#xA;&#xA;    //Apply sRGB gamma correction.&#xA;    sRGB::sRGBGammaCorrection(color, GammaCompanding);&#xA;&#xA;    //Convert to standard 0 - 255 RGB value.&#xA;    sRGB::sRGBStandardRange(color);&#xA;&#xA;    delete colorMatchingFunction;&#xA;&#xA;    return color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can seen, I already apply a gamma correction to the color obtained. &#xA;Do you have any idea why my image rendered is so dark? My concerns are in the part where I convert the SPD sum, obtained from the samples, into RGB color. Do you see any error? Am I missing something? Do I need other operation to execute a correct conversion from the SPD obtained from the sampling to an RGB color?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;To avoid to write a too long question, I will link the main classes used by the path tracer for the calculation:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Lighting/shading class used to recursive calculate the rendering equation &lt;a href=&quot;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Shading/PathBRDF.cpp&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Shading/PathBRDF.cpp&lt;/a&gt; &lt;/li&gt;&#xA;&lt;li&gt;CIE XYZ conversion &lt;a href=&quot;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Color/CIEColorSpaces/CIEXYZ.cpp&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Color/CIEColorSpaces/CIEXYZ.cpp&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;sRGB utils &lt;a href=&quot;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Color/sRGB.hpp&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/blob/luminance/SCLT/Color/sRGB.hpp&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The other files/classes used are all on this repository (branch luminance)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/tree/luminance&quot; rel=&quot;nofollow&quot;&gt;https://github.com/chicio/Spectrum-Clara-Lux-Tracer/tree/luminance&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Thanks you all guys, I hope someone could help me.&lt;/p&gt;&#xA;" OwnerUserId="2237" LastEditorUserId="2237" LastEditDate="2016-03-20T23:21:29.253" LastActivityDate="2016-03-21T14:18:13.233" Title="Spectral path tracing - image color/brightness incorrect" Tags="&lt;color&gt;&lt;color-science&gt;&lt;brightness&gt;&lt;pathtracing&gt;&lt;global-illumination&gt;" AnswerCount="1" CommentCount="15" FavoriteCount="2" />
  <row Id="2208" PostTypeId="1" CreationDate="2016-03-21T10:34:33.770" Score="0" ViewCount="44" Body="&lt;p&gt;I have an .STL file (ascii and binary format) that contains several different CAD models. How can I read the file and create separate .stl files for each of the single different models. Even some hints/reference would be helpful.&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="231" LastEditDate="2016-04-21T11:19:54.067" LastActivityDate="2016-04-21T11:19:54.067" Title="Model Separation - Several models reside in a single .stl file" Tags="&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2209" PostTypeId="2" ParentId="2208" CreationDate="2016-03-21T10:58:05.043" Score="1" Body="&lt;p&gt;Software recommendation are not on topic. Instead I will explain how you separate the shapes. Each object is a separate volume so you can just extract all the continuous shells. You can do this simply by picking a vertex marking it as visited and then picking all connected vertices and marking them visited. You continue this visiting to new un-visited vertices until you no longer can find a connected vertex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a shell, in most cases a shell is a separate object. Unless there is a empty non connected cavity inside the object. This is rare but possible in 3d printing though not most other manufacturing methods. In this case you need to check that the shells do not intersect (they shouldn't or it means your model is broken anyway, nothing to fix) and if a shell lies inside another. You can do this by picking a point on the suspected interior shell and shooting a ray in a arbitrary direction. The shells that you hit a odd number of times are surrounding your main shell.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-03-21T10:58:05.043" CommentCount="0" />
  <row Id="2210" PostTypeId="1" AcceptedAnswerId="2211" CreationDate="2016-03-21T12:18:35.620" Score="14" ViewCount="2244" Body="&lt;p&gt;Apparently Macintosh computers cannot handle the Oculus Rift, because of their 'inferior' graphics cards. But should VR not just be like an external monitor? And concerning computer graphics, how are VR games any different from other games the Macintosh can run easily?&#xA;Where is the problem?&lt;/p&gt;&#xA;" OwnerUserId="2915" LastEditorUserId="48" LastEditDate="2016-03-21T17:59:26.230" LastActivityDate="2016-03-22T11:34:42.403" Title="How is VR different from a monitor" Tags="&lt;virtual-reality&gt;" AnswerCount="4" CommentCount="1" />
  <row Id="2211" PostTypeId="2" ParentId="2210" CreationDate="2016-03-21T12:25:53.360" Score="13" Body="&lt;p&gt;VR requires a much higher resolution and framerate, and a much lower latency (start of draw to showing on screen).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If any of those things are missing then people tend to get nauseated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;All of those require much higher bandwidth and compute power than Apple's cards are supposed to be able to handle.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="-1" LastEditDate="2016-03-22T00:21:05.530" LastActivityDate="2016-03-22T00:21:05.530" CommentCount="2" />
  <row Id="2212" PostTypeId="2" ParentId="2206" CreationDate="2016-03-21T14:18:13.233" Score="5" Body="&lt;p&gt;The problem lies mainly in &lt;code&gt;CIE1931XYZ::tristimulusValues()&lt;/code&gt; function, where you normalize the resulting color to the luminance of your illuminant which causes that directly observed light source has luminance 1, but everything else is much darker. That is a nice thing to do if you just want to visualize colours of various reflectance spectra under a given illumination, but is probably not the best thing to do in a global illumination renderer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be more specific, you cannot expect a reflectance profile to be rendered always completely white just because it reflects all incident energy (constant spectral reflectance equal to 1 everywhere). Neglecting the material directional properties (BSDF, etc.), the properties of the light reflected from a point depend both on reflectance spectral profile of the material, and on the spectral and angular properties of the incident light. The incident light therefore depends not only on the SPD of the illuminant used for your light source, but also on its size and distance from the shaded point. The points closer to the light source will be brighter, while distant points will be darker.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would suggest removing the luminance normalization in your Spectrum-to-XYZ conversion completely and then adjusting the overall brightness of your scene by multiplying the normalized SPD of the used illuminant (D65 in your case) with some brightness parameter. Whether you multiply it before rendering, during rendering or during the XYZ conversion step is completely up to you. The choice shouldn't affect the result.&lt;/p&gt;&#xA;" OwnerUserId="2479" LastActivityDate="2016-03-21T14:18:13.233" CommentCount="7" />
  <row Id="2213" PostTypeId="2" ParentId="2210" CreationDate="2016-03-21T15:43:27.157" Score="3" Body="&lt;p&gt;A external monitor in most cases is only one viewport. Means you have a monoscopic image displayed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Having a HMD (Head-Mounted Display) like Oculus Rift requires you to render your virtual scene for both eyes in order to have stereoscopic 3D. This is essential for the immersion and having immersion is the only reason to use a HMD.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But rendering two viewports (one for each eye) does take additional computational power. Taking into account what ratchet freak already said it is even harder to maintain the high framerate and resolution at a low latency.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To overcome this you need serious computational power and well some people think the hardware used by Apple does not provide enough of it.&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="-1" LastEditDate="2016-03-22T05:38:25.193" LastActivityDate="2016-03-22T05:38:25.193" CommentCount="0" />
  <row Id="2214" PostTypeId="2" ParentId="2210" CreationDate="2016-03-21T20:19:13.870" Score="6" Body="&lt;p&gt;A VR headset is a monitor in terms of connections that just requires special software, but the issue is more of software and the capabilities of Mac hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You are most likely referring to the founder of Oculus, Palmer Luckeey, and his comment on Macs. He said:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;You can buy a $6,000 Mac Pro with the top of the line AMD FirePro&#xA;  D700, and it still doesn't match our recommended specs. So if they&#xA;  prioritize higher-end GPUs like they used to for a while back in the&#xA;  day, we'd love to support Mac. But right now, there's just not a&#xA;  single machine out there that supports it.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;What he means is that not a single Mac has the high end gaming hardware necessary to provide the performance needed for a quality VR experience like the Rift.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VR requires a very tight coupling of high performance hardware to provide low latency imagery. Macs have actually been behind other computers in terms of computer graphics performance and quality. A VR game requires a higher framerate than the average game, with minimums being much higher than the 30 or 60 frames per second that most normal games aim for. VR games need lower latency than the 60 Hz latency of 16.67 ms to be immersive and help prevent motion sickness. These requirements are not flexible. 60 Hz is doable, but it's often seen as the minimum, with many reporting sickness and physical symptoms from framerates of 60 Hz or lower that are reduced or minimized with higher framerates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another factor is simply of market size. The percentage of the Mac market that has hardware capable of handling higher end games at all is an even smaller segment of an already smaller market. Apple's most popular products do not contain high end graphics chips, and instead use lower end integrated or mobile chips.&lt;/p&gt;&#xA;" OwnerUserId="2920" LastEditorUserId="2920" LastEditDate="2016-03-21T21:00:52.663" LastActivityDate="2016-03-21T21:00:52.663" CommentCount="0" />
  <row Id="2217" PostTypeId="2" ParentId="2210" CreationDate="2016-03-22T11:34:42.403" Score="8" Body="&lt;p&gt;Macs are not gaming machines.  They never have been.  There was a time maybe 10-15 years ago when you could build a mid-tier gaming Mac, at significantly higher expense than a PC, but Macs have never catered to anything more than very casual gamers.  They don't run other games &quot;easily&quot; - they just adequately run &lt;em&gt;some&lt;/em&gt; games, &lt;em&gt;barely&lt;/em&gt; run others, and simply can't run a lot of popular titles (if they exist at all).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every Mac, with two exceptions, uses integrated graphics on their Intel chips.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The MacBookPro can optionally can be fitted with a Radeon R9 370X, and the MacPro has the FirePro option above.  Let's look at 3D performance of these various options (The FirePro D700 is a Mac special - it's roughly in line with the specs of the W9000 PC variant):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/fiOdu.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fiOdu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The multi-thousand dollar FirePro is sitting there next to an old GTX 670 that you can pick up in used bargain bins these days.  So what gives?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The FirePro is a professional GPU - it competes with NVidia's Quadro and Tesla line of cards.  It is designed specifically for precision workstation GPU tasks, particularly for double-precision calculations.  These are important for CAD/Architechtural and Engineering applications as well as for scientific computing.  The cards &lt;strong&gt;can&lt;/strong&gt; be used just fine to play 3D games, but they are not optimized for that task and naturally underperform when compared to high end gaming GPUs like the GTX970/980, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gaming GPUs dedicate resources to lower precision calculations (which they can do more of, and more quickly) and eye-candy goodies.  The tasks which they set themselves to are largely at odds with each other in terms of the style of hardware that is needed to get the job done.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you wanted to build a gaming mac right now, you really can't.  None of the systems available even have slots anymore to add in aftermarket GPUs (and they were twice the price of PC GPUs back in the days when you could, and came out months and months later than their PC counterparts so were always behind the cutting edge).  The only expandable Mac right now is the MacPro, and it is so ludicrously expensive that you would be crazy to try to make a gaming machine out of it (in fact, the hardware simply can't be had).  With expensive Xeon processors and FirePro graphics, the system is a highly specialized workstation that is firmly aimed at creative, scientific, and engineering professionals.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So where does that leave the Mac?  Absolutely, completely out of the gaming arena - full stop.  It's got enough juice to run a few very casual games at very modest quality for home users, but that's just about it.  The Oculus Rift needs major GPU horsepower.  For 3D it needs to compute every scene from two unique perspectives (one for each eye), instantly doubling the amount of pre-rendering calculation it needs to do.  It then needs to render each of these independent scenes to two independent displays and keep everything moving fast and smooth enough to not make you want to vomit or have some sort of seizure.  That means it needs a powerful gaming GPU, and these just aren't available for the Mac anywhere.&lt;/p&gt;&#xA;" OwnerUserId="2936" LastActivityDate="2016-03-22T11:34:42.403" CommentCount="0" />
  <row Id="2218" PostTypeId="2" ParentId="2160" CreationDate="2016-03-22T13:34:43.987" Score="1" Body="&lt;p&gt;&lt;a href=&quot;https://www.shadertoy.com/&quot; rel=&quot;nofollow&quot;&gt;Shadertoy&lt;/a&gt; is a good website for playing with real-time visualizations. It uses WebGL and an in-page code editor, so you can edit a fragment shader and then run it live in your browser. One feature that was added (relatively) recently is the ability to connect a shader input to an audio stream - either from a pre-defined source, or from your microphone. You can also output sound from your shader by writing to a special &lt;code&gt;vec2&lt;/code&gt; output. Here's &lt;a href=&quot;https://www.shadertoy.com/results?query=&amp;amp;sort=popular&amp;amp;filter=soundinput&quot; rel=&quot;nofollow&quot;&gt;a list of examples that use mic input&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastActivityDate="2016-03-22T13:34:43.987" CommentCount="0" />
  <row Id="2219" PostTypeId="1" AcceptedAnswerId="2220" CreationDate="2016-03-22T18:44:38.660" Score="6" ViewCount="135" Body="&lt;p&gt;I clip a 3D triangle against a 3D Axis-Aligned Bounding Box (AABB) to obtain the largest planar polygon of the triangle contained in the AABB. My clipping algorithm is a (slightly modified) version of the robust (e.g. clipping planes have a small finite thickness) Sutherland-Hodgman algorithm as described in C. Ericson's Real-Time Collision Detection. I clip the triangle against each of the 6 planes constituting the AABB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to avoid heap (de)allocation, I allocated a fixed size point buffer on the stack in advance for all the vertices of the obtained planar polygon. My question now is: what is the maximum number of vertices possible one can obtain after clipping a triangle against an AABB? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on the control flow, every examined vertex can result in two vertices during a polygon plane clipping. Thus $3*2^6$ vertices. Due to symmetry this becomes $3*2^3=24$ vertices. However, I always obtain less vertices in practice.&lt;/p&gt;&#xA;" OwnerUserId="2287" LastEditorUserId="127" LastEditDate="2016-03-22T20:09:13.100" LastActivityDate="2016-03-22T20:09:13.100" Title="Maximum number of vertices after clipping a triangle against an AABB" Tags="&lt;3d&gt;&lt;geometry&gt;&lt;clipping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2220" PostTypeId="2" ParentId="2219" CreationDate="2016-03-22T19:22:55.100" Score="8" Body="&lt;p&gt;Funnily enough, I asked this exact question on Math.SE a couple years ago: &lt;a href=&quot;http://math.stackexchange.com/questions/859454/maximum-number-of-vertices-in-intersection-of-triangle-with-box/&quot;&gt;Maximum number of vertices in intersection of triangle with box&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The answer is 9 vertices, because each of the 6 planes of the box can cut off one corner of the polygon, replacing one vertex with two. So 3 vertices + 6 added vertices due to clipping = 9 total.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-22T19:22:55.100" CommentCount="0" />
  <row Id="2221" PostTypeId="1" AcceptedAnswerId="2294" CreationDate="2016-03-23T14:26:57.420" Score="1" ViewCount="98" Body="&lt;p&gt;Currently for a university assignment I have been given a 3D rendering engine, capable of rendering a 3D world. One task is to create a first person camera for it (I know with OpenGL you don't have a camera, you have to effect the modelview). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyway, I have managed to get a first person camera working, however rotation is becoming a bit of a problem. I have spent a good few hours trying to fix the problem, but can't seem to find a solution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rotation works perfectly, until you move forward about -10 into the z axis. When you do, the camera rotation seemingly rotates around a set point, so it orbits the actual model, rather than just changing the direction the camera is looking at. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if you move out, then rotate, the camera will move around the object, not just move the object out of sight. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Not sure what code you'll need to see to help, but here's some: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Camera Rotation:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void Camera::RotateCamera(float yaw, float pitch, float roll)&#xA;{&#xA;//left and right rotation&#xA;Quaternion a;&#xA;Quaternion b;&#xA;Quaternion c;&#xA;Quaternion up;&#xA;Quaternion right;&#xA;Quaternion temp;&#xA;float s;&#xA;float vx;&#xA;float vy;&#xA;float vz;&#xA;&#xA;if (yaw != 0)&#xA;{&#xA;    s = cos(yaw / 2);&#xA;    c.SetQuaternion(m_direction, s);&#xA;    vx = m_upVector[0] * sin(yaw / 2);&#xA;    vy = m_upVector[1] * sin(yaw / 2);&#xA;    vz = m_upVector[2] * sin(yaw / 2);&#xA;    a.SetQuaternion(vx, vy, vz, s);&#xA;    b.SetQuaternion(vx, -vy, -vz, s);&#xA;&#xA;    up = a*c*b;&#xA;&#xA;    m_direction.SetVector(up[0], up[1], up[2]);&#xA;}&#xA;&#xA;//up and down rotation&#xA;if (pitch != 0 &amp;amp;&amp;amp; curDown &amp;lt; maxDown &amp;amp;&amp;amp; curDown &amp;gt; -maxDown)&#xA;{&#xA;    curDown += pitch;&#xA;    s = cos(pitch / 2);&#xA;&#xA;    a.SetQuaternion(m_direction, s);&#xA;    vx = m_rightVector[0] * sin(pitch / 2);&#xA;    vy = m_rightVector[1] * sin(pitch / 2);&#xA;    vz = m_rightVector[2] * sin(pitch / 2);&#xA;    b.SetQuaternion(vx, vy, vz, s);&#xA;    c.SetQuaternion(-vx, -vy, -vz, s);&#xA;&#xA;    right;&#xA;    temp;&#xA;    right = b * a * c;&#xA;    m_direction.SetVector(right[0], right[1], right[2]);&#xA;}&#xA;m_direction.Normalise();&#xA;m_rightVector = m_direction.CrossProduct(m_upVector);&#xA;m_upVector = m_rightVector.CrossProduct(m_direction);&#xA;&#xA;m_rightVector.Normalise();&#xA;m_upVector.Normalise();&#xA;UpdateViewMatrix();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The quaternion class is correct (so I'm assuming as it was already created for us). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Camera rotation code&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (x - ox &amp;gt; 0)&#xA;{&#xA;    cam_pos-&amp;gt;RotateCamera(0.05, 0, 0);&#xA;}&#xA;else if (x - ox &amp;lt; 0)&#xA;{&#xA;    cam_pos-&amp;gt;RotateCamera(-0.05, 0, 0);&#xA;}&#xA;&#xA;ox = x;&#xA;oy = y;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Applying the camera&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;modelview = (*cam_pos-&amp;gt;GetViewMatrix()) * modelview;&#xA;modelview = modelview*translation;&#xA;&#xA;float aspect_ratio = (float)m_width / (float)m_height;&#xA;projection.SetPerspective(60.0f, aspect_ratio, 1.0f, 1000.0f);&#xA;&#xA;glUniformMatrix4fv( m_uniform_projection, 1, GL_FALSE, projection.ToPtr() );&#xA;glBindSampler(0, m_texDefaultSampler);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;*cam_pos&lt;/code&gt; is the actual camera. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code works, just rotation goes off.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is what currently happens on rotation: &#xA;&lt;a href=&quot;http://i.stack.imgur.com/5ptng.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5ptng.jpg&quot; alt=&quot;weird rotation effect&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2953" LastEditorUserId="2953" LastEditDate="2016-03-23T19:39:30.300" LastActivityDate="2016-04-08T19:21:25.237" Title="OpenGL C++ Camera Rotation Problem" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;c++&gt;&lt;maths&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2222" PostTypeId="1" AcceptedAnswerId="2225" CreationDate="2016-03-23T20:28:13.507" Score="4" ViewCount="77" Body="&lt;p&gt;I am trying to write a ray tracer to render boxes that are arbitrarily rotated, i.e. not necessarily axis aligned. While I am reasonably comfortable ray tracing axis-aligned bounding boxes (AABB), I don't know how to handle non-axis aligned objects. The geometry becomes quite difficult. I've looked through a textbook (&lt;em&gt;Ray Tracing from the Ground Up&lt;/em&gt;) but couldn't find an answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How should I do it? Detailed suggestions would be greatly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="2954" LastEditorUserId="209" LastEditDate="2016-03-24T15:38:23.423" LastActivityDate="2016-03-24T15:38:23.423" Title="How to convert Non-Axis Aligned Bounding Boxes to AABB" Tags="&lt;raytracing&gt;&lt;geometry&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2223" PostTypeId="1" CreationDate="2016-03-24T01:21:13.737" Score="0" ViewCount="114" Body="&lt;p&gt;Windows 10 ships with &quot;3D Builder&quot;, a Universal App that contains utility functions to prepare STL, OBJ, 3DS, and other files that represent geometries for 3D printing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The utility looks like this:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/N3VXd.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/N3VXd.png&quot; alt=&quot;3D Builder&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In particular, they have triangle mesh functions there that:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&quot;Detect&quot; when a mesh is not suitable for 3D printing;&lt;/li&gt;&#xA;&lt;li&gt;&quot;Repair&quot; so that the mesh isn't right it prepares it, by removing internal faces, closing &quot;holes&quot; in the outer surface, etc.;&lt;/li&gt;&#xA;&lt;li&gt;&quot;Simplify&quot; by removing redundant triangles, such as those found to be adjacent and co-planar; and&lt;/li&gt;&#xA;&lt;li&gt;&quot;Plane Cut&quot;, slicing a mesh through an arbitrarily positioned plane and filling in a surface on that plane to re-enclose the geometry, sometimes followed by a &quot;Detect&quot; and &quot;Repair&quot;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Every copy of Windows 8.1 and 10 has this utility at no additional cost. That says to me that the functions are well-known and as far as I've been able to test, reliable for virtually all inputs to the 3D printing use case. I used some of the messiest STL files I could find to prove this thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Which algorithms did they use for those functions? Where are they found for a .NET/WPF/UWP environment?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More to the point, did they expose the functions in an API I can use for a slightly dissimilar use case? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the builder the functions look like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simplify (the tractor top and sides have fewer facets):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qAAr2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qAAr2.png&quot; alt=&quot;Simplify Output&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Slice, UI before processing:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/GYHQV.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/GYHQV.png&quot; alt=&quot;Slice, UI before processing&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Slice, after processing and then one &quot;Simplify&quot; pass:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/PDt95.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/PDt95.png&quot; alt=&quot;Slice, after processing&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2957" LastEditorUserId="48" LastEditDate="2016-03-24T18:45:08.370" LastActivityDate="2016-04-27T09:59:41.317" Title="Which 3D algorithms does Windows 10's &quot;3D Builder&quot; application use?" Tags="&lt;algorithm&gt;&lt;3d&gt;&lt;mesh&gt;&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2224" PostTypeId="1" CreationDate="2016-03-24T01:30:05.193" Score="2" ViewCount="33" Body="&lt;p&gt;I need to find an approach to the problem of generating texture maps for non-convex polyhedrons without using a design tool like Maya. Specifically, I am mapping simulation results data onto a 3D surface, and I need to write code which generates the maps.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One generic approach uses a gradient color brush as an overall texture to just map colors, but if the facets in the polyhedron are very narrow or the mapped (u,v) coordinates are very far apart on the same facet, the image gets noisy. Another approach employs Phong or Gouraud Shading, but those features are missing in some 3D libraries which favor texturing over specifying colors at each vertex. So, I'm looking for a way to get a good texture map generated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it helps in identifying a correct algorithm, all of the surfaces entirely enclose a 3-D space, there is no facet normal data, and I'll be able to identify a &quot;parting line&quot; which segregates one part of the surface from the other, as one might in order to create a mold for the shape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The simulation results are uniform (non-sparse) 3D arrays containing single precision values. There is a neighboring array of short integers which approximates the 3D mesh; a &quot;1&quot; value in that array corresponds to a part of the enclosed region of the polyhedron. I'm mapping to non-convex 3D surfaces more complex than can be mapped with a simple unrolling texture mapper or a spherical ray casting approach. Think of an American gallon milk jug or similar shapes that can be produced in plastic or metal molds.&lt;/p&gt;&#xA;" OwnerUserId="2957" LastEditorUserId="48" LastEditDate="2016-03-24T18:45:05.230" LastActivityDate="2016-03-24T18:45:05.230" Title="Generate complex (non-convex) polyhedron UV mapping" Tags="&lt;texture&gt;&lt;algorithm&gt;&lt;3d&gt;&lt;computational-geometry&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2225" PostTypeId="2" ParentId="2222" CreationDate="2016-03-24T07:59:15.040" Score="6" Body="&lt;p&gt;You can assign a coordinate system to each nAABB in such a way that the nAABB becomes an AABB in its own coordinate system. We call this a &lt;em&gt;local coordinate system&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I assume rays are expressed in a &lt;em&gt;world&lt;/em&gt; or &lt;em&gt;global coordinate system&lt;/em&gt;. In order to test an nAABB for intersection, one first needs to apply the &lt;em&gt;world-to-local transformation&lt;/em&gt; on the ray (origin and direction). This way we obtain a transformed ray which we can intersect with an AABB. Once an intersection point is found, we need to transform this back to world space via the inverse transformation (i.e. &lt;em&gt;local-to-world transformation&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In practice an nAABB is thus expressed as an AABB with a pair of world-to-local and local-to-world transformation matrices.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If one applies a scaling ($S$) or translation ($T$) on an AABB, one obtains another AABB. If one applies a rotation ($R$) on an AABB, one obtains a nAABB.&#xA;Typically the corresponding transformation matrices are multiplied as follows $T * R * S$. Thus translation is applied after rotation. Since the $R$ component must be included in the world-to-local transformation, you need to include the $T$ component as well. The $S$ component can be included or directly applied to a unit AABB centered at the origin of the world-coordinate system. Including a $T$ component requires your world-to-local and local-to-world transformation matrices to be of size $4$x$4$ (expressed in &lt;em&gt;homogeneous coordinates&lt;/em&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/mmp/pbrt-v2/blob/master/src/core/transform.cpp&quot;&gt;Here&lt;/a&gt; you can find C++ code to construct all the $4$x$4$ world-to-local matrices and their inverses you probably need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this approach works for other geometrical objects as well (if the transformations involve scaling, rotation and translation).&lt;/p&gt;&#xA;" OwnerUserId="2287" LastEditorUserId="2287" LastEditDate="2016-03-24T08:24:24.030" LastActivityDate="2016-03-24T08:24:24.030" CommentCount="0" />
  <row Id="2226" PostTypeId="5" CreationDate="2016-03-24T19:00:26.560" Score="0" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_geometry&quot; rel=&quot;nofollow&quot;&gt;Computational geometry&lt;/a&gt; encompasses a wide variety of topics, including:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Modeling surfaces and solids using meshes, splines, and other geometry representations&lt;/li&gt;&#xA;&lt;li&gt;Generating geometry from implicit or parametric functions, point sets, or other data&lt;/li&gt;&#xA;&lt;li&gt;Processing geometry to extract information about orientation, topology, curvature, or other quantities&lt;/li&gt;&#xA;&lt;li&gt;Performing operations on geometry such as tessellation or triangulation, simplification, CSG operations, feature extraction, UV mapping, and cleaning up mesh errors&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-03-24T19:00:26.560" LastActivityDate="2016-03-24T19:00:26.560" CommentCount="0" />
  <row Id="2227" PostTypeId="4" CreationDate="2016-03-24T19:00:26.560" Score="0" Body="Problems involving meshes and other geometry representations, and manipulating, transforming, or extracting information from them; algorithms for solving geometrical problems such as computing intersections, filling holes, triangulating a shape, etc." OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-03-24T19:00:26.560" LastActivityDate="2016-03-24T19:00:26.560" CommentCount="0" />
  <row Id="2228" PostTypeId="5" CreationDate="2016-03-24T19:02:43.390" Score="0" Body="" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-03-24T19:02:43.390" LastActivityDate="2016-03-24T19:02:43.390" CommentCount="0" />
  <row Id="2229" PostTypeId="4" CreationDate="2016-03-24T19:02:43.390" Score="0" Body="Questions and problems dealing with three-dimensional space, including 3D meshes and other data structures, vector math, transformations, etc." OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-03-24T19:02:43.390" LastActivityDate="2016-03-24T19:02:43.390" CommentCount="0" />
  <row Id="2231" PostTypeId="1" AcceptedAnswerId="2232" CreationDate="2016-03-25T02:13:12.723" Score="15" ViewCount="3219" Body="&lt;p&gt;Games and other graphically intensive applications use frameworks like OpenGL and DirectX. Also they require features like pixel shader and DX12.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But why would we need all these frameworks and GPU features when we could just draw everything pixel by pixel?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, the game would have to be compiling in a way so it is drawn pixel by pixel. This is likely to make the game executable big, but will it be faster and work on any 32-bit color GPU (even old ones)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that the first 3D games were drawn pixel by pixel, but why aren't they doing it now?&lt;/p&gt;&#xA;" OwnerUserId="2964" LastEditorUserId="127" LastEditDate="2016-03-25T09:40:55.547" LastActivityDate="2016-03-27T12:44:07.743" Title="Why do we have graphics frameworks like OpenGL and DirectX, when games could just draw pixels directly?" Tags="&lt;3d&gt;&lt;gpu&gt;&lt;performance&gt;&lt;pixel-shader&gt;&lt;pixels&gt;" AnswerCount="3" CommentCount="5" FavoriteCount="3" />
  <row Id="2232" PostTypeId="2" ParentId="2231" CreationDate="2016-03-25T06:40:42.843" Score="19" Body="&lt;p&gt;Speed is the most common reason why this is &lt;strong&gt;not&lt;/strong&gt; done. In fact you can do what you propose, if you make your own operating system, its just going to be very slow for architectural reasons. So the assumption that its faster is a bit flawed. Even if it would be faster, it would be less efficient in terms of development (like 1% speed increase for 10 times the work).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Copying the data over from the CPU to the graphics card is a relatively slow operation. The less you copy the faster your update speed can be. So ideally you would have most of the data on your GPU and only update small chunks of data. There is a world of difference between copying over 320x200 pixels compared to 1920x1200 or more. See the number of pixels you need to update grows quadratically when the sides grow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example: It's cheaper to tell the GPU to move the image 10 pixels right than copy the pixels manually to the video memory in different locations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why do you have to go trough an API? Simply because it's not your system. The operating system can not allow you to do whatever you want for safety reasons. Secondly because the operating system needs to abstract hardware away, even the OS is talking to the driver trough some abstracted system, an API if you will.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact I would rate the likelihood that your system would be faster, if you just did all the work yourself, close to near zero. It's a bit like comparing C and assembly. Sure you can write assembly, but compilers are pretty smart these days and optimize better and better all the time. It's hard to be better manually, even if you can your productivity will be down the drains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: An API does not make it impossible to do this update just like old games did it. It's just inefficient that's all. Not because of the API mind but because it is inefficient period.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PPS: This is why they are rolling out Vulkan.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="137" LastEditDate="2016-03-25T15:48:11.280" LastActivityDate="2016-03-25T15:48:11.280" CommentCount="17" />
  <row Id="2233" PostTypeId="2" ParentId="2231" CreationDate="2016-03-25T08:14:09.057" Score="12" Body="&lt;p&gt;Just to add to &lt;a href=&quot;http://computergraphics.stackexchange.com/a/2232/&quot;&gt;joojaa's answer&lt;/a&gt;, things &lt;em&gt;are&lt;/em&gt; still being drawn pixel by pixel. You're just generating the pixels using a vertex shader/assembler/rasterizer, then texturing and lighting them using a fragment shader. This was all done in software in the 90's when your video card wasn't much more than a blitter and a frame buffer, but it was slow as hell. Hence the invention of modern GPUs. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The drawing maths that's happening is basically the same as it was back in the Doom days, but now it's running on hundreds/thousands of shader ALUs rather than a handful of CPU cores. The APIs map down to basically the same set of GPU instructions behind the scenes. They're just there to stop you having to write a ton of gnarly GPU assembly across multiple vendor platforms.&lt;/p&gt;&#xA;" OwnerUserId="1937" LastEditorUserId="127" LastEditDate="2016-03-25T09:32:34.763" LastActivityDate="2016-03-25T09:32:34.763" CommentCount="0" />
  <row Id="2234" PostTypeId="2" ParentId="2231" CreationDate="2016-03-25T13:04:06.423" Score="14" Body="&lt;p&gt;&lt;em&gt;work on any 32-bit color GPU (even old ones)?&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bit of history here: this is how games were done on PC up until graphical accelerators started to become available in the mid-90s. It did indeed work on all hardware, because the hardware wasn't doing much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A graphical accelerator allows the drawing of pixels considerably &lt;em&gt;faster&lt;/em&gt; than a CPU can, by using specialized hardware and parallelism. The accelerator contains a number of processor cores. A desktop PC will have between 1-8 cores depending on age. My GTX970Ti graphics card has 1664 (one thousand six hundred and sixty-four!) cores. This obviously beats the PC for raw speed by a long way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, accelerators aren't standardized, and often include weird computer architecture tricks in order to achieve their speed. In order to write a game which isn't customized to a specific make and model of the card, an API needs to exist. And that's what DirectX, GL and the shader languages are used for. In fact, writing shaders &lt;em&gt;is&lt;/em&gt; the closest thing to writing a program that draws pixels directly - it's just that the card will run a thousand copies of that program for you in parallel, one per pixel.&lt;/p&gt;&#xA;" OwnerUserId="2971" LastActivityDate="2016-03-25T13:04:06.423" CommentCount="9" />
  <row Id="2235" PostTypeId="2" ParentId="67" CreationDate="2016-03-25T21:25:36.990" Score="4" Body="&lt;p&gt;If one examines old CRT television displays, one will observe red, green, and blue phosphor dots in a triangular lattice.  Some LCD television sets had a somewhat similar arrangement; pixels were rectangular rather than square, but successive rows of pixels were staggered so that the horizontal position of a red pixel on one row would be halfway between the positions of the nearest red pixels on the row above, and would match the position of a red pixel two rows up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When showing &quot;analog&quot; pictures, such arrangements will improve the visual quality of image that can be obtained with a given number of pixels.  On computer displays, however, there are a lot of lines that should appear perfectly horizontal, and a lot of lines that should appear perfectly vertical.  A rectangular grid can accommodate both easily.  A hexagonal grid, however, can only accommodate one or the other, and the one which isn't accommodated will appear rather nastily jagged.&lt;/p&gt;&#xA;" OwnerUserId="2984" LastActivityDate="2016-03-25T21:25:36.990" CommentCount="1" />
  <row Id="2236" PostTypeId="2" ParentId="65" CreationDate="2016-03-25T21:41:43.557" Score="4" Body="&lt;p&gt;Imagine that one is rendering a picture of a flat floor with a uniform black and white checkerboard pattern that extends to the horizon; the checkers are large enough that they should be clearly visible at points near the camera but not large enough to be distinguishable near the horizon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Near the horizon, the floor should simply appear as uniform gray.  Near the camera, the checkers should appear distinct.  Between the camera and the horizon the appearance of the floor must somehow transition between those two extremes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the scene is rendered a spacial filter that has a very sheep cut-off, there will be a certain distance where the floor goes from being checkered to being gray.  If one uses a shallower filter, the transition will be much more gradual, but things near the original &quot;cut-off&quot; distance will be less sharp than they would have been otherwise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If one were to add a &quot;wall&quot; or crop the scene to hide the distant parts of the floor, such that there was no need to have any parts of the checkered floor blurred to gray, the best results would be obtained by using the steepest filter, yielding the sharpest image.  Using a shallower filter would give up image sharpness for the purpose of preventing a nasty transition that wasn't going to be visible anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Figuring out what kind of filtering to use thus requires that one know something about the spacial frequency content of the information to be displayed.  If the image contains nothing of interest that would approach Nyquist, using a steep filter will produce the sharpest results.  If, however, image content exceeds Nyquist, using a gradual filter will avoid ugly &quot;transitions&quot;.  No single approach will be optimal for all cases.&lt;/p&gt;&#xA;" OwnerUserId="2984" LastActivityDate="2016-03-25T21:41:43.557" CommentCount="0" />
  <row Id="2237" PostTypeId="1" CreationDate="2016-03-26T16:57:36.530" Score="1" ViewCount="122" Body="&lt;p&gt;I did a quick investigation about the topic but there doesn't seem a decent resource to find related problems without digging into latest CG papers (unlike CS problems you can easily find a Wikipedia list)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With open problems I mean phenomenas that still do not have a tractable solution/approximation to find its way into CG and better yet real time CG.&lt;/p&gt;&#xA;" OwnerUserId="8" LastEditorUserId="2287" LastEditDate="2016-03-28T07:28:02.187" LastActivityDate="2016-03-28T07:28:02.187" Title="What are the current open problems in Computer Graphics?" Tags="&lt;real-time&gt;&lt;physically-based&gt;&lt;render&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2239" PostTypeId="2" ParentId="2237" CreationDate="2016-03-26T17:45:40.710" Score="4" Body="&lt;p&gt;Computer Graphics can be divided in multiple subdomains of which I will only talk about physically-based rendering (the one I am the most familiar with and probably the one you are referring to based on the tags of the question). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Physically-based rendering&lt;/strong&gt; (also referred to as global illumination) is &lt;em&gt;generally&lt;/em&gt; far from real time at the moment. The speed we achieve is in fact no way close to what we actually try to simulate: the propagation of light through scenes (see &lt;a href=&quot;https://waskul.tv/portfolio/steve-waskul-ravi-ramamoorthi-2013/&quot; rel=&quot;nofollow&quot;&gt;interview with prof. Ravi Ramamoorthi&lt;/a&gt;). Scenes are rendered for over multiple hours or even days, far below the actual speed of light. Current research in this domain does however not aim at these speeds, a non-flickering framerate perceived by a human observer will do. Since the equations we try to solve are defined a long time ago, effort is mostly put in optimizing current algorithms which can be at a very high (introducing new formalisms) or low level (changing the right 1s to 0s and vice versa).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the ones who actually want to dive in the literature, see this wonderful up-to-date &lt;a href=&quot;http://kesen.realtimerendering.com/&quot; rel=&quot;nofollow&quot;&gt;site&lt;/a&gt; of the papers accepted at the most important conferences in the field of computer graphics.&lt;/p&gt;&#xA;" OwnerUserId="2287" LastEditorUserId="2287" LastEditDate="2016-03-26T18:18:42.260" LastActivityDate="2016-03-26T18:18:42.260" CommentCount="0" />
  <row Id="2240" PostTypeId="1" AcceptedAnswerId="2249" CreationDate="2016-03-26T23:22:58.243" Score="0" ViewCount="86" Body="&lt;p&gt;Im stuck on this question asking me to calculate the intensity of a pixel using Ray Tracing. It gives values such as I&lt;sub&gt;i&lt;/sub&gt; but doesnt give any coordinates. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know for the diffuse component, I&lt;sub&gt;i&lt;/sub&gt;k&lt;sub&gt;d&lt;/sub&gt;(n.l) can be written as I&lt;sub&gt;i&lt;/sub&gt;k&lt;sub&gt;d&lt;/sub&gt;(cos(Theta)) but for the specular component I&lt;sub&gt;i&lt;/sub&gt;k&lt;sub&gt;s&lt;/sub&gt;(h.n) is there any other way to write it so that it doesnt require any vectors such as h and n??&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the question:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/3s0AC.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3s0AC.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2359" LastEditorUserId="2287" LastEditDate="2016-03-28T08:59:07.193" LastActivityDate="2016-03-29T17:20:31.733" Title="phong equation of illumination specular component" Tags="&lt;raytracing&gt;&lt;specular&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="2242" PostTypeId="2" ParentId="2237" CreationDate="2016-03-27T15:48:30.603" Score="8" Body="&lt;p&gt;There is a vast amount of open problems in real-time graphics: shadows, aliasing, reflections, global illumination, transparencies (blending order and lighting) etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SIGGRAPH annually hosts a course called &quot;Open Problems in Real-Time Rendering&quot;, which describes current issues in real-time graphics (mostly game dev industry standpoint). You can find last year's materials here: &lt;a href=&quot;http://openproblems.realtimerendering.com&quot;&gt;http://openproblems.realtimerendering.com&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="93" LastActivityDate="2016-03-27T15:48:30.603" CommentCount="0" />
  <row Id="2243" PostTypeId="2" ParentId="2187" CreationDate="2016-03-28T08:47:03.953" Score="2" Body="&lt;p&gt;This is a hard problem since the systems do not in fact tell you how it should operate. The problem is that there are several ways to address this issue:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Apply a LUT to all output&lt;/li&gt;&#xA;&lt;li&gt;Let the system color manage&lt;/li&gt;&#xA;&lt;li&gt;Let each application color manage&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Nearly all of our color management systems are borderline braindead. There is really no sane way to ensure that you are doing the proper thing without extensively testing the system. You can not ask or specify your intent. In case of multi operating system situations. If there is reason for me to color manage something myself im at the mercy of each operating system peculiarities.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a alternate discussion on this see this &lt;a href=&quot;http://stackoverflow.com/questions/23026151/do-i-need-to-gamma-correct-the-final-color-output-on-a-modern-computer-monitor&quot;&gt;stackoverflow post&lt;/a&gt;. But its not really a robust enough talk to cover all cases.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-03-28T08:47:03.953" CommentCount="2" />
  <row Id="2244" PostTypeId="2" ParentId="2223" CreationDate="2016-03-28T09:55:53.623" Score="1" Body="&lt;p&gt;Well obviously nobody except the makers of this program can answer the exact algorithms used. I have done a quick explanation on each. Each separate task really deserves its own question and answer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just iterate each polygon, for each vertex check if all of them are above the cut line eliminate the face. To make this check easy you can transom each vertex to the plane orientation. If not all vertices are above the cut line then the triangle is split along one of the edge that has a eliminated vertex. Then use the internal face closing algorithm. &lt;a href=&quot;http://geomalgorithms.com/a06-_intersect-2.html&quot; rel=&quot;nofollow&quot;&gt;Heres some math&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Face closing algorithm works by finding open edges and then tessellating the new polygon closed. Please note: That this produces somewhat erratic results if the open holes are not planar. See &lt;a href=&quot;https://en.wikipedia.org/wiki/Polygon_triangulation&quot; rel=&quot;nofollow&quot;&gt;polygon triangulation&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Simplifying planar regions is basically the same face closing algorithm except they you first build up polygons out of regions. Also decimation might come to play. There is a lot of lore, and different algorithms for simplification and going trough all of it its way too bread for a post like this. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Self intersection is a sorting problem. Quite a bit of lore on this exists too. The one that i mostly do is putting the polygons one by one into a BSP self organizing tree and checking against polygons against items in that tree to reduce the check complexity to &lt;em&gt;n&lt;/em&gt; log &lt;em&gt;n&lt;/em&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Internal cavities can be detected by splitting each mesh into shells and then for each shell check if the other shell is contained within each other. See &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/2208/model-separation-several-models-reside-in-a-single-stl-file/2209#2209&quot;&gt;this post&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-03-28T09:55:53.623" CommentCount="0" />
  <row Id="2245" PostTypeId="1" CreationDate="2016-03-29T11:19:37.950" Score="1" ViewCount="20" Body="&lt;p&gt;In my graphics engine, based on OpenGL ES 2.0 I am using standardized name for shader attributes like it is done in libGDX (such as a_position, a_normal, etc.).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When 'activating' my vertex buffer for drawing, just before I call &lt;code&gt;glVertexAttribPointer()&lt;/code&gt; I need to enable an attribute I'm going to use by calling &lt;code&gt;glEnableVertexAttribArray()&lt;/code&gt;, and then disable unused ones. Let's say there's two shaders I'm gonna use:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Shader1 is simple texture shader, which single attribute is &lt;code&gt;a_position_uv&lt;/code&gt; which is vec4, &lt;code&gt;xy&lt;/code&gt; are for position, and the remaining &lt;code&gt;zw&lt;/code&gt; are for texture coordinates (attribute location is bound by name to 0).&lt;/li&gt;&#xA;&lt;li&gt;Shader2 is solid shader, which single attribute is &lt;code&gt;a_position&lt;/code&gt; which is &lt;code&gt;vec2&lt;/code&gt; (attribute location is bound by name to 1).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Then something strange happens. When I'm drawing with Shader1, I'm enabling 0th, and disabling 1st attribute, and it worked just fine. But then, when I'm drawing with Shader2 (and obviously I'm enabling 1st, and disabling 0th attributes) it draws nothing. Just nothing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact: when I'm not disabling 0th attribute for solid shader it works fine. In both cases the remaining code is identical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In fact (2): this happens only when deploying game on devices (iPhone simulator too).  I'm not a newbie in graphics programming and OpenGL, but in this case the behaviour is very strange for me.&lt;/p&gt;&#xA;" OwnerUserId="333" LastActivityDate="2016-03-29T11:19:37.950" Title="Strange behaviour of glDisableVertexAttribArray() on devices" Tags="&lt;opengl&gt;&lt;shader&gt;&lt;opengl-es&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2246" PostTypeId="1" CreationDate="2016-03-29T12:48:44.053" Score="1" ViewCount="39" Body="&lt;p&gt;I was about to scale this image down to 64x64 pixels:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/jAk2Z.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jAk2Z.jpg&quot; alt=&quot;this&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The result I got on MS Paint on Windows 7 was surprisingly good (i.e. crisp edges):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/RaQLu.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RaQLu.png&quot; alt=&quot;ms paint rescaled&quot;&gt;&lt;/a&gt; , whereas scaling it with GIMP resulted in &lt;a href=&quot;http://i.stack.imgur.com/8rPmx.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8rPmx.png&quot; alt=&quot;cubic GIMP&quot;&gt;&lt;/a&gt; for cubic and &lt;a href=&quot;http://i.stack.imgur.com/EQ33f.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/EQ33f.png&quot; alt=&quot;lanczos3 GIMP&quot;&gt;&lt;/a&gt; for Lanczos3 interpolation, both of which look significantly worse to me. Linear and nearest-neighbor was obviously much worse still.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also written a small program to check all of the interpolation modes available &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/system.drawing.drawing2d.interpolationmode.aspx&quot; rel=&quot;nofollow&quot;&gt;in the System.Drawing.Drawing2D.InterpolationMode enumeration&lt;/a&gt;, to no avail. Unfortunately, most web search results on the topic of interpolation in MS Paint are still from the days before Vista, when nearest neighbor was the only option.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know which interpolation algorithm the Windows 7 version of MS Paint uses?&lt;/p&gt;&#xA;" OwnerUserId="3019" LastActivityDate="2016-03-29T12:48:44.053" Title="Which interpolation algorithm does MS Paint on Windows 7 use for image rescaling?" Tags="&lt;algorithm&gt;&lt;interpolation&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2247" PostTypeId="1" AcceptedAnswerId="2254" CreationDate="2016-03-29T13:24:36.920" Score="2" ViewCount="38" Body="&lt;p&gt;I'm making a game that is supposed to like like a early 90s 3D game, so i'm rendering in software on an 8-bit indexed bitmap, using lookup tables for translucency and lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I have to draw a translucent, lit sprite in front of an already-lit background (the light level of said background is lost at that point). I'm unsure whether to first darken all the pixels and then average them with the background or the other way round.&lt;/p&gt;&#xA;" OwnerUserId="3020" LastActivityDate="2016-03-30T18:16:14.740" Title="In what order to apply lighting and translucency?" Tags="&lt;rendering&gt;&lt;texture&gt;&lt;real-time&gt;&lt;lighting&gt;&lt;transparency&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2249" PostTypeId="2" ParentId="2240" CreationDate="2016-03-29T17:20:31.733" Score="3" Body="&lt;p&gt;Assuming that the classic Phong model is desired here, the dot product that goes into the specular calculation should be R·L, rather than N·H (which was introduced by Blinn). That is, Phong calculates specular using the angle between the reflected eye vector and the light vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the diagram you posted, these vectors are shown and the angle is given as 15&amp;deg;. So, R·L = cos(15&amp;deg;) = 0.96 according to the table below the diagram.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-29T17:20:31.733" CommentCount="0" />
  <row Id="2251" PostTypeId="1" AcceptedAnswerId="2252" CreationDate="2016-03-30T11:16:43.963" Score="4" ViewCount="61" Body="&lt;p&gt;This question bothers me since the first time I got to know about derivatives in pixel shader. Suppose we have a 2d texture:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;5   10  20  30&#xA;6   11  50  100&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I correctly understand, the derivatives for the pixel [0, 0] will be:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;dx = pixel[1, 0] - pixel[0, 0] = 10 - 5 = 5&lt;/li&gt;&#xA;&lt;li&gt;dy = pixel[0, 1] - pixel[0, 0] = 6 - 5 = 1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;And this will be 100% guaranteed because gpu executes pixels in 2x2 block. But what is the &lt;strong&gt;dx-derivative&lt;/strong&gt; for the pixel[1, 0]? The pixel right to it is from another block which maybe not executed yet? And what about pixel[3, 0]? It doesn't have a right neighbor at all!&lt;/p&gt;&#xA;" OwnerUserId="386" LastActivityDate="2016-03-30T11:35:24.423" Title="Gpu derivatives. How it's done across 2x2 boundary?" Tags="&lt;gpu&gt;&lt;pixel-shader&gt;&lt;derivatives&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2252" PostTypeId="2" ParentId="2251" CreationDate="2016-03-30T11:35:24.423" Score="7" Body="&lt;p&gt;In Vulkan the shader only looks at each 2x2 and won't attempt to look beyond the neighbourhood: &lt;a href=&quot;http://vulkan-spec-chunked.ahcox.com/ch15s05.html&quot;&gt;http://vulkan-spec-chunked.ahcox.com/ch15s05.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$dPdx_{0,0}=dPdx_{1, 0} = P_{1,0}−P_{0,0}\\&#xA;dPdx_{2,0}=dPdx_{3, 0} = P_{3,0}−P_{2,0}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For times when a pixel would fall off the geometry the shader is invoked for the values it would have if the triangle extended a pixel further. &lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-03-30T11:35:24.423" CommentCount="1" />
  <row Id="2253" PostTypeId="2" ParentId="2198" CreationDate="2016-03-30T16:43:19.203" Score="3" Body="&lt;p&gt;This is more of a long comment&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yes you can do this. The problem lies in how you measure the surface. I tried to do this manually by progressively extruding tubes along the edges, and finding their intersection with the surface. It would work but is a bit hard to compute. Obviously you may want to take more steps than i did for a more accurate solution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IbXPj.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IbXPj.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt; Straight skeleton (magenta)  and insets by finding the intersection of tubes along edges, and then the created edges on mesh (blue) &#xA;From this experiment I can conclude that are many complex cases to consider.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-03-30T18:15:13.220" LastActivityDate="2016-03-30T18:15:13.220" CommentCount="0" />
  <row Id="2254" PostTypeId="2" ParentId="2247" CreationDate="2016-03-30T18:16:14.740" Score="4" Body="&lt;p&gt;When blending multiple layers, physically the &quot;right&quot; thing to do is calculating lighting on each layer separately, then composite the lit layers together. This way, for instance, you can have a translucent sprite standing in a spotlight, in front of a dark background, and the sprite will be well-lit while the background visible through it stays dark. Or conversely, you can have a dark sprite silhouetted against a brightly lit background.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you blended the sprite with the background before lighting, it wouldn't be clear which lighting environment you should use if the sprite and background have different ones.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-03-30T18:16:14.740" CommentCount="0" />
  <row Id="2255" PostTypeId="1" CreationDate="2016-03-30T19:32:11.707" Score="6" ViewCount="85" Body="&lt;p&gt;My goal is to create 3D meshes without the use of a 3D editor.  So I want to know what representation languages and formats there are that are designed for this or otherwise that would be good for this.  I would prefer one that is open and widely supported.  Something like a 3D version of SVG or Postscript would be ideal.&lt;/p&gt;&#xA;" OwnerUserId="3031" LastEditorUserId="3031" LastEditDate="2016-03-30T21:44:32.470" LastActivityDate="2016-03-31T14:28:35.870" Title="Human Readable/Writable 3D Representation Formats/Languages?" Tags="&lt;3d&gt;&lt;mesh&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2256" PostTypeId="2" ParentId="2255" CreationDate="2016-03-30T21:35:06.380" Score="6" Body="&lt;p&gt;There are a number of formats that might fit the bill. Depends on what you want to achieve.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most likely your looking for a scene description language for renderers. Many of them are for that one renderer but at least one is a standard. So you might be looking for something like:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;RIB, &lt;a href=&quot;https://renderman.pixar.com/products/rispec/rispec_pdf/RISpec3_2.pdf&quot; rel=&quot;nofollow&quot;&gt;Renderman Bytestream&lt;/a&gt; although it comes with as a programming api as well. This is a based on a standard set by Pixar and is used by a number of render engines some of which are open source.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Most other rendering solutions also have textual formats I'm somewhat familiar with the mi (mental ray text or binary stream) formats and the now archaic Povray formats. But many others exist.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;It is a bit debatable whether these really are human readable but given that you specify SVG then I guess they are at least and readable as that. Just more complex. Note like SVG these describe scenes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we have &lt;code&gt;dae&lt;/code&gt; also known as &lt;a href=&quot;https://www.khronos.org/collada/&quot; rel=&quot;nofollow&quot;&gt;collada&lt;/a&gt;. Which is sort of human readable has a similar niche as &lt;code&gt;RIB&lt;/code&gt; but is aimed for game assets. A number of other somewhat similar formats exist like the mostly deprecated &lt;a href=&quot;http://gun.teipir.gr/VRML-amgem/spec/index.html&quot; rel=&quot;nofollow&quot;&gt;VRML&lt;/a&gt; format, and its successors.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we have a bunch of polygon formats, many of them are binary. But atleast &lt;code&gt;obj&lt;/code&gt; and &lt;code&gt;ply&lt;/code&gt; are human readable and widely accepted. Obj is a very simple format. Although polygon meshes are seldom really easy to understand or craft manually if they are more complex. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we have a bunch of application specific formats. For example the &lt;code&gt;ma&lt;/code&gt;, Maya ascii, format which is human readable if you understand how maya node hierarchy works. Cant say much of the others.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then we have the CAD transport formats &lt;code&gt;IGES&lt;/code&gt; and &lt;code&gt;STEP&lt;/code&gt;. I wouldn't go and claim either to be human readable. But I have hand crafted both on a occasion. Both of these are ISO standards, although STEP is insanely big and was in fact the biggest of ISO standards in page count last time I checked.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are quite many of these formats because 3D is not as uniform as 2D is. But, yes, I have hand crafted all of these at one point or another. Does not mean i would consider doing so as a productivity tool. Unlike EPS or SVG, which I craft bi weekly by hand. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I would have to guess your motives then i would say RIB is probably what you want. Or then your looking to use obj.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-03-31T14:28:35.870" LastActivityDate="2016-03-31T14:28:35.870" CommentCount="1" />
  <row Id="2258" PostTypeId="1" CreationDate="2016-03-31T10:35:49.703" Score="1" ViewCount="27" Body="&lt;p&gt;I am trying to align two motion capture files from different sources.&#xA;The motion capture file consist of a skeleton specified as a hierarchy of joints. Each joint i.e. &quot;shoulder&quot; has three pieces of data: rotation order (XYZ), a translation plus direction from a parent joint (the bone vector) and an axis, which is a 3-tuple of rotations $(X, Y, Z)$ that describe the transformation from the parent coordinate system to the local coordinate system. For each frame, a local rotation 3-tuple $m_x, m_y, m_z$ is specified which describes how the bone vector changes from frame to frame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To calculate the position of a particular bone for a single frame, I calculate:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$M = M_x \times M_y \times M_z$ where $M_x$, $M_y$, $M_z$ are the rotation matrices correspond to $m_x$, $m_y$ and $m_z$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$L = C^{-1} \times M \times C$, where $C$ is the multiplication of 3 $X$, $Y$, $Z$ rotation matrices corresponding to the bone axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is done for each bone in the hierarchy (i.e shoulder -&gt; clavicle -&gt; spine -&gt; root) and multiplied together to get the global rotation matrix for a particular bone. The bone vector is then multiplied by this rotation matrix to get the $x, y, z$ position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is now I have two motion capture files with corresponding joints: say, &lt;code&gt;shoulderA&lt;/code&gt; and &lt;code&gt;shoulderB&lt;/code&gt;. Since the joint axes are different, the local per-frame rotations are also different, which means I can't use the frame data for file B for skeleton A. I would like to &quot;refer&quot; &lt;code&gt;shoulderB&lt;/code&gt; to &lt;code&gt;shoulderA&lt;/code&gt;, so I'm wondering how I can change the per-frame rotations to do so.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Reference: &#xA;&lt;a href=&quot;http://research.cs.wisc.edu/graphics/Courses/cs-838-1999/Jeff/ASF-AMC.html&quot; rel=&quot;nofollow&quot;&gt;http://research.cs.wisc.edu/graphics/Courses/cs-838-1999/Jeff/ASF-AMC.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3036" LastEditorUserId="182" LastEditDate="2016-04-09T09:27:12.450" LastActivityDate="2016-04-09T09:27:12.450" Title="Coordinate system transformation in motion capture files" Tags="&lt;animation&gt;&lt;motion&gt;&lt;capture&gt;&lt;matrix&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2259" PostTypeId="1" CreationDate="2016-03-31T15:38:59.380" Score="3" ViewCount="23" Body="&lt;p&gt;What I am interested in doing is taking a 360 degree stereo video (Oculus Rift etc), and a description of how the viewing direction changes with time (rather than reading from gyroscopes) and then rendering that to a simple side by side rectangular video (so that you can stick the result on a smartphone and view it with the inbuilt video viewer).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This will naturally involve something like sphere mapping the source, and rendering the result, as the smartphone apps do. I am interested in doing this transformation offline. Is there software around that already does this? Or what are the most suitable libraries to use if I want to program it myself?&lt;/p&gt;&#xA;" OwnerUserId="3038" LastActivityDate="2016-03-31T15:38:59.380" Title="Converting from 360 degree vr to rectangular" Tags="&lt;virtual-reality&gt;&lt;conversion&gt;&lt;video&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="2262" PostTypeId="5" CreationDate="2016-04-03T18:04:03.003" Score="0" Body="" OwnerUserId="127" LastEditorUserId="127" LastEditDate="2016-04-03T18:04:03.003" LastActivityDate="2016-04-03T18:04:03.003" CommentCount="0" />
  <row Id="2263" PostTypeId="4" CreationDate="2016-04-03T18:04:03.003" Score="0" Body="For questions specifically about the Geometry Shader stage in the GPU pipeline." OwnerUserId="127" LastEditorUserId="127" LastEditDate="2016-04-03T18:04:03.003" LastActivityDate="2016-04-03T18:04:03.003" CommentCount="0" />
  <row Id="2264" PostTypeId="1" AcceptedAnswerId="2266" CreationDate="2016-04-03T20:15:54.950" Score="6" ViewCount="59" Body="&lt;p&gt;I recently watched &lt;a href=&quot;https://www.youtube.com/watch?v=LKnqECcg6Gw&quot;&gt;this&lt;/a&gt; video that talked from a physics perspective how most of the ways we deal with color on the computer is incorrect because brightness is on a logarithmic not linear scale.  Being a novice graphics programmer I want to know how to deal with the information to make my graphics even better. I figure ray-tracers but for rasterizing, blur, grading, and light adjusting like HDR what am I supposed to do with this information?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I guess I have some questions. (sorry its a lot)&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Most texture data is stored in a format where the rgb brightness is incorrect because it has not been square rooted (right?) therefore if we want to display it correctly &lt;/li&gt;&#xA;&lt;li&gt;When using textures is the gaussian formula technically incorrect? If so what would be the mathematically correct method.&lt;/li&gt;&#xA;&lt;li&gt;What part of this problem is the monitor responsible for? Does it square or squareroot the values it gets?&lt;/li&gt;&#xA;&lt;li&gt;What part of the problem is the GPU responsible for when you ask it to sample a color at a point? Does it square or squareroot the values it gets?&lt;/li&gt;&#xA;&lt;li&gt;Do people know about this property and just not act on it because square rooting a number is an expensive computation? If so do we have a good way to quickly approximate the squareroot of numbers from 0-1?&lt;/li&gt;&#xA;&lt;li&gt;How do you apply the knowledge from that video in your graphics?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2308" LastActivityDate="2016-04-04T05:41:06.400" Title="How are we supposed to fix brightness with square roots?" Tags="&lt;physics&gt;&lt;color-science&gt;&lt;blur&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2266" PostTypeId="2" ParentId="2264" CreationDate="2016-04-04T05:41:06.400" Score="10" Body="&lt;p&gt;What the video is talking about is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_correction&quot;&gt;gamma correction&lt;/a&gt; and it's a very familiar topic for graphics programmers. The first 30 minutes of John Hable's &lt;a href=&quot;http://www.gdcvault.com/play/1012351/Uncharted-2-HDR&quot;&gt;talk on Uncharted 2 rendering&lt;/a&gt; is my favorite introduction to &quot;why graphics programmers should care about gamma&quot;, as well as being a good introduction to HDR rendering. If you don't want to watch the talk (though I highly recommend it), &lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html&quot;&gt;The Importance of Being Linear&lt;/a&gt; is also a good article, and Charles Poynton has a &lt;a href=&quot;http://www.poynton.com/notes/colour_and_gamma/GammaFAQ.html&quot;&gt;Gamma FAQ&lt;/a&gt; that goes into more technical detail and explains the terminology used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way, square/squareroot is only an approximation; the &lt;a href=&quot;https://en.wikipedia.org/wiki/SRGB&quot;&gt;sRGB gamma curve&lt;/a&gt; is what's most commonly used in practice (see the formulas there mentioning $C_\text{srgb}$ and $C_\text{linear}$). See also &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/68/is-gamma-still-important-to-take-into-account&quot;&gt;this related question&lt;/a&gt; about gamma and color spaces. It's a bit more complicated of a topic than the video makes it sound. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer your specific questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Yes, most image data is stored in gamma format, so the pixel values do &lt;em&gt;not&lt;/em&gt; represent linear luminance. I wouldn't describe it as &quot;incorrect&quot;, since gamma encoding has the useful property of putting more values in the darks, which is better for perceptual precision (as mentioned in the video). Gamma encoding is absolutely necessary to make good-looking images with only 8 bits per RGB component; if you stored linear values you would need more like 12 bits per component to get enough precision in the darks.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;To be more physically correct, when blurring an image, one should convert it to linear luminance, then do the blur (Gaussian or otherwise), then convert it back to the original color space.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The monitor accepts image data in gamma format. So the pixel values do not represent linear luminance, and the output brightness is not linearly proportional to the pixel values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it's a good-quality, calibrated monitor, then it accepts data in a specific color space such as sRGB or Adobe RGB and reproduces the colors accurately based on the definition of that color space. If it's a crap uncalibrated monitor, then who knows what you'll get, but it'll be something &lt;em&gt;kinda&lt;/em&gt; like sRGB.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;GPUs can be configured to do various things here. Modern GPUs have hardware support to automatically convert values between gamma and linear when sampling a texture, or storing a value out to the framebuffer, but the programmer has to explicitly enable that using an API such as Direct3D or OpenGL.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Computer graphics people certainly do know about this topic, and it's critical to get it right when building a realistic renderer for film, AAA games, and so on. However, a lot of more &quot;everyday&quot; apps like web browsers and image editors (even Photoshop, as the video mentions) don't really handle gamma correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's not an issue of processing power; maybe that was a problem 20 years ago, but today even mobile phones have ample processing power to convert images between gamma and linear. The reason why web browsers and image editors still don't get this right is some combination of (a) ignorance, (b) not believing that gamma is important enough to be worth getting right, and (c) simply being stuck with bad behavior because it's been that way for a long time and users have come to expect it, so they freak out if something changes.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;As mentioned, we have to think a lot about gamma when building a renderer. Most computations (e.g. lighting) should be done in linear space, but the input textures and output image should be in gamma space. Also, maybe some parts of rendering such as UI compositing should happen in gamma space because it's more familiar to artists (it works more like Photoshop layer compositing and such). Also, we have to decide if internal render targets are going to be stored in linear or gamma space, as it will affect precision. And we have to get all the conversions right, so that data gets converted between linear and gamma at the right spots in the rendering process.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-04T05:41:06.400" CommentCount="1" />
  <row Id="2267" PostTypeId="1" AcceptedAnswerId="2269" CreationDate="2016-04-04T12:16:01.040" Score="5" ViewCount="34" Body="&lt;p&gt;In &lt;a href=&quot;https://hal.inria.fr/hal-01024289/document#page=8&quot;&gt;Understanding the Masking-Shadowing Function in Microfacet-Based BRDFs, Section 2.2&lt;/a&gt;, Mr Eric Heitz defines the distribution of normals as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zgYQM.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zgYQM.png&quot; alt=&quot;Definition of D&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then, he goes on with the following assertion:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/cDXlW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cDXlW.png&quot; alt=&quot;Property of D&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I fail to understand how can D have such a property. Is it an assumption based on common sense or on geometrical considerations? Or can this equality be proven by manipulating the formulation of the integral?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2016-04-05T05:46:47.927" Title="Why does the integral of NDF over a solid angle equals the area where micronormals belong to that angle?" Tags="&lt;distribution&gt;&lt;micronormal&gt;&lt;integral&gt;&lt;equals&gt;&lt;microsurface&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2269" PostTypeId="2" ParentId="2267" CreationDate="2016-04-05T05:15:23.133" Score="4" Body="&lt;p&gt;$D(\omega)$ is defined as the area ($m^2$ unit in the numerator) of the microsurface with normals pointing in the direction $\omega$. $\mathcal{M}'$ is defined as the portion of the microsurface with normals point in the direction $\omega \in \Omega'$. So it's natural that the integral of $D(\omega)$ over $\Omega'$ gives the area of $\mathcal{M}'$.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Substituting the definition of $D$ into (4) gives:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega'} \int_{\mathcal{M}} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A fundamental property of the Dirac delta function is that it integrates to 1 over its domain. Unfortunately, we are integrating over $\Omega'$, not $\Omega$. Here I will stop pretending that I am actually able to use formal mathematical terms, and I will just note that the integrand has non-zero values only where both $d p_m \in \mathcal{M'}$ and $d\omega_m \in \Omega'$, which is what (3) says. As long as at least one of our integration domains is restricted to the set we care about, we will get the same result. So I'll just move the prime around a tad:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega} \int_{\mathcal{M}'} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now we can apply the Dirac delta identity, since we're integrating over the whole domain:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\mathcal{M}'} d p_m$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ta-da!&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2016-04-05T05:46:47.927" LastActivityDate="2016-04-05T05:46:47.927" CommentCount="0" />
  <row Id="2270" PostTypeId="1" AcceptedAnswerId="2272" CreationDate="2016-04-05T08:42:57.990" Score="0" ViewCount="74" Body="&lt;p&gt;1) Update the front buffer will cause screen tearing, every one knows it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) But V-Sync, I think it still get the same issue. Why I get this idea? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;2.1) When GPU flips to the back buffer, current screen displays the old image which was in previous front buffer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2.2) Now GPU sends image from &quot;current front buffer&quot; (previous back buffer) to monitor. This will last 16.7ms (60Hz). During this period, Human eyes will see half &quot;current front buffer&quot; and half &quot;previous front buffer&quot;. Is it right?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2.3) If it is right, it would be same as non-V-sync.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) But, in fact, when I switch &quot;V-sync&quot; on, I can't see screen tearing. Why?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot for the answers.&lt;/p&gt;&#xA;" OwnerUserId="3058" LastActivityDate="2016-04-05T09:18:37.017" Title="Can V-Sync avoid tearing? I think it can't. Am I wrong?" Tags="&lt;rendering&gt;&lt;gpu&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2271" PostTypeId="1" CreationDate="2016-04-05T09:09:45.963" Score="0" ViewCount="89" Body="&lt;p&gt;According to the following assumptions, I think Cloud based VR would be the future:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) Real world scene rendering needs ray tracing or other complex computing graphics technologies. It needs a lot of computing power.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Single GPU can't afford enough GFLOPS for that. And the powerful GPU is costly and hot. Since the moore's law is ending soon, the process of GPU will also end evolution, that means GPU can't get much faster than needed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) Mobility is the king. But mobile GPU speed will be limited by the power and process. It won't get much faster too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) Cloud gaming would be promise for that. The network bandwidth and latency would be the biggest issues. Since Google Fiber has been deployed, I don't think network is the issue in the future.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;5) Rendering a real world needs unlimited computing power, but human eye resolution is limited. Limited resolution means limited network bandwidth consumption. For example, 1Gbps is enough to transmite 8K/16K H265 streaming video in stereo format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I believe Cloud based VR (Rendering in Cloud and stream to the client) would be the future. Am I correct?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;added more detail @20160406&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I did some research on it. The motion-to-photon has to be less than 20ms. Maybe only 5ms is left for round trip network. So that the Cloud DC has to be very close to the client. In case optical fiber access, only two hops ( switch and BRAS ) to local router and then to local Cloud DC. (Transmission speed in optical is 200K KM/s, so 100KM radius would take only 0.5ms. That means Cloud DC would cover 30K square KM area.)If no network congestion with good QoS policy, the network latency would be very low. At the mean time, use distributed GPU clusters to render the VR content and then stream it to the client in parallel to reduce the time further. Let's count the time consumptions during each stages. 1.5ms for head tracking and local cpu processing. 1.5ms for network transmission to Cloud. 1.5 ms for distributed GPU cluster rendering and encoding. 5ms for network transmission to client. 1ms for decoding with special decoder since current video decoder designed for video playing not for VR. 5ms for transmission to monitor (equal 200Hz monitor, or very high bandwidth transmission as display port 1.4). The total time is 1.5+1.5+1.5+5+1+5 = 15.5ms. Of course, if using Foveated Rendering technology, addition 3ms for eye tracking delay, but the network bandwidth and latency will be reduced.&lt;/p&gt;&#xA;" OwnerUserId="3058" LastEditorUserId="3058" LastEditDate="2016-04-06T01:03:53.073" LastActivityDate="2016-04-06T17:07:41.003" Title="Cloud based VR would be the future?" Tags="&lt;gpu&gt;&lt;virtual-reality&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="2272" PostTypeId="2" ParentId="2270" CreationDate="2016-04-05T09:18:37.017" Score="3" Body="&lt;p&gt;V-sync means that the GPU won't switch buffer until the current frame is done showing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you tell to swap the buffer with V-Sync enabled the actual swap doesn't happen until V-Blank, a small window between frames where nothing is being sent to the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason I think you cannot see the update tear is that it moves across the screen during the update. While a non v-sync tear stays in the same place on the screen for 16 milliseconds.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-04-05T09:18:37.017" CommentCount="3" />
  <row Id="2273" PostTypeId="2" ParentId="2271" CreationDate="2016-04-05T11:06:05.663" Score="3" Body="&lt;p&gt;Strictly speaking I think that the latency coming from transferring data over the network will not be low enough for VR. The device will need to send all the head tracking data and the user input over the network to the cloud based rendering farm. Then that data and input would need to be processed, rendered and the video streamed back to the user. Can you imagine playing a game and the the actual game stopping for buffering?&lt;/p&gt;&#xA;" OwnerUserId="204" LastActivityDate="2016-04-05T11:06:05.663" CommentCount="1" />
  <row Id="2275" PostTypeId="1" AcceptedAnswerId="2277" CreationDate="2016-04-06T09:16:19.873" Score="0" ViewCount="82" Body="&lt;p&gt;In virtual reality, the motion-to-photon time is very important. Oculus says it has to be less than 20ms. Maybe 10~15ms is better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some people try to introduce Frameless rendering technology into VR. I think it's good to reduce the latency. But I don't know how the display quality is. According to the paper: &lt;a href=&quot;http://www.cs.virginia.edu/~luebke/publications/pdf/afr.egsr.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;Adaptive Frameless Rendering&quot;&lt;/a&gt; and the video (&lt;a href=&quot;https://www.youtube.com/watch?v=ycSpSSt-yVs&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=ycSpSSt-yVs&lt;/a&gt;), it seems not very good. But, in this paper &quot;&lt;a href=&quot;http://discovery.ucl.ac.uk/1474834/1/Construction%20and%20Evaluation%20of%20an%20Ultra%20Low%20Latency%20Frameless%20Renderer%20for%20VR.pdf&quot; rel=&quot;nofollow&quot;&gt;Construction and Evaluation of an Ultra Low Latency Frameless Renderer for VR&lt;/a&gt;&quot;, the authors used FPGA instead of GPU to render and used Frameless Rendering to reduce the latency to 1ms. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my mind, FPGA is very hard to replace GPU in 3D rendering area. And, Frameless Rendering needs &quot;racing the beam&quot;, it is also very hard to program. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What do you think? Is it realistic to use FPGA and frameless rendering?&lt;/p&gt;&#xA;" OwnerUserId="3058" LastEditorUserId="48" LastEditDate="2016-04-06T17:23:15.133" LastActivityDate="2016-04-06T17:52:49.340" Title="Can Frameless rendering reduce latency? And, can FPGA do 3D rendering instead of GPU?" Tags="&lt;rendering&gt;&lt;gpu&gt;&lt;virtual-reality&gt;" AnswerCount="1" CommentCount="5" FavoriteCount="1" />
  <row Id="2276" PostTypeId="2" ParentId="2271" CreationDate="2016-04-06T17:07:41.003" Score="2" Body="&lt;p&gt;As other people have mentioned, due to network latency issues, I think that full rendering on a cloud server and streaming video to a client device is unlikely to be workable. Even if the latency can be kept low enough in a best-case scenario, it would only work for a very limited number of users with very high-quality internet connections. (Don't forget that e.g. in the USA, there are still tens of millions of people on dialup or DSL, with no access to cable internet, let alone fiber. It will be decades still before fiber is widespread.) Also, note that many &lt;a href=&quot;https://en.wikipedia.org/wiki/Cloud_gaming&quot; rel=&quot;nofollow&quot;&gt;cloud gaming services&lt;/a&gt; do exist, but that according to players, input latency is noticeably worse on these services than playing locally on a PC.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, all that being said, rendering entirely on a cloud server is not the only way to approach the problem. The most latency-sensitive aspect of VR is head tracking. Oculus's &lt;a href=&quot;https://developer.oculus.com/blog/asynchronous-timewarp-examined/&quot; rel=&quot;nofollow&quot;&gt;&quot;timewarp&quot;&lt;/a&gt; technology shows that you can quickly and cheaply update a previously rendered image to respond to head rotation, as a 2D image operation: if the user turns their head to the right, you shift the image to the left, and so on. With a depth buffer, you can do positional timewarp as well, although this creates various artifacts with parallax because it's based on a single-layered 2D image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Timewarp is certainly cheap enough to run on a mobile device. So, one approach is to render frames in the cloud, stream the video to the client, then timewarp the video on the client device to obtain very low-latency head tracking. This wouldn't help the latency of other inputs such as keyboard/mouse/controller inputs, but those don't induce nausea when they're delayed, the way delayed head-tracking does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are also other possibilities along the lines of splitting rendering between the cloud and the client. For example, object-space lighting and shading could be done on the cloud and streamed to the client as textures; then the client only needs to render simple textured geometry. This would work better than positional timewarp for getting correct parallax when you move your head around, though you'd still have some artifacts with specular shading and reflections. There are a lot of possibilities for offloading rendering to the cloud in VR-friendly ways.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-06T17:07:41.003" CommentCount="3" />
  <row Id="2277" PostTypeId="2" ParentId="2275" CreationDate="2016-04-06T17:52:49.340" Score="4" Body="&lt;p&gt;First of all, the &quot;frameless rendering&quot; technique is in the context of raytracing, not rasterization. It's not obvious how it could be made to work effectively with rasterization, given that the basic idea of it is to update an image by a combination of temporal reprojection plus firing rays specifically at areas where the algorithm thinks the image is undersampled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So this technique is, prima facie, not compatible with rasterization-based graphics applications. But if you're already doing raytracing for other reasons, this technique would be interesting to look at; it certainly appears to improve quality relative to an image raytraced from scratch each frame, with the same number of rays per second.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Raytracing on a GPU is certainly possible; you don't need an FPGA for that. I've only skimmed the second paper, but my reading of it is that the main reason for the FPGA is to get a close coupling between the display scanout and the rendering activity: namely they &quot;race the beam&quot; and evaluate pixels just before they're about to be scanned out, thus obtaining low latency.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The GPU equivalent of this is probably to split the image in thin horizontal strips, and kick off a compute dispatch to render each strip just before it starts to be scanned out. Today, this is difficult to accomplish as it requires either millisecond-precise scheduling that desktop OSes are not currently set up for, or it requires the GPU to be able to dispatch based on an interrupt from the scanout unit&amp;mdash;a hardware feature that doesn't currently exist AFAIK (or if it does, it isn't exposed in any graphics APIs). Or you might be able to make it work with a long-running asynchronous compute dispatch, if you can find a way to stay in sync with scanout.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, there are obstacles, but they aren't insurmountable, and I think if there was sufficient interest in racing-the-beam-style rendering, then OS and GPU vendors could come up with a way to do it in the future. So I don't think an FPGA is required for this kind of technique to work. On the other hand, the fact that it's based on raytracing is a much bigger obstacle to using it in &quot;real-world&quot; apps and games.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-06T17:52:49.340" CommentCount="1" />
  <row Id="2278" PostTypeId="2" ParentId="2161" CreationDate="2016-04-07T02:45:58.850" Score="3" Body="&lt;p&gt;This bachelor thesis briefly reviews six SSAO techniques. It could be a good start.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CryEngine 2 AO&lt;/li&gt;&#xA;&lt;li&gt;StarCraft II AO&lt;/li&gt;&#xA;&lt;li&gt;HBAO&lt;/li&gt;&#xA;&lt;li&gt;Volumetric Obscurance&lt;/li&gt;&#xA;&lt;li&gt;Alchemy AO&lt;/li&gt;&#xA;&lt;li&gt;Unreal Engine 4 AO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://frederikaalund.com/a-comparative-study-of-screen-space-ambient-occlusion-methods/&quot; rel=&quot;nofollow&quot;&gt;http://frederikaalund.com/a-comparative-study-of-screen-space-ambient-occlusion-methods/&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-07T02:45:58.850" CommentCount="0" />
  <row Id="2279" PostTypeId="1" AcceptedAnswerId="2291" CreationDate="2016-04-07T12:47:27.967" Score="4" ViewCount="224" Body="&lt;p&gt;I am trying to implement for research purposes a gradient-domain path tracer. In order to achieve this step I first need a working Path tracer. I have been creating one so far but results are wrong and I will explain you why. Some concepts:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am working on paths that are generated before sampling them. I mean that the first step of my algorithm consists in calculating a path for a certain pixel (x,y). This path will perform some bounces within the scene and, if terminates on the light, will be considered valid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First of all: the following is the definition of a struct (will be declared and initialized later), containing some scene info needed to cast ray from the camera into the scene.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;struct RenderData&#xA;{&#xA;    vec3 E;&#xA;    vec3 p1;&#xA;    vec3 dx;&#xA;    vec3 dy;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is initialized in the InitializeScene method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void InitializeScene(){ // setup virtual screen plane&#xA;  vec3 E( 2, 8, -26 ); //Eye position&#xA;  vec3 V( 0, 0, 1 ); //LookAt vector&#xA;  float d = 0.5f, ratio = SCRWIDTH / SCRHEIGHT, focal = 18.0f;&#xA;  vec3 p1(E + V * focal + vec3(-d * ratio * focal, d * focal, 0)); // top-left screen corner in SCREEN SPACE&#xA;  vec3 p2(E + V * focal + vec3(d * ratio * focal, d * focal, 0)); // top-right screen corner&#xA;  vec3 p3(E + V * focal + vec3(-d * ratio * focal, -d * focal, 0)); // bottom-left screen corner&#xA;  mat4 M = rotate( mat4( 1 ), r, vec3( 0, 1, 0 ) );&#xA;  p1 = vec3(M * vec4(p1, 1.0f)); //rotating the above points&#xA;  p2 = vec3(M * vec4(p2, 1.0f));&#xA;  p3 = vec3(M * vec4(p3, 1.0f));&#xA;  renderData.dx = (p2 - p1) / (float)SCRWIDTH;&#xA;  renderData.dy = (p3 - p1) / (float)SCRHEIGHT;&#xA;  renderData.E = vec3(M * vec4(E, 1.0f));&#xA;  renderData.p1 = p1;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code above is to make you understand how I initialized the scene.&#xA;I also have structs to store informations about my paths:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;struct PathVert {&#xA;   vec3 p; vec3 n; //hit point and normal of the surface hit &#xA;};&#xA;&#xA;struct Path {&#xA;   PathVert verts[MAX_DEPTH]; //maxDepth is 15 for now&#xA;   int vertCount;&#xA;   int x, y; //which pixel this path is referring to&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;therefore I start consider pixel after pixel.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for (int y = 0; y &amp;lt; SCRHEIGHT; y++) for (int x = 0; x &amp;lt; SCRWIDTH; x++)&#xA;{     &#xA;   Path path;&#xA;   if(generatePath(x,y, path)){&#xA;      Sample(path);&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;generatePath()&lt;/strong&gt; method indeed tracks the path into the scene and checks all the vertices it hits. The &lt;strong&gt;checkIfRayIntersectSomething(t)&lt;/strong&gt; method you will see used, it's just a pseudo method implemented in my framework and that I omit posting cause of its length. I use it to check if my ray hits something in the scene, if it does, it update the &quot;t&quot; with the distance to that object. NOTE: the light is not considered an object itself. Hence, I also have a &lt;strong&gt;checkRayLightIntersection(hitLightPoint)&lt;/strong&gt; which checks the intersection with the light, if there is any, the hitLightPoint is updated with the point on the light I have been hitting.&#xA;The light is a 2D surface.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec lightPos = Vec(5, 15, 2); //hard coded position of the light&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the light, as said, is a surface, but exactly a square surface, whose 4 angles are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec P1 = Vec(lightPos.x - 20, lightPos.y, lightPos.z + 20);&#xA;Vec P2 = Vec(lightPos.x + 20, lightPos.y, lightPos.z + 20);&#xA;Vec P3 = Vec(lightPos.x + 20, lightPos.y, lightPos.z - 20);&#xA;Vec P4 = Vec(lightPos.x - 20, lightPos.y, lightPos.z - 20);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Quite big, I know, so the first question relies on this aspect, is it correct having such a big light?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But let's go to the main methods. Hereby you can see the GeneratePath method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool GeneratePath(int x, int y, Path &amp;amp;path){&#xA;   path.x = x;&#xA;   path.y = y;&#xA;   path.vertCount = 0;&#xA;&#xA;   vec3 P = renderData.p1 + renderData.dx * ((float)(x) + Rand(1)) +  renderData.dy * ((float)(y) + Rand(1));&#xA;   vec3 O = renderData.E + vec3(Rand(0.4f) - 0.2f, Rand(0.4f) - 0.2f, Rand(0.4f) - 0.2f);&#xA;   vec3 D = normalize(P - O); //direction of the first ray, the one from the camera towards the pixel we are considering&#xA;&#xA;   for (int depth = 1; depth &amp;lt;= MAXDEPTH; depth++){&#xA;    float t;&#xA;    Vec hitLightPoint;&#xA;    PathVert vert;&#xA;    if (!checkIfRayIntersectSomething(t)){&#xA;        //we didn't find any object.. but we still may have found the light which is an object non represented in the scene&#xA;        //the depth check avoids me rendering the light as a white plane&#xA;        if (depth &amp;gt; 1 &amp;amp;&amp;amp; checkRayLightIntersection(O, D, hitLightPoint)){&#xA;            //update the vertex since we realized it's the light&#xA;            vert.p = hitLightPoint;&#xA;            vert.n = Vec(0, -1, 0);//cause the light is pointing down&#xA;            path.verts[depth - 1] = vert;&#xA;            path.vertCount++;&#xA;            return true; //light hit, path completed    &#xA;        }&#xA;        return false; //nothing hit, path non valid&#xA;    }&#xA;    //otherwise I got a hit into the scene&#xA;    vert.p = O + D * t; //reach the hitPoint&#xA;    vert.n = methodToFindTheNormal();&#xA;    vert.color = CalculateColor(vert.p); //according to the material properties (only diffuse objects so far)&#xA;    path.verts[depth - 1] = vert;&#xA;    path.vertCount++;&#xA;&#xA;    //since I have the light, and a path terminates when it hits the light, I have to check out also if my ray hits this light,&#xA;    //and if does, I have to check whether it first hits the light or the object just calculated above&#xA;    //moreover with the &quot;depth &amp;gt; 1&quot; check, I avoid again rendering the light which otherwise would be visible as a white plane&#xA;&#xA;    if (depth &amp;gt; 1 &amp;amp;&amp;amp; checkRayLightIntersection(O, D, hitLightPoint)){&#xA;        float distFromObj = length(vert.p);&#xA;        float distFromLight = length(hitLightPoint);&#xA;        if (distFromLight &amp;lt; distFromObj){&#xA;            //update the vertex since we realized it's the light&#xA;            vert.p = hitLightPoint;&#xA;            vert.n = Vec(0, -1, 0);&#xA;            vert.color = Vec(1, 1, 1);// TODO light color? or light emission?&#xA;&#xA;            path.verts[depth - 1] = vert;&#xA;            return true; //light hit, path completed&#xA;        }&#xA;    }&#xA;    if (depth == MAXDEPTH) return false;&#xA;       Vec newDir = BSDFDiffuseReflectionCosineWeighted(vert.n, D);//explained later&#xA;       D = newDir;&#xA;       O = vert.p;&#xA;   }&#xA;   return false;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The BSDFDiffuseReflectionCosineWeighted() just calculate the new directions, tested and working. What remains last is the Sample method which calculates the final color of the pixel.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec Sampling(Path &amp;amp;path){&#xA;&#xA;   Vec color(1, 1, 1);&#xA;&#xA;   for (int vert = 0; vert &amp;lt; path.vertCount - 1; vert++) { //considers the last vertex as the light&#xA;      const PathVert &amp;amp;currVert = path.verts[vert];&#xA;      const PathVert &amp;amp;nextVert = path.verts[vert + 1];&#xA;      Vec wo = (nextVert.p - currVert.p).norm();&#xA;      double cosTheta = fabs(wo.dot(currVert.n));&#xA;      float PDF = cosTheta/PI;&#xA;      if (cosTheta &amp;lt;= 1e-6) return Vec();&#xA;      //considering only DIFFUSE objects&#xA;      color = color.mult(currVert.color * (cosTheta / M_PI) / PDF);&#xA;   }&#xA;   return color.mult(Vec(10.0f, 10.0f, 10.0f)); //multiplication for the light emission?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result with 16SPP is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/HNOah.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HNOah.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see the results are not that bad but there is one main problem: shadows are missing.. tried many combinations but no improvements. There is some error in the algorithm itself. Can you help me understanding why? Thanks in advance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: this is my target reference:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/QHG2z.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/QHG2z.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT 2: this is what happens in case the &lt;strong&gt;depth &gt; 1&lt;/strong&gt; check is omitted:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/hiqEY.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/hiqEY.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT 3: after fixing the &lt;strong&gt;distFromLight&lt;/strong&gt; and &lt;strong&gt;distFromObj&lt;/strong&gt; I got the following results:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JvGhf.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JvGhf.jpg&quot; alt=&quot;some clue of shadows&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One of the 2 problems as suggested by @trichoplax could be too soft shadows. So I tried to lower the size of the light, increasing his power to compensate. Results are not good and shown here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8aYPH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8aYPH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover as requested I can post some more code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool checkRayLightIntersection(Vec O, Vec D, Vec &amp;amp; hitLightPoint){&#xA;  //getting the 4 corners of my light&#xA;  Vec P1 = Vec(lightPos.x - 20, lightPos.y, lightPos.z + 10);&#xA;  Vec P2 = Vec(lightPos.x + 20, lightPos.y, lightPos.z + 10);&#xA;  Vec P3 = Vec(lightPos.x + 20, lightPos.y, lightPos.z - 10);&#xA;  Vec P4 = Vec(lightPos.x - 20, lightPos.y, lightPos.z - 10);&#xA;&#xA;  //the majority of the methods first find out where the ray intersects the plane that the rectangle lies on Ax + By + Cz + D = 0&#xA;  //in our case the equation of that plane is easy -&amp;gt; D = 20&#xA;  // answers.google.com/answers/threadview?id=18979&#xA;&#xA;  float t = -(-O.y + lightPos.y) / (-D.y);&#xA;  if (t &amp;gt; 0){&#xA;    Vec hitPoint = O + D * t;&#xA;    Vec V1 = (P2 - P1).norm();&#xA;    Vec V2 = (P3 - P2).norm();&#xA;    Vec V3 = (P4 - P3).norm();&#xA;    Vec V4 = (P1 - P4).norm();&#xA;    Vec V5 = (hitPoint - P1).norm();&#xA;    Vec V6 = (hitPoint - P2).norm();&#xA;    Vec V7 = (hitPoint - P3).norm();&#xA;    Vec V8 = (hitPoint - P4).norm();&#xA;&#xA;    if (V1.dot(V5) &amp;gt; 0.0 &amp;amp;&amp;amp; V2.dot(V6) &amp;gt; 0.0 &amp;amp;&amp;amp; V3.dot(V7) &amp;gt; 0.0 &amp;amp;&amp;amp; V4.dot(V8) &amp;gt; 0.0){&#xA;        hitLightPoint = hitPoint;&#xA;        return true;&#xA;    }&#xA;  }&#xA;  return false;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Instead, the &lt;strong&gt;checkIfRayIntersectSomething&lt;/strong&gt; method is part of the framework itself (it has just another name), it's not done by me but it's tested and working.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT 4: tried to render a small part of the scene with high number of spp. Tried 512 spp for the bottom right corner where should appear shadows according to the target image. Results show that shadows are not there:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ohZJu.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ohZJu.png&quot; alt=&quot;Bottom right corner at 512spp&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT 5:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I hereby post the code used to render the target image, it does at the same time my &lt;strong&gt;Sampling()&lt;/strong&gt; and &lt;strong&gt;GeneratePath()&lt;/strong&gt; methods using a recursive structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec Sample(vec3 O, vec3 D, int depth)&#xA;{&#xA;  vec3 color(0, 0, 0);&#xA;  float t;&#xA;  if (checkIfIntersectSomething(D, t))&#xA;  {&#xA;&#xA;    vec3 I = O + t * D; //get to the intersection point on the object&#xA;    vec3 diffuse = getMaterial(I);&#xA;&#xA;    vec3 L = vec3(-1 + Rand(2.0f), 20, 9 + Rand(2.0f)) - I; //(-1,20,9) is Hard-code of the light position, and I add Rand(2.0f) on X and Z axis&#xA;    //so that I have an area light instead of a point light&#xA;    L = normalize(L);&#xA;    float ndotl = dot(I.getNormal(), L); //the closer the dotProdutc is to 1.0, the more the light and surface face each other&#xA;    if (ndotl &amp;gt; 0)&#xA;    {&#xA;        if (!checkRayLightIntersection(L)) {&#xA;            float dist = distFromLight(I);&#xA;            color += diffuse * ndotl * vec3(1000.0f, 1000.0f, 850.0f) * (1.0f / (dist * dist));&#xA;        }&#xA;    }&#xA;&#xA;    // russian roulette&#xA;    float Psurvival = CLAMP((diffuse.r + diffuse.g + diffuse.b) * 0.33333f, 0.2f, 0.8f);&#xA;&#xA;    // continue random walk &#xA;    float rand = Rand(1.0f);&#xA;    if (depth &amp;lt; 10 &amp;amp;&amp;amp; rand &amp;lt; Psurvival)&#xA;    {&#xA;        //Besides russian roulette, I also do another weight, because rays that go towards the horizon will bring back very little energy&#xA;        //so I make a random distribution that favours those rays who are close to the normal of the hit point, this is DiffuseReflectionCosineWeighted(). It creates a Random bounce but proportional to N dot R&#xA;        vec3 R = DiffuseReflectionCosineWeighted(I.getNormal());//there is a weight&#xA;&#xA;        float prob = 1.0;&#xA;        float cosTheta = fabs(dot(I.getNormal(), R));&#xA;        if (cosTheta &amp;gt; 1e-6) prob = cosTheta / M_PI;&#xA;&#xA;        color += diffuse * Sample(I + R * EPSILON, R, depth + 1) * (1.0f / Psurvival); //the cosTheta() of the attenuation of the rendering equation gets simplified with the cosTheta of the &quot;prob&quot;&#xA;        //the PI of the prob gets simplified with the BRDF where we are using the ideal BRDF = diffuse/PI&#xA;    }&#xA;}&#xA;return color;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT 6: Trichoplax was right, I had a problem calculating the &lt;strong&gt;distanceFromObject&lt;/strong&gt; and &lt;strong&gt;distanceFromLight&lt;/strong&gt;. After solving this, we realized that some sort of shadows appeared (shown in one of the images above). He then suggested that such a broad light might have been washing out the shadows, and so it was. By reducing the light size and by using a high amount of SPP we realized that shadows are there and visibile: &lt;a href=&quot;http://imgur.com/y88zgAZ&quot; rel=&quot;nofollow&quot;&gt;http://imgur.com/y88zgAZ&lt;/a&gt; . As a drawback tho, way less rays reach the light, giving back an image with high noise, but this is another kind of problem and I created another question for it. Thanks to @trichoplax and @julien for their help&lt;/p&gt;&#xA;" OwnerUserId="3069" LastEditorUserId="3069" LastEditDate="2016-04-17T15:19:19.143" LastActivityDate="2016-04-17T15:19:19.143" Title="Path tracer not rendering shadows" Tags="&lt;c++&gt;&lt;lighting&gt;&lt;shadow&gt;&lt;pathtracing&gt;&lt;global-illumination&gt;" AnswerCount="2" CommentCount="9" />
  <row Id="2280" PostTypeId="2" ParentId="2008" CreationDate="2016-04-07T14:59:33.647" Score="4" Body="&lt;p&gt;The problem with your calculations is that you use a uniform for &lt;em&gt;viewPos&lt;/em&gt; which is the camera position.&#xA;The rest of your calculations is in Camera-Space, so the &lt;em&gt;viewPos&lt;/em&gt; should always be $(0.0, 0.0, 0.0)$ and thereby the $viewDir$ becomes $-1*\text{fragPos}$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In general: Always be careful about the different reference frames your calculations can be done in and aim to keep this consistent.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-04-07T14:59:33.647" CommentCount="0" />
  <row Id="2281" PostTypeId="2" ParentId="2279" CreationDate="2016-04-07T16:55:14.700" Score="4" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;is it correct having such a big light?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I don't see any problem with having a big area light. That said, it also depends on the scale of your scene. If the light is large compared to it, shadows will tend to be more diffuse, like under an overcast sky.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;shadows are missing.. [...] Can you help me understanding why?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I haven't found any blatant mistake, but there are two elements that look suspicious to me.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (depth &amp;gt; 1 &amp;amp;&amp;amp; checkRayLightIntersection(O, D, hitLightPoint)) {&#xA;    // ...&#xA;    return true; //light hit, path completed&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Although this part seems to be correct as far as I can tell, I would try to make sure there is no bug introducing a bias. For every bounce, it checks if there is a ray hitting the light first, and if so it breaks out of the loop: in case the code is incorrect and it happens to hit the light more often than it should, the shadows will disappear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, in the first if:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;return false; //nothing hit, path non valid&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the related loop:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for (int y = 0; y &amp;lt; SCRHEIGHT; y++) for (int x = 0; x &amp;lt; SCRWIDTH; x++)&#xA;{     &#xA;  Path path;&#xA;   if(generatePath(x,y, path)){&#xA;      Sample(path);&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Make sure you still take the paths that hit nothing into account when averaging the color of the pixel, otherwise this will also introduce a bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As joojaa said in the comment, you should try using a simpler scene: a few spheres or cubes, no texture. You can also reduce the number of bounces to one or two to see only direct lighting. If necessary, maybe even use a constant sky dome first instead of a light area.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Other remarks:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;struct Path {&#xA;   PathVert verts[MAX_DEPTH]; //maxDepth is 15 for now&#xA;   int vertCount;&#xA;   int x, y; //which pixel this path is referring to&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you need to store x and y?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float distFromObj = length(vert.p);&#xA;float distFromLight = length(hitLightPoint);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Don't you already have two &lt;code&gt;t&lt;/code&gt; that you can compare?&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="182" LastEditDate="2016-04-11T01:58:24.743" LastActivityDate="2016-04-11T01:58:24.743" CommentCount="1" />
  <row Id="2283" PostTypeId="1" AcceptedAnswerId="2284" CreationDate="2016-04-07T22:22:11.080" Score="6" ViewCount="119" Body="&lt;p&gt;I am trying to implement this formula to generate bump but I am facing some issue. The result doesn't look the same it's much darker.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/kUWt5.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kUWt5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my result (without same parameters) but it is much darker and I don't get why.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9GUF2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9GUF2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And here is the associated code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// randx, randy and frequencies are array with some random values for each sin wave.&#xA;for (int x = 0; x &amp;lt; _width; ++x)&#xA;{&#xA;    for (int y = 0; y &amp;lt; _height; ++y)&#xA;    {&#xA;        float color = 0.0f;&#xA;        for (int i = 0; i &amp;lt; _iterations; ++i)&#xA;        {&#xA;            val += Mathf.Sin(Mathf.Sqrt(Mathf.Pow(x - randx[i], 2.0f) + Mathf.Pow(y - randy[i], 2.0f)) * 1.0f / (2.08f + 5.0f * frequencies[i]));&#xA;        }&#xA;        color /= (float)_iterations;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any idea why I am getting this result ?&lt;br&gt;&#xA;Thanks a lot !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Thanks to @trichoplax it works by doing this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float tmp = Mathf.Sin(Mathf.Sqrt(Mathf.Pow(x - randx[i], 2.0f) + Mathf.Pow(y - randy[i], 2.0f)) * 1.0f / (2.08f + 5.0f * frequencies[i]));&#xA;tmp = tmp * 0.5f + 0.5f;&#xA;val += tmp;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="2372" LastEditDate="2016-04-08T12:17:27.073" LastActivityDate="2016-04-08T12:17:27.073" Title="Sine-based Tiled Procedural Bump" Tags="&lt;texture&gt;&lt;shader&gt;&lt;maths&gt;&lt;procedural-generation&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2284" PostTypeId="2" ParentId="2283" CreationDate="2016-04-07T23:10:28.537" Score="8" Body="&lt;p&gt;As you are taking the mean of a number of sine waves, your colour values will range from -1 to 1. From your example image, it looks like only the top half of this range of values (from 0 to 1) is resulting in colour, with everywhere else remaining black.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If whatever you are using to display the result can only handle positive values, then you will need to convert the result to the correct range of values. For example, to convert from the range [-1 to 1] into the range [0 to 1], you would add 1 and divide by 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the context of your example code, this could be by following the line&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;color /= (float)_iterations;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;with&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;color += 1;&#xA;color /= 2;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="231" LastActivityDate="2016-04-07T23:10:28.537" CommentCount="1" />
  <row Id="2285" PostTypeId="1" AcceptedAnswerId="2289" CreationDate="2016-04-08T05:13:10.747" Score="8" ViewCount="220" Body="&lt;p&gt;As far as I understand, in a BRDF the Fresnel term is telling us the probability for a photon to be reflected or refracted when it hits a surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reflected photons will contribute to the specular term, while the refracted ones will contribute to the diffuse term. Therefore when determining, in a physically based manner, the contribution of a light to the color of material, I feel tempted to just write:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Assuming for example:&#xA;//   diffuse = dot(L, N);&#xA;//   specular = pow(dot(H, N), alpha) * (alpha + 2.0) / 8.0;&#xA;//   fresnel = f0 + (1.0 - f0) * pow(1.0 - dot(E, H), 5.0);&#xA;color = lightIntensity * Lerp(diffuse * albedo, specular, fresnel);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Yet, I don't think I've ever seen it written this way. I have seen the specular term being weighted according to the Fresnel term, but not the diffuse one. In his largely referenced &lt;a href=&quot;https://seblagarde.wordpress.com/2011/08/17/hello-world/&quot;&gt;article on PBR&lt;/a&gt;, Sébastien Lagarde even states that using $(1 - F)$ to weight the diffuse term is incorrect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I missing?&lt;br/&gt;&#xA;I would much welcome an explanation that highlights in an obvious manner why this would be incorrect.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-11T20:25:29.450" Title="How to properly combine the diffuse and specular terms?" Tags="&lt;brdf&gt;&lt;specular&gt;&lt;pbr&gt;&lt;diffuse&gt;&lt;fresnel&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="2" />
  <row Id="2286" PostTypeId="2" ParentId="2285" CreationDate="2016-04-08T05:19:06.857" Score="4" Body="&lt;p&gt;While browsing to properly write my question, I actually &lt;a href=&quot;https://seblagarde.wordpress.com/2011/08/17/hello-world/#comment-2405&quot; rel=&quot;nofollow&quot;&gt;found the answer&lt;/a&gt;, which happens to be very simple.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another Fresnel term is also going to weight in as the photons make their way out of the material (so being refracted into the air) and become the diffuse term. Thus the correct factor for the diffuse term would be:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$(1 - F_{in}) * (1 - F_{out})$$&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-08T05:19:06.857" CommentCount="0" />
  <row Id="2287" PostTypeId="2" ParentId="2010" CreationDate="2016-04-08T06:34:40.257" Score="4" Body="&lt;p&gt;For modern GPUs it's commonly stored in VRAM which for discrete GPUs are separate from the CPU memory. This usually means there is a penalty associated with letting the CPU access this as the data typically has to be copied across some bus. Mobile devices tend to use shared RAM for both GPU and CPU.&lt;/p&gt;&#xA;" OwnerUserId="3073" LastActivityDate="2016-04-08T06:34:40.257" CommentCount="4" />
  <row Id="2288" PostTypeId="1" AcceptedAnswerId="2295" CreationDate="2016-04-08T10:13:43.037" Score="1" ViewCount="80" Body="&lt;p&gt;I want to generate video similar to OSX Flurry screensaver (&lt;a href=&quot;http://i.imgur.com/yEFvLTr.jpg&quot; rel=&quot;nofollow&quot;&gt;screenshot&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/JWzBK87t19U&quot; rel=&quot;nofollow&quot;&gt;video&lt;/a&gt;), but I have no idea how it is made. Does anyone here has an idea about how it's made, its algorithm and method?&lt;/p&gt;&#xA;" OwnerUserId="3074" LastEditorUserId="3074" LastEditDate="2016-04-08T10:24:24.723" LastActivityDate="2016-04-09T09:10:49.433" Title="How to generate OSX Flurry screensaver" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2289" PostTypeId="2" ParentId="2285" CreationDate="2016-04-08T11:41:51.023" Score="7" Body="&lt;p&gt;Using two Fresnel terms is correct in the sense that any given diffuse path will pass through the surface twice. If you're solving diffusion by tracing a path through the medium until it bounces out again then that you will get two (or more) Fresnel terms for that path as it interacts with the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, that's not what you're doing with a diffuse BRDF. A diffuse BRDF is intended to represent the average of all those possible diffusion paths. In the case of an Lambertian that average is modeled as uniform reflection and a single albedo value measuring the internal energy loss during diffusion, but more complicated models are possible. Crucially: a diffuse BRDF will already include the aggregate effect of some paths being reflected back into the medium to diffuse further and some passing out immediately. $1 - F_{out}$ is already &quot;baked in&quot; to the BRDF¹ and you do not need factor it in again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What the Lambertian term does not include is the portion of energy that is lost by being reflected before light enters the medium of diffusion. This is view-dependent, but depends on the precise glossy lobe above it. There is no energy loss at a (non-metallic) surface interface so everything that isn't reflected will be refracted, meaning that what you actually want is to integrate the total energy loss at the surface over all outgoing directions, i.e. $1 - \int \texttt{glossy_bsdf}(\texttt{in}, \texttt{out}) \: \mathrm{d} \texttt{out}$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's possible to pre-compute approximations to that integral for specific BRDFs. The end result will in general depend on view direction, material roughness and IOR at least. As a first approximation you can assume that the glossy lobe is a perfectly specular reflector. That gives a weighting of $1 - \int \texttt{glossy} \: \mathrm{d}\texttt{out} = 1 - F_{in}$, which is exactly what you first suggested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Additionally, note that the Lambertian BRDF is the albedo divided by $\pi$ and that the cosine term is a measure of how attenuated the incoming light is on the surface; it applies to both glossy and diffuse reflectance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, roughly:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Assuming for example:&#xA;//   diffuse = albedo / PI;&#xA;//   specular = my_favorite_glossy_brdf(in_dir, out_dir, roughness);&#xA;//   fresnel = f0 + (1.0 - f0) * pow(1.0 - dot(E, H), 5.0);&#xA;//   total_surface_reflection = fresnel&#xA;color = lightIntensity * dot(L, N) * Lerp(diffuse, specular, total_surface_reflection);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;¹ &lt;em&gt;Really the integral of $F$ over all possible incident internal diffusion exit paths that result in your out direction, but I digress.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="3075" LastEditorUserId="3075" LastEditDate="2016-04-11T20:25:29.450" LastActivityDate="2016-04-11T20:25:29.450" CommentCount="1" />
  <row Id="2290" PostTypeId="2" ParentId="2087" CreationDate="2016-04-08T12:52:30.253" Score="0" Body="&lt;p&gt;If multi-texturing is supported, texture coordinate generation will affect the active texture unit (quote from the documentation, emphasis is mine):&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;When the ARB_multitexture extension is supported, &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glTexGen.xml&quot; rel=&quot;nofollow&quot;&gt;glTexGen&lt;/a&gt; sets the texture generation parameters &lt;strong&gt;for the currently active texture unit, selected with glActiveTexture&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So you simply need to make sure texture coordinates generation is disabled for the color texture, but enabled for the shadow texture. As you guessed, this is done with &lt;a href=&quot;https://www.opengl.org/sdk/docs/man2/xhtml/glActiveTexture.xml&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;glActiveTexture()&lt;/code&gt;&lt;/a&gt;, by writing for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glActiveTexture(GL_TEXTURE0); // Assuming the color texture to be #0&#xA;glDisable(GL_TEXTURE_GEN_S);&#xA;glDisable(GL_TEXTURE_GEN_T);&#xA;glDisable(GL_TEXTURE_GEN_R);&#xA;glDisable(GL_TEXTURE_GEN_Q);&#xA;&#xA;glActiveTexture(GL_TEXTURE1); // Assuming the shadow texture to be #1&#xA;glTexGeni(GL_S, GL_TEXTURE_GEN_MODE, GL_EYE_LINEAR);&#xA;glTexGenfv(GL_S, GL_EYE_PLANE, textureMatrix.GetRow(0));&#xA;glEnable(GL_TEXTURE_GEN_S);&#xA;// [...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;This being said, you're adding to yourself a lot of trouble by sticking with fixed pipeline instead of using GLSL.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-08T12:52:30.253" CommentCount="0" />
  <row Id="2291" PostTypeId="2" ParentId="2279" CreationDate="2016-04-08T14:06:10.073" Score="4" Body="&lt;h2&gt;The problem appears to be unintentionally transparent surfaces&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Although the image is grainy, it is sufficiently clear to estimate that all of the darker regions are due to surfaces facing away from the light, rather than due to shadows cast onto surfaces facing towards the light. So it does seem that there is a problem, and the lack of shadows is not just due to having soft shadows from a large area light that don't show up well, or excessively reflective surfaces that provide too much ambient light. Somehow light is getting through your surfaces.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;The probable source of this problem&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Since you check for intersection with the light and the objects separately, the crucial point where this could go wrong is where you check&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (distFromLight &amp;lt; distFromObj)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You correctly terminate the path and return the light colour if this check is true (that is, if the light is closer than the nearest object in that direction). So the problem is likely to be with one of the two variables you are comparing. If they were both correct, then you would see shadows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;These are calculated on the preceding lines&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float distFromObj = length(vert.p);&#xA;float distFromLight = length(hitLightPoint);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As pointed out in &lt;a href=&quot;http://computergraphics.stackexchange.com/a/2281/231&quot;&gt;Julien Guertault's answer&lt;/a&gt;, these calculations may be redundant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;length(vert.p)&lt;/code&gt; should still be available as &lt;code&gt;t&lt;/code&gt;, used 11 lines previously in the calculation of &lt;code&gt;vert.p&lt;/code&gt;. This not only avoids unnecessary calculation, but eliminates a potential source of error. Then this length is likely to be correct unless there is a problem with &lt;code&gt;checkIfRayIntersectSomething()&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This leaves only &lt;code&gt;length(hitLightPoint)&lt;/code&gt; as a source of error, unless there is a more subtle problem. It would be useful to be able to eliminate the possibility of problems with &lt;code&gt;length()&lt;/code&gt; or &lt;code&gt;hitLightPoint&lt;/code&gt;, which is calculated by &lt;code&gt;checkRayLightIntersection()&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;We need to see the code to be able to help&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;length()&lt;/code&gt;, &lt;code&gt;checkIfRayIntersectSomething()&lt;/code&gt; and &lt;code&gt;checkRayLightIntersection()&lt;/code&gt; are not currently included with the code in your question, so we can't analyse these. If you are able to share these that would help narrow down the source of the problem.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;A simple way to check whether this is the real problem&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;You could also verify whether this analysis of the problem is correct by rendering a scene which will eliminate the other possibilities (shadows too soft to distinguish, or shadows washed out by too much reflected light from the rest of the scene). If the light getting through the surfaces is the source of your problem, then it will apply even if there is no path from the light to a surface, provided that surface is facing the light. So render a simple scene that has a light with a plane beneath it, and another plane beneath that, separated by a gap big enough to easily be seen. The lower plane is completely occluded by the upper plane, so should receive no light (no soft shadows and no reflected light from elsewhere). If the lower plane is lit, then you know that this is indeed the problem and you can focus your efforts here.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the lower plane is &lt;em&gt;not&lt;/em&gt; lit, then either this is not the problem, or there is a problem with the scale and the occluding plane may need to be at a different distance from the light in order for the problem to show up. You can use your existing scene as a source of estimated distances for which this problem occurs, and you can use inclined planes in order to see the effect for a variety of ratios of object distance to light distance within a single image.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sharing the images resulting from such simple scenes may help us identify the problem more precisely.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I notice that in a comment on the question you mention that you do not have a straightforward way of generating scenes yourself. Perhaps there are much simpler scenes already publicly available in the same file format, that you can use to test this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If not, it sounds like the only thing you have control over is the light. In that case, you could try putting the light underground. If surfaces are not transparent, the scene should be uniformly black. If surfaces are being lit despite an occluding surface, you may see evidence for it this way that would be hard to discern when there is lots of secondary light bouncing around.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-04-10T22:48:51.737" LastActivityDate="2016-04-10T22:48:51.737" CommentCount="8" />
  <row Id="2294" PostTypeId="2" ParentId="2221" CreationDate="2016-04-08T19:21:25.237" Score="1" Body="&lt;p&gt;A common mistake that causes this behavior is to first apply the translation and then the rotation when rendering the scene. I think that's what's happening. If you apply the rotation first and then the translation it should behave as expected. Give that a try&lt;/p&gt;&#xA;" OwnerUserId="3083" LastActivityDate="2016-04-08T19:21:25.237" CommentCount="4" />
  <row Id="2295" PostTypeId="2" ParentId="2288" CreationDate="2016-04-09T09:10:49.433" Score="5" Body="&lt;p&gt;Looking at the video you provided, I would tend to think it's something along those lines:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A particle emitter, to which all threads are attached, is moving around. I haven't really figured what its motion is dictated by.&lt;/li&gt;&#xA;&lt;li&gt;Each thread is made of particles flowing away from the emitter. The thread has a direction, used for the initial speed of the particles. Once created,  they are independent from the emitter.&lt;/li&gt;&#xA;&lt;li&gt;The particles use a quad or a rectangle textured with a fuzzy ellipse shape (you can see them individually when a thread suddenly changes direction) oriented in the direction of the motion. Their color is based on the duration since their creation, and they use additive alpha blending (the saturation to white is typical). After a certain duration, they fade out and die, only to be recycled into new particles.&lt;/li&gt;&#xA;&lt;li&gt;I suspect the whole thing might be in a vector field, or something similar blowing wind onto the particles, which would explain why the threads sometimes suddenly change direction.&lt;/li&gt;&#xA;&lt;li&gt;There might be some feedback post-processing effect to smooth everything out.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-09T09:10:49.433" CommentCount="0" />
  <row Id="2296" PostTypeId="2" ParentId="2166" CreationDate="2016-04-10T02:55:44.603" Score="2" Body="&lt;p&gt;The short answer is It's Complicated™. :)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A lot of factors can affect frame timing (and the associated problem of animation juddering, due to the game's animation delta-times not matching actual frame delivery times). Some of the factors are: how CPU-limited versus GPU-limited the game is, how the game's code measures time, how much variability there is in frame time on both the CPU and GPU, how many frames of queue-ahead the CPU is allowed to build up, double versus triple buffering, and driver behavior.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the specific question of whether you get locked to a divisor of 60 fps (30, 20, 15, etc), this seems to depend mainly on whether the application is GPU-limited or CPU-limited. If you're GPU-limited, then assuming the application is running steadily (not producing any hitches or rapid performance shifts in itself), it does lock to the highest divisor of the vsync rate that it can sustain. This timing diagram shows how it works:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Rm3tV.png&quot; alt=&quot;vsync in GPU limited case&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Time runs from left to right and the width of each block represents the duration of one frame's work. After the first few frames, the system settles into a state where the frames are turned out at a steady rate, one every two vsync periods (i.e. if vsync was 60 Hz, the game would be running at exactly 30 fps).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note the empty spaces (idle time) in both CPU and GPU rows. That indicates that the game could run faster if vsync were turned off. However, with vsync on, the swapchain puts back-pressure on the GPU&amp;mdash;the GPU can't start rendering the next frame until vsync releases a backbuffer for it to render into. The GPU in turn puts back-pressure on the CPU via the command queue, as the driver won't return from &lt;code&gt;Present()&lt;/code&gt; / &lt;code&gt;SwapBuffers()&lt;/code&gt; until the GPU starts rendering the next frame, and opens a queue slot for the CPU to continue issuing commands. The result is that everything runs at a steady 30 fps rate, and everyone's happy (except maybe the 60fps purist gamer ).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compare this to what happens when the application is CPU-limited:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/mXZJo.png&quot; alt=&quot;vsync in CPU-limited case&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This time we ended up with the game running at 36 fps, the maximum speed possible given the CPU load in this case. But this isn't a divisor of the refresh rate, so we get uneven frame delivery to the display, with some frames shown for two vsync periods and some for only one. This will cause judder.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here the GPU goes idle because it's waiting for the CPU to give it more work&amp;mdash;not because it's waiting for a backbuffer to be available to render into. The swapchain thus doesn't put back-pressure on the GPU (or at least doesn't do so consistently), and the GPU in turn doesn't put back-pressure on the CPU. So the CPU just runs as fast as it can, oblivious to the irregular frame times appearing on the display.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's possible to &quot;fix&quot; this so that the game still discretizes to vsync-friendly rates even in the CPU-limited case. The game code could try to recognize this situation and compensate by sleeping its CPU thread to artificially lock itself to 30 fps, or by increasing its swap interval to 2. Alternatively, the driver could do the same thing by inserting extra sleeps in &lt;code&gt;Present()&lt;/code&gt; / &lt;code&gt;SwapBuffers()&lt;/code&gt; (the NVIDIA drivers do have this as a control panel option; I don't know about others). But in any case, &lt;em&gt;someone&lt;/em&gt; has to detect the problem and do something about it; the game won't naturally settle on a vsync-friendly rate the way it does when it's GPU-limited.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This was all a simplified case in which both CPU and GPU workloads are perfectly steady. In real life, of course, there's variability in the workloads and frame times, which can cause even more interesting things to happen. The CPU render-ahead queue and triple buffering can come into play then.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-10T02:55:44.603" CommentCount="0" />
  <row Id="2300" PostTypeId="1" CreationDate="2016-04-10T17:26:45.990" Score="3" ViewCount="88" Body="&lt;p&gt;I am new to OpenGL and Computer Graphics in general. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lately I was learning how to model a camera, specifically how to model the rotation of camera. I was introduced to Euler angles for this purpose. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I got a rough idea about how to use Euler angles to model camera rotation from this &lt;a href=&quot;https://www.opengl.org/discussion_boards/showthread.php/133072-Camera-Position-Camera-Angle-Camera-Target-Position?p=986100&amp;amp;viewfull=1#post986100&quot; rel=&quot;nofollow&quot;&gt;post&lt;/a&gt;. Based on that, I wrote something like below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLfloat yaw, pitch;&#xA;...&#xA;void mouse_callback(GLFWwindow* window, double xpos, double ypos)&#xA;{&#xA;        /* &#xA;           Here I am updating the yaw, and pitch values &#xA;           based on mouse movement and previous mouse position&#xA;        */&#xA;&#xA;        glm::mat4 yawPitchRotMat;&#xA;        /* We want to do yaw * pitch. */&#xA;&#xA;        //y-axis as yaw axis&#xA;        yawPitchRotMat = glm::rotate(yawPitchRotMat, glm::radians(yaw), &#xA;                         glm::vec3(0.0f, 1.0f, 0.0f));&#xA;&#xA;        //x-axis as pitch axis&#xA;        yawPitchRotMat = glm::rotate(yawPitchRotMat, glm::radians(pitch), &#xA;                         glm::vec3(1.0f, 0.0f, 0.0f));&#xA;&#xA;        cameraFrontDirection = glm::normalize(-1.0f * &#xA;                 glm::vec3(yawPitchRotMat[2].x, yawPitchRotMat[2].y, yawPitchRotMat[2].z));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above code seems to work. However, I would like to hear your opinion about this approach. Specially as my math is not really very strong, so any mathematical insight regarding Euler angles would be very helpful. Also appreciate any reference to online resource on the same, that is easy to understand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Though later I also came to know about certain limitation of modelling rotation using Euler angles and use quaternions instead. However I would like to save that topic for future discussion, as I am yet to read about that in details.&lt;/p&gt;&#xA;" OwnerUserId="3098" LastEditorUserId="3098" LastEditDate="2016-04-10T20:37:38.120" LastActivityDate="2016-04-12T14:35:32.873" Title="Computing camera front direction from Euler angles" Tags="&lt;opengl&gt;&lt;transformations&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="0" />
  <row Id="2301" PostTypeId="1" CreationDate="2016-04-10T22:53:17.550" Score="1" ViewCount="26" Body="&lt;p&gt;Hello everybody i have a trouble making datatransmition with mine program just to give the idea im trying to move this blocks when i move mine cars&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/hiOLG.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/hiOLG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here I leave the encoding of mine server/client program &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package com.mygdx.Xrace;&#xA;&#xA;import java.io.BufferedReader;&#xA;&#xA;import java.io.IOException;&#xA;&#xA;import java.io.InputStreamReader;&#xA;&#xA;import java.io.PrintWriter;&#xA;&#xA;import java.net.ServerSocket;&#xA;&#xA;import java.net.Socket;&#xA;&#xA;public class ScreenServidor extends Pantallabase{&#xA;&#xA;        ScreenCarrera carrera;&#xA;        MyTextInputListener listener;&#xA;        ServerSocket server ;&#xA;        float delta = 0.01f;&#xA;        int puerto=9600;&#xA;        public ScreenServidor(Main game) {&#xA;            super(game);&#xA;        }&#xA;        @Override       &#xA;        public void show()&#xA;        {&#xA;            carrera=new ScreenCarrera(game);&#xA;            carrera.show();&#xA;&#xA;            try {&#xA;                server = new ServerSocket(puerto);&#xA;                new User(server.accept()).start();&#xA;            } catch (IOException e) {&#xA;                 System.out.println(e);&#xA;                }finally{&#xA;                        try {&#xA;                            server.close();&#xA;                        } catch (IOException e) {&#xA;                             System.out.println(e);&#xA;                        }&#xA;                }&#xA;&#xA;        }   &#xA;&#xA;&#xA;        @Override&#xA;        public void render(float delta)&#xA;        {&#xA;            carrera.render(delta);&#xA;        }&#xA;        @Override&#xA;        public void dispose(){&#xA;            carrera.dispose();&#xA;            }&#xA;        @Override&#xA;        public void resize(int width, int height) {}&#xA;        @Override&#xA;        public void pause() {}&#xA;        @Override&#xA;        public void resume() {}&#xA;        @Override&#xA;        public void hide() {&#xA;        }&#xA;}&#xA;class User extends Thread{&#xA;&#xA;    BufferedReader in;&#xA;    PrintWriter out;&#xA;    Socket conexion;&#xA;    ScreenCarrera carrera;&#xA;    public User(Socket serv)&#xA;    {&#xA;        this.conexion=serv;&#xA;    }&#xA;    public void run() {&#xA;&#xA;        try {&#xA;&#xA;            in = new BufferedReader(new InputStreamReader(conexion.getInputStream())); &#xA;            out=new PrintWriter(conexion.getOutputStream(),true);&#xA;            **carrera.m.posx2=in.read();** //receiving data&#xA;            **out.print(carrera.m.posx1);**//sending data&#xA;            } catch (IOException e) {&#xA;                 System.out.println(e);&#xA;        }finally{&#xA;            try {&#xA;            in.close();&#xA;            out.close();&#xA;            conexion.close();&#xA;            } catch (IOException e) {}&#xA;    }&#xA;&#xA;    }&#xA;&#xA;}&#xA;&#xA;package com.mygdx.Xrace;&#xA;&#xA;import java.net.*;&#xA;&#xA;import java.io.*;&#xA;&#xA;import com.badlogic.gdx.Gdx;&#xA;&#xA;public class ScreenCliente extends Pantallabase{&#xA;&#xA;    Socket conexion;&#xA;    ScreenCarrera carrera;&#xA;    PrintStream out ;&#xA;    MyTextInputListener listener;&#xA;    BufferedReader in;&#xA;    float delta = 0.01f;&#xA;    int puerto=9600;&#xA;&#xA;    static String ip;&#xA;&#xA;    public ScreenCliente(Main game) {&#xA;        super(game);&#xA;    }&#xA;&#xA;    @Override&#xA;    public void show() {&#xA;        carrera=new ScreenCarrera(game);&#xA;        carrera.show();&#xA;        conexion=new Socket();&#xA;        listener = new MyTextInputListener();&#xA;        Gdx.input.getTextInput(listener, &quot;ingrese ip&quot;, ip, ip);&#xA;        try {&#xA;&#xA;            conexion =new Socket(ip,puerto);&#xA;            out = new PrintStream(conexion.getOutputStream(),true);&#xA;            in = new BufferedReader(new InputStreamReader(conexion.getInputStream()));&#xA;&#xA;        }catch(Exception e){e.printStackTrace();}&#xA;    }&#xA;    @Override&#xA;    public void render(float delta) {&#xA;        carrera.render(delta);&#xA;        try{&#xA;        **carrera.m.posx2=in.read()**;&#xA;        } catch (IOException e) {&#xA;            e.printStackTrace();&#xA;            game.m.mensaje=&quot;error de conexion&quot;;&#xA;            game.setScreen(game.m);&#xA;        }try {&#xA;            run();&#xA;        } catch (IOException e) {&#xA;            e.printStackTrace();&#xA;        }&#xA;&#xA;    }&#xA;    private void run() throws IOException{&#xA;&#xA;         **out.print(carrera.m.posx1);**&#xA;    }&#xA;&#xA;    @Override&#xA;    public void resize(int width, int height) {&#xA;    }&#xA;    @Override&#xA;    public void pause() {&#xA;    }&#xA;    @Override&#xA;    public void resume() {&#xA;    }&#xA;    @Override&#xA;    public void hide() {&#xA;    }&#xA;    @Override&#xA;    public void dispose() {&#xA;        carrera.dispose();&#xA;        try {&#xA;            in.close();&#xA;            out.close();&#xA;            conexion.close();&#xA;    } catch (IOException e) {&#xA;        e.printStackTrace();&#xA;    }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3101" LastEditorUserId="16" LastEditDate="2016-04-11T07:01:07.523" LastActivityDate="2016-04-11T07:01:07.523" Title="Trouble using java connectivity and data transmition on libgdx" Tags="&lt;javascript&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2016-04-16T06:49:49.237" />
  <row Id="2302" PostTypeId="2" ParentId="2300" CreationDate="2016-04-12T14:35:32.873" Score="5" Body="&lt;p&gt;This kind of navigation works well for situations that are centered about objects. It keeps the up vector always pointing upward which makes it less possible for confusion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Bringing quaternions into this mix is not necessarily a good idea. People often totally exaggerate the problems of Euler angles (or taint Tait–Bryan angles). Its true that Euler did not even consider the implications in interpolating the angles. True you can get into big problems if you rotate angles into certain positions and interpolate angle by angle. But this didn't bother Euler so much as he did mathematical modeling where the problem cancels itself out.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;So Euler angles are perfectly fine for storing the rotation, also ok in differential rotation. And in case as no gimbal lock can really happen anyway, you get other problems before it. Euler angles are just not good at linear interpolation in a curved space. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;In this case there should not be any bigger problems. Euler angles are much easier to understand. Unless you need to interpolate between 2 recorded positions. However this is a no issue as your collection and interpolation dont need to be based on the same model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Its possible to look at this model in another light, you can think of it as a vector that records camera position in relation to a center as long as you keep the vector equal length you achieve same thing. You can then just form the matrix by multiplying by up vector and then that by front to get new up. This is even easier to interpolate unless you move trough the actual center but again you can keep normalizing the vector to a length.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-04-12T14:35:32.873" CommentCount="1" />
  <row Id="2303" PostTypeId="1" AcceptedAnswerId="2304" CreationDate="2016-04-13T12:09:54.340" Score="5" ViewCount="217" Body="&lt;p&gt;I have a vertex buffer like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0.0, 0.0,&#xA;1.0, 0.0,&#xA;0.0, 0.6,&#xA;1.0, 0.6,&#xA;0.5, 1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have the following index buffer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0, 2,&#xA;2, 4,&#xA;4, 3,&#xA;3, 2,&#xA;2, 1,&#xA;1, 0,&#xA;0, 3,&#xA;3, 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I know I want to draw &lt;code&gt;gl.LINES&lt;/code&gt; using WebGL, means multiple seperated line segments.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gl.drawElements(gl.LINES, 16, gl.UNSIGNED_SHORT, indexBuffer);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It seems to enable drawing of multiple line segments in a single draw call in WebGL.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could someone ELI5 to me, what is an index buffer and how is it related to vertex buffers? How to generate index buffers from my primitives?&lt;/p&gt;&#xA;" OwnerUserId="361" LastActivityDate="2016-04-14T17:27:49.293" Title="What is an index buffer and how is it related to vertex buffers?" Tags="&lt;webgl&gt;&lt;line-drawing&gt;&lt;mapping&gt;&lt;vertex-buffer-object&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2304" PostTypeId="2" ParentId="2303" CreationDate="2016-04-13T12:42:50.983" Score="8" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Could someone ELI5 to me, what is an index buffer and how is it&#xA;  related to vertex buffers&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Your vertex buffer contains the X and Y coordinates of 5 vertices. They are:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;index |  X  |  Y&#xA;  0   | 0.0 | 0.0 &#xA;  1   | 1.0 | 0.0&#xA;  2   | 0.0 | 0.6&#xA;  3   | 1.0 | 0.6&#xA;  4   | 0.5 | 1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Your index buffer contains information about which lines to draw between these vertices. It uses the index of each vertex in the vertex buffer as a value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since you are drawing lines, each pair of consecutive values in your index buffer indicate a line segment. For example, if the index buffer starts with &lt;code&gt;0, 2&lt;/code&gt;, it means draw a line between the 0th and 2nd vertices in the vertex array, which in this case would draw a line going from &lt;code&gt;[0.0, 0.0]&lt;/code&gt; to &lt;code&gt;[0.0, 0.6]&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the following graphic each pair of indices is color coordinated with the line it indicates:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/0H9FS.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/0H9FS.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly, if you were drawing triangles instead of lines, you would need to supply an index buffer where each 3 consecutive values indicate the indices of three vertices in the vertex buffer, e.g. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;0, 1, 2,&#xA;2, 1, 3,&#xA;2, 3, 4,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="457" LastEditorUserId="457" LastEditDate="2016-04-14T17:27:49.293" LastActivityDate="2016-04-14T17:27:49.293" CommentCount="0" />
  <row Id="2305" PostTypeId="2" ParentId="2303" CreationDate="2016-04-13T12:48:19.650" Score="4" Body="&lt;p&gt;If you have a vertex buffer like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var vertices = [&#xA;  0.0, 0.0, 0.0,&#xA;  1.0, 0.0, 0.0,&#xA;  0.0, 0.6, 0.0,&#xA;  1.0, 0.6, 0.0,&#xA;  0.5, 1.0, 0.0&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And simply draw it as it is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Create an empty buffer object&#xA;var vertex_buffer = gl.createBuffer();&#xA;&#xA;// Bind appropriate array buffer to it&#xA;gl.bindBuffer(gl.ARRAY_BUFFER, vertex_buffer);&#xA;&#xA;// Pass the vertex data to the buffer&#xA;gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW);&#xA;&#xA;/* [...] */&#xA;&#xA;// Draw the lines&#xA;gl.drawArrays(gl.LINES, 0, 5);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It would require two dedicated coordinates for each line segment. With the &lt;code&gt;vertices&lt;/code&gt; as defined above, it would only be possible two draw &lt;strong&gt;two lines&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/HTDG6.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HTDG6.png&quot; alt=&quot;two lines&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have the following indices defined:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var indices = [&#xA;  0, 2,&#xA;  2, 4,&#xA;  4, 3,&#xA;  3, 2,&#xA;  2, 1,&#xA;  1, 0,&#xA;  0, 3,&#xA;  3, 1&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is possible to draw lines which intersect the same vertices again and again. This reduces redundancy. If you bind the index buffer and tell the GPU to draw line segments connecting the vertices by the order specified in the indecies array:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var index_buffer = gl.createBuffer();&#xA;&#xA;gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, index_buffer);&#xA;&#xA;gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), gl.STATIC_DRAW);&#xA;&#xA;// draw geometry lines by indices&#xA;gl.drawElements(gl.LINES, 16, gl.UNSIGNED_SHORT, index_buffer);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;One can draw complexer figures without redefining the same vertices over and over again. This is the result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/oBLfK.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oBLfK.png&quot; alt=&quot;a house&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To achieve the same result without indices, the vertex buffer should look like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var vertices = [&#xA;  0.0, 0.0, 0.0,&#xA;  0.0, 0.6, 0.0,&#xA;  0.0, 0.6, 0.0,&#xA;  0.5, 1.0, 0.0,&#xA;  0.5, 1.0, 0.0,&#xA;  1.0, 0.6, 0.0,&#xA;  1.0, 0.6, 0.0,&#xA;  0.0, 0.6, 0.0,&#xA;  0.0, 0.6, 0.0,&#xA;  1.0, 0.0, 0.0,&#xA;  1.0, 0.0, 0.0,&#xA;  0.0, 0.0, 0.0,&#xA;  0.0, 0.0, 0.0,&#xA;  1.0, 0.6, 0.0,&#xA;  1.0, 0.6, 0.0,&#xA;  1.0, 0.0, 0.0&#xA;]&#xA;&#xA;/* [...] */&#xA;&#xA;// Draw the lines&#xA;gl.drawArrays(gl.LINES, 0, 16);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Which results in the same image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/QOezA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/QOezA.png&quot; alt=&quot;yet another house&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note the huge redundancy in stored vertices. &lt;/p&gt;&#xA;" OwnerUserId="361" LastActivityDate="2016-04-13T12:48:19.650" CommentCount="3" />
  <row Id="2306" PostTypeId="1" AcceptedAnswerId="2307" CreationDate="2016-04-13T21:53:50.753" Score="6" ViewCount="59" Body="&lt;p&gt;What is the best way to draw unlit objects with deferred? Like skybox, particles or just glowing objects?&#xA;I thought about mask in alpha channel, check it in light pass and just set diffuse color but it won't work with light blending.&#xA;My second idea was stencil buffer with pass after light pass that only fill unlit space. But switching pso (dx12) only for color copy doesn't sound like a good idea.&#xA;And of course pass only for unlit objects after light pass, but I don't want to create another list only for this (and again, i have to switch pso).&lt;/p&gt;&#xA;" OwnerUserId="3123" LastActivityDate="2016-04-14T03:21:57.747" Title="Unlit objects in deferred rendering" Tags="&lt;lighting&gt;&lt;deferred-rendering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2307" PostTypeId="2" ParentId="2306" CreationDate="2016-04-14T03:21:57.747" Score="8" Body="&lt;p&gt;Typically in a deferred engine, you would render unlit objects directly to the color buffer, bypassing the lighting system entirely. You would likely also output zeroes to the G-buffer so that deferred lights don't add anything to those pixels (and possibly also mark them in the stencil buffer, to avoid running the lighting shaders on them, for efficiency).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Don't worry about switching PSOs for this. I'm guessing you're worried about the performance, but it's not a big deal. Game engines typically switch PSOs (or switch shaders, in DX11/GL) hundreds or thousands of times per frame. Switching a couple of times for an extra color rendering pass isn't even a blip on the radar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, if you want to really simplify your code (possibly at the expense of performance), you could just bind the color buffer together with your G-buffers and render to it all the time, as if it were an extra G-buffer. Ordinary deferred shaders would output zero to it, while unlit objects would output their color to it.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-14T03:21:57.747" CommentCount="1" />
  <row Id="2308" PostTypeId="1" CreationDate="2016-04-14T07:02:25.657" Score="4" ViewCount="40" Body="&lt;p&gt;A 3D model is sliced against a plane and the resulting 2D contour is projected onto the plane. I am looking for an efficient algorithm to identify the inside and outside region of the contour. Simultaneously I need to identify outer and inner loops within the contours. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any references from articles/books is earnestly requested.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-04-14T08:11:28.427" Title="inside and outside region identification from 2D contour" Tags="&lt;vector-graphics&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2309" PostTypeId="2" ParentId="2308" CreationDate="2016-04-14T08:11:28.427" Score="3" Body="&lt;p&gt;In general, I recommend the book &lt;a href=&quot;http://realtimecollisiondetection.net/&quot; rel=&quot;nofollow&quot;&gt;Real-Time Collision Detection&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For your particular case, my first choice would be to use the 3D test for point-inside-mesh. I assume that the contour is a sequence of 2D segments (a sequence of vertices, essentially) and that the model mesh is &quot;sealed&quot;, i.e. there are no holes in its surfaces that would make distinguishing the &quot;inside&quot; and &quot;outside&quot; of it impossible.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Find a bounding box for your mesh.&lt;/li&gt;&#xA;&lt;li&gt;Cast rays from an edge of the bounding box to the tested point. In your case, the ray would be coplanar with the slicing plane, and the point would be one of the vertices of the contour.&lt;/li&gt;&#xA;&lt;li&gt;Count how many times the ray crosses mesh triangles.&lt;/li&gt;&#xA;&lt;li&gt;Odd number of crossings means that the point is on the inside; even number means it's on the outside.&lt;/li&gt;&#xA;&lt;li&gt;Repeat points 2-4 for all contour vertices.&lt;/li&gt;&#xA;&lt;li&gt;Classify contour regions by whether their vertices are on the inside or outside.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2817" LastActivityDate="2016-04-14T08:11:28.427" CommentCount="6" />
  <row Id="2310" PostTypeId="1" AcceptedAnswerId="2323" CreationDate="2016-04-14T09:15:04.660" Score="4" ViewCount="96" Body="&lt;p&gt;Would it be, at least theoretically, possible to render without a near clipping plane? Could raytracing ignore this perchance? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Quite many budding 3d artists have a &lt;em&gt;WTF&lt;/em&gt; moment the first time they encounter the clipping plane. If this could be avoided then it would be a big win for usability of 3D authoring apps. Its not one of those things a normal artist would think about until explained.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-04-17T19:31:05.250" Title="Could we dispense the near clipping plane?" Tags="&lt;raytracing&gt;&lt;clipping&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="2311" PostTypeId="2" ParentId="2310" CreationDate="2016-04-14T10:15:00.100" Score="4" Body="&lt;p&gt;The near clipping plane in rasterizing setups with perspective projection is there to avoid a divide by 0 and bound the possible depths for orthogonal projection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With a bit of extra math you can make the depth stored in the depth buffer linear again. However you still need the guard against the divide by 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In raytracing the near plane can be at 0 no problem. There is nothing technical stopping you from putting it at -10 however (besides that it would be very odd).&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-04-14T10:15:00.100" CommentCount="6" />
  <row Id="2314" PostTypeId="1" AcceptedAnswerId="2341" CreationDate="2016-04-15T14:29:35.037" Score="5" ViewCount="86" Body="&lt;p&gt;This might be a general question but I have some difficulties about understanding baked bent normal maps and how to use it inside a shader in Unity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what I understand, bent normals are especially used with occlusion but I don't really get what they contain and how to use them.&#xA;The other strange thing is that when baking them with different tools, it gives me different output, so it's hard for me to interpret them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some examples:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tHXeT.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tHXeT.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;This is made from Substance Designer in World Space. Seems really close to a simple normal map in World Space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1AAcV.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1AAcV.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;This is made from Substance Designer in Tangent Space. It doesn't seem to contain really relevant informations as the one above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Zenrb.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Zenrb.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;This is made from Maya Turtle. It seems to contain interesting information but... what space is that ?!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BCZmP.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BCZmP.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;This is the same as the one above but converted to Unity using Handplane. What can I do with this...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems that Unity is using tangent space normal maps but I don't have any control on the import process and I don't know what they are doing. Regular normals are then unpacked in converted into world space normals using TBN matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fact is that at this point, any clue or advice should be really useful.&#xA;I don't know if my baked maps are right or usable and I don't know how to use them inside my shader.&lt;/p&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="2372" LastEditDate="2016-04-15T15:16:12.770" LastActivityDate="2016-04-20T05:44:23.073" Title="How could I use bent normal map" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;shader&gt;&lt;texture&gt;&lt;unity&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2315" PostTypeId="1" CreationDate="2016-04-16T11:26:50.253" Score="0" ViewCount="43" Body="&lt;p&gt;I need to load a model &quot;somemodel.json&quot; into my &quot;movie.js&quot;.Can someone tell me where i am going wrong.I followed the steps given &lt;a href=&quot;http://threejs.org/docs/#Reference/Loaders/ObjectLoader&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;index.html&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-html prettyprint-override&quot;&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;&#xA;&amp;lt;html&amp;gt;&#xA;    &amp;lt;head&amp;gt;&#xA;         &amp;lt;meta charset=utf-8&amp;gt;&#xA;         &amp;lt;title&amp;gt;Test&amp;lt;/title&amp;gt;&#xA;         &amp;lt;style&amp;gt;&#xA;            body { margin: 0; }&#xA;            canvas { width: 100%; height: 100% }&#xA;         &amp;lt;/style&amp;gt;&#xA;    &amp;lt;/head&amp;gt;&#xA;    &amp;lt;body&amp;gt;&#xA;        &amp;lt;script src=&quot;three.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&#xA;        &amp;lt;script src=&quot;ObjectLoader.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&#xA;        &amp;lt;script src=&quot;movie.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&#xA;    &amp;lt;/body&amp;gt;&#xA;&amp;lt;/html&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;movie.js&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-js prettyprint-override&quot;&gt;&lt;code&gt;var camera;&#xA;var scene;&#xA;var renderer;&#xA;var loader;&#xA;&#xA;frame();&#xA;animate();&#xA;&#xA;function frame() {&#xA;&#xA;scene = new THREE.Scene();&#xA;camera = new THREE.PerspectiveCamera( 45, window.innerWidth /     window.innerHeight, 0.1, 1000);&#xA;camera.position.y = 0;&#xA;camera.position.z = 10;&#xA;camera.rotation.x = 0;&#xA;&#xA;var loader = new THREE.ObjectLoader();&#xA;loader.load(&#xA;'somemodel.json',&#xA;function () {&#xA;    var geometry = new THREE.SphereGeometry(3,32,32);&#xA;    var material = new THREE.MeshBasicMaterial();&#xA;    var object = new THREE.Mesh( geometry, material );&#xA;    scene.add( object );&#xA;}&#xA;);&#xA;&#xA;renderer = new THREE.WebGLRenderer();&#xA;renderer.setSize( window.innerWidth, window.innerHeight );&#xA;document.body.appendChild( renderer.domElement );&#xA;render();&#xA;}&#xA;&#xA;function animate() {&#xA;render();&#xA;requestAnimationFrame( animate );&#xA;}&#xA;&#xA;function render() {&#xA;renderer.render( scene, camera );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or can your give an example with a sample model ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When i run my file it raises an error in the console saying &lt;code&gt;not well-formed        male.json&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="3141" LastEditorUserId="3141" LastEditDate="2016-04-16T12:10:44.253" LastActivityDate="2016-04-16T23:14:54.010" Title="How to load a model in .json in three.js" Tags="&lt;webgl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2316" PostTypeId="1" CreationDate="2016-04-16T21:40:19.773" Score="9" ViewCount="122" Body="&lt;p&gt;I have seen that in some implementations of Path Tracing, an approach called Russian Roulette is used to cull some of the paths and share their contribution among the other paths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that rather than following a path until it drops below a certain threshold value of contribution, and then abandoning it, a different threshold is used and paths whose contribution is below that threshold are only terminated with some small probability. The other paths have their contribution increased by an amount corresponding to sharing the lost energy from the terminated path. It isn't clear to me whether this is to correct a bias introduced by the technique, or whether the whole technique is itself necessary to avoid bias.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Does Russian Roulette give an unbiased result?&lt;/li&gt;&#xA;&lt;li&gt;Is Russian Roulette &lt;em&gt;necessary&lt;/em&gt; for an unbiased result?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;That is, would using a tiny threshold and just terminating a path the moment it drops below that threshold give a more biased or less biased result?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given an arbitrarily large number of samples, would both approaches converge on an unbiased resulting image?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking to understand the underlying reason for using the Russian Roulette approach. Is there a significant difference in speed or quality?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I understand that the energy is redistributed among other rays in order to preserve total energy. However, could this redistribution not still be done if the ray were terminated on dropping below a fixed threshold, rather than having a randomly determined life span after reaching that threshold?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Conversely, if the energy that would be lost by terminating a ray without redistributing its energy is eventually lost anyway (as the rays to which it is redistributed are also eventually terminated), how does this improve the situation?&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-04-17T13:07:37.037" LastActivityDate="2016-05-20T20:26:14.153" Title="Is Russian Roulette really the answer?" Tags="&lt;monte-carlo&gt;&lt;pathtracing&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="3" />
  <row Id="2317" PostTypeId="2" ParentId="2315" CreationDate="2016-04-16T23:14:54.010" Score="3" Body="&lt;p&gt;The error &lt;code&gt;not well-formed        male.json&lt;/code&gt; doesn't appear to have anything to do with the code you've posted. I don't see any references to a JSON file called &lt;code&gt;male.json&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this file is in your code somewhere, the error is telling you the problem. It is malformed. Paste the contents of the file into an online validation tool such as &lt;a href=&quot;http://jsonlint.com/&quot; rel=&quot;nofollow&quot;&gt;JSONLint&lt;/a&gt;. It will tell you what is making it malformed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, you're passing JSON into &lt;code&gt;ObjectLoader.load&lt;/code&gt; but you don't do anything with the results. Instead try this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var loader = new THREE.ObjectLoader();&#xA;loader.load('somemodel.json', function (object) {&#xA;    scene.add(object);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Check out &lt;a href=&quot;http://threejs.org/docs/#Reference/Loaders/ObjectLoader&quot; rel=&quot;nofollow&quot;&gt;the documentation&lt;/a&gt; for more information.&lt;/p&gt;&#xA;" OwnerUserId="3143" LastActivityDate="2016-04-16T23:14:54.010" CommentCount="0" />
  <row Id="2318" PostTypeId="1" AcceptedAnswerId="2319" CreationDate="2016-04-16T23:42:21.557" Score="1" ViewCount="27" Body="&lt;p&gt;I found many sites, books and articles, all about especifics algorithms, I listed some of them (like &lt;em&gt;bounding boxes&lt;/em&gt;, &lt;em&gt;bounding spheres&lt;/em&gt;, &lt;em&gt;octrees&lt;/em&gt; and &lt;em&gt;KD-tree&lt;/em&gt;), but any of these founds lists every algorithm for collision detection, and thats what I need.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Someone knows the name of all algorithms (or at least some of them, any other algorithm would help) for collision detection, or knows where I can find it?&lt;/p&gt;&#xA;" OwnerUserId="3144" LastActivityDate="2016-04-16T23:53:54.400" Title="Collision detection methods for 2D and 3D environments" Tags="&lt;algorithm&gt;" AnswerCount="1" CommentCount="1" ClosedDate="2016-04-17T08:43:38.453" />
  <row Id="2319" PostTypeId="2" ParentId="2318" CreationDate="2016-04-16T23:53:54.400" Score="2" Body="&lt;p&gt;There is a book about real-time collision detection that contains a variety of algorithms for all sorts of intersection tests and lists a large number of collision detection algorithms. It's a decent overview if that's what you are looking for:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://realtimecollisiondetection.net&quot; rel=&quot;nofollow&quot;&gt;http://realtimecollisiondetection.net&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://books.google.com/books?isbn=0080474144&quot; rel=&quot;nofollow&quot;&gt;https://books.google.com/books?isbn=0080474144&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3145" LastActivityDate="2016-04-16T23:53:54.400" CommentCount="1" />
  <row Id="2320" PostTypeId="1" CreationDate="2016-04-17T11:42:57.663" Score="3" ViewCount="69" Body="&lt;p&gt;I am trying to implement for research purposes a path tracer, but so far but results are not so good and I will explain you why. The general idea before getting to the code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am working on paths that are generated before sampling them. I mean that the first step of my algorithm consists in calculating a path for a certain pixel (x,y). This path will perform some bounces within the scene and, if terminates on the light, will be considered valid, which means that I can calculate its contribution for the pixel (x,y) which otherwise will be black.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But first, my aim is to reach an &quot;acceptable&quot; image with a few number of SPP (around 16-32 spp). This is because I am following the code of a target framework, easy implemented, that manages to reach such good results in few steps (explained later). The following is the  target image that I want to reach, produced by the target framework:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/QHG2z.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/QHG2z.jpg&quot; alt=&quot;Target image&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's rendered with 16SPP. The core code behind it is quite straightforward. It just implements the rendering equation and it is shown and commented below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec Sample(vec3 O, vec3 D, int depth)//oriding, direction, depth of the recursive step&#xA;{&#xA;  vec3 color(0, 0, 0);&#xA;  float t;&#xA;  if (checkIfIntersectSomething(D, t))&#xA;  {&#xA;&#xA;    vec3 I = O + t * D; //get to the intersection point on the object&#xA;    vec3 diffuse = getMaterial(I);&#xA;&#xA;    vec3 L = vec3(-1 + Rand(2.0f), 20, 9 + Rand(2.0f)) - I; //(-1,20,9) is Hard-code of the light position, and I add Rand(2.0f) on X and Z axis&#xA;    //so that I have an area light instead of a point light&#xA;    L = normalize(L);&#xA;    float ndotl = dot(I.getNormal(), L); //the closer the dotProdutc is to 1.0, the more the light and surface face each other&#xA;    if (ndotl &amp;gt; 0)&#xA;    {&#xA;        if (!checkRayLightIntersection(L)) {&#xA;            float dist = distFromLight(I);&#xA;            color += diffuse * ndotl * vec3(1000.0f, 1000.0f, 850.0f) * (1.0f / (dist * dist));&#xA;        }&#xA;    }&#xA;&#xA;    // russian roulette&#xA;    float Psurvival = CLAMP((diffuse.r + diffuse.g + diffuse.b) * 0.33333f, 0.2f, 0.8f);&#xA;&#xA;    // continue random walk &#xA;    float rand = Rand(1.0f);&#xA;    if (depth &amp;lt; 10 &amp;amp;&amp;amp; rand &amp;lt; Psurvival)&#xA;    {&#xA;        //Besides russian roulette, I also do another weight, because rays that go towards the horizon will bring back very little energy&#xA;        //so I make a random distribution that favours those rays who are close to the normal of the hit point, this is DiffuseReflectionCosineWeighted(). It creates a Random bounce but proportional to N dot R&#xA;        vec3 R = DiffuseReflectionCosineWeighted(I.getNormal());//there is a weight&#xA;&#xA;        float prob = 1.0;&#xA;        float cosTheta = fabs(dot(I.getNormal(), R));&#xA;        if (cosTheta &amp;gt; 1e-6) prob = cosTheta / M_PI;&#xA;&#xA;        color += diffuse * Sample(I + R * EPSILON, R, depth + 1) * (1.0f / Psurvival); //the cosTheta() of the attenuation of the rendering equation gets simplified with the cosTheta of the &quot;prob&quot;&#xA;        //the PI of the prob gets simplified with the BRDF where we are using the ideal BRDF = diffuse/PI&#xA;    }&#xA;  }&#xA;return color;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;DiffuseReflectionCosineWeighted&lt;/strong&gt; implies that we have to apply a PDF to our recursive step. This PDF is &lt;strong&gt;cosTheta/PI&lt;/strong&gt; and we have to divide for it in:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;color += diffuse * Sample(I + R * EPSILON, R, depth + 1) * (1.0f / Psurvival);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You can't see it in the code because it gets simplified witht the &lt;strong&gt;cosTheta&lt;/strong&gt; attenuation term of the rendering equation and &lt;strong&gt;PI&lt;/strong&gt; of the diffuse BRDF where my BRDF is indeed &lt;strong&gt;diffuse/PI&lt;/strong&gt; because we want to consider only DIFFUSE objects.&#xA;(For any question about the code do not hesitate to ask.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to emulate this code with the difference that I don't calculate the path at the moment I want to calculate the color of a pixel (like the target framework does above), but I calculate a path beforehand:&#xA;My &lt;strong&gt;generatePath()&lt;/strong&gt; method indeed tracks the path into the scene and checks all the vertices it hits. The &lt;strong&gt;checkIfRayIntersectSomething(t)&lt;/strong&gt; method you will see used, it's just a pseudo method implemented in my framework and that I omit posting cause of its length. I use it to check if my ray hits something in the scene, if it does, it update the &quot;t&quot; with the distance to that object. NOTE: the light is not considered an object itself. Hence, I also have a &lt;strong&gt;checkRayLightIntersection(hitLightPoint)&lt;/strong&gt; which checks the intersection with the light, if there is any, the hitLightPoint is updated with the point on the light I have been hitting.&#xA;The light is a 2D surface of area 2x2 placed at the same position (-1,20,9), as the target framework does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First the definition of a couple of struct to store some info:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;struct PathVert {&#xA;   vec3 p; vec3 n; //hit point and normal of the surface hit &#xA;};&#xA;&#xA;struct Path {&#xA;   PathVert verts[MAX_DEPTH]; //maxDepth is 15 for now&#xA;   int vertCount;&#xA;   int x, y; //which pixel this path is referring to&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And now the main code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool GeneratePath(int x, int y, Path &amp;amp;path){&#xA;   path.x = x;&#xA;   path.y = y;&#xA;   path.vertCount = 0;&#xA;&#xA;   vec3 P = renderData.p1 + renderData.dx * ((float)(x) + Rand(1)) +  renderData.dy * ((float)(y) + Rand(1));&#xA;   vec3 O = renderData.E + vec3(Rand(0.4f) - 0.2f, Rand(0.4f) - 0.2f, Rand(0.4f) - 0.2f);&#xA;   vec3 D = normalize(P - O); //direction of the first ray, the one from the camera towards the pixel we are considering&#xA;&#xA;   for (int depth = 1; depth &amp;lt;= MAXDEPTH; depth++){&#xA;    float t;&#xA;    Vec hitLightPoint;&#xA;    PathVert vert;&#xA;    if (!checkIfRayIntersectSomething(t)){&#xA;        //we didn't find any object.. but we still may have found the light which is an object non represented in the scene&#xA;        //the depth check avoids me rendering the light as a white plane&#xA;        if (depth &amp;gt; 1 &amp;amp;&amp;amp; checkRayLightIntersection(O, D, hitLightPoint)){&#xA;            //update the vertex since we realized it's the light&#xA;            vert.p = hitLightPoint;&#xA;            vert.n = Vec(0, -1, 0);//cause the light is pointing down&#xA;            path.verts[depth - 1] = vert;&#xA;            path.vertCount++;&#xA;            return true; //light hit, path completed    &#xA;        }&#xA;        return false; //nothing hit, path non valid&#xA;    }&#xA;    //otherwise I got a hit into the scene&#xA;    vert.p = O + D * t; //reach the hitPoint&#xA;    vert.n = methodToFindTheNormal();&#xA;    vert.color = CalculateColor(vert.p); //according to the material properties (only diffuse objects so far)&#xA;    path.verts[depth - 1] = vert;&#xA;    path.vertCount++;&#xA;&#xA;    //since I have the light, and a path terminates when it hits the light, I have to check out also if my ray hits this light,&#xA;    //and if does, I have to check whether it first hits the light or the object just calculated above&#xA;    //moreover with the &quot;depth &amp;gt; 1&quot; check, I avoid again rendering the light which otherwise would be visible as a white plane&#xA;&#xA;    if (depth &amp;gt; 1 &amp;amp;&amp;amp; checkRayLightIntersection(O, D, hitLightPoint)){&#xA;        float distFromObj = length(vert.p - O);&#xA;        float distFromLight = length(hitLightPoint - O);&#xA;        if (distFromLight &amp;lt; distFromObj){&#xA;            //update the vertex since we realized it's the light&#xA;            vert.p = hitLightPoint;&#xA;            vert.n = Vec(0, -1, 0);&#xA;            vert.color = Vec(1, 1, 1);// TODO light color? or light emission?&#xA;&#xA;            path.verts[depth - 1] = vert;&#xA;            return true; //light hit, path completed&#xA;        }&#xA;    }&#xA;    if (depth == MAXDEPTH) return false;&#xA;       Vec newDir = BSDFDiffuseReflectionCosineWeighted(vert.n, D);//explained later&#xA;       D = newDir;&#xA;       O = vert.p;&#xA;   }&#xA;   return false;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;BSDFDiffuseReflectionCosineWeighted()&lt;/strong&gt; just calculates the new direction like in the target framework, tested and working. What remains last is the Sample method which calculates the final color of the pixel using the path calculated right above:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec Sampling(Path &amp;amp;path){&#xA;&#xA;   Vec color(1, 1, 1);&#xA;&#xA;   for (int vert = 0; vert &amp;lt; path.vertCount - 1; vert++) { //considers the last vertex as the light&#xA;      const PathVert &amp;amp;currVert = path.verts[vert];&#xA;      const PathVert &amp;amp;nextVert = path.verts[vert + 1];&#xA;      Vec wo = (nextVert.p - currVert.p).norm();&#xA;      double cosTheta = fabs(wo.dot(currVert.n));&#xA;      float PDF = cosTheta/PI;&#xA;      if (cosTheta &amp;lt;= 1e-6) return Vec();&#xA;      //considering only DIFFUSE objects&#xA;      color = color.mult(currVert.color * (cosTheta / M_PI) / PDF);&#xA;   }&#xA;   return color.mult(Vec(10.0f, 10.0f, 10.0f)); //multiplication for the light emission?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is that the image is really dark, you can see how small the light is, but somehow in the target framework it works correctly while in mine:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8aYPH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8aYPH.png&quot; alt=&quot;Dark render&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is because not many rays reach the light, so many paths are not considered valid. If instead I make my light broad (20x20) results are better but I lose the shadows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/JvGhf.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/JvGhf.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am sure that less rays reach the light cause I have been calculating them in both cases: small area light and big area light. I also tried to compensate the dark image by increasing the light contribution when reached by the path, but it brings to white pixels (for paths that reach the light) and the black ones (cause of invalid path) keep remaining black, so not a good approach. It seems that (as far as my understanding arrives) the target framework applies direct lighting, that's why such a good image BUT according to theory direct lighting only decrease variance, reducing the noise.. which means that using or not direct lighting shouldn't change the resulting image so much. Is there a way according to you to fix this problem? Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="3069" LastActivityDate="2016-04-18T04:49:34.327" Title="How to compensate low amount of rays reaching the light in a Path Tracer" Tags="&lt;c++&gt;&lt;lighting&gt;&lt;shadow&gt;&lt;pathtracing&gt;&lt;global-illumination&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2322" PostTypeId="2" ParentId="2316" CreationDate="2016-04-17T17:40:13.913" Score="5" Body="&lt;p&gt;The Russian roulette technique itself is a way of terminating paths without introducing systemic bias. The principle is fairly straightforward: if at a particular vertex you have a 10% chance of arbitrarily replacing the energy with 0, and if you do that an infinite number of times, you will see 10% less energy. The energy boost just compensates for that. If you did not compensate for the energy lost due to path termination, then Russian roulette would be biased, but the whole technique is a useful method of avoiding bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I was an adversary looking to prove that the &quot;terminate paths whose contribution is less than some small fixed value&quot; technique is biased, I would construct a scene with lights so dim that the contributing paths are &lt;em&gt;always&lt;/em&gt; less than that value. Perhaps I'm simulating a low-light camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But of course you could always expose the fixed value as a tweakable parameter to the user, so they can drop it even further if their scene happens to be low-light. So let's disregard that example for a minute.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What happens if I consider an object that is illuminated by a lot of very low-energy paths that are collected by a &lt;a href=&quot;https://en.wikipedia.org/wiki/Parabolic_reflector&quot;&gt;parabolic reflector&lt;/a&gt;? Low energy paths don't &lt;em&gt;necessarily&lt;/em&gt; bounce around indiscriminately in a manner that you can completely neglect. Similarly reasoning applies for, e.g., cutting off paths after a fixed number of bounces: you can construct a scene with a path that bounces off a series of 20 mirrors before hitting an object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another way of looking at it: if you set the contribution of a path to 0 after it falls below some fixed epsilon, how do you correct for that energy loss? You aren't simply reducing the total energy by some fraction. You don't know anything about how much energy you are neglecting, because you are cutting off at some &lt;em&gt;contribution&lt;/em&gt; threshold before you know the other factor: the incident energy.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2016-04-17T17:40:13.913" CommentCount="0" />
  <row Id="2323" PostTypeId="2" ParentId="2310" CreationDate="2016-04-17T18:17:05.423" Score="3" Body="&lt;p&gt;The near clipping plane is a fundamental feature of projective rasterization. To borrow a diagram from Eric Lengyel's &lt;a href=&quot;http://www.terathon.com/gdc07_lengyel.pdf&quot; rel=&quot;nofollow&quot;&gt;Projection Matrix Tricks&lt;/a&gt; presentation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/wd9gx.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/wd9gx.png&quot; alt=&quot;Diagram of camera space to NDC transform&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The displayed region of clip space (that you can think of rasterization as happening in) is a box which has a &quot;near&quot; face. This is associated with the near clip plane. But we can push the far clip plane out to infinity, so can we pull the near clip plane in to zero?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Mathematically, you absolutely can! The only point that presents a problem is the eye itself: you can get infinitely close to the eye and still resolve to a single point in clip space. However, if you have a triangle that passes through the eye, you still need to clip it, and to clip it you need an actual plane, not a concept like &quot;infinitely close to the eye.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;More critically for the current rasterization pipeline, the perspective divide does fun things with numerical precision. See the line at NDC 0 in the diagram above? It's halfway through the viewing volume in clip space, but not at all near halfway through the view frustum in camera space. In fact, its position depends on the near clip plane's distance to the eye! This means that over half of the depth range in clip space is in that region near the eye. As you get the near clip plane closer and closer to the eye, that plane pulls in even closer and you waste more and more depth range. This isn't necessarily fundamental—you don't have to store z/w in the depth buffer—but it is convenient. Here's an &lt;a href=&quot;https://developer.nvidia.com/content/depth-precision-visualized&quot; rel=&quot;nofollow&quot;&gt;article from Nathan Reed&lt;/a&gt; about depth precision, and a few notes about &lt;a href=&quot;http://www.humus.name/index.php?ID=255&quot; rel=&quot;nofollow&quot;&gt;why the projected z/w depth is useful&lt;/a&gt; from Emil Persson.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, I'll plug &lt;a href=&quot;https://fgiesen.wordpress.com/2011/07/05/a-trip-through-the-graphics-pipeline-2011-part-5/&quot; rel=&quot;nofollow&quot;&gt;a post in the excellent &quot;A trip through the Graphics Pipeline&quot;&lt;/a&gt; series, which is very interesting and references homogeneous rasterization algorithms which, in theory, could avoid the need to do clipping in projected clip space, which would totally avoid some of these traps.&lt;/p&gt;&#xA;" OwnerUserId="196" LastEditorUserId="196" LastEditDate="2016-04-17T19:31:05.250" LastActivityDate="2016-04-17T19:31:05.250" CommentCount="1" />
  <row Id="2324" PostTypeId="1" AcceptedAnswerId="2327" CreationDate="2016-04-17T18:31:13.193" Score="1" ViewCount="80" Body="&lt;p&gt;So I have a vector of &lt;code&gt;glm::vec3&lt;/code&gt; containing the triangles for the classic Cornell Box called triangles. The Draw method casts a ray for each pixel on the screen and calls &lt;code&gt;ClosestIntersection&lt;/code&gt; which returns true or false and updates a struct containing things like colour and the point of that intersection. Then I simply draw the pixels based on the colours (black for no intersection). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The result is very inconsistent and I can't find the error:&#xA;&lt;a href=&quot;http://i.imgur.com/nQlyeC4.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.imgur.com/nQlyeC4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void Draw()&#xA;{&#xA;    if (SDL_MUSTLOCK(screen))&#xA;        SDL_LockSurface(screen);&#xA;&#xA;    // Loop through each pixel and cast ray&#xA;    for (int y = 0; y&amp;lt;SCREEN_HEIGHT; ++y)&#xA;    {&#xA;        for (int x = 0; x&amp;lt;SCREEN_WIDTH; ++x)&#xA;        {&#xA;            vec3 dir(x - SCREEN_WIDTH / 2, y - SCREEN_HEIGHT / 2, focalLength);&#xA;            Intersection isect;&#xA;            if (ClosestIntersection(cameraPos, dir, triangles, isect))&#xA;            {&#xA;                vec3 color = triangles[isect.triangleIndex].color;&#xA;                PutPixelSDL(screen, x, y, color);&#xA;            }&#xA;            else&#xA;            {&#xA;                PutPixelSDL(screen, x, y, vec3(1, 1, 1));&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    if (SDL_MUSTLOCK(screen))&#xA;        SDL_UnlockSurface(screen);&#xA;&#xA;    SDL_UpdateRect(screen, 0, 0, 0, 0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the function called from Draw:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bool ClosestIntersection(vec3 start,&#xA;    vec3 dir,&#xA;    const vector&amp;lt;Triangle&amp;gt;&amp;amp; triangles,&#xA;    Intersection&amp;amp; closestIntersection)&#xA;{&#xA;    bool foundIntersection = false;&#xA;&#xA;    // Loop through all the triangles and check for intersection with ray&#xA;    for (size_t i = 0; i &amp;lt; triangles.size(); i++)&#xA;    {&#xA;        vec3 v0 = triangles[i].v0;&#xA;        vec3 v1 = triangles[i].v1;&#xA;        vec3 v2 = triangles[i].v2;&#xA;        vec3 e1 = v1 - v0;&#xA;        vec3 e2 = v2 - v0;&#xA;        vec3 b = start - v0;&#xA;        mat3 A(-dir, e1, e2);&#xA;        vec3 x = glm::inverse(A) * b;&#xA;&#xA;        // Check that found scalars are within bounds&#xA;        if ((x.x &amp;gt;= 0) &amp;amp;&amp;amp; (x.y &amp;gt; 0) &amp;amp;&amp;amp; (x.z &amp;gt; 0) &amp;amp;&amp;amp; (x.y + x.z &amp;lt; 1))&#xA;        {&#xA;            if (foundIntersection)  // If there's a previvous intersection for this ray check that new one is closer to camera&#xA;            {&#xA;                if (glm::distance(start, x) &amp;lt; closestIntersection.distance)&#xA;                {&#xA;                    closestIntersection.distance = glm::distance(start, x);&#xA;                    closestIntersection.triangleIndex = i;&#xA;                    closestIntersection.position = x;&#xA;                }&#xA;            }&#xA;            else&#xA;            {&#xA;                foundIntersection = true;&#xA;                closestIntersection.distance = glm::distance(start, x);&#xA;                closestIntersection.triangleIndex = i;&#xA;                closestIntersection.position = x;&#xA;            }&#xA;        }&#xA;    }&#xA;    return foundIntersection;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3153" LastEditorUserId="182" LastEditDate="2016-04-18T05:40:09.957" LastActivityDate="2016-04-18T05:40:09.957" Title="Ray-tracing the Cornell Box results in really inconsistent image" Tags="&lt;raytracing&gt;&lt;c++&gt;&lt;glm&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2325" PostTypeId="2" ParentId="2316" CreationDate="2016-04-17T18:59:48.360" Score="11" Body="&lt;p&gt;In order to understand Russian Roulette, let's look at a very basic forward path tracer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void RenderPixel(uint x, uint y, UniformSampler *sampler) {&#xA;    Ray ray = m_scene-&amp;gt;Camera.CalculateRayFromPixel(x, y, sampler);&#xA;&#xA;    float3 color(0.0f);&#xA;    float3 throughput(1.0f);&#xA;&#xA;    // Bounce the ray around the scene&#xA;    for (uint bounces = 0; bounces &amp;lt; 10; ++bounces) {&#xA;        m_scene-&amp;gt;Intersect(ray);&#xA;&#xA;        // The ray missed. Return the background color&#xA;        if (ray.geomID == RTC_INVALID_GEOMETRY_ID) {&#xA;            color += throughput * float3(0.846f, 0.933f, 0.949f);&#xA;            break;&#xA;        }&#xA;&#xA;        // We hit an object&#xA;&#xA;        // Fetch the material&#xA;        Material *material = m_scene-&amp;gt;GetMaterial(ray.geomID);&#xA;        // The object might be emissive. If so, it will have a corresponding light&#xA;        // Otherwise, GetLight will return nullptr&#xA;        Light *light = m_scene-&amp;gt;GetLight(ray.geomID);&#xA;&#xA;        // If we hit a light, add the emmisive light&#xA;        if (light != nullptr) {&#xA;            color += throughput * light-&amp;gt;Le();&#xA;        }&#xA;&#xA;        float3 normal = normalize(ray.Ng);&#xA;        float3 wo = normalize(-ray.dir);&#xA;        float3 surfacePos = ray.org + ray.dir * ray.tfar;&#xA;&#xA;        // Get the new ray direction&#xA;        // Choose the direction based on the material&#xA;        float3 wi = material-&amp;gt;Sample(wo, normal, sampler);&#xA;        float pdf = material-&amp;gt;Pdf(wi, normal);&#xA;&#xA;        // Accumulate the brdf attenuation&#xA;        throughput = throughput * material-&amp;gt;Eval(wi, wo, normal) / pdf;&#xA;&#xA;&#xA;        // Shoot a new ray&#xA;&#xA;        // Set the origin at the intersection point&#xA;        ray.org = surfacePos;&#xA;&#xA;        // Reset the other ray properties&#xA;        ray.dir = wi;&#xA;        ray.tnear = 0.001f;&#xA;        ray.tfar = embree::inf;&#xA;        ray.geomID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.primID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.instID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.mask = 0xFFFFFFFF;&#xA;        ray.time = 0.0f;&#xA;    }&#xA;&#xA;    m_scene-&amp;gt;Camera.FrameBuffer.SplatPixel(x, y, color);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;IE. we bounce around the scene, accumulating color and light attenuation as we go. In order to be completely mathematically unbiased, bounces &lt;em&gt;should&lt;/em&gt; go to infinity. But this is unrealistic, and as you noted, not visually necessary; for most scenes, after a certain number of bounces, say 10, the amount of contribution to the final color is very very minimal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in order to save computing resources, many path tracers have a hard limit to the number of bounces. This adds bias.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, it's hard to choose what that hard limit should be. Some scenes look great after 2 bounces; others (say with transmission or SSS) may take up to 10 or 20. &#xA;&lt;a href=&quot;http://i.stack.imgur.com/ikXSo.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ikXSo.jpg&quot; alt=&quot;2 Bounces from Disney&amp;#39;s Big Hero 6&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/BPiZ7.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BPiZ7.jpg&quot; alt=&quot;9 Bounces from Disney&amp;#39;s Big Hero 6&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we choose too low, the image will be visibly biased. But if we choose to high, we're wasting computation energy and time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One way to solve this, as you noted, is to terminate the path after we reach some threshold of attenuation. This also adds bias. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Clamping after a threshold, will &lt;em&gt;work&lt;/em&gt;, but again, how do we choose the threshold? If we choose too large, the image will be visibly biased, too small, and we're wasting resources.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Russian Roulette attempts to solve these problems in an unbiased way. First, here is the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void RenderPixel(uint x, uint y, UniformSampler *sampler) {&#xA;    Ray ray = m_scene-&amp;gt;Camera.CalculateRayFromPixel(x, y, sampler);&#xA;&#xA;    float3 color(0.0f);&#xA;    float3 throughput(1.0f);&#xA;&#xA;    // Bounce the ray around the scene&#xA;    for (uint bounces = 0; bounces &amp;lt; 10; ++bounces) {&#xA;        m_scene-&amp;gt;Intersect(ray);&#xA;&#xA;        // The ray missed. Return the background color&#xA;        if (ray.geomID == RTC_INVALID_GEOMETRY_ID) {&#xA;            color += throughput * float3(0.846f, 0.933f, 0.949f);&#xA;            break;&#xA;        }&#xA;&#xA;        // We hit an object&#xA;&#xA;        // Fetch the material&#xA;        Material *material = m_scene-&amp;gt;GetMaterial(ray.geomID);&#xA;        // The object might be emissive. If so, it will have a corresponding light&#xA;        // Otherwise, GetLight will return nullptr&#xA;        Light *light = m_scene-&amp;gt;GetLight(ray.geomID);&#xA;&#xA;        // If we hit a light, add the emmisive light&#xA;        if (light != nullptr) {&#xA;            color += throughput * light-&amp;gt;Le();&#xA;        }&#xA;&#xA;        float3 normal = normalize(ray.Ng);&#xA;        float3 wo = normalize(-ray.dir);&#xA;        float3 surfacePos = ray.org + ray.dir * ray.tfar;&#xA;&#xA;        // Get the new ray direction&#xA;        // Choose the direction based on the material&#xA;        float3 wi = material-&amp;gt;Sample(wo, normal, sampler);&#xA;        float pdf = material-&amp;gt;Pdf(wi, normal);&#xA;&#xA;        // Accumulate the brdf attenuation&#xA;        throughput = throughput * material-&amp;gt;Eval(wi, wo, normal) / pdf;&#xA;&#xA;&#xA;        // Russian Roulette&#xA;        // Randomly terminate a path with a probability inversely equal to the throughput&#xA;        float p = std::max(throughput.x, std::max(throughput.y, throughput.z));&#xA;        if (sampler-&amp;gt;NextFloat() &amp;gt; p) {&#xA;            break;&#xA;        }&#xA;&#xA;        // Add the energy we 'lose' by randomly terminating paths&#xA;        throughput *= 1 / p;&#xA;&#xA;&#xA;        // Shoot a new ray&#xA;&#xA;        // Set the origin at the intersection point&#xA;        ray.org = surfacePos;&#xA;&#xA;        // Reset the other ray properties&#xA;        ray.dir = wi;&#xA;        ray.tnear = 0.001f;&#xA;        ray.tfar = embree::inf;&#xA;        ray.geomID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.primID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.instID = RTC_INVALID_GEOMETRY_ID;&#xA;        ray.mask = 0xFFFFFFFF;&#xA;        ray.time = 0.0f;&#xA;    }&#xA;&#xA;    m_scene-&amp;gt;Camera.FrameBuffer.SplatPixel(x, y, color);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Russian Roulette randomly terminates a path with a probability inversely equal to the throughput. So path with low throughput that won't contribute much to the scene are more likely to be terminated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we stop there, we're still biased. We 'lose' the energy of the path we randomly terminate. To make it unbiased, we boost the energy of the non-terminated paths by their probability to be terminated. This, along with being random, makes Russian Roulette unbiased.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To answer your last questions:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Does Russian Roulette give an unbiased result?&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Yes&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Is Russian Roulette necessary for an unbiased result?&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Depends on what you mean by unbiased. If you mean mathematically, then yes. However, if you mean visually, then no. You just have to choose you max path depth and cutoff threshold very very carefully. This can be very tedious since it can change from scene to scene.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Can you use a fixed probability (cut-off), and then redistribute the 'lost' energy. Is this unbiased?&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If you use a fixed probability, you are adding bias. By redistributing the 'lost' energy, you reduce the bias, but it is still mathematically biased. To be completely unbiased, it must be random.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;If the energy that would be lost by terminating a ray without redistributing its energy is eventually lost anyway (as the rays to which it is redistributed are also eventually terminated), how does this improve the situation?&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Russian Roulette only stops the bouncing. It doesn't remove the sample completely. Also, the 'lost' energy is accounted for in the bounces up to the termination. So the only way for the the energy to be 'eventually lost anyway' would be to have a completely black room.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;In the end, Russian Roulette is a very simple algorithm that uses a very small amount of extra computational resources. In exchange, it can save a large amount of computational resources. Therefore, I can't really see a reason &lt;em&gt;not&lt;/em&gt; to use it.&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-05-20T20:26:14.153" LastActivityDate="2016-05-20T20:26:14.153" CommentCount="0" />
  <row Id="2326" PostTypeId="1" AcceptedAnswerId="2329" CreationDate="2016-04-17T23:05:46.197" Score="7" ViewCount="105" Body="&lt;p&gt;I'm interested in virtual reality, but according to some sources, less than 1% of computers in use today have the necessary performance to run modern VR games, granted many of them are not intended for gaming, that's still a huge barrier for the widespread adoption of VR headsets into the market. So I was wondering what could be done to reduce the performance requirements of a VR game that hasn't been implemented with such optimizations in mind.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm talking about technologies like foveated rendering, which (ab)uses the fact that we only ever perceive a few degrees in the center of our field of vision as sharp while everything else is blurry. This can be combined with eye tracking so that only the areas where the player is looking are rendered in full quality, while we can reduce the render quality and therefore the performance requirements everywhere else.&#xA;This seems like a great advantage and I honestly wonder why none of the major VR companies have implemented eye tracking into their headsets and foveated rendering into their SDKs into their headsets. The technology exists and I can't imagine that it would add much to the price tag.&#xA;&lt;a href=&quot;http://research.microsoft.com/pubs/176610/foveated_final15.pdf&quot; rel=&quot;nofollow&quot;&gt;Paper about foveated rendering&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's also instanced stereo rendering which basically renders things to both eyes at the same time rather than rendering it for each eye individually.&#xA;&lt;img src=&quot;https://docs.unrealengine.com/latest/images/Support/Builds/ReleaseNotes/2016/4_11/image_16.gif&quot; alt=&quot;Example gif for Instanced Stero Rendering - Epic Games, Unreal Engine 4.11 release notes&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's also possible to use just one frustum for both eyes rather than two as discussed here.&#xA;&lt;a href=&quot;http://computergraphics.stackexchange.com/questions/1736/vr-and-frustum-culling&quot;&gt;VR and frustum culling&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But are there other approaches to the performane problem?&lt;/p&gt;&#xA;" OwnerUserId="3156" LastEditorUserId="3156" LastEditDate="2016-04-18T20:32:32.223" LastActivityDate="2016-04-22T07:13:59.907" Title="What methods/technologies to reduce required performance for virtual reality are there?" Tags="&lt;rendering&gt;&lt;performance&gt;&lt;optimisation&gt;&lt;virtual-reality&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="2327" PostTypeId="2" ParentId="2324" CreationDate="2016-04-17T23:24:54.573" Score="3" Body="&lt;h1&gt;Two symptoms&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;There appear to be two problems with the image.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The background is showing through along the line between adjacent triangles.&lt;/li&gt;&#xA;&lt;li&gt;The colour displayed is not always from the closest intersection.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note that the background colour is white, rather than black, due to the line:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                PutPixelSDL(screen, x, y, vec3(1, 1, 1));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Why this background shows through at the triangle edges I do not know. However, the second problem is identified below, so I recommend you fix that first and then the first problem can be analysed in isolation without the additional distractions.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;Cause of the problem with &lt;code&gt;ClosestIntersection&lt;/code&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;ClosestIntesection&lt;/code&gt; function returns &lt;code&gt;true&lt;/code&gt; when an intersection is found, and updates an &lt;code&gt;Intersection&lt;/code&gt; object called &lt;code&gt;closestIntersection&lt;/code&gt;. However, the intersection stored here is not always the closest to the camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The reason for this is here:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                closestIntersection.distance = glm::distance(start, x);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This appears in two places so it will need to be fixed in both.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;start&lt;/code&gt; is &lt;code&gt;cameraPos&lt;/code&gt;, presumably in world coordinates. &lt;code&gt;x&lt;/code&gt; appears to be in the local coordinates of the triangle being intersected (one of the triangle vertices being the origin). The distance between two points in different coordinate systems is meaningless, so that in different regions of a triangle it will appear to be behind or in front of another triangle for no immediately apparent reason.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If &lt;code&gt;x&lt;/code&gt; is first converted to world coordinates then the distance will be correct and the intersection chosen will be the closest to the camera.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2016-04-17T23:24:54.573" CommentCount="1" />
  <row Id="2328" PostTypeId="2" ParentId="2320" CreationDate="2016-04-18T04:49:34.327" Score="1" Body="&lt;h3&gt;A note first&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;From the look of your screen capture, I suspect there might still be a bug in your code. Noise is to be expected with only 16 spp, but your picture still looks surprisingly dark to me. For comparison, here is what my implementation of SmallPT looks like with 16 spp, 15 bounces, and no next event prediction:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zFFjC.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zFFjC.png&quot; alt=&quot;SmallPT, 16 spp, 15 bounces, no next event predicition&quot;&gt;&lt;/a&gt;&#xA;(&lt;a href=&quot;https://www.shadertoy.com/view/4sfGDB&quot; rel=&quot;nofollow&quot;&gt;here it is on ShaderToy&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Noisy, but not nearly as dark as your picture. It is very different scene, but considering the that the shape is similar (a box with one side open and only one light source), I would expect to get a similar luminosity/noise appearance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;The rest of this answer assumes your code to be correct.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Path tracing and reducing noise&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The balance between light source size, noise and SPP is a classic problem, to which there is no magical solution. From easiest to hardest, you could:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Use a scene that is more suitable to your algorithm:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rendering algorithms have limitations, may shine in certain cases and poorly handle other cases. Knowing the limitations of yours, you can choose to restrict its use to a case that works best for it: a large light source, a sky dome, many light sources, a scene with less occlusion, etc.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Implement next event prediction:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When you know where your light sources are, at each bounce you can try and shoot rays directly toward them. If the ray doesn't hit other geometry first, you get a path that contributes. Since doing so biases the randomness, you will need to use proper factors to take it into account.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Implement &lt;a href=&quot;http://www.graphics.cornell.edu/~eric/Portugal.html&quot; rel=&quot;nofollow&quot;&gt;Bi-Directional Path Tracing&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When tracing rays from the camera, if the light source is small, a large number of paths will never reach it. This is the problem you are having. On the opposite, when tracing rays from the light, many of them will never reach the camera. BDPT consists in first shooting rays both from the camera and from the light source, then trying to connect them. Each pair that can be connected is a contributing path. BDPT will significantly reduce noise.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Implement &lt;a href=&quot;http://graphics.stanford.edu/papers/metro/&quot; rel=&quot;nofollow&quot;&gt;Metropolis Light Transport&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If there is a path between the camera and a light source, it is likely there will be a similar path between them. MLT consists in searching for those contributing paths in the neighborhood of paths that are already known to contribute by mutating them (adding, removing or changing one or more bounces). This is a lot less trivial already, as the math to keep it unbiased gets a bit hairy.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The problem of reducing noise is still under active research though, and there is no silver bullet. At this point you will need to read what are the latest developments.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-18T04:49:34.327" CommentCount="1" />
  <row Id="2329" PostTypeId="2" ParentId="2326" CreationDate="2016-04-18T05:05:56.010" Score="9" Body="&lt;h2&gt;Avoid stereo when possible&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;There was some research published recently measuring when users can or cannot tell whether the specular contribution, which is view point dependent, is different between eyes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://software.intel.com/en-us/articles/perception-of-highlight-disparity-at-a-distance-in-consumer-head-mounted-displays&quot;&gt;Perception of Highlight Disparity at a Distance in Consumer Head-Mounted Displays&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the future, this could be use to share some computation between the two eye renders. Unfortunately this sounds hard to take advantage of in a typical rasterization pipeline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some games already avoid stereo when rendering far objects and background.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Reduce shading when possible&lt;/h2&gt;&#xA;&#xA;&lt;h3&gt;Lens distortion&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Part of the NVidia VRWorks SDK is a feature, &lt;a href=&quot;https://blogs.nvidia.com/blog/2016/04/20/ue-4/&quot;&gt;Multi-Res Shading&lt;/a&gt;, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;a href=&quot;http://media.steampowered.com/apps/valve/2015/Alex_Vlachos_Advanced_VR_Rendering_GDC2015.pdf&quot;&gt;Alex Vlachos' GDC 2015 presentation&lt;/a&gt;, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Foveated rendering&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some &lt;a href=&quot;http://research.lighttransport.com/foveated-real-time-ray-tracing-for-virtual-reality-headset/&quot;&gt;research on the topic&lt;/a&gt;, but this requires eye tracking like in the &lt;a href=&quot;http://www.getfove.com/&quot;&gt;Fove&lt;/a&gt; HMD.&lt;/p&gt;&#xA;" OwnerUserId="182" LastEditorUserId="182" LastEditDate="2016-04-22T07:13:59.907" LastActivityDate="2016-04-22T07:13:59.907" CommentCount="0" />
  <row Id="2330" PostTypeId="2" ParentId="2326" CreationDate="2016-04-18T07:56:36.613" Score="8" Body="&lt;p&gt;Alex Vlachos from Valve has had two great GDC talks about this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://media.steampowered.com/apps/valve/2015/Alex_Vlachos_Advanced_VR_Rendering_GDC2015.pdf&quot;&gt;the one from 2015&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.gdcvault.com/play/1021771/Advanced-VR&quot;&gt;the one from 2016&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Other than that (and what you have linked yourself), there isn't much left to do than to simply optimize your app until you spend at most 10 ms per frame (100 Hz, targeting a 90 Hz display + margin). Standard rendering optimizations apply. Apply a technique, profile, determine its impact on performance, rinse, repeat.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You may also exercise caution with that UE4 stereo instancing from Epic. In my experience, its performance boost depends heavily on the game's rendering workload. I had a statically lit game with lots of instanced foliage and this feature has actually been a net loss, probably due to the overhead of left/right eye dynamic branching in the shaders; while a fully dynamically-lit scene may benefit from it all right. As I said before: profile to ensure that a technique actually helps performance.&lt;/p&gt;&#xA;" OwnerUserId="2817" LastActivityDate="2016-04-18T07:56:36.613" CommentCount="0" />
  <row Id="2331" PostTypeId="1" CreationDate="2016-04-18T07:59:10.987" Score="5" ViewCount="18" Body="&lt;p&gt;In &lt;a href=&quot;https://hal.inria.fr/hal-01024289/document&quot; rel=&quot;nofollow&quot;&gt;Understanding the Masking-Shadowing Function in Microfacet-Based BRDFs&lt;/a&gt;, Eric Heitz derives the Separable Masking and Shadowing function as (P.83-84):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1F17w.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1F17w.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the mentioned &lt;a href=&quot;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&quot; rel=&quot;nofollow&quot;&gt;Microfacet Models for Refraction through Rough Surfaces&lt;/a&gt; by Walter et al., Smith term is computed as:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Sxwgo.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Sxwgo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't see a parameterisation of Λ that could make appear the generic form (Heitz version) from the specific form derived by Walter. What am I missing?&lt;/p&gt;&#xA;" OwnerUserId="110" LastEditorUserId="231" LastEditDate="2016-04-18T13:51:41.537" LastActivityDate="2016-04-18T13:51:41.537" Title="Why does the G1 term have a 2 in the numerator in Walter's formula, but not in Heitz's generalized formula?" Tags="&lt;function&gt;&lt;masking&gt;&lt;shadowing&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2332" PostTypeId="1" CreationDate="2016-04-18T08:53:27.960" Score="0" ViewCount="52" Body="&lt;p&gt;Suppose i have a image and I want to convert the portion of image that is red to blue(or any other color). One way to do this is extract red part of image using HSV or other color values and convert red hsv value to corresponding blue. Is there any other methods(better) for this. What sort of algorithms do Photoshop color replacement tool use?&lt;/p&gt;&#xA;" OwnerUserId="2383" LastActivityDate="2016-04-22T05:32:42.897" Title="Change particular color in an image" Tags="&lt;color&gt;&lt;color-science&gt;&lt;color-management&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="2333" PostTypeId="2" ParentId="2331" CreationDate="2016-04-18T08:58:40.510" Score="2" Body="&lt;p&gt;Actually I just had to solve the following equation to find a solution, consistent with the presence of the &quot;2&quot; in the numerator:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\frac{1}{1+\Lambda (m)}=\frac{2}{1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$1+\Lambda (m)=\frac{1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}{2}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$\Lambda (m)=\frac{-1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}{2}$&lt;/p&gt;&#xA;" OwnerUserId="110" LastEditorUserId="110" LastEditDate="2016-04-18T10:21:12.167" LastActivityDate="2016-04-18T10:21:12.167" CommentCount="0" />
  <row Id="2334" PostTypeId="2" ParentId="2332" CreationDate="2016-04-18T14:39:31.007" Score="2" Body="&lt;p&gt;Assuming you are looking to transform a particular component (r, g or b) of any color, this can be easily done by simple vector/matrix multiplication.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The introduction on this page (Chapter 6.1) - from the most excellent immersivemath.com site - illustrates this perfectly:&#xA;&lt;a href=&quot;http://immersivemath.com/ila/ch06_matrices/ch06.html&quot; rel=&quot;nofollow&quot;&gt;http://immersivemath.com/ila/ch06_matrices/ch06.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Think of it as the &quot;Channel Mixer&quot; command in PhotoShop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I described above is one of many ways of changing a given color. The PS Color Replacement Tool simply applies that to any pixel that matches certain conditions defined by the user (source color, destination color, tolerance, etc...). The nifty thing they did is put it in the form of a brush tool, allowing the user artistic control over where to apply the color transform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically it boils down to this: if a pixel under the brush matches the conditions set by the user the color transform is applied. If not, leave the pixel as is.&lt;/p&gt;&#xA;" OwnerUserId="3128" LastEditorUserId="3128" LastEditDate="2016-04-19T08:30:39.597" LastActivityDate="2016-04-19T08:30:39.597" CommentCount="1" />
  <row Id="2336" PostTypeId="1" AcceptedAnswerId="2344" CreationDate="2016-04-19T08:12:14.370" Score="4" ViewCount="38" Body="&lt;p&gt;When I saw an implementation of normal mapping that computed the &lt;em&gt;TBN matrix&lt;/em&gt; in the vertex shader and converted everything (in particular the view vector and light vector) to tangent space at that stage, it seemed like the proper way to do. This way there is no matrix-vector multiplication left to do in the fragment shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Enters image based lighting: for each fragment we want to fetch from an image, typically one or more cube map textures, the light intensity coming from certain directions. I assume this data is in world space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: is it possible to use IBL while retaining the vertex shader transform optimization mentioned above?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It seems to me we still need to transform our fetch direction from tangent space to world space (or whichever space the cube map is in).&lt;/li&gt;&#xA;&lt;li&gt;Conversely, if we want to work in world space, like with deferred shading, we need to transform the normal for each fragment.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Does IBL necessarily imply at least one matrix vector multiplication per fragment and the optimization I mentioned is an uncommon case, or am I missing something?&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-04-19T14:32:18.870" Title="Image based lighting, tangent space coordinates, and optimization" Tags="&lt;optimisation&gt;&lt;matrices&gt;&lt;ibl&gt;&lt;normal-mapping&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2337" PostTypeId="1" CreationDate="2016-04-19T09:07:32.807" Score="1" ViewCount="23" Body="&lt;p&gt;The GL Transmission Format comes along with a JSON styled main file which basicly describes the scene and binary files which contain the buffers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/RtFrg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/RtFrg.png&quot; alt=&quot;gltf basic structure&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm currently writing a WebGL library and I need to work alot with the vertex and index buffers. So, my question now is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Would it be possible to store plain text array buffers in the gltf (e.g., as JSON) instead of generating binary blobs always when the buffers are adjusted?&lt;/p&gt;&#xA;" OwnerUserId="361" LastActivityDate="2016-04-19T09:40:01.340" Title="Is it possible to store the plain buffer data in gltf files?" Tags="&lt;webgl&gt;&lt;data-structure&gt;&lt;vertex-buffer-object&gt;&lt;gltf&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2338" PostTypeId="1" CreationDate="2016-04-19T09:18:40.673" Score="1" ViewCount="25" Body="&lt;p&gt;I'm looking for any way which enables me to quickly generate gltf files including the conversion of plain array buffers (vertex, index, color, etc.) into binary (bgltf, glb, bin) files.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm asking because it currently looks like there are only high level applications like Maya, Blender available which can do that or converters to indirectly write glTF from COLLADA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any way to generate binary gltf array buffers using low level / command line utilities?&lt;/p&gt;&#xA;" OwnerUserId="361" LastEditorUserId="361" LastEditDate="2016-04-20T08:15:42.833" LastActivityDate="2016-04-20T08:34:13.520" Title="How to generate binary gltf array buffers?" Tags="&lt;data-structure&gt;&lt;gltf&gt;&lt;buffers&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2339" PostTypeId="2" ParentId="2337" CreationDate="2016-04-19T09:40:01.340" Score="2" Body="&lt;p&gt;Embedding human readable data is not supported. However you can put the data into a &lt;a href=&quot;https://tools.ietf.org/html/rfc2397&quot; rel=&quot;nofollow&quot;&gt;data uri&lt;/a&gt; to store the data inline in base64 format.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;buffers&quot;: {&#xA;    &quot;a-buffer-id&quot;: {&#xA;        &quot;byteLength&quot;: 1024,&#xA;        &quot;type&quot;: &quot;arraybuffer&quot;,&#xA;        &quot;uri&quot;=&quot;data:application/octet-stream;base64,...&quot;&#xA;    }&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-04-19T09:40:01.340" CommentCount="0" />
  <row Id="2340" PostTypeId="2" ParentId="2338" CreationDate="2016-04-19T09:53:41.033" Score="1" Body="&lt;p&gt;Technically any file format where you can dump the entire file into a VBO and then render from that will work for the .bin files. Unfortunately those formats are less well known than they should be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then you only need to adjust the bufferview elements to reference the proper subset of the file&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-04-19T09:53:41.033" CommentCount="0" />
  <row Id="2341" PostTypeId="2" ParentId="2314" CreationDate="2016-04-19T12:00:41.240" Score="2" Body="&lt;p&gt;Here is an interesting answer from Styves on gamedev.net.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Calculating bent normals is just an extension of AO calculation where&#xA;  the direction of each sample is also averaged along with the occlusion&#xA;  amount.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/HH0zw.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HH0zw.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;The bent normals from substance painter seem correct. In any region&#xA;  that there is no AO, your normal won't be bent in any direction,&#xA;  therefore in tangent space it's pointing straight up along the surface&#xA;  normal. This is why you only see details in the ears.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I have no idea how Unity handles the bent normals but the usual way is&#xA;  to, instead of applying AO as a standard multiplier onto the lighting&#xA;  result (which gives this weird muddy look), use the dot product of the&#xA;  bent normal and the light instead. This is how we handle SSDO in&#xA;  CryENGINE for example.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;I've only ever used a texture-based version of this technique once,&#xA;  however in this version the bent normals also contained information&#xA;  from the original tangent space normal map, and was used directly&#xA;  during lighting in place of the original.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="2372" LastEditDate="2016-04-20T05:44:23.073" LastActivityDate="2016-04-20T05:44:23.073" CommentCount="2" />
  <row Id="2342" PostTypeId="1" AcceptedAnswerId="2353" CreationDate="2016-04-19T12:20:54.443" Score="2" ViewCount="93" Body="&lt;p&gt;I am trying to blend two world space normals inside a shader. One comes from a tangent space normal map converted into world space using a classic TBN matrix and the other one is a mesh normal map in world space.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found some interesting resources here :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://blog.selfshadow.com/publications/blending-in-detail/&quot; rel=&quot;nofollow&quot;&gt;Blending in Detail&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://www.shadertoy.com/view/4t2SzR#&quot; rel=&quot;nofollow&quot;&gt;Shadertoy - Normal Map Blending&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;But those blending technics seem to be only available for tangent space normals, especially the Reoriented Normal Mapping (RNM).&#xA;I tried to apply the RNM technic with unpack already done.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;n1 += vec3(0, 0, 1);&#xA;n2 *= vec3(-1, -1, 1);&#xA;&#xA;return n1 * dot(n1, n2) / n1.z - n2;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But this doesn't give expected results and I don't get why. Is there a way to apply the RNM blending on world space normals ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks a lot.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Here are some output and the result I am having with RNM. The A normal is not disturbed by the B normal, it gives some strange massively reoriented results.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3 nv = dot(normal, viewDir);&#xA;color.rgb = nv;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iFPZp.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iFPZp.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2372" LastEditorUserId="2372" LastEditDate="2016-04-19T20:02:38.247" LastActivityDate="2016-04-21T18:09:25.737" Title="How to blend World Space Normals" Tags="&lt;opengl&gt;&lt;rendering&gt;&lt;shader&gt;&lt;texture&gt;&lt;normal-mapping&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2343" PostTypeId="1" CreationDate="2016-04-19T13:48:31.713" Score="4" ViewCount="42" Body="&lt;p&gt;Let's assume that we have some set of points in 3d space sampled from some arbitrary surface, without self intersections and mostly flat. Is it possible to get any parametrization over this set of points?  And if it is - how?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first idea was to find a plane (Maybe through PCA), or more sophisticated smooth surface which approximate this set of points, project each of these points on this fitted plane/surface and get corresponding parameters. But maybe there is better solutions for this problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for any help.&lt;/p&gt;&#xA;" OwnerUserId="2644" LastEditorUserId="2644" LastEditDate="2016-04-19T14:10:26.560" LastActivityDate="2016-04-19T14:10:26.560" Title="Parametrize set of unordered points in 3d space" Tags="&lt;algorithm&gt;&lt;geometry&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2344" PostTypeId="2" ParentId="2336" CreationDate="2016-04-19T14:32:18.870" Score="3" Body="&lt;p&gt;Yes, you do need to transform the fetch direction into the space of the cubemap. If you could somehow figure out the fetch direction in the vertex shader, then you could do the transformation there instead, but that would produce worse lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It also may be worth optimizing for a smaller number of interpolants between the vertex and pixel shaders (rather than a smaller number of matrix-vector multiplications in the pixel shader). As soon as you have one transformed view vector and two transformed light vectors that must be interpolated (which would otherwise be constant), you've matched the size of the tangent/bitangent/normal matrix that you would otherwise be interpolating. That has the potential to have more performance impact than doing extra matrix multiplications in the shader.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2016-04-19T14:32:18.870" CommentCount="0" />
  <row Id="2345" PostTypeId="2" ParentId="1934" CreationDate="2016-04-19T14:59:49.760" Score="1" Body="&lt;p&gt;You can put the data into a data uri to store the data inline in base64 format.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;buffers&quot;: {&#xA;    &quot;a-buffer-id&quot;: {&#xA;        &quot;byteLength&quot;: 1024,&#xA;        &quot;type&quot;: &quot;arraybuffer&quot;,&#xA;        &quot;uri&quot;=&quot;data:application/octet-stream;base64,...&quot;&#xA;    }&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It's still a bit bloated compared to storing the data in a binary file though.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-04-19T14:59:49.760" CommentCount="0" />
  <row Id="2346" PostTypeId="1" CreationDate="2016-04-19T18:18:33.913" Score="1" ViewCount="39" Body="&lt;p&gt;I'm a newbie to OpenGL and I was following a series by &quot;thebennybox&quot; on Youtube. Specifically his series on modern opengl programming. I thought I would try to implement his Mesh wrapper using the vector in C++ rather than using arrays of data and passing that to OpenGL. It works, but I cant' seem to render multiple meshes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I making an obvious mistake somewhere?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is I do to setup my mesh:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glGenVertexArrays(1, &amp;amp;vao);&#xA;glBindVertexArray(vao);&#xA;&#xA;glGenBuffers(BUFFER_COUNT, vbo);&#xA;&#xA;// vertices buffer&#xA;glBindBuffer(GL_ARRAY_BUFFER, vbo[BUFFER_POSITION]);&#xA;glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(positions.front()), &amp;amp;positions.front(), GL_STATIC_DRAW);&#xA;glEnableVertexAttribArray(0);&#xA;glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 0, 0);&#xA;&#xA;// texture buffer&#xA;glBindBuffer(GL_ARRAY_BUFFER, vbo[BUFFER_TEXCOORD]);&#xA;glBufferData(GL_ARRAY_BUFFER, vertices.size() * sizeof(texes.front()), &amp;amp;texes.front(), GL_STATIC_DRAW);&#xA;glEnableVertexAttribArray(1);&#xA;glVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, 0, 0);&#xA;&#xA;// index buffer&#xA;glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, vbo[BUFFER_INDEX]);&#xA;glBufferData(GL_ELEMENT_ARRAY_BUFFER, indices.size() * sizeof(indices.front()), &amp;amp;indices.front(), GL_STATIC_DRAW);&#xA;&#xA;glBindVertexArray(0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then to render it I bind the VAO and draw the elements:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glBindVertexArray(vao);&#xA;glDrawElementsBaseVertex(GL_TRIANGLES, indices.size(), GL_UNSIGNED_INT, 0, 0);&#xA;glBindVertexArray(0);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And finally I can send it some data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;std::vector&amp;lt;Vertex&amp;gt; vertices = {&#xA;    Vertex(glm::vec3(0, 0, 0), glm::vec2(0, 0)),&#xA;    Vertex(glm::vec3(0, 1, 0), glm::vec2(1, 0)),&#xA;    Vertex(glm::vec3(0, 1, 1), glm::vec2(1, 1)),&#xA;    Vertex(glm::vec3(0, 0, 1), glm::vec2(0, 1)),&#xA;};&#xA;&#xA;std::vector&amp;lt;unsigned int&amp;gt; indices = {&#xA;    0, 1, 2, 2, 3, 0&#xA;};&#xA;&#xA;shape = new Mesh(vertices, indices);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The weird thing is that this works perfectly, but as soon as I try render another mesh it wont render the previous mesh. The weird thing is that say I had mesh A and mesh B. I can render mesh A, then I &lt;em&gt;setup&lt;/em&gt; mesh B, and even if I don't render it, nothing renders at all. Not even mesh A. I assume this is due to the state-based-ness of OpenGL.&lt;/p&gt;&#xA;" OwnerUserId="3173" LastActivityDate="2016-04-19T18:18:33.913" Title="Can't render multiple VAO's?" Tags="&lt;opengl&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2347" PostTypeId="1" CreationDate="2016-04-19T20:58:44.273" Score="4" ViewCount="114" Body="&lt;p&gt;Another code review question.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to draw an arrow to represent a vector. I have started with an unit arrow with vertices defined as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//vertex data for vector with arrow    &#xA;GLfloat vertices[] = {&#xA;    //line&#xA;    0.0f, 0.0f, 0.0f,&#xA;    1.0f, 0.0f, 0.0f,&#xA;&#xA;    //arrow bases&#xA;    0.95f, 0.01f, 0.0f,&#xA;    0.95f,-0.01f, 0.0f&#xA;};&#xA;&#xA;GLuint indices[] = {&#xA;    0, 1,       // Line&#xA;    1, 2, 3    // Arrow Triangle&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have another array that contains the actual vector start and end positions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;GLfloat lineVertices[] = {&#xA;&#xA;    1.0f, 2.0f, 3.0f, //v1 start&#xA;    3.0f, 5.0f,-5.0f, //v1 end&#xA;&#xA;   -1.0f,-1.0f,-1.0f, //v2 start&#xA;   -3.0f, 5.0f,-10.0f //v2 end&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to construct the &lt;code&gt;model&lt;/code&gt; matrix to apply the transformation from &lt;code&gt;lineVertices&lt;/code&gt; for each vectors on the unit arrow.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;//'view', and 'projection' matrix is set out side of the loop&#xA;for (GLint i = 0; i &amp;lt; sizeof(lineVertices) / sizeof(GLfloat); i += 6)&#xA;{&#xA;    glm::mat4 model;&#xA;&#xA;    //apply transformation&#xA;    glm::vec3 start(lineVertices[i], lineVertices[i + 1], lineVertices[i + 2]);&#xA;    glm::vec3 end(lineVertices[i + 3], lineVertices[i + 4], lineVertices[i + 5]);&#xA;    glm::vec3 startToEnd = end - start;&#xA;    GLfloat length = glm::length(startToEnd);&#xA;&#xA;    //translate&#xA;    model = glm::translate(model, start);&#xA;&#xA;    //rotate&#xA;    //GLfloat xyAngle = glm::atan(startToEnd.z, length);&#xA;    //GLfloat xzAngle = glm::atan(startToEnd.y, length);&#xA;    //GLfloat yzAngle = glm::atan(startToEnd.x, length);&#xA;&#xA;    //model = glm::rotate(model, xyAngle, glm::vec3(1.0f, 1.0f, 0.0f));&#xA;    //model = glm::rotate(model, xzAngle, glm::vec3(1.0f, 0.0f, 1.0f));&#xA;    //model = glm::rotate(model, yzAngle, glm::vec3(0.0f, 1.0f, 1.0f));&#xA;&#xA;    //rotate&#xA;    GLfloat angleWithZ = glm::acos(startToEnd.z/length);&#xA;    GLfloat angleWithY = glm::acos(startToEnd.y/length);&#xA;    GLfloat angleWithX = glm::acos(startToEnd.x/length);&#xA;&#xA;    model = glm::rotate(model, angleWithX, glm::vec3(0.0f, 0.0f, 1.0f));&#xA;    model = glm::rotate(model, angleWithY, glm::vec3(0.0f, 0.0f, 1.0f));&#xA;    model = glm::rotate(model, angleWithZ, glm::vec3(1.0f, 0.0f, 0.0f));&#xA;&#xA;    //scale&#xA;    model = glm::scale(model, glm::vec3(length));&#xA;&#xA;    glUniformMatrix4fv(glGetUniformLocation(ourShader.Program, &quot;model&quot;), 1, GL_FALSE, glm::value_ptr(model));&#xA;&#xA;    glDrawElements(GL_LINES, 2, GL_UNSIGNED_INT, 0);&#xA;    glDrawElements(GL_TRIANGLES, 3, GL_UNSIGNED_INT, BUFFER_OFFSET(2));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Though this seems to work, as I can see all those arrows, however, I am not sure whether this is the right implementation or not. Please let me know, if this is correct and/or there is room for improvement, or there are easier ways to achieve this, and so on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; updated my code sample (only the rotation part) based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Direction_cosine&quot; rel=&quot;nofollow&quot;&gt;Direction cosine (wikipedia)&lt;/a&gt;. Old code is still kept commented. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;P.S.: this implementation is based on following stack exchange discussions: &lt;a href=&quot;http://gamedev.stackexchange.com/a/96960/82355&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://math.stackexchange.com/a/799812/298948&quot;&gt;2&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3098" LastEditorUserId="3098" LastEditDate="2016-06-11T08:24:30.833" LastActivityDate="2016-06-11T08:24:30.833" Title="Represent vector with arrow" Tags="&lt;opengl&gt;&lt;c++&gt;&lt;vectors&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2348" PostTypeId="1" CreationDate="2016-04-20T06:59:31.810" Score="11" ViewCount="1407" Body="&lt;p&gt;There are many references related to the physically-based rendering of several natural features of the numan body, such as skin, hair, eyes.&lt;br&gt;&#xA;However, I could not find specific information about the simulation of visually realistic nails. For example, I did not find specificities of the characteristics of the reaction of human nails to light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/a/aa/Daumennagel_mit_Nagelhaut_und_Niednagel.jpg&quot; alt=&quot;Image from the Wikipedia article&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A few ideas I had:  &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;According to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Nail_(anatomy)&quot;&gt;Wikipedia article&lt;/a&gt;, nails are &quot;made of a tough protective protein called keratin. (...) It is made up of dead skin cells&quot;.&lt;br&gt;&#xA;Hair is also &lt;a href=&quot;https://en.wikipedia.org/wiki/Hair&quot;&gt;composed of keratin&lt;/a&gt;, so nail rendering might have a few similarities with hair rendering? However I think the main feature of hair rendering is the coloring of specular highlights due to internal reflections inside the hair fiber. Is that phenomenon significant or does it even happen for nails?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Examination indicates that nails are transparent and that they host sub-surface scattering effects. Are there models combining these phenomenons?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The proximity to skin also seems to suggest that the skin under nails has singular appearance properties. Should this layer require some specific techniques to be simulated too?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;What kind of technology would be involved to simulate accurately the rendering of human nails?&lt;br&gt;&#xA;What kind of tricks digital artists use to simulate the appearance of human nails?&lt;/p&gt;&#xA;" OwnerUserId="110" LastActivityDate="2016-04-20T17:58:44.800" Title="What kind of technology would be involved in the rendering of human nails?" Tags="&lt;rendering&gt;&lt;realistic&gt;&lt;human&gt;&lt;nails&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2349" PostTypeId="2" ParentId="2348" CreationDate="2016-04-20T07:22:17.570" Score="9" Body="&lt;p&gt;I'm not aware of any rendering technology designed specially for fingernails. Just eyeballing it, I would suggest that a combination of subsurface scattering with a relatively smooth glossy specular surface would get you most of the way there. In other words, you could use the same shader as you do for skin, but with different textures and different specular parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wouldn't guess that a hair shader would be too useful. Nails are not in the shape of fibers, nor do they have the kind of complex occlusion and shadowing that hair does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, if the scattering properties of the nails are different enough from that of skin, it might make sense to use a different shader (for instance, doing some sort of volumetric scattering instead of a dipole approximation). I don't know of any references on skin scattering that have investigated nails specifically, though.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-20T07:22:17.570" CommentCount="0" />
  <row Id="2350" PostTypeId="2" ParentId="2348" CreationDate="2016-04-20T07:48:13.090" Score="7" Body="&lt;p&gt;I don't know of any specific technology for rendering fingernails, but I agree that using subsurface scattering with some specular would be a good starting point. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A couple of things I'd take into account are that most natural nails without any coatings or nail polish aren't that shiny and have some ridges, so there should be some anisotropy to the highlights. I think a hair shader might be overkill for that though. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A few things I think would be interesting to look at would be things like inclusions from injury/disease/etc., discoloration, and varying thickness; there's probably some work to be done on how light scatters between the soft tissue of the nailbed and the nail itself. Another interesting thing to look at would be nails with nail polish on them; I bet you could do some cool stuff with car paint effects and long distance scattering (like with a backlit manicured nail).&lt;/p&gt;&#xA;" OwnerUserId="3178" LastActivityDate="2016-04-20T07:48:13.090" CommentCount="1" />
  <row Id="2353" PostTypeId="2" ParentId="2342" CreationDate="2016-04-21T18:09:25.737" Score="2" Body="&lt;p&gt;Huge thanks to @MJP who answered this.&lt;br&gt;&#xA;The aim is to avoid the simplification made when using tangent space normals.&#xA;Here is the paper : &lt;a href=&quot;http://blog.selfshadow.com/publications/blending-in-detail/&quot; rel=&quot;nofollow&quot;&gt;Blending in detail&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But only implement equation (&lt;a href=&quot;http://blog.selfshadow.com/publications/blending-in-detail/#mjx-eqn-eqrotation&quot; rel=&quot;nofollow&quot;&gt;4&lt;/a&gt;) which gives you this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3 ReorientNormal(in float3 u, in float3 t, in float3 s)&#xA;{&#xA;    // Build the shortest-arc quaternion&#xA;    float4 q = float4(cross(s, t), dot(s, t) + 1) / sqrt(2 * (dot(s, t) + 1));&#xA;    // Rotate the normal&#xA;    return u * (q.w * q.w - dot(q.xyz, q.xyz)) + 2 * q.xyz * dot(q.xyz, u) + 2 * q.w * cross(q.xyz, u);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Original answer is &lt;a href=&quot;http://www.gamedev.net/topic/678043-how-to-blend-world-space-normals/?view=findpost&amp;amp;p=5287707&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="2372" LastActivityDate="2016-04-21T18:09:25.737" CommentCount="0" />
  <row Id="2354" PostTypeId="2" ParentId="2332" CreationDate="2016-04-22T05:32:42.897" Score="5" Body="&lt;p&gt;In order to replace a color with another color, you need some sort of &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics)&quot;&gt;distance metric&lt;/a&gt; between the colors and a function for calculating the &lt;a href=&quot;https://en.wikipedia.org/wiki/Blend_modes&quot;&gt;blending&lt;/a&gt; based on that distance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finding a distance between 2 colors can be tricky. RGB is a bad color space to measure the distance between colors for perceptual uses. HSV or YCbCr are better. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have the colors in a color space you like, you need to decide how to calculate the distance. I generally calculate the hue distance for starters. If the hue of the color I'm considering replacing is within +/- some threshold (possibly set by the user), then I check to see if the saturation and value are within a certain range, too. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once I've found a color that's within the ranges specified, I calculate a blending amount. Usually this will be 100% if the color in the image exactly matches the color I want to replace, and will fall off to 0 at the thresholds of hue, saturation and brightness. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;You then need to calculate the color you're going to use as a replacement color. It's useful to offset the hue, saturation and value (or Y, Cb, and Cr components) by the same amount as the color in the image differs from the replacement color.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So now you have the original color in the image, the color to replace it with, and a blending amount. You still need to figure out which blend mode you want to use. I would start with what Photoshop calls &quot;Normal&quot; or &quot;Over&quot;. That would be a linear mix of the two. If you have colors A and B with a mix amount of 25%, it would be calculated like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;newColor = A * .25 + B * (1.0 - .25);&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are some relevant answers I gave to similar questions on Stack Overflow:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/9234724/how-to-change-hue-of-a-texture-with-glsl/9234854#9234854&quot;&gt;How to Change the Hue of a Texture with GLSL&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/questions/9064556/better-image-coloring-logic-algorithm/9075340#9075340&quot;&gt;Better Image Coloring Logic/Algorithm&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3003" LastActivityDate="2016-04-22T05:32:42.897" CommentCount="0" />
  <row Id="2355" PostTypeId="1" CreationDate="2016-04-22T09:39:17.053" Score="6" ViewCount="55" Body="&lt;p&gt;My shape is a slightly concave polygon, and I'd like to know the maximal diameter. I imagine a straight line between two points on the surface of the polygon, such that the line does not pass outside the polygon.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a general algorithm for this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my case I am interested in 2D. My shapes are tumors in medical images. So we can also assume:&#xA;1 the centroid is always inside the polygon. &#xA;2 a high vertex density, i.e. the next vertex is always close to the previous one. &lt;/p&gt;&#xA;" OwnerUserId="3204" LastEditorUserId="3204" LastEditDate="2016-04-23T05:39:47.663" LastActivityDate="2016-04-23T05:39:47.663" Title="Find the longest straight line between two points on surface of polygon" Tags="&lt;maths&gt;&lt;computational-geometry&gt;&lt;polygon&gt;" AnswerCount="0" CommentCount="9" />
  <row Id="2356" PostTypeId="1" CreationDate="2016-04-24T14:21:39.993" Score="2" ViewCount="88" Body="&lt;p&gt;I'm attempting to test out the maths behind bounding volume algorithms (prior to ray tracing) using MATLAB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, I have successfully created the relatively trivial axis aligned bounding volume, and I believe I have successfully created a bounding sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've attempted to then create an object aligned bounding volume - but although I believe I have got the principle axes correct because the box appears to be a suitable shape - I have been unable to translate it correctly &quot;onto&quot; the shape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially my question is - what am I doing wrong in my algorithm &amp;amp; how do I translate my bounding volume onto the shape.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The two sources I have been using are &lt;a href=&quot;https://github.com/tl3shi/books/blob/master/GameDev/Math/Mathematics.for.3D.Game.Programming.and.Computer.Graphics,.Lengyel,.3rd,.2011.pdf&quot; rel=&quot;nofollow&quot;&gt;Maths for 3D Games&lt;/a&gt;, as well as a &lt;a href=&quot;http://jamesgregson.blogspot.co.uk/2011/03/latex-test.html&quot; rel=&quot;nofollow&quot;&gt;blog&lt;/a&gt; which gives some indication on to do the translation - but doesn't seem to have worked very well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have put my source code below - thanks very much!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;%//===================Declare vertices and faces===================&#xA;vp_vtx = [379.379,684.302,319.752,711.497,215.956,439.237,600,600,732.938,418.084,600,600,747.081;&#xA;    225.836,158.305,394.208,337.480,84.834,245.958,212.858,342.251,322.344,330.576,300,416.190,398.885;&#xA;    0.933,0.917,0.904,0.878,0.854,0.923,0.914,0.949,0.943,0.908,0.896,0.940,0.933;&#xA;    1,1,1,1,1,1,1,1,1,1,1,1,1];&#xA;ws_fcs = [2,4,3,6,6,7,7,10,10; %//Faces forming shape&#xA;    4,3,1,7,11,9,13,11,13;&#xA;    5,5,5,11,10,13,11,13,12];&#xA;%//===================Create an AA bounding box===================&#xA;x = vp_vtx(1,:);y = vp_vtx(2,:);z = vp_vtx(3,:); %//Seperate vertex coordinates&#xA;bb_vtx = [min(x),max(x),min(x),max(x),min(x),max(x),min(x),max(x); %//Take min/max from each&#xA;    min(y),min(y),max(y),max(y),min(y),min(y),max(y),max(y); %//To form enclosing box&#xA;    min(z),min(z),min(z),min(z),max(z),max(z),max(z),max(z)];&#xA;&#xA;bb_fcs = [1,2,6,1,1,3; 2,4,5,5,2,4;4,8,7,7,6,8; 3,6,8,3,5,7]; %//Allocate faces of box&#xA;&#xA;figure(); grid on; hold on; xlabel('x'); ylabel('y'); zlabel('z');&#xA;scatter3(vp_vtx(1,:),vp_vtx(2,:),vp_vtx(3,:),'r'); %//Plot shape&#xA;patch('Faces',ws_fcs','Vertices',vp_vtx(1:3,:)', 'Facecolor', 'r','FaceAlpha', 0.1)&#xA;patch('Faces',bb_fcs', 'Vertices',bb_vtx','FaceColor','g','FaceAlpha', 0.05);%//Plot enclosing box&#xA;&#xA;mean_point = sum(vp_vtx,2)/length(vp_vtx);&#xA;C = zeros(4,4); %//Create 4x4 empty matrix&#xA;for i = 1:length(vp_vtx)&#xA;    C = C+(vp_vtx(:,i)-mean_point)*(vp_vtx(:,i)-mean_point)'; %//Sum to get covarience matrix&#xA;end;&#xA;C = C/length(vp_vtx); %//Scale by the number of samples&#xA;&#xA;[y,v] = eig(C(1:3,1:3)) ;%//Get eigenvalues &amp;amp; eigen vectors&#xA;&#xA;R = y(:,1); %//Eigen vectors &amp;amp; values form object aligned axes&#xA;S = y(:,2);&#xA;T = y(:,3);%//T is principle axis as derived from largest eigenvalue&#xA;&#xA;%//========Create an Object Orientated Bounding Box=========&#xA;dot_arr = zeros(size(vp_vtx));&#xA;for i = 1:length(vp_vtx)  %//Create array of dot products with each OO axis&#xA;    dot_arr(1,i) = dot(vp_vtx(1:3,i),T);&#xA;    dot_arr(2,i) = dot(vp_vtx(1:3,i),R);&#xA;    dot_arr(3,i) = dot(vp_vtx(1:3,i),S);&#xA;end&#xA;%//Get min/max variation in each OO axis&#xA;a = 0.5*(min(dot_arr(1,:)) + max(dot_arr(1,:)));&#xA;b = 0.5*(min(dot_arr(2,:)) + max(dot_arr(2,:)));&#xA;c = 0.5*(min(dot_arr(3,:)) + max(dot_arr(3,:)));&#xA;%//Centre is point where the 3 planes of the box intersect? (from book)&#xA;q = a*T + b*R + c*S;&#xA;&#xA;Tr = vertcat(horzcat(T,S,R,q),[0,0,0,1]); %//Transform &amp;amp; translate original AA box&#xA;bb_vtx = Tr*vertcat(bb_vtx, ones(1,length(bb_vtx)));&#xA;patch('Faces',bb_fcs', 'Vertices',bb_vtx(1:3,:)','FaceColor','g','FaceAlpha', 0.05);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2646" LastEditorUserId="2646" LastEditDate="2016-04-25T13:47:39.697" LastActivityDate="2016-04-27T09:34:58.197" Title="Creating an Object Orientated Bounding Volume" Tags="&lt;raytracing&gt;&lt;transformations&gt;&lt;bounding-volume-hierarchy&gt;&lt;matlab&gt;&lt;box&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="2357" PostTypeId="1" CreationDate="2016-04-25T07:39:59.417" Score="7" ViewCount="74" Body="&lt;p&gt;Whenever I try to adjust gamma settings in a game, I am shown three different icons and usually asked to make sure the left-most one is barely visible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tetOP.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tetOP.png&quot; alt=&quot;Image&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the other two icons for?&lt;/p&gt;&#xA;" OwnerUserId="3220" LastActivityDate="2016-05-03T13:17:41.510" Title="Gamma setting in games - Why 3 icons?" Tags="&lt;gamma&gt;&lt;brightness&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2358" PostTypeId="2" ParentId="306" CreationDate="2016-04-25T07:55:10.327" Score="19" Body="&lt;p&gt;Sorry about the long wait, but it has become obvious that although the article I have promised is basically complete, the publishing process will take some time. Therefore, I have instead prepared an open source program with my new multi-channel distance field construction algorithm, &lt;strong&gt;msdfgen&lt;/strong&gt;, which you can try out right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is available on GitHub: &lt;a href=&quot;https://github.com/Chlumsky/msdfgen&quot;&gt;https://github.com/Chlumsky/msdfgen&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I am new to this, so please let me know if there is anything wrong with the repository.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Someone also asked about how it compares to a larger monochrome distance field, so here is a teaser of the quality difference. However, it really depends on the particular font, and I would not say it is always worth the extra data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/dhhiy.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/dhhiy.png&quot; alt=&quot;Multi-channel distance field 16x16&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/ExmTZ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ExmTZ.png&quot; alt=&quot;Monochrome distance field 32x32&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2811" LastActivityDate="2016-04-25T07:55:10.327" CommentCount="0" />
  <row Id="2359" PostTypeId="1" CreationDate="2016-04-26T11:54:22.140" Score="4" ViewCount="53" Body="&lt;p&gt;I have an application in which I am using an octree to store a volume mesh of axis-aligned bounding boxes (AABBs).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given a water-tight manifold triangle mesh, I need to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;find if an AABB is intersected by or completely inside/outside of the surface mesh,&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;clip the surface mesh with the intersected AABBs to generate triangles that completely lie within each AABB.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The triangulation and the octree containing the AABBs are both dynamic. The number of leaf nodes in the octree is huge. The number of triangles in the surface mesh is much smaller (O(10^9 - 10^13) octree nodes, vs O(10^6) triangles).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which data-structures and algorithm are suitable for my problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now I:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;store the triangles in the same octree as the volume mesh,&lt;/li&gt;&#xA;&lt;li&gt;store each triangle in the smallest octree node that contains it,&lt;/li&gt;&#xA;&lt;li&gt;clip the triangle mesh with a single AABB by traversing from that AABB to both the root node and its leafs clipping each triangle contained in the nodes with the AABB. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The triangles in the nodes until the leafs are fully contained within the AABB and don't need any &quot;clipping&quot; (the AABB just contains those triangles), while the ones contained in the nodes from the AABB to the root need clipping. However:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;due to the way I am storing the triangles (in the smallest octree node that fully contains them) I don't have an upper bound in the maximum number of triangles that can be stored within each single octree node, so I don't have an upper bound in the number of triangles that have to be tested against a single AABB.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;if I just want to test if an AABB is intersected by the triangle mesh, I have to test all triangles between that AABB and the root node which might be expensive. Ideally I would like to have a very fast way to test, and then clip the mesh if the test is true.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;currently I have no fast way of determining if a node is inside/outside/intersected by the mesh. I could construct a signed distance field (which is expensive), or perform some ray casting (which is also expensive), maybe there is a better solution or maybe I just need to precompute something to speed this up every time I move the triangle mesh.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;moving the triangle mesh requires manipulating the octree (I don't know if this can be avoided). Ideally I would like to generate an octree for the triangle mesh once, and then just move the mesh using a single transformation matrix. However, then I would need to have some sort of mapping between &quot;the octree containing the triangle mesh&quot; and &quot;the octree of my volume mesh&quot;. Maybe this mapping is trivial but I haven't figured it out yet. This would save me from manipulating the octree when I move the mesh and maybe the coordinate transformations that would be required are negligible with the cost of clipping.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="3225" LastEditorUserId="3225" LastEditDate="2016-05-04T14:12:42.043" LastActivityDate="2016-05-05T06:44:32.243" Title="Data structure and algorithm for clipping triangulation with leaf nodes of an octree" Tags="&lt;algorithm&gt;&lt;geometry&gt;&lt;performance&gt;&lt;data-structure&gt;&lt;clipping&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="2" />
  <row Id="2360" PostTypeId="1" AcceptedAnswerId="2361" CreationDate="2016-04-26T17:11:05.690" Score="1" ViewCount="65" Body="&lt;p&gt;Say I have rendered an HDR ray traced image with dimensions 1920x1080 (width x height) pixels and I want to instead represent it with dimensions 960x540. Assume as well that the image has not yet been gamma corrected (i.e. it is still in linear space). How should I go about downsampling the image such that I don't need to re-render the image in the new dimensions?&lt;/p&gt;&#xA;" OwnerUserId="1924" LastActivityDate="2016-04-26T23:27:46.947" Title="How can I resize a rendered ray traced image?" Tags="&lt;raytracing&gt;&lt;sampling&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2361" PostTypeId="2" ParentId="2360" CreationDate="2016-04-26T23:27:46.947" Score="6" Body="&lt;p&gt;Since you want to downsample the image by a factor of 2 along each axis, a simple and easy thing to do is just average a 2&amp;times;2 box of source pixels to generate each destination pixel. In pseudocode this would look like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for dest_y = 0 to 540&#xA;    src_y = dest_y*2&#xA;    for dest_x = 0 to 960&#xA;        src_x = dest_x*2&#xA;        average = 0.25 * (src[src_y][src_x] + src[src_y][src_x+1] +&#xA;                          src[src_y+1][src_x] + src[src_y+1][src_x+1])&#xA;        dest[dest_y][dest_x] = average&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will do an okay job, but for higher-quality downsampling or resizing images to any size (not just neat integer factors), I suggest looking at the &lt;a href=&quot;https://github.com/nothings/stb/blob/master/stb_image_resize.h&quot;&gt;stb_image_resize&lt;/a&gt; library. It's quite easy to use, supports either 8-bit or floating-point images with any number of channels, and gives good results.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-26T23:27:46.947" CommentCount="3" />
  <row Id="2362" PostTypeId="1" CreationDate="2016-04-27T10:12:31.790" Score="1" ViewCount="75" Body="&lt;p&gt;Thanks for the &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/2140/intersection-between-line-segments-narrowed-precondition&quot;&gt;suggestions&lt;/a&gt;. Now I am doing it as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First off it is identified if two line segments intersect or not. And I am using the algorithm described in the book - &quot;Introduction to Algorithms&quot; chapter 33 (Computational Geometry). &lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;int orientation(glm::i16vec2 &amp;amp;p1,&#xA;                glm::i16vec2 &amp;amp;p2,&#xA;                glm::i16vec2 &amp;amp;p3)&#xA;{&#xA;   return (((p3.x - p1.x) * (p2.y - p1.y)) - ((p2.x - p1.x) * (p3.y - p1.y)));&#xA;}&#xA;&#xA;&#xA;bool onSegment(const glm::i16vec2 &amp;amp;pi,&#xA;                 const glm::i16vec2 &amp;amp;pj,&#xA;                 const glm::i16vec2 &amp;amp;pk)&#xA;{&#xA;    if((std::min(pi.x,pj.x) &amp;lt;= pk.x &amp;amp;&amp;amp; pk.x &amp;lt;= std::max(pi.x,pj.x))  &amp;amp;&amp;amp;&#xA;    (std::min(pi.y,pj.y) &amp;lt;= pk.y &amp;amp;&amp;amp; pk.y &amp;lt;= std::max(pi.y,pj.y)))&#xA;        return true;&#xA;    else&#xA;        return false;&#xA;}&#xA;&#xA;bool intersects(const glm::i16vec2 &amp;amp;p1,const glm::i16vec2 &amp;amp;p2,&#xA;                const glm::i16vec2 &amp;amp;p3,const glm::i16vec2 &amp;amp;p4)&#xA;{&#xA;&#xA;    int d1 = orientation(p1,p2,p3);&#xA;    int d2 = orientation(p1,p2,p4);&#xA;    int d3 = orientation(p3,p4,p1);&#xA;    int d4 = orientation(p3,p4,p2);&#xA;&#xA;    if(((d1 &amp;gt; 0 &amp;amp;&amp;amp; d2 &amp;lt; 0) || (d1 &amp;lt; 0 &amp;amp;&amp;amp; d2 &amp;gt; 0)) &amp;amp;&amp;amp;&#xA;        ((d3 &amp;gt; 0 &amp;amp;&amp;amp; d4 &amp;lt; 0) || (d3 &amp;lt; 0 &amp;amp;&amp;amp; d4 &amp;gt; 0)))&#xA;          return true;&#xA;    else if(d1 == 0 &amp;amp;&amp;amp; onSegment(p3,p4,p1))&#xA;          return true;&#xA;    else if(d2 == 0 &amp;amp;&amp;amp; onSegment(p3,p4,p2))&#xA;          return true;&#xA;    else if(d3 == 0 &amp;amp;&amp;amp; onSegment(p1,p2,p3))&#xA;          return true;&#xA;    else if(d4 == 0 &amp;amp;&amp;amp; onSegment(p1,p2,p4))&#xA;          return true;&#xA;    else&#xA;          return false;    &#xA;}&#xA;&#xA;int main()&#xA;{&#xA;    glm::i16vec2 hs(5,1);  // p1&#xA;    glm::i16vec2 he(5,3); // p2&#xA;&#xA;    glm::i16vec2 cs(5,1);   // p3&#xA;    glm::i16vec2 ce(5,3); // p4&#xA;&#xA;    if(intersects(hs,he,cs,ce))&#xA;      std::cout &amp;lt;&amp;lt; &quot;It has intersection&quot; &amp;lt;&amp;lt; std::endl;&#xA;    else&#xA;      std::cout &amp;lt;&amp;lt; &quot;No intersection&quot; &amp;lt;&amp;lt; std::endl;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once the boolean flag mentions that the intersection point is found, then the exact intersection point is extracted as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;short int getXIntersection(float x1,&#xA;                           float y1,&#xA;                           float x2,&#xA;                           float y2,&#xA;                           float yPos)&#xA;{&#xA;    float t = (x2 - x1) / (y2 - y1);&#xA;&#xA;    float xDbl = (static_cast&amp;lt;double&amp;gt;(yPos) - y1) * t + x1;&#xA;&#xA;    return static_cast&amp;lt;short int&amp;gt;(std::floor(0.5+xDbl));&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above function snippet is for the case when one of the line segments is parallel to the X-axis and the other segment is of arbitrary orientation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other case is when one of the line segments is parallel to Y-axis and the other segment is of arbitrary orientation. For the sake of brevity, the code snippet is not written here. I hope you can imagine how it will be? Yet it is included in the following snippet as for some coordinate values I am having wrong output:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;short int getYIntersection(float x1,&#xA;                           float y1,&#xA;                           float x2,&#xA;                           float y2,&#xA;                           float xPos)&#xA;{&#xA;    float t = (y2 - y1)/(x2 - x1);&#xA;&#xA;    float yDbl = (static_cast&amp;lt;double&amp;gt;(xPos) - x1) * t + y1;&#xA;&#xA;    return static_cast&amp;lt;short int&amp;gt;(std::floor(0.5+yDbl));&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;RECENT : Lets a line segment that is parallel to the Y-axis and it has the following coordinates as start and end point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;hs(3124,-3168)&#xA;he(3124,-3094)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other arbitrarily oriented line segment has the following coordinates:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;cs(3124,-3168)&#xA;ce(3125,-3116)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the line segment parallel to Y-axis has the constant X-value, should not the function getYIntersection() get the correct Y-value . But I am not getting it- instead I am getting -3213 as the Y-intersection value. Is there anything I am missing here ? The Y-value of the intersection must be within both the line segment, unfortunately it is not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you imagine any loop hole in this overall algorithm? In practice, I am getting some huge intersection point value that is way out of the region.&#xA;The intersection point is saved as&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt;    short int&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I am getting the  X-intersection value -32768. This value is way out of the bounding area where all the intersection points must lie within.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any thoughts?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After several comments and one answer , some adjustments is made within the code. It is found that with overlapping line segments the algorithm return true. It is not intended in my case, all overlapping line segments must not be flagged as intersection. Some hint would be nice. The issue of getting the intersection point is still in the pipeline. I believe that I missed the part that must be done after the initial true/false algorithm - that is to test if the calculated intersectin point lies within both the line segments or not. If it does, only then we eventually have the true intersection otherwise no intersection at all. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some feed-back would be great! &lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="2712" LastEditDate="2016-05-10T15:17:40.067" LastActivityDate="2016-05-10T15:17:40.067" Title="Line segment intersection test and find the point of intersection" Tags="&lt;mathematics&gt;" AnswerCount="0" CommentCount="11" />
  <row Id="2364" PostTypeId="1" AcceptedAnswerId="2376" CreationDate="2016-04-28T11:41:14.680" Score="2" ViewCount="51" Body="&lt;p&gt;I have implemented line segments &lt;a href=&quot;http://computergraphics.stackexchange.com/questions/2362/line-segment-intersection-test-and-find-the-point-of-intersection&quot;&gt;intersetion test&lt;/a&gt; and at the very same test I want to include the fact that if two line segments overlap each other partially or fully , it will be flagged as no intersection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some hint to it would be great to have!&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-04-30T01:13:58.043" Title="Line segments overlapping" Tags="&lt;2d-graphics&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="2365" PostTypeId="1" CreationDate="2016-04-28T16:18:55.477" Score="7" ViewCount="81" Body="&lt;p&gt;I am writing an OpenCL program for use with my AMD Radeon HD 7800 series GPU. According to AMD's &lt;a href=&quot;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2013/07/AMD_Accelerated_Parallel_Processing_OpenCL_Programming_Guide-rev-2.7.pdf&quot; rel=&quot;nofollow&quot;&gt;OpenCL programming guide&lt;/a&gt;, this generation of GPU has two hardware queues that can operate asynchronously.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&lt;strong&gt;5.5.6 Command Queue&lt;/strong&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;For Southern Islands and later, devices support at least two hardware compute&#xA;  queues. That allows an application to increase the throughput of small dispatches&#xA;  with two command queues for asynchronous submission and possibly execution.&#xA;  The hardware compute queues are selected in the following order: first queue =&#xA;  even OCL command queues, second queue = odd OCL queues.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;To do this, I've created two separate OpenCL command queues to feed data to the GPU. Roughly, &lt;a href=&quot;https://github.com/Mokosha/GenTC/blob/master/codec/codec.cpp#L242&quot; rel=&quot;nofollow&quot;&gt;the program running on the host thread&lt;/a&gt; looks something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static const int kNumQueues = 2;&#xA;cl_command_queue default_queue;&#xA;cl_command_queue work_queue[kNumQueues];&#xA;&#xA;static const int N = 256;&#xA;cl_mem gl_buffers[N];&#xA;cl_event finish_events[N];&#xA;&#xA;clEnqueueAcquireGLObjects(default_queue, gl_buffers, N);&#xA;&#xA;int queue_idx = 0;&#xA;for (int i = 0; i &amp;lt; N; ++i) {&#xA;  cl_command_queue queue = work_queue[queue_idx];&#xA;&#xA;  cl_mem src = clCreateBuffer(CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, ...);&#xA;&#xA;  // Enqueue a few kernels&#xA;  cl_mem tmp1 = clCreateBuffer(CL_READ_WRITE);&#xA;  clEnqueueNDRangeKernel(kernel1, queue, src, tmp1);&#xA;&#xA;  clEnqueueNDRangeKernel(kernel2, queue, tmp1, tmp1);&#xA;&#xA;  cl_mem tmp2 = clCreateBuffer(CL_READ_WRITE);&#xA;  clEnqueueNDRangeKernel(kernel2, queue, tmp1, tmp2);&#xA;&#xA;  clEnqueueNDRangeKernel(kernel3, queue, tmp2, gl_buffer[i], finish_events + i);&#xA;&#xA;  queue_idx = (queue_idx + 1) % kNumQueues;&#xA;}&#xA;&#xA;clEnqueueReleaseGLObjects(default_queue, gl_buffers, N);&#xA;clWaitForEvents(N, finish_events);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With &lt;code&gt;kNumQueues = 1&lt;/code&gt;, this application pretty much works as intended: it collects all of the work into a single command queue that then runs to completion with the GPU being fairly busy the whole time. I'm able to see this by looking at the output of the CodeXL profiler:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IP41r.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IP41r.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I set &lt;code&gt;kNumQueues = 2&lt;/code&gt;, I expect the same thing to happen but with the work evenly split across two queues. If anything, I expect each queue to have the same characteristics individually as the one queue: that it starts work sequentially until everything is done. However, when using two queues, I can see that not all of the work is split across the two hardware queues:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eDhDI.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eDhDI.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the beginning of the GPU's work, the queues do manage to run some kernels asynchronously, although it seems like neither ever fully occupies the hardware queues (unless my understanding is mistaken). Near the end of the GPU work, it seems like the queues are adding work sequentially to only one of the hardware queues, but there are even times that no kernels are running. What gives? Do I have some fundamental misunderstanding of how the runtime is supposed to behave?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a few theories as to why this is happening:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;The interspersed &lt;code&gt;clCreateBuffer&lt;/code&gt; calls are forcing the GPU to allocate device resources from a shared memory pool synchronously which stalls the execution of individual kernels.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The underlying OpenCL implementation does not map logical queues to physical queues, and only decides where to place objects at runtime.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Because I'm using GL objects, the GPU needs to synchronize access to specially allocated memory during writes.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Are any of these assumptions true? Does anyone know what could be causing the GPU to wait in the two-queue scenario? Any and all insight would be appreciated!&lt;/p&gt;&#xA;" OwnerUserId="197" LastEditorUserId="197" LastEditDate="2016-05-03T03:51:59.007" LastActivityDate="2016-05-03T03:51:59.007" Title="What is my GPU waiting on?" Tags="&lt;gpgpu&gt;&lt;opencl&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="2366" PostTypeId="1" AcceptedAnswerId="2367" CreationDate="2016-04-28T18:11:12.010" Score="4" ViewCount="83" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/t5yOY.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/t5yOY.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;I'm currently trying to develop a game using Cocos2d-x using C++&#xA;I really need to generate this burst or (donut-shaped) pulse.&#xA;I'm having a difficult time how to generate such shape.&#xA;At first, I was thinking of scaling the image but that didnt work out because the line or &quot;diameter&quot; of the donut shape gets bigger accordingly..&#xA;Any experienced coders who can provide an equation of this thing? or maybe &quot;mathematics&quot; is more exact.&#xA;If you guys can provide ANY help, suggestion, or alternative recommendation, I'll be so glad^^&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ewnEE5mt1ZM&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=ewnEE5mt1ZM&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3248" LastEditorUserId="3248" LastEditDate="2016-05-04T10:29:14.240" LastActivityDate="2016-05-04T10:29:14.240" Title="Generate this kind of 2d burst or pulse algorithm?" Tags="&lt;c++&gt;&lt;2d&gt;&lt;mathematics&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2367" PostTypeId="2" ParentId="2366" CreationDate="2016-04-28T19:49:11.807" Score="7" Body="&lt;p&gt;Rather than using an image, I would suggest doing this kind of effect using a shader. I'm not familiar with Cocos2d-x, but some quick googling suggests that it can work with shaders. You'd use a pixel shader that calculates the distance of each pixel to the center of the pulse effect, then applies a function based on that distance to define the shape and appearance of the pulse.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I made a &lt;a href=&quot;https://www.shadertoy.com/view/MdtXWX&quot;&gt;quick shadertoy&lt;/a&gt; as a proof of concept and example.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/49koo.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-28T19:49:11.807" CommentCount="4" />
  <row Id="2368" PostTypeId="1" AcceptedAnswerId="2374" CreationDate="2016-04-28T20:24:24.297" Score="5" ViewCount="85" Body="&lt;p&gt;I have a 3D scene I use to generate some images which I then process in Matlab. I noticed that when global illumination (GI) is not enabled, rendering always generates exactly the same pixel-for-pixel image (as far as I can tell). However, when I enable GI, the image is not exactly the same. I have been told (I originally accidentally posted &lt;a href=&quot;http://graphicdesign.stackexchange.com/questions/70432/gi-leading-to-image-variation/70480#70480&quot;&gt;here&lt;/a&gt;) that GI is stochastic and so this is to be somewhat expected. However, I am curious if there's any way to avoid it (while still using GI).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="3251" LastEditorUserId="38" LastEditDate="2016-04-29T03:56:50.823" LastActivityDate="2016-04-29T07:25:08.900" Title="Global illumination leading to image variation" Tags="&lt;rendering&gt;&lt;global-illumination&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2369" PostTypeId="2" ParentId="2368" CreationDate="2016-04-28T20:43:54.790" Score="6" Body="&lt;p&gt;If you can make your samples deterministic (the same from run to run - don't base random sample points on time or any other non deterministic value), it will be a stable rendering that you get every time.&lt;/p&gt;&#xA;" OwnerUserId="56" LastEditorUserId="56" LastEditDate="2016-04-28T22:08:45.650" LastActivityDate="2016-04-28T22:08:45.650" CommentCount="0" />
  <row Id="2370" PostTypeId="1" AcceptedAnswerId="2375" CreationDate="2016-04-28T20:50:53.533" Score="3" ViewCount="76" Body="&lt;p&gt;What I'm trying to do is to simulate refraction through biconcave lens described by spheres &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt; where &lt;code&gt;C&lt;/code&gt; is a sphere in between &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. So far I've gathered that a good approach would be to use CSG when it comes to modeling such object. I'm failing to understand how to implement this though. There are mentions of using Interval Arithmetics but I don't understand how exactly are they supposed to help me.&lt;/p&gt;&#xA;" OwnerUserId="3233" LastActivityDate="2016-04-29T13:54:57.470" Title="How to implement Constructive Solid Geometry in ray tracing with implicit surfaces (spheres)?" Tags="&lt;raytracing&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="2372" PostTypeId="2" ParentId="2364" CreationDate="2016-04-29T03:32:36.057" Score="0" Body="&lt;p&gt;If you're working in 2D, it should be enough to just take the dot product of the normals of the lines and if it is -1 or 1 then the lines are parallel, and either don't overlap or fully overlap. 2 lines cannot &quot;partially overlap&quot;. They either meet at a single point, at every point, or no points.&lt;/p&gt;&#xA;" OwnerUserId="3003" LastActivityDate="2016-04-29T03:32:36.057" CommentCount="2" />
  <row Id="2374" PostTypeId="2" ParentId="2368" CreationDate="2016-04-29T07:08:27.200" Score="2" Body="&lt;p&gt;Yes, computation of global illumination is usually done in a stochastic way and as such it relies on randomly generated numbers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Typically pseudo random generators are employed, which generate deterministic sequences, and usually have so called &quot;seed&quot; parameter. The seed basically says where to start in the sequence, which in turn affects the noise pattern of your rendered image. If you can use the same seed for each run, there is a high chance of getting identical results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is whether the software that you use allows you to do that. In the rendering settings, I would suggest looking for for words like seed, random or deterministic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Since you said you use Vray, it seems to me that using &quot;Deterministic Monte Carlo&quot; engine should be the way to go.&lt;/p&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-04-29T07:25:08.900" LastActivityDate="2016-04-29T07:25:08.900" CommentCount="1" />
  <row Id="2375" PostTypeId="2" ParentId="2370" CreationDate="2016-04-29T13:54:57.470" Score="4" Body="&lt;p&gt;I'm not sure what exactly you are asking but, IIRC, to do CSG with ray tracing, you maintain not just the closest intersection with an object, but a list of ordered pairs of [Inpoint, OutPoint] 'interval' distances. With a single sphere test (or any solid convex primitive for that matter), this list would have at most one [Inpoint, OutPoint] entry &lt;em&gt;or&lt;/em&gt; is empty if there is a miss.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To implement CSG you then apply the binary operator to a pair of lists, which in turn means applying them to the entries in the lists in a manner akin to merge-sorting two lists. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, for intersection, if you had  [a,b] in one list and [c,d] in the other, compute  e=Max(a,c) f=Min(b,d).  If e &gt; f there's no intersection else return the intersection, [e,f].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does that help?&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2016-04-29T13:54:57.470" CommentCount="3" />
  <row Id="2376" PostTypeId="2" ParentId="2364" CreationDate="2016-04-30T01:06:36.060" Score="1" Body="&lt;p&gt;The first step in your existing code calculates the orientations of one line segment relative to each of the two end points of the other line segment, using the cross product. This allows testing whether the other line segment has one end point anticlockwise and the other end point clockwise from the first line segment, indicating that the line on which the first segment lies cuts the other segment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rather than writing a new test, you can use these same orientations to test for line segments that overlap (including partial overlap). If both segments lie on the same line, then all four orientations calculated in your first step will be zero, because none of the end points are either clockwise or anticlockwise from the line, since they are on the line. Then it remains only to check whether the two segments on the same line overlap, by checking whether either of them has an end point between the end points of the other segment.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-04-30T01:13:58.043" LastActivityDate="2016-04-30T01:13:58.043" CommentCount="0" />
  <row Id="2377" PostTypeId="1" AcceptedAnswerId="2380" CreationDate="2016-04-30T10:43:15.217" Score="6" ViewCount="79" Body="&lt;p&gt;Recently I'm developing a Monte-Carlo path tracer. To measure my tracer's performance, I decided to implement a simple mechanism to count how many rays can it trace in a second. Then here comes the problem, there are two ways to define a single ray:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;ol&gt;&#xA;  &lt;li&gt;&lt;p&gt;A complete ray, i.e., starting from camera and bouncing around the scene until it terminates.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;The ray originated from each intersection point.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ol&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Although many people use &quot;Rays per Second&quot; to measure ray/path tracer's performance, how they recognize &quot;rays&quot; is unclear to me. Perhaps anybody knows?&lt;/p&gt;&#xA;" OwnerUserId="3267" LastEditorUserId="3267" LastEditDate="2016-04-30T18:59:36.110" LastActivityDate="2016-04-30T22:55:42.313" Title="How does everyone count &quot;Rays per Second&quot;?" Tags="&lt;raytracing&gt;&lt;performance&gt;&lt;pathtracing&gt;&lt;monte-carlo&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="0" />
  <row Id="2378" PostTypeId="1" AcceptedAnswerId="2379" CreationDate="2016-04-30T19:06:49.000" Score="4" ViewCount="38" Body="&lt;p&gt;We have a matrix      &lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This matrix represents a transformation in homogeneous coordinates.&#xA;My question is whether the above matrix is affine or not and an example too for this.                  &lt;/p&gt;&#xA;" OwnerUserId="3271" LastEditorUserId="48" LastEditDate="2016-04-30T22:02:29.723" LastActivityDate="2016-04-30T22:02:29.723" Title="Affine Transformation" Tags="&lt;transformations&gt;&lt;affine-transformations&gt;&lt;matrix&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2379" PostTypeId="2" ParentId="2378" CreationDate="2016-04-30T22:00:59.013" Score="5" Body="&lt;p&gt;It is not necessarily affine. An affine matrix in homogeneous coordinates has a form like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$\begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(assuming you use a column vector convention). Here, the upper-left 2&amp;times;2 submatrix is the linear part, and $(a_{13}, a_{23})$ is the translation vector of the affine transform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the lower row of the matrix has some values other than $[0, 0, 1]$, then it is in general a projective transform, not an affine one.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-04-30T22:00:59.013" CommentCount="0" />
  <row Id="2380" PostTypeId="2" ParentId="2377" CreationDate="2016-04-30T22:55:42.313" Score="5" Body="&lt;p&gt;I've seen both, unfortunately. I'm not fond of rays per second as meaning exclusively primary rays and I'd suggest &quot;paths per second&quot; or better yet &quot;samples per second&quot; instead. &quot;Complete ray&quot; is not a common term: a ray is a (potentially unbounded) line segment and a sequence of rays is a path.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Rays per second in your second sense of total ray casts is not well-specified for a path tracer: do shadow rays count, for instance? It's a useful metric for evaluating an acceleration structure or intersection testing framework (i.e. Embree or OptiX) but I'd avoid it for a renderer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, be aware that samples per second still isn't a great metric of actual performance since sample quality will vary wildly depending on implementation details. It's probably the best thing you can do starting out though, as the better solutions involve fairly complex variance estimates.&lt;/p&gt;&#xA;" OwnerUserId="3075" LastActivityDate="2016-04-30T22:55:42.313" CommentCount="2" />
  <row Id="2381" PostTypeId="1" CreationDate="2016-05-01T03:48:18.253" Score="2" ViewCount="52" Body="&lt;p&gt;In a plane, we have a &lt;strong&gt;convex&lt;/strong&gt; polygon with n sides. What would be the maximum number of disconnected pieces / fragments if this polygon is clipped against a square?&lt;/p&gt;&#xA;" OwnerUserId="3271" LastEditorUserId="3304" LastEditDate="2016-05-06T21:11:45.187" LastActivityDate="2016-05-08T20:18:18.947" Title="Finding the maximum number of disconnected fragments" Tags="&lt;polygon&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2383" PostTypeId="1" AcceptedAnswerId="2385" CreationDate="2016-05-01T22:02:24.773" Score="6" ViewCount="109" Body="&lt;p&gt;I have been studying computer graphics, from the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1482229390&quot;&gt;Fundamentals of Computer Graphic&lt;/a&gt; (but the third edition), and I lastly read about projections. Though, I didn't exactly understand what's the difference between orthographic and perspective projection? Why do we need both of them, where are they used? I also would like to learn what is &lt;em&gt;perspective transform&lt;/em&gt; that is applied before orthographic projection in perspective projection. Lastly, why do we need the viewport transformation? I mean we use view transformation if the camera/viewer is not looking at $-z$-direction, but what about the viewport?&lt;/p&gt;&#xA;" OwnerUserId="3277" LastActivityDate="2016-05-03T15:32:59.583" Title="What's the difference between orthographic and perspective projection?" Tags="&lt;3d&gt;&lt;projections&gt;&lt;perspective&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2385" PostTypeId="2" ParentId="2383" CreationDate="2016-05-02T10:18:12.757" Score="7" Body="&lt;p&gt;Orthographic projections are parallel projections. Each line that is originally parallel will be parallel after this transformation. The orthographic projection can be represented by a affine transformation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In contrast a perspective projection is not a parallel projection and originally parallel lines will no longer be parallel after this operation. Thus perspective projection can not be done by a affine transform.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why would you need orthographic projections? It is useful for several artistic and technical reasons. Orthographic projections are used in CAD drawings and other technical documentations. One of the primary reasons is to verify that your part actually fits in the space that has been reserved for it on a floor plan for example. Orthographic projections are often chosen so that the dimensions are easy to measure. In many cases this is just a convenient way to represent a problem in a different basis so that coordinates are easier to figure out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5zqm3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5zqm3.png&quot; alt=&quot;ortho&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: A number of useful orthographic projections for same object (and projection rule). The last on on the right is a special case called isometric having the property that cardinal axe directions are all in same scale.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A perspective projection is needed to be able to do 2 and 3 point perspectives, which is how we experience the world. A specific perspective projection can be decomposed as being a combination of a orthographic projection and a perspective divide.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/urTYN.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/urTYN.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: 2 point perspective note how the lines in prespective direction no longer are parallel&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Viewport transformation allows you to pan/rotate/scale the resulting projection. Maybe because you want a off center projection like in cameras with film offset, or you have a anisotropic medium for example. It can also be convenient for the end user to zoom into image without changing the perspective in the process.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-05-03T15:32:59.583" LastActivityDate="2016-05-03T15:32:59.583" CommentCount="0" />
  <row Id="2386" PostTypeId="1" AcceptedAnswerId="2387" CreationDate="2016-05-02T19:56:03.910" Score="2" ViewCount="133" Body="&lt;p&gt;I have been studying computer graphics, from the book &lt;a href=&quot;http://rads.stackoverflow.com/amzn/click/1482229390&quot; rel=&quot;nofollow&quot;&gt;Fundamentals of Computer Graphic&lt;/a&gt; (but the third edition), and I lastly read about texture mapping and shadow maps. Though, I didn't exactly understand how they function. I know there is also another type of texture map called projective texture map which allows to cast hard shadows, but how they function? On the other hand, how are soft shadows cast? And what's exactly the difference between hard and soft shadows?&lt;/p&gt;&#xA;" OwnerUserId="3277" LastActivityDate="2016-05-03T03:27:35.803" Title="How are hard and soft shadows cast?" Tags="&lt;texture&gt;&lt;3d&gt;&lt;shadow&gt;&lt;shadow-mapping&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2387" PostTypeId="2" ParentId="2386" CreationDate="2016-05-02T20:16:32.843" Score="2" Body="&lt;p&gt;Hard shadows are simple that only needs a point light. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How it's done is by rendering the scene from the point of view of the light and only keep only the depth information. This is the shadow map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then when doing the actual rendering you calculate the point on the triangle in world space and find where it would be on the shadow map. Then you sample the depth from the shadowmap at that point and compare it to the distance to the light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Soft shadows are harder. They require figuring out how close you are to the edge of what would be a hard shadow and adjusting the light value based on that. There are several techniques for that. One of which is simply sampling multiple times. &lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-05-02T20:16:32.843" CommentCount="1" />
  <row Id="2388" PostTypeId="2" ParentId="2386" CreationDate="2016-05-03T03:27:35.803" Score="1" Body="&lt;p&gt;@ratchetfreak's answer describes shadow mapping, however, there's another not so difficult to implement way of drawing hard shadows. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Shadow_volume&quot; rel=&quot;nofollow&quot;&gt;&lt;em&gt;shadow volume&lt;/em&gt;&lt;/a&gt; technique uses extruded geometry plus some &lt;a href=&quot;http://computergraphics.stackexchange.com/q/12/54&quot;&gt;stencil buffer&lt;/a&gt; tricks to simulate the area occluded by an object. The idea behind it is to generate a volume from the light source's perspective and then render it as you would render other geometry. The resulting shadow geometry will be as accurate as your graphics hardware is capable of rendering it, so the shadows will be very clear cut. There's no simple way of producing soft shadow edges with this technique though.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/sRO84.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/sRO84.png&quot; alt=&quot;shadow volume&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="54" LastActivityDate="2016-05-03T03:27:35.803" CommentCount="2" />
  <row Id="2389" PostTypeId="2" ParentId="2357" CreationDate="2016-05-03T13:17:41.510" Score="3" Body="&lt;p&gt;This has to do with gamma correction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If one pixel has red component with value of 1 (where 255 is max), the next pixel has value for red that is 2, there no guarantee that exactly twice as much photons are exiting from the second pixel. Displays have different curves that predict what's the expected brightness. It also has to do with how our eyes work: they're more sensitive for darker shades than brighter shades.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To read more about these phenomena:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://gamedevdaily.io/the-srgb-learning-curve-773b7f68cf7a#.di88vr58t&quot; rel=&quot;nofollow&quot;&gt;https://gamedevdaily.io/the-srgb-learning-curve-773b7f68cf7a#.di88vr58t&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://marcinignac.com/blog/pragmatic-pbr-setup-and-gamma/&quot; rel=&quot;nofollow&quot;&gt;http://marcinignac.com/blog/pragmatic-pbr-setup-and-gamma/&lt;/a&gt; (gamma section)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://colour-science.org/posts/the-importance-of-terminology-and-srgb-uncertainty/&quot; rel=&quot;nofollow&quot;&gt;http://colour-science.org/posts/the-importance-of-terminology-and-srgb-uncertainty/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://www.essentialmath.com/GDC2015/VanVerth_Jim_DoingMathwRGB.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.essentialmath.com/GDC2015/VanVerth_Jim_DoingMathwRGB.pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The three images are used as low value, medium grey and bright examples. Indeed, the lowest brightness is used so you are able do see the difference between the black background and &quot;barely lit&quot; object, and the other two images are just used to display other important values for the set gamma curve.&lt;/p&gt;&#xA;" OwnerUserId="2838" LastActivityDate="2016-05-03T13:17:41.510" CommentCount="0" />
  <row Id="2391" PostTypeId="1" CreationDate="2016-05-04T21:54:04.517" Score="11" ViewCount="273" Body="&lt;p&gt;What is the best currently known and ideally also production-verified approach for sampling environment maps (EM) in a MIS-based uni-directional path tracer and similar types of renderers? I would prefer solutions which are reasonably complicated while reasonably functional to those which provide perfect sampling at cost of super complicated and hard-to-understand implementation.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;What I know so far&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;There are some easy ways of sampling EMs. One can sample the needed hemisphere in a cosine-weighted manner,&#xA;   which ignores both the BSDF and the EM function shapes. As a result,&#xA;   it doesn't work for dynamic EMs:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TBYYn.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TBYYn.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To improve&#xA;   the sampling to a usable level, one can sample the luminance of the EM&#xA;   over the whole sphere. It is relatively easily implemented and the&#xA;   results are quite good. However, the sampling strategy is still&#xA;   ignoring the hemispherical visibility information and the cosine&#xA;   factor (and also the BSDF), resulting in high noise on the surfaces&#xA;   which are not directly lit by high-intensity areas of the EM:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1DOSr.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1DOSr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Papers&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I have found a few papers on the topic, but have not read them yet. Is any of these worth reading and implementing in a forward uni-directional path tracer, or is there something even better?&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://cseweb.ucsd.edu/~ravir/papers/structured/&quot; rel=&quot;nofollow&quot;&gt;Structured Importance Sampling of Environment Maps&lt;/a&gt; (2003) by&#xA;Agarwal et al.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://home.eps.hw.ac.uk/~ks400/research.html#SteerableImportance&quot; rel=&quot;nofollow&quot;&gt;Steerable Importance Sampling&lt;/a&gt; (2007) by&#xA;Kartic Subr and Jim Arvo. They claim to present “...an algorithm for&#xA;efficient stratified importance sampling of environment maps that&#xA;generates samples in the positive hemi-sphere defined by local&#xA;orientation of arbitrary surfaces while accounting for cosine&#xA;weighting. “ The paper “Importance Sampling Spherical Harmonics”&#xA;comments on it: ”They create a triangulated representation of the&#xA;environment map and store the illumination premultiplied by each of&#xA;the first nine spherical harmonic basis functions at every vertex.&#xA;This forms a steerable basis where the clamped-cosine can be&#xA;efficiently rotated to any orientation.”&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://fileadmin.cs.lth.se/graphics/research/papers/2008/pps/&quot; rel=&quot;nofollow&quot;&gt;Practical Product Importance Sampling for Direct Illumination&lt;/a&gt; (2008) by&#xA;Petrik Clarberg and Tomas Akenine-Möller. An algorithm for sampling&#xA;the product of environment map lighting and surface reflectance. Uses&#xA;wavelet-based importance sampling.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://www.cs.dartmouth.edu/~wjarosz/publications/jarosz09importance.html&quot; rel=&quot;nofollow&quot;&gt;Importance Sampling Spherical Harmonics&lt;/a&gt; (2009)&#xA;by Jarosz, Carr, and Jensenn. The abstract says: “...we present the&#xA;first practical method for importance sampling functions represented&#xA;as spherical harmonics (SH)...”&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/26584494&quot; rel=&quot;nofollow&quot;&gt;Tone-Mapped Mean-shift Based Environment Map Sampling&lt;/a&gt; (2015) by Feng&#xA;et al. This is pretty new and I have found neither a reference to it&#xA;nor the paper itself.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-05-14T11:03:35.480" LastActivityDate="2016-05-21T19:29:57.323" Title="Importance Sampling of Environment Maps" Tags="&lt;rendering&gt;&lt;sampling&gt;&lt;pathtracing&gt;&lt;importance-sampling&gt;&lt;ibl&gt;" AnswerCount="3" CommentCount="2" FavoriteCount="2" />
  <row Id="2392" PostTypeId="2" ParentId="1473" CreationDate="2016-05-04T22:38:30.980" Score="1" Body="&lt;p&gt;Ship your application with a benchmarking sequence testing the actual platform. (Possible answer for many questions I guess...)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suspect the performance is highly dependent on &lt;strong&gt;how&lt;/strong&gt; you use the hardware. Since the hardware is unlikely to somehow instrument your application backwards, telling you what to do, I´d go with whatever looks good in your design.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;...command queues can be executed concurrently by the device...&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Keyword is CAN. I see no reason why any vendor would screw this up. In the end it is the platform provider (Intel/AMD/Nvidia) who is responsible of making you a good enough driver for you not to consider switching vendor. If they do have a &quot;know issue&quot; with this functionality (which by the way has no functional meaning, only performance) then they &lt;strong&gt;should&lt;/strong&gt; also solve it using what they know. I mean for crying out loud, the fallback is something they have already implemented; synchrounous execution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hardware is enough voodoo as it is for us developers.&lt;/p&gt;&#xA;" OwnerUserId="3041" LastActivityDate="2016-05-04T22:38:30.980" CommentCount="1" />
  <row Id="2393" PostTypeId="1" AcceptedAnswerId="2401" CreationDate="2016-05-05T06:44:00.977" Score="6" ViewCount="122" Body="&lt;p&gt;Tessellation has been touted as one of the major features in newer graphics APIs like DirecX 11, and it is shown as a standalone stage in a modern graphics pipeline.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Compared to the amount of hardware and software attention given to this feature, it doesn't seem to be heavily used in real-time graphics. So this doesn't seem to be a feature that arose out of graphical demand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then why did tessellation become such a prominent feature? To cater to the demands of non-realtime rendering? As a side-effect of the increasing shift in GPU architecture as generalized parallel processors in heterogeneous computing? Or is this a forward-thinking feature that will be used in graphics as tessellation-capable GPUs become increasingly common?&lt;/p&gt;&#xA;" OwnerUserId="88" LastActivityDate="2016-05-06T17:16:17.913" Title="Why did tessellation come to be a prominent feature?" Tags="&lt;gpu&gt;&lt;directx11&gt;&lt;tesselation&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2394" PostTypeId="2" ParentId="2359" CreationDate="2016-05-05T06:44:32.243" Score="1" Body="&lt;p&gt;Rather than making one spatial subdivision structure do double duty in representing both the voxels and the triangles, I would suggest creating a separate BVH for the mesh. You can find many articles and papers about BVH-building algorithms on the web. It's likely to be a more efficiently queryable representation of the mesh than the octree would be.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given the BVH, it's easy to determine whether a given voxel might intersect the mesh by starting at the BVH root and traversing to child nodes that intersect the voxel box. Depending on the quality of the BVH, i.e. how tightly it fits the mesh, many voxels (or even higher-level octree nodes) may be able to be eliminated with only a few checks. For voxels that do intersect the mesh surface, the BVH traversal will reach down to the leaves, where you can accumulate the triangles into a list for clipping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the BVH is built in the mesh's local space, the mesh can easily be moved or transformed without needing to update the octree. (If translation is the only transform needed, then the BVH nodes will always be axis-aligned relative to the voxels, which simplifies the intersection test to an AABB overlap test; if more general transforms are needed, you'd have to use a more general OBB vs OBB test.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, note that a BVH doesn't lend itself that well to determining whether a things are inside versus outside a watertight mesh. If that's important to you (it wasn't clear to me whether it is), then you might want to look into using a BSP tree instead of a BVH. A BSP tree can be used for intersection testing similarly to a BVH, but additionally can represent the exact boundary of the mesh so that it can be used to determine whether a point (or voxel) is inside or outside. This property of BSP trees is widely used for collision detection.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An alternate approach to determining inside/outsideness is to use raycasting and count the number of intersections with the mesh (even = outside, odd = inside). The raycasting can be accelerated by the BVH as well.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-05T06:44:32.243" CommentCount="2" />
  <row Id="2395" PostTypeId="2" ParentId="2393" CreationDate="2016-05-05T14:49:18.677" Score="10" Body="&lt;p&gt;The main purpose of tesselation is to increase the resolution of the mesh, while only transferring a small amount of triangle data around. In addition, tessellation allows you to dynamically change the LOD of the mesh, so you can &lt;a href=&quot;http://www.g-truc.net/post-0662.html&quot;&gt;optimize your shader calls.&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, we can pass the GPU, say, 3000 triangles, and have it tesselate it to 300000 triangles. We are essentially trading storage space/bandwidth for compute power. Since GPUs have lots and lots of compute, and memory is limited/slow, this is a pretty good tradeoff.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for &quot;not being heavily used in real-time graphics&quot;. I somewhat disagree. Many AAA games have been using tessellation for a long time. That said, tessellation is &lt;em&gt;hard&lt;/em&gt; to get right. Done wrong, tesselation can lead to lots of problems, such as &lt;a href=&quot;https://developer.nvidia.com/content/dynamic-hardware-tessellation-basics&quot;&gt;cracking&lt;/a&gt;, or over tessellating to sub-pixel triangles, which &lt;a href=&quot;http://www.g-truc.net/post-0662.html&quot;&gt;destroys your fill rate&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Combined, these problems make it difficult to implement good and fast tessellation for your everyday person. Thus, most of the uses of tessellation you see nowadays are in AA or AAA games, game engines, and offline rendering tools.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, there is active research going on that is trying to better utilize the tessellation hardware, and make it easier to use. For example: &lt;a href=&quot;http://www.graphics.stanford.edu/~niessner/brainerd2016efficient.html&quot;&gt;Efficient GPU Rendering of Subdivision Surfaces using Adaptive Quadtrees&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end, tessellation is a great feature, but is difficult to get right&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2016-05-05T14:49:18.677" CommentCount="1" />
  <row Id="2397" PostTypeId="2" ParentId="1862" CreationDate="2016-05-06T01:23:24.967" Score="0" Body="&lt;p&gt;While the other answers given are correct I'm not sure they entirely answer your question. In order to not get black bars you have 2 options if the destination is not the same shape (or aspect ratio) as the source:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Fill the destination leaving some pixels from the source out of the destination.&lt;/strong&gt; In your example, the source is 1024x768 and the destination is 1920x1080. If you scale the original image up so the result is 1920x1440, you can then crop the top and/or bottom off to make it 1920x1080.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Stretch the destination image to fit.&lt;/strong&gt; So you'd scale vertically by 1080/768 = 1.40625, and scale horizontally by 1920/1024 = 1.875. This is almost never what you want, and looks terrible in most cases. (This is what makes people look much wider than they should, and turns circles into ovals and rotations into shears.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;There's another option, but it doesn't fit your criteria - scale uniformly, but only until 1 dimension is as big as the destination. In this case you would scale both horizontally and vertically by 1.40625 to get a result of 1440x1080. This is usually referred to as &quot;scale to fit&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For any of these options you can use whichever scaling algorithm meets your needs.&lt;/p&gt;&#xA;" OwnerUserId="3003" LastActivityDate="2016-05-06T01:23:24.967" CommentCount="2" />
  <row Id="2398" PostTypeId="1" CreationDate="2016-05-06T10:21:44.147" Score="1" ViewCount="34" Body="&lt;p&gt;I implemented the algorithm shown in given link to fit given image to specified quadrilateral but the output is blank. Here is the link to the algorithm&#xA;&lt;a href=&quot;http://math.ewha.ac.kr/~jylee/SciComp/dip-crane/chapter8/list8_4.c&quot; rel=&quot;nofollow&quot;&gt;http://math.ewha.ac.kr/~jylee/SciComp/dip-crane/chapter8/list8_4.c&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I implemented it in C# as follows&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-csharp prettyprint-override&quot;&gt;&lt;code&gt;    Bitmap inputImage = new Bitmap(&quot;D:\\Image\\A2.jpg&quot;);&#xA;        Bitmap outputBitmap = new Bitmap(inputImage.Width, inputImage.Height);&#xA;        double a11, a21, a31, a41;&#xA;        double a12, a22, a32, a42;&#xA;        double first, second;&#xA;        double denom;&#xA;        int u, v;&#xA;        double floatu, floatv;&#xA;        int i, j;&#xA;        ulong sourcebase; /// this has been replaced by u,v i think its same&#xA;        double x, y;&#xA;        double A, B, C;&#xA;        int x0 = 50, y0 = 100, x1 = 65, y1 = 110, x2 = 55, y2 = 120, x3 = 450, y3 = 110;// fix value quadrilaeral for checking purpose.&#xA;        a11 = x0 - x1 - x3 + x2;&#xA;        a21 = x1 - x0;&#xA;        a31 = x3 - x0;&#xA;        a41 = x0;&#xA;        a12 = y0 - y1 - y3 + y2;&#xA;        a22 = y1 - y0;&#xA;        a32 = y3 - y0;&#xA;        a42 = y0;&#xA;        Console.WriteLine(&quot;inside of here&quot;);&#xA;        A = a11 * a22 - a21 * a12;&#xA;        if (A == 0)&#xA;            A = 0.00001;&#xA;        int rows = inputImage.Height, cols = inputImage.Width;&#xA;        for (i = 0; i &amp;lt; rows; i++)&#xA;        {&#xA;            y = Convert.ToDouble(i) / Convert.ToDouble(rows);&#xA;            for (j = 0; j &amp;lt; cols; j++)&#xA;            {&#xA;                x = Convert.ToDouble(j) / Convert.ToDouble(cols);&#xA;                B = a12 * x - a11 * y + a11 * a42 - a41 * a12 + a31 * a22 - a21 * a32;&#xA;                C = a32 * x - a31 * y + a31 * a42 - a41 * a32;&#xA;                first = B * B;&#xA;                second = 4.0 * A * C;&#xA;                if (first &amp;lt; second)&#xA;                    floatu = B / (-2.0 * A);&#xA;                else&#xA;                    floatu = (-1.0 * B + Math.Sqrt(first - second)) / (2.0 * A);&#xA;                if((floatu &amp;lt; 0.0) | (floatu &amp;gt; 1.0))&#xA;                {&#xA;                    if (first &amp;lt; second)&#xA;                        floatu = B / -2.0 * A;&#xA;                    else&#xA;                        floatu = (1.0 * B - Math.Sqrt(first - second)) / (2.0 * A);&#xA;                }&#xA;                denom = (a11 * floatu + a31);&#xA;                if (denom == 0.0)&#xA;                    denom = 0.000001;&#xA;                floatv = (x - a21 * floatu - a41) / denom;&#xA;                u = Convert.ToInt32(floatu * cols + 0.5);&#xA;                v = Convert.ToInt32(floatv * rows + 0.5);&#xA;                Color pixelColor;&#xA;                if ((u &amp;gt;= cols) | (u &amp;lt; 0) | (v &amp;gt;= rows) | (v &amp;lt; 0))&#xA;                {&#xA;                    // Console.WriteLine(&quot;escapaed&quot;);&#xA;                    pixelColor = Color.Black;&#xA;                    continue;&#xA;                }&#xA;                else { /// program ne&#xA;                    Console.WriteLine(&quot;here&quot;);&#xA;                    //sourcebase = ((ulong)u * (ulong)cols + (ulong)v);&#xA;                    pixelColor = inputImage.GetPixel((int)u, (int)v);&#xA;                    outputBitmap.SetPixel(i, j, pixelColor);// =pixelColor;&#xA;                                                            // pictureBox1.Image = outputBitmap;&#xA;                }&#xA;            }&#xA;          //  Console.WriteLine(i);&#xA;        }&#xA;        pictureBox1.Image = outputBitmap;&#xA;        outputBitmap.Save(&quot;output.jpg&quot;);&#xA;        Console.WriteLine(&quot;Out Of here&quot;);&#xA;        pictureBox1.Invalidate();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2383" LastEditorUserId="2383" LastEditDate="2016-05-10T00:04:00.297" LastActivityDate="2016-05-10T00:04:00.297" Title="Bilinear Transformation not working" Tags="&lt;image-processing&gt;&lt;perspective&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="2399" PostTypeId="1" AcceptedAnswerId="2404" CreationDate="2016-05-06T15:05:48.190" Score="2" ViewCount="52" Body="&lt;p&gt;I am using matrix for performing 3D rotations. I know that in 3D space the matrix product order is important - changing the order of the matrices can effect the rotate result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I am interesting about how can I create a rotate matrix that perform rotation (clockwise) around some vector, say $(1, 0, 1)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought that $R_{(1,0,1)}(\theta) = R_X(\theta) \cdot R_Z(\theta)$ but it seems wrong because if I tries to rotate the $(1, 0, 1)$ with $45^{\circ}$ it moves out from $(1, 0, 1)$ but it should stay in place.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I build such a rotation matrix as product of 3D trasformation matrices?&lt;/p&gt;&#xA;" OwnerUserId="3305" LastActivityDate="2016-05-06T21:01:50.843" Title="3D rotation matrix around vector" Tags="&lt;3d&gt;&lt;transformations&gt;&lt;matrices&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2401" PostTypeId="2" ParentId="2393" CreationDate="2016-05-06T17:16:17.913" Score="0" Body="&lt;p&gt;History: ATI Introduced PN triangles (a basic approach to tessellation) in its first generation to include programmable HW - so it's been around about as long as programmable HW shaders. It was deprecated in ATI's next generation, but tessellation was revived in the HW that became the basis of the Xbox 360 (a few ATI demos showed it off on PCs). Microsoft then incorporated the feature into DX11 (although not compatible with ATI/AMD's existing HW), making tessellation support a de-facto requirement for all GPU makers.&lt;/p&gt;&#xA;" OwnerUserId="2500" LastActivityDate="2016-05-06T17:16:17.913" CommentCount="4" />
  <row Id="2402" PostTypeId="1" CreationDate="2016-05-06T17:20:52.417" Score="1" ViewCount="34" Body="&lt;p&gt;I am trying to find an algorithm as mentioned in the subject. It is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; int orientation(glm::i16vec2 &amp;amp;p1,&#xA;                 glm::i16vec2 &amp;amp;p2,&#xA;                 glm::i16vec2 &amp;amp;p3)&#xA; {&#xA;   return (((p3.x - p1.x) * (p2.y - p1.y)) - ((p2.x - p1.x) * (p3.y - p1.y)));&#xA; }&#xA;&#xA;&#xA; bool onSegment(const glm::i16vec2 &amp;amp;pi,&#xA;                const glm::i16vec2 &amp;amp;pj,&#xA;                const glm::i16vec2 &amp;amp;pk)&#xA; {&#xA;   if((std::min(pi.x,pj.x) &amp;lt;= pk.x &amp;amp;&amp;amp; pk.x &amp;lt;= std::max(pi.x,pj.x)) &amp;amp;&amp;amp;&#xA;      (std::min(pi.y,pj.y) &amp;lt;= pk.y &amp;amp;&amp;amp; pk.y &amp;lt;= std::max(pi.y,pj.y)))&#xA;    return true;&#xA;   else&#xA;    return false;&#xA; }&#xA;&#xA; int main()&#xA; {&#xA;   glm::i16vec2 hs(5,2);  // p1&#xA;   glm::i16vec2 he(5,3); // p2&#xA;&#xA;&#xA;   glm::i16vec2 cs(5,1);   // p3&#xA;   glm::i16vec2 ce(5,3); // p4&#xA;&#xA;   int d1 = orientation(cs,ce,hs);&#xA;   int d2 = orientation(cs,ce,he);&#xA;   int d3 = orientation(hs,he,cs);&#xA;   int d4 = orientation(hs,he,ce);&#xA;&#xA;   if((d1 == 0 &amp;amp;&amp;amp; d2 == 0 &amp;amp;&amp;amp; d3 == 0 &amp;amp;&amp;amp; d4 == 0) &amp;amp;&amp;amp;&#xA;        (onSegment(cs,ce,hs) &amp;amp;&amp;amp; onSegment(cs,ce,he)))&#xA;   {&#xA;    std::cout &amp;lt;&amp;lt; &quot;HPs is entirely inside CPs&quot; &amp;lt;&amp;lt; std::endl;&#xA;   }&#xA;&#xA;   return 0;&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So far it working fine. Is there any leak within procedure when it may not work as intended in some situations ? Would like  to know about those scenarios ...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks &lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-05-06T17:20:52.417" Title="Check if one line segments is entirely on the other line segment" Tags="&lt;line-segment-overlapping&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="2403" PostTypeId="2" ParentId="2399" CreationDate="2016-05-06T20:14:27.637" Score="1" Body="&lt;p&gt;Build a matrix with one of the vectors a=(1,0,1) now you need another vector to define the matrix vector say b=(1, 0, 0) take the cross product of a  and b lets call this c. Then take the cross product of a and c for a rectified b'. Normalize a, b' and c.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your matrix T is now [a b' c] plus any affine part you deem neccesery.  Then do T&lt;sup&gt;-1&lt;/sup&gt; * Rx(Angle) * T. where Rx(a) is a function that returns a rotation matrix around the x axis with a angle of a.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-05-06T20:14:27.637" CommentCount="1" />
  <row Id="2404" PostTypeId="2" ParentId="2399" CreationDate="2016-05-06T21:01:50.843" Score="3" Body="&lt;p&gt;There is a direct formula for the rotation matrix for an arbitrary axis and angle. Given a unit vector $a = (a_x, a_y, a_z)$ and angle $\theta$, the matrix can be constructed as follows (derivation from &lt;a href=&quot;https://en.wikipedia.org/wiki/Rodrigues&amp;#39;_rotation_formula#Matrix_notation&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First build a matrix $C$ from the components of $a$ according to the following formula:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ C =&#xA;\begin{bmatrix}&#xA;  0 &amp;amp; -a_z &amp;amp; a_y \\&#xA;  a_z &amp;amp; 0 &amp;amp; -a_x \\&#xA;  -a_y &amp;amp; a_x &amp;amp; 0&#xA;\end{bmatrix} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then you can construct the rotation matrix from $C$ as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ R_a(\theta) = I + C \sin \theta + C^2 (1 - \cos \theta) $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;where $I$ is the 3&amp;times;3 identity matrix. This assumes you're using a column-vector convention; if you're using row vectors instead, transpose $C$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By the way, this formula has a neat geometric interpretation. The matrix $C$ has the effect of calculating cross products with $a$. In other words, for any vector $v$, $Cv = a \times v$. If you imagine rotating some vector $v$ about $a$, the tip of $v$ traces out a circle perpendicular to $a$; $Cv$ is a vector tangent to that circle, and $C^2 v$ (which equals $a \times (a \times v)$) is a vector normal to the circle. So with those two vectors, you have a basis for the plane of the circle; then you can apply a variation of the usual sine/cosine formula for points on a circle.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-06T21:01:50.843" CommentCount="1" />
  <row Id="2405" PostTypeId="1" CreationDate="2016-05-06T21:46:39.237" Score="7" ViewCount="100" Body="&lt;p&gt;So I been thinking about this for a while and tried to google for an answer but without any success.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If all your textures are 8bit LDR images, like JPEGs, couldn't that potentially cause conflicts with exposure control/tone mapping when rendering. That is if you adjust the rendering exposure of your image that should expose detail in the textures that aren't really there, since they been clamped out by the low dynamic range. So wouldn't it make sense to also have the textures as HDR images, saved as .exr, in linear colour space with 16bit half-float to get a good colour representation (32bit &quot;full&quot; float might be overkill?). To have more detailed and correct colour values might also, I figure, have an effect on GI and how colour bleed is calculated?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or is it simply not necessary since the end result of the rendering we want is probably going to be similar to the exposure level of the texture when it was photographed any way? And since camera mostly shoot in 12-14bit you would have to take multiple exposures of the texture and do all that extra work to piece them all together to one HDRI. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;To clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines.&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="2736" LastEditorUserId="2736" LastEditDate="2016-05-10T07:32:26.140" LastActivityDate="2016-05-10T18:27:56.747" Title="16bit half-float linear HDR images as (diffuse/albedo) textures?" Tags="&lt;texture&gt;&lt;physically-based&gt;&lt;pbr&gt;&lt;diffuse&gt;" AnswerCount="3" CommentCount="0" FavoriteCount="1" />
  <row Id="2406" PostTypeId="2" ParentId="2405" CreationDate="2016-05-07T06:17:59.660" Score="8" Body="&lt;p&gt;Yes, it's possible in some extreme cases for HDR lighting and tonemapping to expose banding issues in color textures. In those cases, having a higher bit depth for the textures could be useful. However, in my experience the majority of materials and ordinary lighting situations don't exhibit this problem, and most textures in a typical game are fine in 8-bit (or even less&amp;mdash;games often use BC1 compression, which reduces them to 5-6-5-bit).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;People use HDR render targets because a single scene can contain vastly different magnitudes of brightness, such as a dark room in which you can see through a window to a sunlit exterior 10&amp;ndash;100 times brighter than the room. However, color textures don't have such a wide range of magnitudes. They represent reflectances, which are inherently in the [0, 1] range, and in practice few everyday materials are lower than about 2&amp;ndash;5% reflectance. So an 8-bit image (with gamma encoding) can usually represent diffuse and specular colors with enough precision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's true that the combination of a quite dark texture with very bright lighting or an extremely overexposed camera setting can show banding in the final frame, but that would be a more unusual case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A case where you probably &lt;em&gt;would&lt;/em&gt; want an HDR texture is emissive materials, especially for neon signs and similar light sources. The texture would appear with its value amped up to appear as a bright light source in game, so in that case an 8-bit image could easily show banding.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, it can still be useful to work at higher precision (e.g. 16-bit precision) if possible when capturing and creating textures, simply because it gives you more headroom to process the image without causing precision problems. For instance, if you need to adjust levels or color balance, you lose a little precision; that can introduce banding (especially if you do it multiple times) when starting from an 8-bit source image. A 16-bit source would be more resilient to such problems. However, the final texture used in the game would still probably be compressed to 8-bit.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-07T06:17:59.660" CommentCount="4" />
  <row Id="2407" PostTypeId="2" ParentId="2381" CreationDate="2016-05-07T06:40:03.413" Score="3" Body="&lt;p&gt;A convex polygon has the property:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;A line drawn through a convex polygon will intersect the polygon exactly twice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From this follows that any line trough splits the convex polygon in 2 pieces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uyM6r.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uyM6r.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: convex polygon split by a line.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Using this, and assuming the inner piece is discarded we can craw the conclusion that there are five cases:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;When the lines of the square do not exit the polygon. You now have a piece with a hole or the entire polygon is contained and theres no hole and no piece.&lt;/li&gt;&#xA;&lt;li&gt;When the lines exit 2 times you have one fragment*&lt;/li&gt;&#xA;&lt;li&gt;when the lines exit 4 times you have two fragments*&lt;/li&gt;&#xA;&lt;li&gt;when the exits are 6 times you have three fragments* &lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;when the exits are 8 times you have four fragments *&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/OwHAq.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/OwHAq.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: the five cases, clipping rectangle and remaining shards in black. Even a squares has same cases if the polygons vary, just easier to draw this way.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;The last case is the maximum so answer is &lt;strong&gt;four&lt;/strong&gt; or five if you count the clipped part as one piece. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;* Since we haven't tackled the cases very rigorously we know that this is the highest possible answer, there could be less for each case. There shouldn't be but I so far haven't given evidence for that. But thats okay it wasn't asked.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-05-08T20:18:18.947" LastActivityDate="2016-05-08T20:18:18.947" CommentCount="4" />
  <row Id="2409" PostTypeId="1" AcceptedAnswerId="2439" CreationDate="2016-05-07T10:24:23.030" Score="4" ViewCount="39" Body="&lt;p&gt;I have a Optix Raycasting renderer which loads and renders VTK files.&#xA;For a single static model this works fine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But now my dataset consists of multiple timesteps which I want to display in a animated fashion.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Each couple of frames or seconds the displayed model has to be switched out with the next one.&#xA;If possible I want to refrain from rebuilding the acceleration structure each time this happens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So my question: Can I just load all models at once, place them in the global acceleration structure and then just switch all off except the one currently displayed.&#xA;My hope is that this would be more efficient.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit 1:&#xA;I have now build a variant in which I create &lt;em&gt;optix::Acceleration&lt;/em&gt; and &lt;em&gt;optix::Group&lt;/em&gt; for each of the timesteps separate and each time the active timestep changes I just &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_context[&quot;top_object&quot;]-&amp;gt;set(mesh_group[timestep]);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But each this call takes nearly half a second. I think optix does quite some work in the background when I switch this object out.&#xA;This delay is not acceptable for me as it makes the playback of my &quot;animation&quot; stutter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Has anyone a idea how to do this more efficient?&lt;/p&gt;&#xA;" OwnerUserId="273" LastEditorUserId="273" LastEditDate="2016-05-09T11:45:01.930" LastActivityDate="2016-05-13T11:34:19.540" Title="Optix: Rendering time-variant data" Tags="&lt;optix&gt;&lt;time-variant&gt;&lt;acceleration&gt;&lt;vtk&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2410" PostTypeId="1" CreationDate="2016-05-07T12:46:39.143" Score="3" ViewCount="33" Body="&lt;p&gt;Specify individually the translation and scaling matrices required to transform a 2D&#xA;window of [Xmin=-234, Ymin=156] and [Xmax=66, Ymax=456] to a display viewport&#xA;of [Umin=45, Vmin=35] and [Umax=245, Vmax=185].&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ignore the question above since I solved the matrix, the information is just relevant for the question I'm stuck on&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was asked to compute the view-port positions (U1,V1) and (U2,V2) for two points A (-100,300)&#xA;and B (30,-40) in a 2D window and determine if these two points are inside the view-port.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on the transformation Matrix I found (U1,V1) to be (403/3, 407) and (U2,v2) to be (221, -103). It turns out that both these points are outside our view-port but part of the line between them is inside.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I'm confused about this part below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apply a 2D clipping method to the line segment between the two points A and B as&#xA;given in above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My Attempt&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;delta x = 221-(403/3) = 260/3&lt;/p&gt;&#xA;&#xA;&lt;p&gt;delta y = -103 - 406 = -510&lt;/p&gt;&#xA;&#xA;&lt;p&gt;m = delta y / delta x = -5.88&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I started with U1,V1 since it is above our viewport:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Y = 185&lt;/p&gt;&#xA;&#xA;&lt;p&gt;X = 403/3 + (185-407)*(delta x/ delta y)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;X = 279.48&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Point 1 - (172, 185) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this correct? Since the point is now within the view-port. Do I then do the same for the second point?&lt;/p&gt;&#xA;" OwnerUserId="1971" LastActivityDate="2016-05-07T12:46:39.143" Title="Cohen-Sutherland Clipping" Tags="&lt;clipping&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2411" PostTypeId="1" AcceptedAnswerId="2413" CreationDate="2016-05-08T12:23:55.243" Score="8" ViewCount="263" Body="&lt;p&gt;This is how the rendering equation is written in the textbook&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$L(p,\omega) = L_e(p,\omega) + \int f(p,\omega_i,\omega) \, L(p*,-\omega_i)\cos \theta \, d\omega_i$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;which component of this equation handles the shadowing?&lt;/p&gt;&#xA;" OwnerUserId="2359" LastEditorUserId="48" LastEditDate="2016-05-08T18:31:34.063" LastActivityDate="2016-05-08T18:31:34.063" Title="How does the rendering equation incorporate shadowing" Tags="&lt;rendering&gt;&lt;shadow&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2412" PostTypeId="1" CreationDate="2016-05-08T12:39:12.340" Score="2" ViewCount="55" Body="&lt;p&gt;In perspective projection, group of parallel lines have the same vanishing point. I am interesting about the reverse calculation: Getting the group of parallel lines equations that their vanishing point specific point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say I know that the camera is perspective camera at $(0,0,0)$ and it's direction is $(0,0,1)$, the view plane is $z = 1$ and I am interesting about the lines in plane $y= y_0$ that their vanishing point is $P = (p_x,p_y,p_z)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to calculate the projection point of some point $(x,y_0,z)$ and get the equations:&lt;br&gt;&#xA;(i) $p_x = x(\frac1z)$&lt;br&gt;&#xA;(ii) $p_y = y_0(\frac1z)$&lt;br&gt;&#xA;(iii) $p_z = 1$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But it seems wrong because if the vanishing point is like $(x_0,0,*)$ then form (ii) we will get $z\rightarrow \infty$ but then (i) is wrong because $x(\frac1z)\rightarrow0$ but it need to be equals to $x_0$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how can I get the group of parallel lines have the same vanishing point in these conditions?&lt;/p&gt;&#xA;" OwnerUserId="3305" LastActivityDate="2016-05-08T13:42:24.037" Title="Calculate vanishing point" Tags="&lt;3d&gt;&lt;perspective&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2413" PostTypeId="2" ParentId="2411" CreationDate="2016-05-08T12:53:05.680" Score="11" Body="&lt;p&gt;Light that is blocked will mean that $L(p*, -w_i)$ under the integral is 0 plus how much light the blocking object itself reflects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words the shadowing is embedded in the incoming light function.&lt;/p&gt;&#xA;" OwnerUserId="137" LastEditorUserId="137" LastEditDate="2016-05-08T15:40:27.513" LastActivityDate="2016-05-08T15:40:27.513" CommentCount="0" />
  <row Id="2414" PostTypeId="2" ParentId="2412" CreationDate="2016-05-08T13:42:24.037" Score="3" Body="&lt;p&gt;Here's a hint to get you started: Parallel lines include the line through the camera. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So really all you need is the direction from the camera to the vanishing point on the view plane. Then create lines parallel to that line.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-05-08T13:42:24.037" CommentCount="3" />
  <row Id="2416" PostTypeId="1" CreationDate="2016-05-09T14:39:36.810" Score="2" ViewCount="83" Body="&lt;p&gt;Here's an odd intellectual puzzle.  Something I was daydreaming about on a train - but the maths are somewhat beyond me.  How would someone go about un-winding a spiral to produce a long flat image?  Like a rope on a spool, I'd want to uncoil the rope so that it lay long and flat.  The function would have to take in the image data (imageData) and the imageHeight (the height of the output strip - and hence also the thickness of the 'rope' on the spool).  Assumptions that could be made are that the input image is square, containing a circle whose top and bottom touch the top and bottom of the square - and left and right similarly.  The hub of circle would be in the absolute centre of the image.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(char*) unwindImage (char* imageData, int imageHeight)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Has anyone else puzzled over this?  And, if so, what solution did your come up with?&lt;/p&gt;&#xA;" OwnerUserId="3319" LastActivityDate="2016-05-12T14:40:56.900" Title="Unwinding an image on a spiral to make it long and flat" Tags="&lt;algorithm&gt;&lt;transformations&gt;&lt;maths&gt;&lt;image-processing&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2417" PostTypeId="1" CreationDate="2016-05-09T16:45:13.933" Score="3" ViewCount="40" Body="&lt;p&gt;I'm searching a way to implement a linear gradient shader that behaves as the linear gradient in Photoshop (only the vertical case is necessary). It will be applied to 2D sprites.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I'm passing to the pixel shader these parameters:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;StartColor.&lt;/li&gt;&#xA;&lt;li&gt;EndColor.&lt;/li&gt;&#xA;&lt;li&gt;Offset: an offset applied to the gradient starting point.&lt;/li&gt;&#xA;&lt;li&gt;Length: the gradient length (the range inside where the colors will be&#xA;interpolated).&lt;/li&gt;&#xA;&lt;li&gt;isFixed: a boolean parameter that indicates if the&#xA;gradient must be influenced by the camera position or not.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here a first attempt of the vertex shader and the pixel shader that I've implemented:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;struct VsInput&#xA;{&#xA;    float3 position : VES_POSITION;&#xA;    float2 uv       : VES_TEXCOORD0;&#xA;    float4 color    : VES_COLOR;&#xA;};&#xA;&#xA;struct VsOutput&#xA;{&#xA;    float4 position : HPOS;&#xA;    float2 uv       : TEXCOORD0;&#xA;    float4 color    : COLOR0;&#xA;    float4 outPos   : TEXCOORD1;&#xA;};&#xA;&#xA;VsOutput main(VsInput input)&#xA;{&#xA;    VsOutput vsOutput;&#xA;    vsOutput.position = MUL(float4(input.position, 1.0f), WorldViewProjection);&#xA;    vsOutput.uv = input.uv;&#xA;    vsOutput.color = input.color;&#xA;    vsOutput.outPos = vsOutput.position;&#xA;&#xA;    return vsOutput;&#xA;}&#xA;struct PsInput&#xA;{&#xA;    float4 Position : HPOS;&#xA;    float2 UV       : TEXCOORD0;&#xA;    float4 Color    : COLOR0;&#xA;    float4 outPos   : TEXCOORD1;&#xA;};&#xA;&#xA;float4 startColor;&#xA;float4 endColor;&#xA;float offset;&#xA;float len;&#xA;bool isFixed;&#xA;&#xA;float4 main(PsInput psInput) : COLOR&#xA;{&#xA;    psInput.outPos = psInput.outPos / psInput.outPos.w;&#xA;    float yScreenSize = 900.0f;&#xA;    float yPixelCoordinate = 0.0f;&#xA;    if (isFixed)&#xA;    {&#xA;        yPixelCoordinate = 0.5f * (1.0f - psInput.UV.y) * yScreenSize;&#xA;    }&#xA;    else&#xA;    {&#xA;        yPixelCoordinate = 0.5f * (psInput.outPos.y + 1.0f) * yScreenSize;&#xA;    }&#xA;&#xA;    float gradient = (yPixelCoordinate + offset) / len;&#xA;    gradient = clamp(gradient, 0.0f, 1.0f);&#xA;    return lerp(startColor, endColor, gradient);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When &lt;strong&gt;isFixed&lt;/strong&gt; is false I want the gradient influenced by the camera position. But my shader is wrong, since the starting point of the gradient is the bottom of the window instead of the bottom of the sprite.&#xA;The question is: how can I modify the shader in order to have the gradient starting from the bottom of the sprite? Maybe I need the size of the sprite in pixel? Or there are other convenient ways?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other question regards the &quot;fixed gradient&quot;: if I want the gradient not influenced by the camera position, what is the convenient way? It's possible to have these two behaviors in the same shader?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Maybe the sentence &lt;em&gt;gradient influenced by the camera&lt;/em&gt;, it is not correct, so I'll try to be more clear with a few images (taken using the shaders above).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the first image you can see a base sprite (in black) with an orange gradient applied to it. On top of the base you can see the player sprite; the camera is attached to this one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the second image the player has jumped and the camera follows him: the &lt;strong&gt;isFixed&lt;/strong&gt; attribute is false and, as you can see, the gradiend moves with the player and the camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The third image shows the same situation of the second image, but the &lt;strong&gt;isFixed&lt;/strong&gt; attribute is true and, as you can see, the gradient don't moves with the player and the camera.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/IhsGb.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IhsGb.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/PptVQ.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/PptVQ.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/MzWOl.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/MzWOl.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="3323" LastActivityDate="2016-05-09T16:45:13.933" Title="Linear gradient shader ( Photoshop-like)" Tags="&lt;shader&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2419" PostTypeId="1" CreationDate="2016-05-09T21:54:21.997" Score="5" ViewCount="56" Body="&lt;p&gt;Once a 3D model is sliced against a plane and projected onto it, we get a contour curve at certain height. The contour curve I am generating in this process is made of several line segments in closed loop. Now I want to generate another contour curve with the following characteristics:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Each of the line segment of the newly generated inner contour curve must be parallel to the corresponding line segments at the outer contour curve.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Each of the parallel line segments on both outer and inner contour curve has an adjacent distance in between and defined by a constant value as shown in the attached picture (green mark) &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://imgur.com/YqRlHwL&quot;&gt;Image Link&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need some suggested algorithms to generate it . I hope that I have explained the issue properly. Please do let me know if it is not clear enough.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="127" LastEditDate="2016-05-17T15:35:18.937" LastActivityDate="2016-05-17T15:35:18.937" Title="generate multiple border contour" Tags="&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2420" PostTypeId="2" ParentId="2405" CreationDate="2016-05-10T00:42:44.313" Score="4" Body="&lt;p&gt;I'd like to invite readers to read this article about &lt;a href=&quot;http://fabiensanglard.net/quake2/quake2_software_renderer.php&quot; rel=&quot;nofollow&quot;&gt;Quake 2 engine rasterization technology explained in details&lt;/a&gt;, if they have the time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If TLDR, please pay attention to this image:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/j4i57.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/j4i57.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What we see is the &lt;strong&gt;Albedo&lt;/strong&gt; channel, that's what you want to encode in 16 bits if I understand your question correctly.&lt;br&gt;&#xA;I'm not going to say &quot;&lt;em&gt;if it could be encoded in 256 colors for games in the past, why would we need 281474976710656 (that's 281 trillion) in new games?&lt;/em&gt;&quot; even if I want to but that sounds like pixar's grumpy guy from Up. More constructively, if you noted in this image, &lt;strong&gt;everything is at the same lighting level&lt;/strong&gt;. More particularly, the maximum intensity possible that preserves the desired saturation. (meaning in HSV space, V is maxed out)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The emphasis is key, we barely need any bits because albedo has no depth to encode anyway. The dynamic comes from the shading, it makes sense to work in &lt;code&gt;f32&lt;/code&gt; per components within shaders, and output to &lt;code&gt;f16&lt;/code&gt; render targets. But storing albedo textures in &lt;code&gt;f16&lt;/code&gt; that's not only overkill, that's a severe unjustified performance hog for our precious bandwidth.&lt;/p&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2016-05-10T00:42:44.313" CommentCount="1" />
  <row Id="2421" PostTypeId="1" CreationDate="2016-05-10T10:09:35.760" Score="3" ViewCount="35" Body="&lt;p&gt;I am reading &lt;a href=&quot;http://reality.cs.ucl.ac.uk/projects/two-shot-svbrdf/two-shot-svbrdf.pdf&quot; rel=&quot;nofollow&quot;&gt;two shot svbrdf capture&lt;/a&gt; . I want to understand the data fitting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, I have my input image 3000x2000 pixels, and I divide it to create my source tiles of 192x192 pixels. The way I understand this paper is, each pixel of each tile will be the svbrdf. Now we have to find the BRDF parameters for each position with the L-M method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, we get svbrdf parameters for 192x192 pixels.&#xA;After the data fitting what did they do to obtain the normal map, specular albedo and diffuse albedo.&lt;/p&gt;&#xA;" OwnerUserId="3328" LastActivityDate="2016-05-10T10:09:35.760" Title="Data fitting SVBRDF with L-M" Tags="&lt;optimisation&gt;&lt;brdf&gt;&lt;specular&gt;&lt;diffuse&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="2422" PostTypeId="1" CreationDate="2016-05-10T15:35:53.373" Score="3" ViewCount="69" Body="&lt;p&gt;I am working on a small rendering engine for a personal project and I am having issues with the texture mapping part of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It seems to work for some cases, but not for others. For example, when one of the vertices is behind the camera, the texture is stretched.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Seemingly&lt;/em&gt; &lt;strong&gt;Correct case&lt;/strong&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/IJtuU.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IJtuU.png&quot; alt=&quot;Seemingly correct case&quot;&gt;&lt;/a&gt;&#xA;&lt;strong&gt;Incorrect case&lt;/strong&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/5M70C.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5M70C.png&quot; alt=&quot;Incorrect case&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am guessing that it has something to do with the texture mapping not being perspective-correct. I have tried various changes mainly involving the z-distance to the camera, but I could not find any quick fix to my code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my code for the perspective projection:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public double[] project(double x, double y, double z) {&#xA;    double tx = x - camera.x;&#xA;    double ty = z - camera.z;&#xA;    double tz = y - camera.y;&#xA;&#xA;    double cx = Math.cos(camera.pitch);&#xA;    double cy = Math.cos(camera.yaw);&#xA;    double cz = Math.cos(camera.roll);&#xA;&#xA;    double sx = Math.sin(camera.pitch);&#xA;    double sy = Math.sin(camera.yaw);&#xA;    double sz = Math.sin(camera.roll);&#xA;&#xA;    double dx = cy * (sz * ty + cz * tx) - sy * tz;&#xA;    double dy = sx * (cy * tz + sy * (sz * ty + cz * tx)) + cx * (cz * ty - sz * tx);&#xA;    double dz = cx * (cy * tz + sy * (sz * ty + cz * tx)) - sx * (cz * ty - sz * tx);&#xA;&#xA;    double ez = 1.0 / Math.tan(FOV / 2.0);&#xA;&#xA;    double bx = ez / dz * dx;&#xA;    double by = ez / dz * dy;&#xA;&#xA;    if (dz &amp;lt; 0.0) {&#xA;        bx = -bx;&#xA;        by = -by;&#xA;    }&#xA;&#xA;    int px = (int) (width + bx * height) / 2;&#xA;    int py = (int) (height + by * height) / 2;&#xA;&#xA;    return new double[] { px, py, dz };&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and here my code for the texture mapping:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public double[] map(double x, double y, double x0, double y0, double x1, double y1, double x2, double y2, double x3, double y3) {&#xA;    double A = (x0 - x) * (y0 - y2) - (y0 - y) * (x0 - x2);&#xA;    double B = ((x0 - x) * (y1 - y3) - (y0 - y) * (x1 - x3) + (x1 - x) * (y0 - y2) - (y1 - y) * (x0 - x2)) / 2.0;&#xA;    double C = (x1 - x) * (y1 - y3) - (y1 - y) * (x1 - x3);&#xA;&#xA;    double det = A - 2.0 * B + C;&#xA;&#xA;    double u;&#xA;    if (det == 0.0) {&#xA;        u = A / (A - C);&#xA;        if (Double.isNaN(u) || u &amp;lt; 0.0 || u &amp;gt; 1.0)&#xA;            return null;&#xA;    } else {&#xA;        double u1 = ((A - B) + Math.sqrt(B * B - A * C)) / det;&#xA;        boolean u1valid = !Double.isNaN(u1) &amp;amp;&amp;amp; u1 &amp;gt;= 0.0 &amp;amp;&amp;amp; 1.0 &amp;gt;= u1;&#xA;&#xA;        double u2 = ((A - B) - Math.sqrt(B * B - A * C)) / det;&#xA;        boolean u2valid = !Double.isNaN(u2) &amp;amp;&amp;amp; u2 &amp;gt;= 0.0 &amp;amp;&amp;amp; 1.0 &amp;gt;= u2;&#xA;&#xA;        if (u1valid &amp;amp;&amp;amp; u2valid)&#xA;            u = u1 &amp;lt; u2 ? u2 : u1;&#xA;        else if (u1valid)&#xA;            u = u1;&#xA;        else if (u2valid)&#xA;            u = u2;&#xA;        else&#xA;            return null;&#xA;    }&#xA;&#xA;    double v1 = ((1.0 - u) * (x0 - x) + u * (x1 - x)) / ((1.0 - u) * (x0 - x2) + u * (x1 - x3));&#xA;    boolean v1valid = !Double.isNaN(v1) &amp;amp;&amp;amp; v1 &amp;gt;= 0.0 &amp;amp;&amp;amp; 1.0 &amp;gt;= v1;&#xA;&#xA;    double v2 = ((1.0 - u) * (y0 - y) + u * (y1 - y)) / ((1.0 - u) * (y0 - y2) + u * (y1 - y3));&#xA;    boolean v2valid = !Double.isNaN(v2) &amp;amp;&amp;amp; v2 &amp;gt;= 0.0 &amp;amp;&amp;amp; 1.0 &amp;gt;= v2;&#xA;&#xA;    double v;&#xA;    if (v1valid &amp;amp;&amp;amp; v2valid)&#xA;        v = v1 &amp;lt; v2 ? v2 : v1;&#xA;    else if (v1valid)&#xA;        v = v1;&#xA;    else if (v2valid)&#xA;        v = v2;&#xA;    else&#xA;        return null;&#xA;&#xA;    return new double[] { u, v };&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and here is my quad drawing code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public void renderFace(Screen screen, int x0, int y0, int z0, int x1, int y1, int z1, int x2, int y2, int z2, int x3, int y3, int z3) {&#xA;    boolean render = true;&#xA;&#xA;    double[] p0 = screen.project(x0, y0, z0);&#xA;    int px0 = (int) p0[0], py0 = (int) p0[1];&#xA;    render |= p0[2] &amp;gt;= ZCLIP &amp;amp;&amp;amp; px0 &amp;gt;= 0 &amp;amp;&amp;amp; px0 &amp;lt; screen.width &amp;amp;&amp;amp; py0 &amp;gt;= 0 &amp;amp;&amp;amp; py0 &amp;lt; screen.height;&#xA;&#xA;    double[] p1 = screen.project(x1, y1, z1);&#xA;    int px1 = (int) p1[0], py1 = (int) p1[1];&#xA;    render |= p1[2] &amp;gt;= ZCLIP &amp;amp;&amp;amp; px1 &amp;gt;= 0 &amp;amp;&amp;amp; px1 &amp;lt; screen.width &amp;amp;&amp;amp; py1 &amp;gt;= 0 &amp;amp;&amp;amp; py1 &amp;lt; screen.height;&#xA;&#xA;    double[] p2 = screen.project(x2, y2, z2);&#xA;    int px2 = (int) p2[0], py2 = (int) p2[1];&#xA;    render |= p2[2] &amp;gt;= ZCLIP &amp;amp;&amp;amp; px2 &amp;gt;= 0 &amp;amp;&amp;amp; px2 &amp;lt; screen.width &amp;amp;&amp;amp; py2 &amp;gt;= 0 &amp;amp;&amp;amp; py2 &amp;lt; screen.height;&#xA;&#xA;    double[] p3 = screen.project(x3, y3, z3);&#xA;    int px3 = (int) p3[0], py3 = (int) p3[1];&#xA;    render |= p3[2] &amp;gt;= ZCLIP &amp;amp;&amp;amp; px3 &amp;gt;= 0 &amp;amp;&amp;amp; px3 &amp;lt; screen.width &amp;amp;&amp;amp; py3 &amp;gt;= 0 &amp;amp;&amp;amp; py3 &amp;lt; screen.height;&#xA;&#xA;    if (!render)&#xA;        return;&#xA;&#xA;    int minX = Math.min(Math.min(px0, px1), Math.min(px2, px3));&#xA;    if (minX &amp;lt; 0)&#xA;        minX = 0;&#xA;    if (minX &amp;gt; screen.width)&#xA;        minX = screen.width;&#xA;&#xA;    int minY = Math.min(Math.min(py0, py1), Math.min(py2, py3));&#xA;    if (minY &amp;lt; 0)&#xA;        minY = 0;&#xA;    if (minY &amp;gt; screen.height)&#xA;        minY = screen.height;&#xA;&#xA;    int maxX = Math.max(Math.max(px0, px1), Math.max(px2, px3));&#xA;    if (maxX &amp;lt; 0)&#xA;        maxX = 0;&#xA;    if (maxX &amp;gt; screen.width)&#xA;        maxX = screen.width;&#xA;&#xA;    int maxY = Math.max(Math.max(py0, py1), Math.max(py2, py3));&#xA;    if (maxY &amp;lt; 0)&#xA;        maxY = 0;&#xA;    if (maxY &amp;gt; screen.height)&#xA;        maxY = screen.height;&#xA;&#xA;    if (minX == maxX || minY == maxY)&#xA;        return;&#xA;&#xA;    for (int py = minY; py &amp;lt; maxY; ++py)&#xA;        for (int px = minX; px &amp;lt; maxX; ++px) {&#xA;            double[] uv = screen.map(px + 0.5, py + 0.5, px0, py0, px1, py1, px2, py2, px3, py3);&#xA;            if (uv == null)&#xA;                continue;&#xA;            double u = uv[0], v = uv[1];&#xA;&#xA;            double pz = (1 - u) * ((1 - v) * p0[2] + v * p2[2]) + u * ((1 - v) * p1[2] + v * p3[2]);&#xA;            if (pz &amp;lt; ZCLIP)&#xA;                continue;&#xA;&#xA;            int texX = 15 - Math.min(15, (int) (16 * u));&#xA;            int texY = 15 - Math.min(15, (int) (16 * v));&#xA;            screen.setPixel(px, py, pz, Art.WALLS.getPixel(texX, texY) * BRICKS);&#xA;        }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can anyone point out what I am doing wrong? I am not very experienced as this is my first try at implementing a game engine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for any insight.&lt;/p&gt;&#xA;" OwnerUserId="3330" LastActivityDate="2016-05-10T15:35:53.373" Title="Perspective-Correct Texture Mapping" Tags="&lt;texture&gt;&lt;perspective&gt;&lt;uv-mapping&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="2423" PostTypeId="2" ParentId="2405" CreationDate="2016-05-10T18:27:56.747" Score="5" Body="&lt;p&gt;In film production, we almost never use 8-bit textures for color/albedo, because of banding, etc. (JPEG is especially problematic since by spec, it's sRGB rather than linear values.) We either use 'half' (16 bit float) or 16-bit unsigned integer values for color/albedo textures.&lt;/p&gt;&#xA;" OwnerUserId="1781" LastActivityDate="2016-05-10T18:27:56.747" CommentCount="3" />
  <row Id="2424" PostTypeId="1" CreationDate="2016-05-11T06:00:33.290" Score="6" ViewCount="163" Body="&lt;p&gt;I'm currently reading some books on radiometry. They mention that radiance is constant along a ray. It doesn't change with distance.&#xA;However, I've seen some raytracer and they put the 1/r² factor when they deal with point sources. I don't get why. I didn't find a great explanation on the Internet. &lt;/p&gt;&#xA;" OwnerUserId="3339" LastActivityDate="2016-05-25T18:45:30.017" Title="Why does the 1/r² term appear with point sources?" Tags="&lt;raytracing&gt;&lt;color&gt;&lt;brdf&gt;&lt;physics&gt;" AnswerCount="5" CommentCount="2" />
  <row Id="2425" PostTypeId="2" ParentId="2424" CreationDate="2016-05-11T09:26:05.633" Score="5" Body="&lt;p&gt;It is the &lt;a href=&quot;http://hyperphysics.phy-astr.gsu.edu/hbase/vision/isql.html&quot;&gt;inverse square law of light&lt;/a&gt; for a pure point light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$E = \frac{I}{r^2}$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where E is illuminance and I is pointance or power/flux per unit solid angle.&lt;/p&gt;&#xA;" OwnerUserId="3331" LastActivityDate="2016-05-11T09:26:05.633" CommentCount="1" />
  <row Id="2426" PostTypeId="1" AcceptedAnswerId="2440" CreationDate="2016-05-11T09:29:43.657" Score="5" ViewCount="72" Body="&lt;p&gt;The slicing algorithm is done on the STL model and it is ensured that the result is a series of simple polygons that do not intersect each other. In other words all of those polygons generate closed loops. I need to identify which of those loops are outer and inner loops. There have been some references suggesting tree depth can be used to identify external and internal loops, without any outline of the algorithm. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be great if anyone can point to more detailed references where the tree depth can be utilized to filter out inner and outer contour lines.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following images will provide more information:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/so8C2Gl.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.imgur.com/so8C2Gl.jpg&quot; alt=&quot;Partition areas&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.imgur.com/rM661UP.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.imgur.com/rM661UP.jpg&quot; alt=&quot;Depth Tree Structure&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to the article that I am following - a, b, e, f, and j are outer loops, while the rest are inner loops. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the current situation, I am identifying the inner and outer loops while doing the scanline over each layer after the slicing operation. But I want to identify the inner and outer loops ahead of the scanline operation, so that I can create inner/outer offset for the identified loops and then run the scanline to generate the fill pattern.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The article that I am following is titled - &quot;Equidistant path generation for improving scanning efficiency in layered manufacturing&quot;. I googled up and I am afraid that it is not public. I downloaded the article through the university database.&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="231" LastEditDate="2016-05-13T00:59:24.057" LastActivityDate="2016-05-13T12:11:09.990" Title="Identify different contour cycle after model slicing" Tags="&lt;computational-geometry&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="2427" PostTypeId="1" AcceptedAnswerId="2428" CreationDate="2016-05-11T12:14:42.920" Score="0" ViewCount="29" Body="&lt;p&gt;Usually GearVR + SamsungPhone are used both for rendering and displaying but I would like to use it only for displaying when it is plugged to PC. The reason is to get performance similar to Oculus Rift or HTC Vive and not spend money for it. Is it possible?&lt;/p&gt;&#xA;" OwnerUserId="3341" LastActivityDate="2016-05-13T07:10:30.100" Title="Using GPU in PC for GearVR" Tags="&lt;virtual-reality&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2428" PostTypeId="2" ParentId="2427" CreationDate="2016-05-11T13:48:05.083" Score="1" Body="&lt;p&gt;Since you're asking in this Stack Exchange, I assume you intend to build a VR app yourself that utilizes this scenario.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While it is perfectly possible to utilize PC for rendering and stream the output to a mobile VR headset, it will come at the relatively high cost of increased latency (see e.g. &lt;a href=&quot;http://arstechnica.com/gaming/2013/01/how-fast-does-virtual-reality-have-to-be-to-look-like-actual-reality/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; for why this is important; the tl;dr is that high latency → nausea). This is because wireless networks have intrinsically higher latency than wired ones, and you need to do the following in a loop:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Collect head tracking (gyroscope/accelerometer) data from the VR headset.&lt;/li&gt;&#xA;&lt;li&gt;Send the head tracking data over the network to the PC.&lt;/li&gt;&#xA;&lt;li&gt;Render a frame on the PC.&lt;/li&gt;&#xA;&lt;li&gt;Stream the frame from the PC to the VR headset. This involves the following substeps:&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Encode video and audio on the PC.&lt;/li&gt;&#xA;&lt;li&gt;Transmit the encoded media frame to the VR headset over the network.&lt;/li&gt;&#xA;&lt;li&gt;Decode video and audio on the VR headset.&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;As I mentioned previously, steps 2 and 4 introduce additional latency (probably from several to a dozen milliseconds, from stream encoding/decoding and network transmission), which – depending on your specific application and the individual tolerance of the user – may or may not be a deal breaker.&lt;/p&gt;&#xA;" OwnerUserId="2817" LastEditorUserId="2817" LastEditDate="2016-05-13T07:10:30.100" LastActivityDate="2016-05-13T07:10:30.100" CommentCount="0" />
  <row Id="2429" PostTypeId="2" ParentId="2424" CreationDate="2016-05-11T17:49:05.713" Score="7" Body="&lt;p&gt;The concept of a point source is an approximation. Physically, light sources are extended objects and emit light from every point on their surface; but when you're far enough away (i.e. the distance to the source is large compared to its size) it's useful to approximate it as a point source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can get the $1/r^2$ law out of it as follows. Imagine a spherical area light with some radius $r_\text{light}$, and you're looking at it from a distance $r$ away. Then, we can approximate the solid angle that it subtends from your point of view as the area of a circle of radius $r_\text{light}/r$ (just using similar triangles). This area will be $\pi (r_\text{light}/r)^2$, so it's proportional to $1/r^2$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that this approximation becomes exact in the limit $r_\text{light}/r \to 0$, i.e. when the light source is very far away or very small. It breaks down if the source is too large or too close.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the source emits a constant radiance from every point on its surface, then when you integrate over solid angle in the rendering equation, you'll get a total irradiance proportional to $1/r^2$. In order to approximate it as a point source in a renderer, we skip the integration and just add an irradiance proportional to $1/r^2$ directly.&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-05-11T18:04:08.223" LastActivityDate="2016-05-11T18:04:08.223" CommentCount="0" />
  <row Id="2430" PostTypeId="2" ParentId="2416" CreationDate="2016-05-12T07:43:31.303" Score="6" Body="&lt;p&gt;A quickly formulated method, read first one that popped in my brain (not best), could be. Find the closest points on a parametric spiral for each sample (read &lt;a href=&quot;http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf&quot; rel=&quot;nofollow&quot;&gt;A Pixel Is Not A Little Square&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;). Then place the samples on a line by placing the pints in one axis by how far they are from your spiral line and the other by what the closest point is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/m8TYh.png&quot; alt=&quot;spiral[1&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can then use something like poisson disk sampling to reconstruct the image (maybe the didsk should be longer ovals the further away they are form the center line and less long as you progress further into the spiral).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now that I think of the problem a bit more. A simpler approach could be to map the image on mesh, using UV mapping, then unwrap the mesh. A triangular mesh might exhibit some artifacts so using a B-spline interpolation could be better. Several image mesh algorithms exist and you even have these in Photoshop.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;http://i.stack.imgur.com/Qc5qt.png&quot; alt=&quot;unwound spiral&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: A proof of concept of unwound spiral with the mesh method. Note i scaled the unwound spirals length to make it fit the site requirements better.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From initial test in my 3D app of choice seems that a dense enough triangular mesh works fine.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-05-12T14:40:56.900" LastActivityDate="2016-05-12T14:40:56.900" CommentCount="1" />
  <row Id="2431" PostTypeId="1" AcceptedAnswerId="2436" CreationDate="2016-05-12T09:58:02.670" Score="4" ViewCount="89" Body="&lt;p&gt;I am trying to implement my own path tracer but before arriving to the question I want to give you a short overview:&lt;br&gt;&#xA;In the implementation of the rendering equation I use some particular technique in order to sample surfaces. For example: when one of my rays hits a diffuse surface, the next ray bouncing from that surface will be calculated using a &lt;em&gt;Cosine-weighted Random Direction&lt;/em&gt;. This implies that in my rendering equation I have to take into account the PDF that this specific random distribution implies and specifically divide for this PDF in my equation. So far everything is okey.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, my question. I want to implement a particular technique called &quot;next event estimation&quot; which simply samples the lights. In order to do so, I want to pick a &lt;strong&gt;random point&lt;/strong&gt; on my light, which is spherical, by using the following code (C++):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vec3&amp;lt;float&amp;gt; randomPoint() const&#xA;{&#xA;    float x;&#xA;    float y;&#xA;    float z;&#xA;&#xA;        // random vector in unit sphere&#xA;    do&#xA;    {&#xA;        x = ((float)rand() / (RAND_MAX)) * 2 - 1;           // random coordinates between -1 and 1&#xA;        y = ((float)rand() / (RAND_MAX)) * 2 - 1;&#xA;        z = ((float)rand() / (RAND_MAX)) * 2 - 1;&#xA;    } while (pow(x, 2) + pow(y, 2) + pow(z, 2) &amp;gt; 1);        // simple rejection sampling&#xA;&#xA;    return centerOfSphere + Vec3&amp;lt;float&amp;gt;(x, y, z) * radius;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As far as I understand (please correct me if wrong), this implements a &lt;strong&gt;uniformly random sampling&lt;/strong&gt;. Also, as far as I have read, this kind of sampling has a PDF = 1 / (b - a) . My question: do I have to use this PDF as well like I do for my &lt;em&gt;Cosine-weighted Random Distribution&lt;/em&gt; or not, since it is a uniform distribution? if yes, what's the range (b - a) that PDF talks about? Thanks in advance!&lt;/p&gt;&#xA;" OwnerUserId="3069" LastActivityDate="2016-05-13T02:36:03.227" Title="Role of PDF of Uniform Random Sampling in a path tracer" Tags="&lt;rendering&gt;&lt;c++&gt;&lt;pathtracing&gt;&lt;distribution&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2432" PostTypeId="2" ParentId="2424" CreationDate="2016-05-12T11:33:27.583" Score="2" Body="&lt;p&gt;Say the point light is at $P_L$, the shading is happening at $P_S$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It's true that the radiance is constant along a shadow ray $P_L \rightarrow P_S$, but that's not the key property for solving the rendering equation at $P_S$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The rendering equation, somewhat simplified, is: $L_o(\omega_o) = L_e(\omega_o) + \int_{\Omega} \, f_r(\omega_i, \omega_o)\, L_i(\omega_i)\, (n \cdot \omega_i) \, d \omega_i $&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While $L_o$ is expressed in radiance, you are actually integrating the &lt;em&gt;irradiance&lt;/em&gt; $L_i$ of the incoming light at $P_S$, which is expressed in $Wm^{-2}$. The incoming &lt;em&gt;radiance&lt;/em&gt; -- call it $\hat{L}_i$ -- is in $W m^{-2} sr^{-1}$ and, while constant along the shadow ray, it is not directly relevant. The difference between $\hat{L}_i$ and $L_i$ is a $1/r^2$ term since the area of 1 $sr$ increases quadratically with distance from $P_L$ .&lt;/p&gt;&#xA;" OwnerUserId="3075" LastEditorUserId="3075" LastEditDate="2016-05-16T21:29:20.397" LastActivityDate="2016-05-16T21:29:20.397" CommentCount="0" />
  <row Id="2433" PostTypeId="2" ParentId="2419" CreationDate="2016-05-12T18:36:13.147" Score="5" Body="&lt;p&gt;Polygon offsetting is certainly possible to do yourself. It is not nearly as involved as Bezier or B-spline offsetting is. First you want to decide how to deal with:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Areas with no info, in essence how to deal with the joins. Do you just extend the line to the common intersection (miter join) or do you just connect the endpoints (bevel). You could also use round, parabolic or other join styles if you wish. Options here are unlimited. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the purposes of this discussion I will assume you will want a miter or bevel join.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How do you want to deal with sidedness. Is an open curve's offset closed or just on the other side. Other than that the convention is usually in 2D that outside is on the clockwise side of the path&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;So lets look at what happens when 2 straight lines join and they are offset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/jagc9.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jagc9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Situation in 2D.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a beveled corner join vector, from any point to its offset is magnitude of $a$. Take the direction of the segment rotated counterclockwise 90 degrees then multiply that with the length of offset. Or in the case of the picture in mathematical notation&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ &#xA; A' = A + a\bigg(\big|B-A\big|   \begin{bmatrix}&#xA;    0 &amp;amp; 1\\&#xA;    -1 &amp;amp; 0 &#xA;  \end{bmatrix}\bigg)&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $a$ is the amount of offset. This assumes lines go from $A$ to $B$ to $C$. Connecting segments must be processed on both sides separately. The same formula can be used for all the points. Then just connect $B'^1$ to $B'^2$ with a line. In case you are matrix illiterate, the matrix says swap x and y coordinate and invert the x value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a miter join the following applies:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/r6eG1.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/r6eG1.png&quot; alt=&quot;sss&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Situation of miter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ &#xA; B^{B-B'^{1}} = a\bigg(\big|B-A\big| \begin{bmatrix}&#xA;    0 &amp;amp; 1\\&#xA;    -1 &amp;amp; 0 &#xA;  \end{bmatrix}\bigg)&#xA;$$&#xA;$$&#xA;B' = B+\dfrac{ B^{B-B'^{1}}}{B^{B-B'^{1}}  \cdot (|B-A|-|C-B|)}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The above can be heavily simplified but I'll leave that to the reader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the reason to use a library is that when the curves start to self intersect it gets a bit tedious to manage this. So just find a library that does this for you and you'll save a lot of time.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="231" LastEditDate="2016-05-14T11:16:23.827" LastActivityDate="2016-05-14T11:16:23.827" CommentCount="0" />
  <row Id="2434" PostTypeId="1" AcceptedAnswerId="2438" CreationDate="2016-05-12T21:13:17.540" Score="3" ViewCount="23" Body="&lt;p&gt;I am making a 2d game in opengl es 2.0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Inside are tons of rectangles defined by 4 points and one 4 component color. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using vertex buffer objects, and I have heard that it is efficent to interlace the data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So like traditionally you would do&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Corner.x&#xA;Corner.y&#xA;Corner.z&#xA;Corner.rgba&#xA;(repeat for each corner) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However in my situations two assumptions can be made that can probably make things faster&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. All the rectangles z values are 0&#xA;2. All corners of the rectangle have the same color. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it possible, and what would it look like to have a buffer object structured like this. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Corner.xy&#xA;Corner.xy&#xA;Corner.xy&#xA;Corner.xy&#xA;Color.rgba? &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it even possible to have opengl assume that the Z is always 0? Is it possible to reuse the color like to hat? &lt;/p&gt;&#xA;" OwnerUserId="2308" LastActivityDate="2016-05-13T09:27:56.793" Title="Interlacing vertex buffer data with extra efficiency" Tags="&lt;opengl-es&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2436" PostTypeId="2" ParentId="2431" CreationDate="2016-05-12T21:31:21.500" Score="5" Body="&lt;p&gt;Firstly, as @trichoplax correctly pointed out, your randomPoint function calculates a point in a cube, then uses rejection sampling to return &lt;strong&gt;&lt;em&gt;all&lt;/em&gt;&lt;/strong&gt; points that are &lt;strong&gt;&lt;em&gt;inside&lt;/em&gt;&lt;/strong&gt; a unit sphere. In order to return points on a sphere, you would need to change the greater than to an equals. That said, rejection sampling is very inefficient. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;A better way to sample a sphere, is to sample in spherical space, then transform to cartesian. ie:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float phi = PI * randf();&#xA;float theta = 2 * PI * randf();    &#xA;&#xA;float x = radius * std::sinf(phi) * std::sinf(theta);&#xA;float y = radius * std::cosf(phi);&#xA;float z = radius * std::sinf(phi) * std::cosf(theta);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, this is not uniform, and will cause samples to clump at the poles. To prevent this, we transform phi:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float phi = std::acosf(2.0f * randf() - 1.0f);&#xA;float theta = 2 * PI * randf();    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We can use some trig identities to make it a bit more efficient to calculate with the computer:&#xA;$$\begin{align*}&#xA;\sin(\cos^{-1}(x)) \equiv&amp;amp; \ \sqrt{1 - x^2}\\&#xA;\cos(\cos^{-1}(x)) \equiv&amp;amp; \ x\\&#xA;\\&#xA;\phi =&amp;amp; \ \cos^{-1}(2\xi - 1)\\&#xA;\sin(\phi) =&amp;amp; \ sin(\cos^{-1}(2 \xi - 1))\\&#xA;=&amp;amp; \ \sqrt{1 - (2 \xi - 1)^2}\\&#xA;\cos(\phi) =&amp;amp; \ 2 \xi - 1&#xA;\end{align*}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the full change is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float cosPhi = 2.0f * randf() - 1.0f;&#xA;float sinPhi = std::sqrt(1.0f - cosPhi * cosPhi);&#xA;float theta = 2 * PI * randf();    &#xA;&#xA;float x = radius * sinPhi * std::sinf(theta);&#xA;float y = radius * cosPhi;&#xA;float z = radius * sinPhi * std::cosf(theta);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in the case of next event estimation, uniform sampling the whole sphere is inefficient, because a ray can only 'see' half of the sphere at a time. So if we generate a point on the 'back' of the sphere, the sphere will occlude the point, and your calculation will be wasted.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead, you generate samples in a cone, which covers the great circle of the sphere, as viewed from the starting point of the ray.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Sample sphere uniformly inside subtended cone&#xA;float rand1 = randf();&#xA;float rand2 = randf();&#xA;&#xA;// Compute theta and phi values for sample in cone&#xA;float distanceSquared = DistanceSquared(rayOrigin, sphereCenter);&#xA;&#xA;// We use geometry to calculate the angle of the cone (aka, the maximum phi can be when we sample)&#xA;// It's easier / cheaper to use geometry to calculate sin/cos phi directly, rather than generating phi and using sin/cos&#xA;float sinPhiMaxSquared = radius * radius / distanceSquared;&#xA;float cosPhiMax = std::sqrt(1.0f - sinPhiMaxSquared);&#xA;float cosPhi = (1.0f - rand1) + rand1 * cosPhiMax;&#xA;float sinPhi = std::sqrt(1.0f - cosPhi * cosPhi);&#xA;&#xA;// Phi can be anything in 2 PI&#xA;float theta = 2 * PI * rand2;&#xA;&#xA;float x = radius * sinPhi * std::sinf(theta);&#xA;float y = radius * cosPhi;&#xA;float z = radius * sinPhi * std::cosf(theta);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;NOTE: x, y, z are in LOCAL coordinate space.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So we need to transform to world coordinates:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3 zAxis = normalize(sphereCenter - rayOrigin);&#xA;float3 xAxis;&#xA;if (std::abs(zAxis.x) &amp;gt; std::abs(zAxis.y))&#xA;    xAxis = float3(-zAxis.z, 0.0f, zAxis.x) / std::sqrt(zAxis.x * zAxis.x + zAxis.z * zAxis.z);&#xA;else&#xA;    xAxis = float3(0.0f, zAxis.z, -zAxis.y) / std::sqrt(zAxis.y * zAxis.y + zAxis.z * zAxis.z);&#xA;yAxis = cross(zAxis, xAxis);&#xA;&#xA;&#xA;float3 samplePoint = x * xAxis + y * yAxis + z * zAxis;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the pdf is calculated as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float pdf = 1 / (2 * PI * (1 - cosPhiMax));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code here is heavily influenced / copied from &lt;a href=&quot;https://github.com/mmp/pbrt-v3/blob/master/src/shapes/sphere.cpp&quot; rel=&quot;nofollow&quot;&gt;PBRT v3&lt;/a&gt;. They have a series of classes and functions for sampling from shapes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, the pdf. In monte-carlo integration, we need to combine the pdf's of each integration we do. In path tracing, we can integrate over many many things. For example, the general rendering equation integrates the incoming light over the hemisphere, depth of field can be treated as an integration over a focal distance, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For next event estimation, you explicitly split the rendering equation into two integrands, direct lighting, and indirect lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Standard rendering equation:&#xA;$$ L_{\text{o}}(p, \omega_{\text{o}}) =  L_{e}(p, \omega_{\text{o}}) \ + \  \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Next Event Estimation:&#xA;$$ L_{\text{o}}(p, \omega_{\text{o}}) =  L_{e}(p, \omega_{\text{o}}) \ + \ \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, direct}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}}\ \ + \ \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, indirect}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}} $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In simple naive forward path tracing, everything is is treated as indirect light. In next event estimation, we directly calculate the direct lighting and add it to the indirect lighting. And if we hit a light, we ignore the contribution, since we're calculating the direct lighting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since we have two integrations, each will have its own pdf. Aka:&#xA;$$L_{\text{o}}(p, \omega_{\text{o}}) =  L_{e}(p, \omega_{\text{o}}) \ \ + \ \sum_{k=0}^{\infty } \frac{f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, direct}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | }{pdf_{direct}}\ \ + \ \sum_{k=0}^{\infty } \frac{f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, indirect}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | }{pdf_{indirect}}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want to see how this is implemented, you can check out my implementation &lt;a href=&quot;https://github.com/RichieSams/lantern/blob/master/source/renderer/renderer.cpp#L92&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. Note: my light sampling is a bit more complicated, since it does multiple importance sampling. But it can be as simple as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3 Renderer::EstimateDirect(Light *light, UniformSampler *sampler, float3a &amp;amp;surfacePos, float3a &amp;amp;surfaceNormal, float3a &amp;amp;wo, Material *material) const {&#xA;    float pdf;&#xA;    float3 wi;&#xA;&#xA;    // Sample a point on the light and get the pdf&#xA;    float3 Li = light-&amp;gt;SampleLi(sampler, m_scene, surfacePos, surfaceNormal, &amp;amp;wi, &amp;amp;pdf);&#xA;&#xA;    // Calculate the brdf value&#xA;    float f = material-&amp;gt;Eval(wi, wo, surfaceNormal);&#xA;    return f * Li * / pdf;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-05-13T02:36:03.227" LastActivityDate="2016-05-13T02:36:03.227" CommentCount="8" />
  <row Id="2438" PostTypeId="2" ParentId="2434" CreationDate="2016-05-13T09:27:56.793" Score="4" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;Is it even possible to have opengl assume that the Z is always 0?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Yes it is. Just set the component count to 2 in &lt;code&gt;glVertexAttribPointer&lt;/code&gt; and the other 2 components (&lt;code&gt;z&lt;/code&gt; and &lt;code&gt;w&lt;/code&gt;) will be auto filled with 0 and 1 resp.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Is it possible to reuse the color like to hat? &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;No it is not. Opengl (and most other graphics apis) require that each vertex is referenced by only a single index.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-05-13T09:27:56.793" CommentCount="4" />
  <row Id="2439" PostTypeId="2" ParentId="2409" CreationDate="2016-05-13T11:34:19.540" Score="4" Body="&lt;p&gt;With help from the Nvidia Forum and two very helpful and friendly Nvidia employees I found the perfect solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In OptiX there is a so called selector node. This is a special scene-graph node that utilizes a user specified program to select which child a ray should traverse and which not.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;optix.h&amp;gt;&#xA;&#xA;rtDeclareVariable(unsigned int, timestep, , );&#xA;&#xA;RT_PROGRAM void animation_selector()&#xA;{&#xA;    rtIntersectChild(timestep);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is a simple version of such a selection program. From the CPP-API side the current timestep will be set and can be used to make the animation. And then when the scene graph is traversed the program just says &quot;Traverse only the child with index 'timestep'&quot;. This does not trigger a recompile or refit and therefore is nearly instant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The selector node is simply created in following way:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;optix::Selector selector = context_-&amp;gt;createSelector();&#xA;&#xA;std::string path_to_ptx = VRTPtxFinder::GetPtxPath(&quot;VistaRayTracing_generated_animation_selector.cu.ptx&quot;);&#xA;&#xA;optix::Program selection_program = context_-&amp;gt;createProgramFromPTXFile(path_to_ptx, &quot;animation_selector&quot;);&#xA;selector[&quot;timestep&quot;]-&amp;gt;setUint(0u);&#xA;selector-&amp;gt;setVisitProgram(selection_program);&#xA;&#xA;for(auto group : groups)&#xA;{&#xA;  selector-&amp;gt;addChild(group);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The only cost is one additional method call for each traversal and thereby also a slightly bigger stack which caused me some problems with a stack overflow so when using this, keep an eye on the stack size.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-05-13T11:34:19.540" CommentCount="0" />
  <row Id="2440" PostTypeId="2" ParentId="2426" CreationDate="2016-05-13T11:57:41.403" Score="3" Body="&lt;p&gt;Take a random vertex on your loop, shoot a ray trough all the other loops. For loops you crossed a odd number of times your inside for loops you crossed a even number of times your outside.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The loops that are outside of all others is in the root of your tree. Then find loops that are inside that, but not inside some other. Then find next level and next level and so on. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/eY5kH.gif&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/eY5kH.gif&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Animation of arranging in tree by shooting rays&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can optimize this by using bounding boxes/ bsp trees for the test areas. But for a small number of areas that might be overkill. just sorting them left to right top to bottom should suffice. You can use the same algorithms as your scan line operators.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/FGFVR.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/FGFVR.png&quot; alt=&quot;offset&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: And now you can offset the shape. But you need to recheck that there is no intersection left after this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the topology is somewhat stable across levels of slices so you can use that to your benefit. Just follow the 3D surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or you could do the offset in 3D on the polygons so you do not need this info. Also if you just need the winding for knowing which way to offset rule you can derive it from the 3d model itself, each vertex has a normal so project the normal for one triangle in each loop with your slicing algorithm per loop. That then gives you the direction of loops outside which is inside for nearly free. You wouldn't get the tree but then again you might not need it.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-05-13T12:11:09.990" LastActivityDate="2016-05-13T12:11:09.990" CommentCount="0" />
  <row Id="2441" PostTypeId="1" CreationDate="2016-05-13T15:40:05.353" Score="6" ViewCount="87" Body="&lt;p&gt;I have a scalar function defined over the vertices of a surface mesh. I want to compute an approximate (and generalized, I suppose) &quot;Gaussian blur/convolution&quot; of this function over the surface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can imagine for each vertex, taking an average of the function at vertices in it's (multi-hop) neighborhood weighted according to the Gaussian of their Euclidean distance (in R3) from the current vertex multiplied by the local area they represent. This approximation would be good if the surface isn't too curved at the scale of the Gaussian kernel. However it would still be computationally prohibitive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a more efficient convolving algorithm, perhaps based on some kind of iterated &quot;flow&quot; or partial exchange through edges of the mesh?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The mesh can have either triangular or quadrilateral faces - whichever is more convenient in answering this question. But the edge lengths are not constant, so local neighborhood geometry differs between vertices.&lt;/p&gt;&#xA;" OwnerUserId="3294" LastEditorUserId="3294" LastEditDate="2016-05-16T18:21:30.267" LastActivityDate="2016-05-16T18:21:30.267" Title="Gaussian blurring a function defined over a surface mesh" Tags="&lt;mesh&gt;&lt;gaussian-blur&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="2443" PostTypeId="2" ParentId="2391" CreationDate="2016-05-13T22:34:31.890" Score="0" Body="&lt;p&gt;While the product sampling methods provides better (perfect) distribution for rays I would say that using MIS (multiple importance sampling) is a method verified in production. Since shadowing information is unknown product sampling doesn't become perfect anyway and it is quite hard to implmenet. Shooting more rays might be worth more! Depends on your situation and ray budgets of course!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Short description of MIS: In essence you trace both a BSDF-ray (as you would anyway for doing indirect lighting) and an explicit ray towards the EM. MIS give you weights so that you can combine them in a way that removes a lot of the noise. MIS is especially good at choosing &quot;technique&quot; (implicit or explicit sampling) based on the situation that arises. This happens naturally without the user having to make hard choices based on roughness etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Chapter 9 of &lt;a href=&quot;http://graphics.stanford.edu/papers/veach_thesis/&quot; rel=&quot;nofollow&quot;&gt;http://graphics.stanford.edu/papers/veach_thesis/&lt;/a&gt; covers this in detail. Also see &lt;a href=&quot;https://www.shadertoy.com/view/4sSXWt&quot; rel=&quot;nofollow&quot;&gt;https://www.shadertoy.com/view/4sSXWt&lt;/a&gt; for a demo of MIS in action with area lights.&lt;/p&gt;&#xA;" OwnerUserId="286" LastActivityDate="2016-05-13T22:34:31.890" CommentCount="1" />
  <row Id="2444" PostTypeId="2" ParentId="2441" CreationDate="2016-05-14T12:59:19.210" Score="4" Body="&lt;h1&gt;A few different approaches&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I'll consider a few variations on your specific request, since you mention efficiency and I suspect your specific request may be the least efficient. I'll also suggest ways of improving the efficiency without varying from your intended approach, so you can weigh up the alternatives.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Blurring the volume instead of the surface&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If you want the distance metric to be 3D Euclidean distance instead of 2D Euclidean distance within the surface, then you could perform the blur on a regular 3D grid to which the scalar function you have in mind has been applied. Then you can use the final result of your Difference of Gaussians to calculate the scalar values at the vertices of your irregular mesh. This avoids having to take into account the mesh shape for the bulk of the calculation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The 3D grid is likely to have a much larger number of vertices than the 2D mesh, but they will all be equally spaced and the large kernel blur can be achieved by repeated application of a small kernel blur taking into account only the 6 nearest neighbours, which will always be at a constant distance away. This approach involves potentially more calculation, but the ease with which the regular grid could be GPU accelerated may appeal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This will give a different result from performing the blur on the mesh vertices using 3D Euclidean distance. For example, the 3D grid approach will be affected by distinctive regions of the 3D scalar function that are near but not on the mesh. This may be desirable or not depending on your specific purpose.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Using 2D distance instead of 3D distance&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If you find that you need the distance metric to be 2D Euclidean within the surface, then you can get a good approximation to a larger kernel Gaussian blur by repeatedly applying a smaller kernel Gaussian blur. If there is not too much variation in the edge lengths within your mesh you may be able to choose a kernel size which allows for only including vertices one edge away at each iteration. This allows for only using single edge lengths to calculate the size of the contribution of a vertex, rather than calculating a 2D multi-edge distance.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;3D distance using the surface without the volume&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If you need the calculation to be precisely as described in your question, being calculated within the mesh rather than within the surrounding volume, but also using the 3D Euclidean distance, then using nearest neighbours and several iterations will not work. Unless the mesh is near to flat, the repeated application of a nearest neighbour blur will result in an approximation to the 2D Euclidean distance case, since the values will only be able to bleed from vertex to vertex, not directly along the shortest path as they would in a single pass. This will give less spread than would be achieved by a single pass that calculates the 3D distance to a vertex 10 edges away. (I have used 10 edges since you mention a 10-hop in your comment on the question.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Implementing the blur in a single pass will mean calculating the 3D Euclidean distance between every vertex and every other vertex within a 10 edge radius. This will be expensive, but perfectly possible. Since you mention efficiency, consider that there are some redundancies you can eliminate provided you have sufficient available memory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The two blurs that you produce prior to taking the Difference of Gaussians will use the same set of 3D distances up to the edge radius of the smaller kernel blur. If you can save these then you only need to calculate them once, rather than once per blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, each distance will be used twice per blur - once in each direction as the length from vertex A to vertex B is the same as the length from B to A. Caching/memoising these distances will avoid calculating them twice.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Effects from arbitrarily many edges away&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If the surface curves such that some vertices which are many edges away are still near enough to affect each other in 3D distance, then rather than considering vertices within a certain number of edges away, you may need to consider all vertices within a certain 3D radius, regardless of how long the path via edges. In this case you can consider fewer vertices by using space partitioning, choosing a specific method which suits the mesh.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you don't want parts of the surface which approach each other to influence each other, then you probably want a 2D distance metric rather than the 3D one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have a wide range of different edge lengths then you may find the same problem of not being able to define a set number of edges to traverse, even if the mesh is fairly flat. Again you may need to define a 3D distance instead of a number of edges, and consider all vertices that lie within that radius.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-14T13:09:26.750" LastActivityDate="2016-05-14T13:09:26.750" CommentCount="10" />
  <row Id="2445" PostTypeId="2" ParentId="2424" CreationDate="2016-05-14T14:27:20.423" Score="3" Body="&lt;p&gt;I'll give an intuitive idea of the reason in this answer. Once this intuitive idea is grasped, it can be easier to absorb the mathematical descriptions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Other people find it easier the other way around, so look at all the answers and see which approach works for you personally.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;A spherical shell of photons&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Imagine a point light source. Picture an instant where it emits a million photons spread evenly in all directions. At that instant, they are all in the same position, at the central point. A moment later, they have all moved the same distance and are now arranged in a small sphere with the point at its centre. A short time later they are still arranged in a sphere, but now a much larger sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the sphere expands it always has the same number of photons, but they are spread out over the increasing area. Each photon has the same amount of energy it had when it first left the point source, but the photons are more spread out so a given area of the sphere now has less energy due to having fewer photons.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When a photon hits a surface, it adds the same amount of energy whether it has traveled 1 metre or 100 metres. The reason the surface looks dimmer when it is further from the light source is that the photons are more spread out across that surface.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Source to eye ray tracing&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If you wrote a ray tracer that started with rays being emitted from a point light source, and then followed them to see what they hit, you wouldn't need the $1/r^2$ term. Objects further from the light would naturally be hit by fewer rays due to the rays spreading out.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Eye to source ray tracing&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Most ray tracers don't start the rays from the light source, as this results in calculating the paths of all the rays that never reach the eye, which is very inefficient. Instead the rays start at the eye and are traced backwards, to see what surface they came from. If the ray was then bounced from that surface in a random direction to see if it hits the point light source, the fact that the light source is a point would make the probability of hitting it zero. So instead $1/r^2$ is used to give a measure of how many rays hit the surface.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Geometry of a point source&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;This isn't a property of light, it is a property of a point source. Light traveling in all directions from a point forms spherical shells of photons, and the surface area of a sphere increases in proportion to the radius squared.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you had light being emitted that was not in all directions then the rule would be different. For example, imagine a line light source instead of a point, with all the light being emitted radially (only in directions perpendicular to the line). Now the light forms cylindrical shells of photons, and the surface area of a cylinder increases in proportion to the radius, not the radius squared. Now you would use a $1/r$ term instead of a $1/r^2$ term, and an object would need to be moved significantly further from the light source before seeing a noticeable drop in brightness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In reality, nearly every light source is equivalent to a collection of point sources - every point on an area light source emits light in all directions. Even cylindrical lights like flourescent strip lights and neon signs still emit light in all directions, so the photons form spherical shells rather than cylindrical ones. So the reduction in light level will nearly always be with $1/r^2$.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-14T14:40:01.353" LastActivityDate="2016-05-14T14:40:01.353" CommentCount="0" />
  <row Id="2446" PostTypeId="1" CreationDate="2016-05-15T04:39:01.593" Score="4" ViewCount="67" Body="&lt;p&gt;OK so I have a simple setup going on.  Basically I am just drawing some textures and shapes.  However sizing is going weird... let me show you how any maybe you can diagnose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically everything is broken into components that work fine by themselves, however when together it is disastrous.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Case #1 Section of actual texture appears with an enlarged resolution in the bottom left corner.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;glViewport(0, 0, GLsizei(width), GLsizei(height))&#xA;layer_shapes.begin()&#xA;glViewport(0, 0, GLsizei(width), GLsizei(height))&#xA;checkFades()&#xA;drawShapes()&#xA;layer_shapes.end()&#xA;drawTexture(layer_shapes, alpha: 0.0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BQqGr.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BQqGr.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;strong&gt;Case #2 Image appears enlarged and full screen&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;layer_shapes.begin()&#xA;checkFades()&#xA;drawShapes()&#xA;layer_shapes.end()&#xA;drawTexture(layer_shapes, alpha: 0.0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/jfR7w.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jfR7w.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Case #3 proving that texture drawing works by drawing to front buffer (I put a texture into a fbo and never drew to the fbo)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;drawTexture(layer_shapes, alpha: 0.0)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/YvdOz.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/YvdOz.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Case #4 Proving that shape drawing works (This is what it should look like, except with an accumulation effect)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;drawShapes()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/9hlqX.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9hlqX.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;(Please note discoloration is due to a debugging thing in the fragment shader, not an error)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;In summary&lt;/strong&gt; Cases 1 and 2 do not work as intended (they enlarge only part of the shape drawing and put it in the wrong proportion on the screen).  Cases 3 and 4 show the individual complnents (drawing shapes, and drawing textures) working when everything is drawn to the front buffer without any 'glViewport' calls.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The desired behavior is&lt;/strong&gt; that an image like case 4 would be drawn, however the image would be an accumulation of previous frames (Notice how i never clear the layer_shapes fbo) so the shapes (that are moving) would leave a trail.  The proportions are just being weird! &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And Incase you want to see it here is how I set up my texture fbo's.  They get initialized and then have .load() called on them&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class TextureBuffer {&#xA;    var framebuffer:GLuint = 0&#xA;    var tex:GLuint = 0&#xA;    var old_fbo:GLint = 0&#xA;    var w:GLsizei = 0&#xA;    var h:GLsizei = 0&#xA;&#xA;&#xA;    init(widthi: GLsizei = 0, heighti: GLsizei = 0)&#xA;    {&#xA;        w = widthi&#xA;        h = heighti&#xA;    }&#xA;&#xA;    func begin()&#xA;    {&#xA;        glGetIntegerv(GLenum(GL_FRAMEBUFFER_BINDING), &amp;amp;old_fbo)&#xA;        glBindFramebuffer(GLenum(GL_FRAMEBUFFER), framebuffer)&#xA;    }&#xA;&#xA;    func checkStatus()&#xA;    {&#xA;        let status = glCheckFramebufferStatus(GLenum(GL_FRAMEBUFFER))&#xA;        if (status != GLenum(GL_FRAMEBUFFER_COMPLETE))&#xA;        {&#xA;            print(&quot;DIDNT GO WELL WITH&quot;)&#xA;            print(status)&#xA;        }&#xA;    }&#xA;&#xA;    func end()&#xA;    {&#xA;        glBindFramebuffer(GLenum(GL_FRAMEBUFFER), GLenum(old_fbo))&#xA;    }&#xA;&#xA;    func loadTexture(name: String, type: String) -&amp;gt; GLuint&#xA;    {&#xA;        var it:GLKTextureInfo = GLKTextureInfo()&#xA;        let pic = UIImage(named: name + &quot;.&quot; + type)!.CGImage&#xA;        let path = NSBundle.mainBundle().pathForResource(name, ofType: type)&#xA;&#xA;        do&#xA;        {&#xA;            try it = GLKTextureLoader.textureWithContentsOfFile(path!, options: nil)&#xA;        }&#xA;        catch&#xA;        {&#xA;            do&#xA;            {&#xA;                //print(&quot;ERR Loading: &quot; + path!)&#xA;                try it = GLKTextureLoader.textureWithCGImage(pic!, options: nil)&#xA;            }&#xA;            catch&#xA;            {&#xA;                print(&quot;Error loading UIImage: &quot; +  pic.debugDescription)&#xA;                it = GLKTextureInfo()&#xA;            }&#xA;        }&#xA;        return it.name&#xA;    }&#xA;&#xA;    func load(width: CGFloat?, height: CGFloat?)&#xA;    {&#xA;        if let lwidth = width&#xA;        {&#xA;            w = GLsizei(lwidth)&#xA;        }&#xA;        if let lheight = height&#xA;        {&#xA;            h = GLsizei(lheight)&#xA;        }&#xA;        glBindTexture(GLenum(GL_TEXTURE_2D), 0)&#xA;        glGetIntegerv(GLenum(GL_FRAMEBUFFER_BINDING), &amp;amp;old_fbo)&#xA;&#xA;        glGenFramebuffers(1, &amp;amp;framebuffer)&#xA;        tex = loadTexture(&quot;Black&quot;, type: &quot;png&quot;)&#xA;&#xA;        glBindFramebuffer(GLenum(GL_FRAMEBUFFER), framebuffer)&#xA;&#xA;&#xA;        glBindTexture(GLenum(GL_TEXTURE_2D), tex)&#xA;        //------&amp;gt;&amp;gt;&amp;gt;&amp;gt;glTexImage2D(GLenum(GL_TEXTURE_2D), 0, GL_RGBA, GLsizei(w), GLsizei(h), 0, GLenum(GL_RGBA), GLenum(GL_UNSIGNED_BYTE), nil)&#xA;        glFramebufferTexture2D(GLenum(GL_FRAMEBUFFER), GLenum(GL_COLOR_ATTACHMENT0), GLenum(GL_TEXTURE_2D), tex, 0)&#xA;&#xA;&#xA;        let status = glCheckFramebufferStatus(GLenum(GL_FRAMEBUFFER))&#xA;        if (status != GLenum(GL_FRAMEBUFFER_COMPLETE))&#xA;        {&#xA;            print(&quot;DIDNT GO WELL WITH&quot;, width, &quot; &quot; , height)&#xA;            print(status)&#xA;        }&#xA;&#xA;        glBindFramebuffer(GLenum(GL_FRAMEBUFFER), GLenum(old_fbo))&#xA;    }&#xA;&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;A thought is there was a line near the end (I put an arrow pointing to it) that I had to remove to get the fbo to work properly.  Previously the code worked fine, however it is not working fine now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do have a suspicion that since I have removed that line that caused nothing to work that the texture that is generated by it is the wrong size since the texture it is loading is of a non-device size.  However I do not know why that call causes everything not to work.&lt;/p&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="2308" LastEditDate="2016-05-15T16:33:53.343" LastActivityDate="2016-05-30T08:37:34.313" Title="What is causing this odd scaling behavior?" Tags="&lt;opengl&gt;&lt;texture&gt;&lt;opengl-es&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="2447" PostTypeId="1" AcceptedAnswerId="2449" CreationDate="2016-05-15T17:06:14.197" Score="16" ViewCount="1360" Body="&lt;p&gt;I tried to implement Perlin Noise &lt;em&gt;by myself&lt;/em&gt; using just the theory (following flafla2.github.io/2014/08/09/perlinnoise.html). Unfortunately I was unable to achieve the look of the &quot;original&quot; Perlin Noise.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's the reason the code below renders a blocky version of Perlin Noise?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What should I improve/change in the code so that it renders Perlin Noise without the artifacts?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I suspect there might be problem either in the way I interpolate or in the &lt;code&gt;grads&lt;/code&gt; vector. The &lt;code&gt;grads&lt;/code&gt; vector contains dot products of (random vector for lattice point) and (the size vector) – for all 4 nearby lattice points. (The random and size vectors are described in the very first link.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GLSL Sandbox: &lt;a href=&quot;http://glslsandbox.com/e#32663.0&quot;&gt;http://glslsandbox.com/e#32663.0&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/14reHUI.png&quot;&gt;&lt;img src=&quot;http://i.imgur.com/14reHUI.png&quot; alt=&quot;Artifacts in the noise&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float fade(float t) { return t * t * t * (t * (t * 6. - 15.) + 10.); }&#xA;vec2 smooth(vec2 x) { return vec2(fade(x.x), fade(x.y)); }&#xA;&#xA;vec2 hash(vec2 co) {&#xA;    return fract (vec2(.5654654, -.65465) * dot (vec2(.654, 57.4), co));&#xA;}&#xA;&#xA;float perlinNoise(vec2 uv) {&#xA;    vec2 PT  = floor(uv);&#xA;    vec2 pt  = fract(uv);&#xA;    vec2 mmpt= smooth(pt);&#xA;&#xA;    vec4 grads = vec4(&#xA;        dot(hash(PT + vec2(.0, 1.)), pt-vec2(.0, 1.)),   dot(hash(PT + vec2(1., 1.)), pt-vec2(1., 1.)),&#xA;        dot(hash(PT + vec2(.0, .0)), pt-vec2(.0, .0)),   dot(hash(PT + vec2(1., .0)), pt-vec2(1., 0.))&#xA;    );&#xA;&#xA;    return 5.*mix (mix (grads.z, grads.w, mmpt.x), mix (grads.x, grads.y, mmpt.x), mmpt.y);&#xA;}&#xA;&#xA;float fbm(vec2 uv) {&#xA;    float finalNoise = 0.;&#xA;    finalNoise += .50000*perlinNoise(2.*uv);&#xA;    finalNoise += .25000*perlinNoise(4.*uv);&#xA;    finalNoise += .12500*perlinNoise(8.*uv);&#xA;    finalNoise += .06250*perlinNoise(16.*uv);&#xA;    finalNoise += .03125*perlinNoise(32.*uv);&#xA;&#xA;    return finalNoise;&#xA;}&#xA;&#xA;void main() {&#xA;    vec2 position = gl_FragCoord.xy / resolution.y;&#xA;    gl_FragColor = vec4( vec3( fbm(3.*position) ), 1.0 );&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3365" LastActivityDate="2016-05-16T21:52:56.627" Title="Why does my Perlin Noise look &quot;blocky&quot;?" Tags="&lt;glsl&gt;&lt;noise&gt;&lt;artifacts&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="5" />
  <row Id="2448" PostTypeId="1" CreationDate="2016-05-15T17:22:22.213" Score="0" ViewCount="27" Body="&lt;h1&gt;Background Information&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I am trying to make a software in which users submit their photographs, and their photographs are perspective projected on moving surfaces in a video. The videos are created by 3d animation artists using 3ds max and Maya, and are prerendered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to achieve this I am following the below steps:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Export FBX file from &lt;em&gt;3ds Max&lt;/em&gt; for the particular scene where the user image plane is visible, and everything else is removed from the scene except camera and lighting. And note down the frame numbers (Eg. between 517th and 705th frames).&lt;/li&gt;&#xA;&lt;li&gt;Load it with OpenGL and render the scene into alpha PNG files.&lt;/li&gt;&#xA;&lt;li&gt;Use ffmpeg to overlay rendered images on top of the video between 517th and 705th frames.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Below is the output video:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://drive.google.com/file/d/0BxIQVP1zErDPRFV4aWMtdjlMQXc/view?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;https://drive.google.com/file/d/0BxIQVP1zErDPRFV4aWMtdjlMQXc/view?usp=sharing&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And a still from the output video:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/U8abr.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/U8abr.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Question&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;There are two major problems in the video. One is near-clipping plane is too far from camera, which is not an issue since I can easily fix it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the other one is, if you watch video till the end, sync goes off.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/qbuRd.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qbuRd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;What might be causing this syncing problem? What are the ways I can debug this, and any other thoughts?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am providing the base video and the FBX in the links below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;FBX file:&#xA;&lt;a href=&quot;https://drive.google.com/file/d/0BxIQVP1zErDPRG5NaTVNTUh1MDQ/view?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;https://drive.google.com/file/d/0BxIQVP1zErDPRG5NaTVNTUh1MDQ/view?usp=sharing&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Base video:&#xA;&lt;a href=&quot;https://drive.google.com/file/d/0BxIQVP1zErDPdFVRaGxSeGJhNTQ/view?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;https://drive.google.com/file/d/0BxIQVP1zErDPdFVRaGxSeGJhNTQ/view?usp=sharing&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3364" LastActivityDate="2016-05-15T17:22:22.213" Title="OpenGL Rendering of FBX scene and overlaying on video" Tags="&lt;opengl&gt;&lt;video&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2449" PostTypeId="2" ParentId="2447" CreationDate="2016-05-15T18:38:14.730" Score="22" Body="&lt;p&gt;The interpolation looks fine. The main problem here is that the hash function you're using isn't very good. If I look at just one octave, and visualize the hash result by outputting &lt;code&gt;hash(PT).x&lt;/code&gt;, I get something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/LcFJV.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/LcFJV.png&quot; alt=&quot;bad hash function&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is supposed to be completely random per grid square, but you can see that it has a lot of diagonal line patterns in it (it almost looks like a checkerboard), so it's not a very random hash, and those patterns will show up in the noise produced by it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other problem is that your hash only returns gradient vectors in [0, 1], while they should be in [&amp;minus;1, 1] to get gradients in all directions. That part's easy to fix by remapping.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To fix those problems, I switched the code to use this hash function (which I learned from Mikkel Gjoel, and &lt;a href=&quot;http://stackoverflow.com/questions/12964279/whats-the-origin-of-this-glsl-rand-one-liner/&quot;&gt;is probably due to a paper by W.J.J. Rey&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;vec2 hash(vec2 co) {&#xA;    float m = dot(co, vec2(12.9898, 78.233));&#xA;    return fract(vec2(sin(m),cos(m))* 43758.5453) * 2. - 1.;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that due to the trig functions it's going to be a bit more expensive than your version. However, it considerably improves the appearance of the resulting noise:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ND8Es.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ND8Es.png&quot; alt=&quot;fbm noise with better hash function&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-05-16T21:52:56.627" LastActivityDate="2016-05-16T21:52:56.627" CommentCount="13" />
  <row Id="2450" PostTypeId="1" AcceptedAnswerId="2452" CreationDate="2016-05-16T17:31:04.707" Score="6" ViewCount="173" Body="&lt;p&gt;I'd like to load arbitrary meshes and draw thick black lines along the edges to get a toon-shading like look.&#xA;I managed to draw a black silhouette around the objects by using the stencil buffer. You can see the result here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/rPEIz.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/rPEIz.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But what's missing are the black lines in the object itself. I thought about checking normal discontinuities: Checking if a neighbouring pixel has a different normal vector than the current one. If yes, an edge has been found. Unfortunately, I've no idea how I could implement this approach, neither in OpenGL nor in GLSL vertex/fragment shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd be very happy for some help regarding this approach or any other regarding edge detection. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I do not use any textures for my meshes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Too be more precise, I would like to create a CAD/CAM solution that looks as much as possible like this (taken from Top Solid &lt;a href=&quot;https://www.youtube.com/watch?v=-qTJZtYUDB4&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=-qTJZtYUDB4&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uIZW7.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uIZW7.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;" OwnerUserId="3377" LastEditorUserId="3377" LastEditDate="2016-05-19T15:10:56.093" LastActivityDate="2016-05-19T15:10:56.093" Title="OpenGL - Detection of edges" Tags="&lt;opengl&gt;&lt;edge-detection&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="2" />
  <row Id="2451" PostTypeId="2" ParentId="2391" CreationDate="2016-05-16T20:30:13.717" Score="2" Body="&lt;p&gt;Disclaimer: I have no idea what is the state of the art in the environmental map sampling. In fact, I have very little knowledge about this topic. So this will not be complete answer but I will formulate the problem mathematically and analyze it. I do this mainly for myself, so I make it clear for my self but I hope that OP and others will find it useful.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\newcommand{\w}{\omega}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We want to calculate direct illumination at a point i.e. we want to know the value of &lt;a href=&quot;https://en.wikipedia.org/wiki/Rendering_equation#Equation_form&quot; rel=&quot;nofollow&quot;&gt;the integral&lt;/a&gt;&#xA;$$&#xA;I =\int_{S^2} f(\omega_i,\omega_o,n) \, L(\omega_i) \,(\omega_i\cdot n)^+ d\omega_i&#xA;$$&#xA;where $f(\omega_i,\omega_o,n)$ is BSDF function(I explicitly state dependance on the normal which will be useful later), $L(\omega_i)$ is radiance of environmental map and $(\omega_i \cdot n)^+$ is the cosine term together with the visibility(&lt;a href=&quot;https://en.wikipedia.org/wiki/Positive_and_negative_parts&quot; rel=&quot;nofollow&quot;&gt;that what the $+$ is for&lt;/a&gt;) i.e. $(\omega_i \cdot n)^+=0$ if $(\omega_i \cdot n)&amp;lt;0$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We estimate this integral by generating $N$ samples  $\omega_i^1,\dots,\omega_i^N$ with the respect to the probability density function $p(\omega_i)$, the estimator is&#xA;$$&#xA;I \approx \frac1N \sum_{k=1}^N \frac{f(\omega_i^k,\omega_o,n) \, L(\omega_i^k) \,(\omega_i^k\cdot n)^+ }{p(\omega_i^k)}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The question is: How do we choose the pdf $p$ such that we are able to generate the samples in acceptable time and the variance of the above estimator is reasonably small.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;s&gt;Best&lt;/s&gt; method&lt;/strong&gt; Pick $p$ proportional to the integrand&#xA;$$&#xA;p(\omega_i) \sim f(\omega_i,\omega_o,n) \, L(\omega_i) \,(\omega_i\cdot n)^+ &#xA;$$&#xA;But most of the times it is very expensive to generate a sample according to this pdf, so it is not useful in practice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Methods suggested by OP:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Method one&lt;/strong&gt;: Choose $p$ proportional to the cosine term&#xA;$$&#xA;p(\omega_i) \sim (\omega_i\cdot n)^+&#xA;$$&#xA;&lt;strong&gt;Method two&lt;/strong&gt;: Choose $p$ proportional to the EM&#xA;$$&#xA;p(\omega_i) \sim L(\omega_i)&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Based on the names of mentioned papers I can partially guess what they do(unfortunately I do not have the time and energy to read them right now). But before discussing what they most probably do, let's talk about power series a little bit :D&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;If we have a function of one real variable e.g. $f(x)$. Then if it is well behaved then it can be expanded into a power series&#xA;$$&#xA;f(x) = \sum_{k=0}^\infty a_k x^k&#xA;$$&#xA;Where $a_k$ are constants. This can be used to approximate $f$ by truncating the sum at some step $n$&#xA;$$&#xA;f(x) \approx \sum_{k=0}^n a_k x^k&#xA;$$&#xA;If $n$ is sufficiently high then the error is really small.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now if we have function in two variables e.g. $f(x,y)$ we can expand it only in the first argument&#xA;$$&#xA;f(x,y) = \sum_{k=0}^\infty b_k(y) \, x^k&#xA;$$&#xA;where  $b_k(y)$ are functions only in $y$. It can be also expanded in both arguments&#xA;$$&#xA;f(x,y) = \sum_{k,l=0}^\infty c_{kl} x^k y^l&#xA;$$&#xA;where $c_{kl}$ are constants. So function with real arguments can be expanded as sum of powers of that argument. Something similar can be done for functions defined on sphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, let's have a function defined on sphere e.g. $f(\omega)$. Such a function can be also expanded in similar fashion as function of one real parameter&#xA;$$&#xA;f(\omega) =\sum_{k=0}^\infty \alpha_k S_k(\omega)&#xA;$$&#xA;where $\alpha_k$ are constants and $S_k(\omega)$ are &lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_harmonics&quot; rel=&quot;nofollow&quot;&gt;spherical harmonics&lt;/a&gt;. Spherical harmonics are normally indexed by two indices and are written as function in spherical coordinates but that is not important here. The important thing is that $f$ can be written as a sum of some known functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now function which takes two points on sphere e.g. $f(\omega,\omega')$ can be expanded only in its first arguments&#xA;$$&#xA;f(\omega,\omega') = \sum_{k=0}^\infty \beta_k(\omega') \, S_k(\omega)&#xA;$$&#xA;or in both its arguments&#xA;$$&#xA;f(\omega,\omega') = \sum_{k,l=0}^\infty \gamma_{kl} \, S_k(\omega)S_l(\omega')&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;So how is this all useful?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I propose the  &lt;strong&gt;CMUNSM&lt;/strong&gt;(Crazy mental useless no sampling method):&#xA;Lets assume that we have expansions for all the function i.e.&#xA;\begin{align}&#xA;f(\omega_i,\omega_o,n) &amp;amp;= \sum_{k,l,m=0}^\infty \alpha_{klm} S_k(\omega_i)S_l(\omega_o) S_m(n) \\&#xA;L(\omega_i ) &amp;amp;= \sum_{n=0}^\infty \beta_n S_n(\omega) \\&#xA;(\omega_i\cdot n)^+ &amp;amp;= \sum_{p,q=0}^\infty \gamma_{pq} S_p(\omega_i)S_q(n)&#xA;\end{align}&#xA;If we plug this into the integral we get&#xA;$$&#xA;I = \sum_{k,l,m,n,p,q=0}^\infty \alpha_{klm} \beta_n  \gamma_{pq} S_l(\omega_o) S_m(n) S_q(n) \int_{S^2} S_k(\omega_i) S_n(\omega)  S_p(\omega_i) d\omega_i&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actually we no longer need Monte Carlo because we can calculate values of the integrals $\int_{S^2} S_k(\omega_i) S_n(\omega)  S_p(\omega_i) d\omega_i$ beforehand and then evaluate the sum(actually approximate the sum, we would sum only first few terms) and we get desired result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is all nice &lt;strong&gt;but&lt;/strong&gt; we might not know the expansions of BSDF or environmental map or the expansions converge very slowly therefore we would have to take a lots of terms in the sum to get reasonably accurate answer.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;So the idea is not to expand in all arguments. One method which might be worth investigating would be to ignore BSDF and expand only the environmental map i.e.&#xA;$$&#xA;L(\omega_i) \approx \sum_{n=0}^K \beta_n S_n(\omega_i)&#xA;$$&#xA;this would lead to pdf:&#xA;$$&#xA;p(\omega_i) \sim \sum_{n=0}^K \beta_n S_n(\omega_i) (\omega \cdot n)^+&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We already know how to do this for $K=0$, this is nothing but the &lt;strong&gt;method one&lt;/strong&gt;. My guess is, it is done in one of the papers for higher $K$.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Further extensions. You can expand different functions in different arguments and do similar stuff as above. Another thing is, that you can expand in different basis, i.e. do not use spherical harmonics but different functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So this is my take on the topic, I hope you have found it at least a little bit useful and now I'm off to GoT and bed.&lt;/p&gt;&#xA;" OwnerUserId="1613" LastActivityDate="2016-05-16T20:30:13.717" CommentCount="2" />
  <row Id="2452" PostTypeId="2" ParentId="2450" CreationDate="2016-05-16T21:02:48.873" Score="9" Body="&lt;p&gt;Generally edge detection boils down to detect areas of the image with high gradient value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In our case we can crudely see the gradient as the derivative of the image function, therefore the magnitude of the gradient gives you an information on &lt;em&gt;how much&lt;/em&gt; your image changes locally (in regards of neighbouring pixels/texels).&lt;br/&gt; Now, an edge is as you say an indication of discontinuity, so now that we defined the gradient is clear that this info is all we need. Once we find the gradient of an image, it's just a matter of applying a threshold to it to obtain a binary value edge/non-edge. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do you find this gradient is really what you are asking and I am yet to answer :) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lots of ways! Here a couple :) &lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Built in shader functions&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Both hlsl and glsl offer derivative functions. In GLSL you have &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/dFdx.xhtml&quot;&gt;dFdx and dFdy&lt;/a&gt; that give you respectively gradient information in x and y direction. Typically these functions are evaluated in a block of 2x2 fragments.&lt;br&gt;&#xA;Unless you are interested in a single direction, a good way to have a compact result that indicates how strong is the gradient in the region is &lt;a href=&quot;https://www.opengl.org/sdk/docs/man/html/fwidth.xhtml&quot;&gt;fwidth&lt;/a&gt; that gives you nothing else but the sum of the absolute value of dFdy and dFdy. &lt;br&gt;You are likely to be interested in an edge on the overall image rather than on a specific channel, so you might want to transform your image function to luma. With this in mind,when it comes to edge detection your shader could include something to the like of:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  float luminance = dot(yourFinalColour,vec3(0.2126, 0.7152, 0.0722));&#xA;  float gradient = fwidth(luminance );&#xA;  float isEdge = gradient &amp;gt; threshold;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With an high threshold you will find coarser edges and you might miss some, conversely, with a low threshold you might detect false edges. You have to experiment to find the threshold that better suits your needs. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;The reason why these functions work is worth mentioning but I don't have time for it now, I am likely to update this answer later on :)&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Screen space post-process&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;You could go fancier than this, now the field of Edge detection in image processing is immense. I could cite you tens of good ways to detect edge detection according to your needs, but let's keep it simple for now, if you are interested I can cite you more options!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the idea would be similar to the one above, with the difference that you could look at a wider neighbourhood and use a set of weights on sorrounding samples if you want. Typically, you run a convolution over your image with a kernel that gives you as a result a good gradient info. &lt;br&gt; A very common choice is the Sobel kernel&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;http://i.stack.imgur.com/kXrkd.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/kXrkd.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which respectively give you gradients in x and y directions: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;http://i.stack.imgur.com/gCQgw.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/gCQgw.png&quot; alt=&quot;From an old pdf I wrote a long time ago.&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can get the single value out of the gradient as $ GradientMagnitude = \sqrt{ (Gradient_x) ^ 2 + (Gradient_y) ^ 2 } $ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then you can threshold as the same way I mentioned above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This kernel as you can see give more weight to the central pixel, so effectively is computing the gradient + a bit of smoothing which traditionally helps (often the image is gaussian blurred to eliminate small edges). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The above works quite well, but if you don't like the smoothing you can use the Prewitt kernels: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;http://i.stack.imgur.com/tQ5WJ.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tQ5WJ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Note I am in a rush, will write proper formatted text instead of images soon! )&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Really there are plenty more kernels and techniques to find edge detection in an image process-y way rather than real time graphics, so I have excluded more convoluted (pun not intended) methods as probably you'd be just fine with dFdx/y functions. &lt;/p&gt;&#xA;" OwnerUserId="100" LastEditorUserId="100" LastEditDate="2016-05-16T21:49:48.597" LastActivityDate="2016-05-16T21:49:48.597" CommentCount="8" />
  <row Id="2453" PostTypeId="1" AcceptedAnswerId="2458" CreationDate="2016-05-17T08:15:04.127" Score="4" ViewCount="39" Body="&lt;p&gt;I know that fragments are rasterized in a 2x2 quad. That's why the seam between two triangles can be rasterized twice. At least &lt;a href=&quot;http://www.humus.name/index.php?page=Comments&amp;amp;ID=228&quot; rel=&quot;nofollow&quot;&gt;that article&lt;/a&gt; states this. I wonder is it still true today on modern hardware?&lt;/p&gt;&#xA;" OwnerUserId="386" LastActivityDate="2016-05-18T03:34:20.290" Title="Double rasterization work on triangles edges." Tags="&lt;rasterizer&gt;" AnswerCount="2" CommentCount="6" />
  <row Id="2454" PostTypeId="2" ParentId="2453" CreationDate="2016-05-17T15:45:12.903" Score="5" Body="&lt;blockquote&gt;&#xA;  &lt;p&gt;That's why the seam between two triangles can be rasterized twice.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The seam won't be &lt;em&gt;rasterized&lt;/em&gt; twice because of the OpenGL or D3D fill/rasterisation rules. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I think Humus is alluding to is that a fragment shader may be executed multiple times on pixels spanning the shared edge, which just implies wasted effort.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The moral of the story is &quot;avoid small or long/thin triangles if you want efficient rendering&quot;.&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2016-05-17T15:45:12.903" CommentCount="1" />
  <row Id="2455" PostTypeId="1" AcceptedAnswerId="2475" CreationDate="2016-05-17T15:48:32.090" Score="3" ViewCount="44" Body="&lt;p&gt;I am going through code snippet that reads in a triangular model, calculates &amp;amp; store the normal with each Triangle object and eventually check the consistencies of winding order. Let us define a triangle with the vertices v1, v2 and v3. The face normal of the triangle is calculated as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void calculateFaceNormal()&#xA;{&#xA;   //vector from v3 to v1&#xA;   double dx13, dy13, dz13;&#xA;&#xA;   //vector from v3 to v2&#xA;   double dx23, dy23, dz23;&#xA;&#xA;    dx13 = v1.x_ - v3.x_;&#xA;    dy13 = v1.y_ - v3.y_;&#xA;    dz13 = v1.z_ - v3.z_;&#xA;&#xA;    dx23 = v2.x_ - v3.x_;&#xA;    dy23 = v2.y_ - v3.y_;&#xA;    dz23 = v2.z_ - v3.z_;&#xA;&#xA;    // Cross product gives normal.&#xA;    x_ = dy13*dz23 - dy23*dz13;&#xA;    y_ = dz13*dx23 - dz23*dx13;&#xA;    z_ = dx13*dy23 - dx23*dy13;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Each Triangle object store the references of its neighboring triangles as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;class Triangle()&#xA;{&#xA;  public:&#xA;    .......&#xA;    .......&#xA;&#xA;    const Triangle *edge12Granne_;&#xA;    const Triangle *edge23Granne_;&#xA;    const Triangle *edge31Granne_;&#xA;&#xA;    bool isGranne12NormalConsistent() const;&#xA;    bool isGranne23NormalConsistent() const;&#xA;    bool isGranne31NormalConsistent() const;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I am writing down only one of the functions whose definition is not clear to me and I believe that if someone could explain the theories behind it,I shall be able to understand the rest of the other two functions.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    bool Triangle::isGranne12NormalConsistent() const&#xA;    {&#xA;&#xA;      // Find opposing vertex in neighbour.&#xA;      if(edge12Granne_-&amp;gt;edge12Granne_ == this)&#xA;      {&#xA;&#xA;        /*&#xA;         * two triangle are neighbors to each other by&#xA;         * the adjecent edge that constitutes both the&#xA;         * triangles.&#xA;         * */&#xA;&#xA;          // v2 | w1&#xA;          // v1 | w2&#xA;          if(v1_.id_ == edge12Granne_-&amp;gt;v2_.id_)&#xA;          {&#xA;            // Ok (reversed order compared to this.)&#xA;            return true;&#xA;          }&#xA;          else&#xA;          {&#xA;            // Not ok, same order!&#xA;            return false;&#xA;          }&#xA;      }&#xA;      else if(edge12Granne_-&amp;gt;edge23Granne_ == this)&#xA;      {&#xA;        // v2 | w2&#xA;        // v1 | w3&#xA;        if(v1_.id_ == edge12Granne_-&amp;gt;v3_.id_)&#xA;        {&#xA;            // Ok (reversed order compared to this.)&#xA;            return true;&#xA;        }&#xA;        else&#xA;        {&#xA;            // Not ok, same order!&#xA;            return false;&#xA;        }&#xA;    }&#xA;    else&#xA;    {&#xA;        // v2 | w3&#xA;        // v1 | w1&#xA;        if(v1_.id_ == edge12Granne_-&amp;gt;v1_.id_)&#xA;        {&#xA;            // Ok (reversed order compared to this.)&#xA;            return true;&#xA;        }&#xA;        else&#xA;        {&#xA;            // Not ok, same order!&#xA;            return false;&#xA;        }&#xA;    }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="2712" LastEditDate="2016-05-17T18:20:38.387" LastActivityDate="2016-05-21T17:08:55.753" Title="face normal consistencies" Tags="&lt;geometry&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2456" PostTypeId="1" AcceptedAnswerId="2463" CreationDate="2016-05-17T16:33:28.920" Score="7" ViewCount="68" Body="&lt;p&gt;I have some experience programming geometry and compute shaders - but never adventured myself in really playing with the fragment shaders. Currently, I am trying to better understand how they work and their potential. One of the things I have read in multiple places is that a fragment (i.e. a screen pixel) cannot extend beyond itself within the fragment shader. Which means, a given fragment being iterated can only affect itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, and for the sake of learning, I would like to know if the following is possible (and if it is, how, in general lines, can it be achieved). Suppose we have, for the sake of simplicity, a point-mesh consisting of only two vertices (located in the 3D World space). Can we program a shader in a way such that each of these two vertices would be painted on screen at their exact WorldToViewport position, but also that a circle around each of them of radius=R is also painted in the surrounding pixels even if they extend beyond the original mesh to which the shader is attached? Like in the figure below (where the red square inthe center of the circles represents the vertices painted on screen):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/GV3QO.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/GV3QO.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If that is possible, can a shader also be programmed in such a way that these circles that extend beyond the vertices influence the color (RGBA) of each other? Like in the figure below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/CtdIW.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CtdIW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As I said, if these are possible things, I would love to hear a bit of how to achieve such a thing - either in conceptual or practical terms. Is it done in the fragment shader, or has to be calculated before at the vertex or geometry shaders? How to calculate and pass &quot;additional fragments&quot; that extend beyond those fragments occupied by the mesh body?&lt;/p&gt;&#xA;" OwnerUserId="3383" LastActivityDate="2016-05-21T17:55:41.620" Title="Can (and how so) shaders paint screen pixels beyond those that occupied by the shaded mesh?" Tags="&lt;rendering&gt;&lt;shader&gt;&lt;pixel-shader&gt;&lt;drawing&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="2457" PostTypeId="1" AcceptedAnswerId="2460" CreationDate="2016-05-17T23:55:45.460" Score="5" ViewCount="135" Body="&lt;p&gt;So I have been wanting to make a rendering engine as a summer project. I'd like to shoot for photo-realism of static scenes rather than real time dynamic game like graphics. So in my research I came across two major techniques for going about this. One is to attempt to directly solve the rendering equation with specialized numeric methods like bidirectional path tracing. Another is the well known ray tracing technique. Ray tracing is a) much simpler and b) more efficient. Additionally it seems as though some kind of interpolation/de-noising is needed to make the images look properly smooth when path tracing techniques are used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand recursive ray tracing to be an algorithm in which rays are shot out from the eye and collide with objects in the space. When they do more rays are shot from that point to calculate the lighting at that point.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand path tracing to be trying to find paths to the viewing plane from light sources. If enough of these paths are sampled without bias then you can use these paths to perform monte carlo integration over the space using the found paths as your sample points.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What effects are captured by path tracing that are not captured by ray tracing&lt;/p&gt;&#xA;" OwnerUserId="3385" LastActivityDate="2016-05-18T13:39:55.867" Title="What effects does path tracing capture that recursive ray tracing does not?" Tags="&lt;raytracing&gt;&lt;pathtracing&gt;&lt;monte-carlo&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2458" PostTypeId="2" ParentId="2453" CreationDate="2016-05-18T03:34:20.290" Score="3" Body="&lt;p&gt;It is true to on today's GPUs, unless one or more vendors has implemented a technique such as this:&#xA;&lt;a href=&quot;http://graphics.stanford.edu/papers/fragmerging/shade_sig10.pdf&quot; rel=&quot;nofollow&quot;&gt;http://graphics.stanford.edu/papers/fragmerging/shade_sig10.pdf&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To clarify, rasterization is not being done multiple times, or redundantly - rather it is the shading work that is done redundantly. Each 2x2 quad will be shaded once for every triangle that covers at least one sub pixel sample in that 2x2 quad. So, for 4x MSAA, that could be as many as 16 times if every sample covers a different (very small) triangle.&lt;/p&gt;&#xA;" OwnerUserId="3386" LastActivityDate="2016-05-18T03:34:20.290" CommentCount="0" />
  <row Id="2460" PostTypeId="2" ParentId="2457" CreationDate="2016-05-18T13:39:55.867" Score="9" Body="&lt;p&gt;Generally speaking, &lt;strong&gt;path tracing&lt;/strong&gt; removes a number of assumptions that ray tracing makes. Ray tracing usually assumes that there is no &lt;strong&gt;indirect lighting&lt;/strong&gt; (or that indirect lighting can be approximated by a constant function), because handling indirect lighting would require casting many additional rays whenever you shade an intersection point. Ray tracing usually assumes that there is no such thing as &lt;strong&gt;non-glossy reflection&lt;/strong&gt;, because while it is fairly easy to handle glossy/mirror reflection (you reflect the ray and continue with the raycast), if you have non-glossy specular reflection you have a problem very much like indirect lighting: you need to trace a bunch more rays and then do lighting wherever those rays hit. Ray tracing also usually assumes that there are no &lt;strong&gt;area lights&lt;/strong&gt;, because it is very cheap to cast a single ray towards a light to determine if an intersection point is in shadow, but if you have an area light you generally need to cast multiple rays towards different points on that light to determine the &lt;em&gt;amount&lt;/em&gt; that the intersection point is shadowed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And as soon as you need to break any of those assumptions with ray tracing, you now have a &lt;strong&gt;sampling problem&lt;/strong&gt;: you need to decide how to distribute the many additional rays you need to trace in order to get an accurate result that doesn't have noticeable aliasing.&lt;/p&gt;&#xA;" OwnerUserId="196" LastActivityDate="2016-05-18T13:39:55.867" CommentCount="6" />
  <row Id="2461" PostTypeId="1" CreationDate="2016-05-18T14:35:29.327" Score="6" ViewCount="113" Body="&lt;p&gt;I am trying to implement my own Gradient Domain Path Tracer by following the code of this guy who already implemented it: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://gist.github.com/BachiLi/4f5c6e5a4fef5773dab1&quot;&gt;https://gist.github.com/BachiLi/4f5c6e5a4fef5773dab1&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already managed to go through various steps but I wanted to do something more. I expanded the code of in the reference by implementing next event estimation and here are some results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Normal Path Tracer image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/qPe9r.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/qPe9r.jpg&quot; alt=&quot;Basic Path Tracer&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gradient Domain resulting image:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/UyaZB.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/UyaZB.png&quot; alt=&quot;Gradient Domain image without Next Event Estimation&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Results are already good.. but as said before, I wanted something more. So I implemented Next Event Estimation and here is the result of the basic Path Tracer:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/HdUSm.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/HdUSm.jpg&quot; alt=&quot;Path Tracer with NEE&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;private Vector3 SampleWithNEE( Ray ray )&#xA;{&#xA;  // prepare&#xA;  Vector3 T = (1,1,1), E = (0,0,0), NL = (0,-1,0);&#xA;  int depth = 0;&#xA;  // random walk&#xA;  while (depth++ &amp;lt; MAXDEPTH)&#xA;  {&#xA;    // find nearest ray/scene intersection&#xA;    Scene.Intersect( ray );&#xA;    if (ray.objIdx == -1) break; //if there is no intersection&#xA;    Vector3 I = ray.O + ray.t * ray.D; //go to the Hit Point on the scene&#xA;    Material material = scene.GetMaterial( ray.objIdx, I );&#xA;    if (material.emissive) //case of a light&#xA;    {&#xA;        E += material.diffuse;&#xA;        break;&#xA;    }&#xA;    // next event estimation&#xA;    Vector3 BRDF = material.diffuse * 1 / PI;&#xA;    float f = RTTools.RandomFloat();&#xA;    Vector3 L = Scene.RandomPointOnLight() - I;&#xA;    float dist = L.Length();&#xA;    L = Vector3.Normalize( L );&#xA;    float NLdotL = Math.Abs( Vector3.Dot( NL, -L ) );&#xA;    float NdotL = Vector3.Dot( ray.N, L );&#xA;    if (NdotL &amp;gt; 0)&#xA;    {&#xA;        Ray r = new Ray( I + L * EPSILON, L, dist - 2 * EPSILON ); //make it a tiny bit shorter otherwise I risk to hit my starting and destination point&#xA;        Scene.Intersect( r );&#xA;        if (r.objIdx == -1) //no occlusion towards the light&#xA;        {&#xA;            float solidAngle= (nldotl * light.getArea()) / (dist * dist);&#xA;            E += T * (NdotL) * solidAngle * BRDF * light.emission;&#xA;        }&#xA;    }&#xA;    // sample random direction on hemisphere&#xA;    Vector3 R = DiffuseReflectionCosWeighted( ray.N );&#xA;    float hemi_PDF = Vector3.Dot( R, ray.N ) / PI;&#xA;    T *= (Vector3.Dot( R, ray.N ) / hemiPDF) * BRDF;&#xA;    ray = new Ray( I + R * EPSILON, R, 1e34f );&#xA;  }&#xA;  return E;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Things work really fine and the results are shown with the picture above. One more thing: I have only diffuse surfaces in my scene. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, the problem is that in this method I use 2 kind of PDF's:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;one is given by randomly sampling the light in the Direct Lighting of the Next Event Estimation, indeed that &lt;strong&gt;SolidAngle&lt;/strong&gt; is our PDF, or better 1/PDF. &lt;/li&gt;&#xA;&lt;li&gt;while the second PDF is the one led by the use of &lt;strong&gt;DiffuseReflectionCosWeighted&lt;/strong&gt; which brings a PDF equals to &lt;strong&gt;CosTheta/PI&lt;/strong&gt;. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Everything fine so far and for any implementation detail you can just look at my code but problems come with my Gradient Domain Path Tracer. Indeed, there, as also in the reference link above implemented by Tzu-Mao Li, I need the &lt;em&gt;final probability density of a whole path&lt;/em&gt; to compute the final gradient image. How did I calculate it in case without Next Event Estimation (NEE) ? In that case (since I have only Diffuse surfaces) this probability is the product of &lt;strong&gt;CosTheta / PI&lt;/strong&gt; at each bounce in the scene. Everything is fine and the resulting gradient image is shown above.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Instead, in case I use NEE things don't work anymore because the Probability Density of my whole path changes and I don't manage to understand how it is. The resulting Gradient Domain Image with Next Event Estimation is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uTMGA.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uTMGA.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to understand how to calculate the final density probability of a path. Can you help me doing it? Thanks in advance!&lt;/p&gt;&#xA;" OwnerUserId="3069" LastActivityDate="2016-05-29T00:53:36.330" Title="Resulting Probabilty Density in Path Tracer for paths using Next Event Estimation" Tags="&lt;rendering&gt;&lt;c++&gt;&lt;physically-based&gt;&lt;physics&gt;&lt;pathtracing&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2462" PostTypeId="1" CreationDate="2016-05-19T01:19:21.283" Score="1" ViewCount="22" Body="&lt;p&gt;I have mapped four rectangular point(R1-&gt;R4) to four quadrilateral points(Q1-&gt;Q4) to generate a perspective view.&#xA;Now I have to translate quadrilateral in x and y direction while maintaing perspective view. How can I achieve this?&lt;/p&gt;&#xA;" OwnerUserId="2383" LastActivityDate="2016-05-19T01:19:21.283" Title="Perspective Translation Of Quadrilateral" Tags="&lt;perspective&gt;&lt;2d-graphics&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2463" PostTypeId="2" ParentId="2456" CreationDate="2016-05-19T12:21:51.260" Score="5" Body="&lt;p&gt;When you use linewidth or line antialiasing or pointwidth or pointsprites, OpenGL creates for you a small rectangle instead of the line or point, with texture coordinates. Nowadays you can even program this yourself using geometry shaders ot even the tesselator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A totally different approach is to use defered shading, using one geometric pass just to store information in the RGBAZ buffer, then a second pass than you run on all the pixels of the screen to do some process. (to act on all pixels, simply draw one full-screen rectangle).&#xA;Nowadays you can even do the first pass as one or several &quot;render to texture&quot;, then MIPmap these textures, so that the final pass can easily access to less local values.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastEditorUserId="1810" LastEditDate="2016-05-20T07:07:12.043" LastActivityDate="2016-05-20T07:07:12.043" CommentCount="3" />
  <row Id="2464" PostTypeId="2" ParentId="95" CreationDate="2016-05-19T17:41:41.510" Score="2" Body="&lt;p&gt;Here's how I've solved this in the past:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Draw the first object (the one that should appear behind the other object) by depth testing but &lt;strong&gt;not&lt;/strong&gt; depth writing&lt;/li&gt;&#xA;&lt;li&gt;Draw the second object by depth testing and depth writing.  This won't cause z-fighting since we didn't write any depth in step 1.  &lt;/li&gt;&#xA;&lt;li&gt;Draw the first object by only writing to the depth buffer and &lt;strong&gt;not&lt;/strong&gt; to the color buffer. This makes sure the depth buffer is up to date for any pixels that are covered by object 1 but not by object 2.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Note, the objects need to be drawn consecutively for this to work.&lt;/p&gt;&#xA;" OwnerUserId="3332" LastEditorUserId="3332" LastEditDate="2016-05-20T13:51:31.210" LastActivityDate="2016-05-20T13:51:31.210" CommentCount="0" />
  <row Id="2465" PostTypeId="2" ParentId="1662" CreationDate="2016-05-19T22:40:47.670" Score="6" Body="&lt;p&gt;The best place to put a look up table for a GPU compute shader depends on the size of the lookup table, and the frequency/coherency of access. In your case (you mentioned 4kb), shared local memory would likely be best (assuming you do not need this memory for other purposes in the same kernel). This memory has different names in different APIs, but is the same architectural thing and follows the same performance guidelines:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CUDA: threadgroup shared memory&lt;/li&gt;&#xA;&lt;li&gt;DirectCompute: groupshared memory&lt;/li&gt;&#xA;&lt;li&gt;OpenCL: local memory&lt;/li&gt;&#xA;&lt;li&gt;Metal: threadgroup memory&lt;/li&gt;&#xA;&lt;li&gt;OpenGL: shared memory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Storing the lookup table in global memory as a read-only buffer may perform just as well, depending on the cache size(s) of the particular GPU you're running on.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that I'm presuming this is a read-only lookup table. A read-write lookup table is a completely different beast, and you don't have any good options there.&lt;/p&gt;&#xA;" OwnerUserId="3386" LastActivityDate="2016-05-19T22:40:47.670" CommentCount="2" />
  <row Id="2466" PostTypeId="1" CreationDate="2016-05-20T07:51:38.263" Score="4" ViewCount="20" Body="&lt;p&gt;Given a triangle mesh, does there exist any algorithm to compute a base mesh (smaller in size than the triangle mesh) that could be subdivided multiple times to get back the original mesh (or an approximate of the original) ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="3401" LastEditorUserId="3401" LastEditDate="2016-05-20T09:43:18.833" LastActivityDate="2016-05-20T09:43:18.833" Title="Constructing base mesh for required subdivision surface" Tags="&lt;compression&gt;&lt;subdivision&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="2467" PostTypeId="1" CreationDate="2016-05-20T14:02:05.440" Score="3" ViewCount="55" Body="&lt;p&gt;Is there a way to make a difference between a photo of an actual object and a photo of an object through a computer or smartphone screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example: how can I detect that the second image is taken from a screen?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/6aTf2.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/6aTf2.jpg&quot; alt=&quot;Actual dog&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/6nH7M.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/6nH7M.jpg&quot; alt=&quot;Screen dog&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you very much!&lt;/p&gt;&#xA;" OwnerUserId="3404" LastActivityDate="2016-05-21T09:26:25.177" Title="How to detect a screen in a photo" Tags="&lt;texture&gt;&lt;lighting&gt;&lt;color&gt;&lt;image-processing&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2468" PostTypeId="1" AcceptedAnswerId="2469" CreationDate="2016-05-20T14:41:18.767" Score="4" ViewCount="39" Body="&lt;p&gt;I am going though a functional source that has the following class:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-cpp prettyprint-override&quot;&gt;&lt;code&gt; class Point3D&#xA; {&#xA;       int x ,y,z;&#xA;&#xA;       int vSize2() const&#xA;       {&#xA;           return x*x +&#xA;                  y*y +&#xA;                  z*z;&#xA;       }&#xA;&#xA;&#xA;       int vSize() const&#xA;       {&#xA;         return sqrt(vSize2());&#xA;       }           &#xA; };&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is hard to understand the functionalities of the above two functions (&lt;code&gt;vSize2()&lt;/code&gt; and &lt;code&gt;vSize()&lt;/code&gt;). It seems to me that it is calculating the length between the point and the origin. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any thoughts?&lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="231" LastEditDate="2016-05-20T15:31:28.283" LastActivityDate="2016-05-20T15:52:07.877" Title="Does a point have a length?" Tags="&lt;geometry&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2469" PostTypeId="2" ParentId="2468" CreationDate="2016-05-20T14:55:55.117" Score="6" Body="&lt;p&gt;No, a point does not have a length. A point is only a location - it has no extent in any direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You are correct in guessing that the function &lt;code&gt;vSize()&lt;/code&gt; returns the distance from the origin to the point. The other function, &lt;code&gt;vSize2()&lt;/code&gt; returns the square of that distance. This is used in calculating the distance, and in some cases it may be useful to work directly with the square of the distance instead of the distance. This avoids calculating square roots, so can lead to improvements in speed and accuracy.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Note that as Nero points out, the functions are using only &lt;code&gt;int&lt;/code&gt;s (integer variables and return values). The distance between a point and the origin is not in general an integer value, even if the coordinates of the point are all integers. The distance squared returned by &lt;code&gt;vSize2()&lt;/code&gt; will be accurate, but the distance itself returned by &lt;code&gt;vSize()&lt;/code&gt; will be accurate only to the nearest integer.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-20T15:52:07.877" LastActivityDate="2016-05-20T15:52:07.877" CommentCount="4" />
  <row Id="2470" PostTypeId="2" ParentId="2467" CreationDate="2016-05-21T09:26:25.177" Score="3" Body="&lt;p&gt;The first things I would try is to see if smartphone pixels are visible (if the photo is high res) or if strange aliasing occurs (your example image shows both). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another classical test (but not adapted to any scenes) is to detect perspective bias: if the camera is not exactly parallel to the smartphone screen, then 3D objects projected on the image on screen are projected again to the camera captor through a slightly different perspective. If the scene contains spheres, discs (physical or as reflects, for instance), perpendicular angles, or any similar invariant, then you can detect this.   (this kind of trick is also used to detect images photoshoped by composition of several images).&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-05-21T09:26:25.177" CommentCount="0" />
  <row Id="2471" PostTypeId="1" CreationDate="2016-05-21T11:04:09.310" Score="1" ViewCount="24" Body="&lt;p&gt;I have some problem in OSG doing my model, which need an rectangular prism, that I do not know the parametric equation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to search with the google, but no usable result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also tried to draw it by using this site: &lt;a href=&quot;http://www.math.uri.edu/~bkaskosz/flashmo/tools/parsur/&quot; rel=&quot;nofollow&quot;&gt;http://www.math.uri.edu/~bkaskosz/flashmo/tools/parsur/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could somebody help to find the parametric equation for (x,y,z) cordinates&lt;/p&gt;&#xA;" OwnerUserId="3408" LastActivityDate="2016-05-21T14:50:40.573" Title="Rectangular prism parametric equation" Tags="&lt;scene-graph&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2472" PostTypeId="1" CreationDate="2016-05-21T14:03:10.723" Score="2" ViewCount="30" Body="&lt;p&gt;I'm recently implementing Phong materials in my path tracer. My implementation of a randomly sampled Phong material works and looks fine. But it doesn't work well if the shininess(aka power) is too high. So I tried to implement an importance sampled Phong material. It also looks fine. &lt;br&gt;&#xA;However, when I compared the random sampled Phong material to the importance sampled one. Both material looks the same if the shininess is high, but the importance sampled one looks darker if the shininess is low. Why? Where am I doing wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Picture 1. Random sampled Phong surface(left)/importance sampled Phong surface(right), both shininess ware set to 0 (3500 SPP, naive path tracing). The importance sampled  surface is too dark.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/fqgRS.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fqgRS.png&quot; alt=&quot;Random sampled vs Importance sampled, power = 0&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Picture 2. andom sampled Phong surface(left)/importance sampled Phong surface(right), both shininess ware set to 23 (6700 SPP, naive path tracing). Both surface looks identical.&lt;br&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/KW0ng.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/KW0ng.png&quot; alt=&quot;Random sampled vs Importance sampled, power = 23&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code.&lt;br&gt;&#xA;1.The code that samples the Phong BRDF randomly&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void PhongMaterial::createSample(const Ray&amp;amp; ray, float4 normal, float4 intersection, Ray&amp;amp; sampleRay, float4&amp;amp; coefficient)&#xA;{&#xA;    sampleRay = createRandomReflect(normal,intersection);&#xA;    float4 reflectDirection = reflect(ray.direction,normal);&#xA;    coefficient = reflectColor * pow(dot(sampleRay.direction,reflectDirection),power+1)&#xA;        *(float)((power+1));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;2.The code that importance samples the Phong BRDF&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;void PhongMaterial::createImportanceSample(const Ray&amp;amp; ray, float4 normal, float4 intersection, Ray&amp;amp; sampleRay, float4&amp;amp; coefficient)&#xA;{&#xA;    float4 reflectDirection = reflect(ray.direction,normal);&#xA;    float2 rnd(rand1(),rand1());&#xA;    float phi = 2.0f*M_PI*rnd.x;&#xA;    float theta = acos(pow(rnd.y,1.0f/(power+1.f)));&#xA;    float4 rotX,rotY;&#xA;    ons(reflectDirection,rotX,rotY);&#xA;    float4 result = cos(phi)*sin(theta)*rotX&#xA;        + sin(phi)*sin(theta)*rotY&#xA;        + cos(theta)*reflectDirection;&#xA;    sampleRay.origin = intersection + result*0.00001f;&#xA;    sampleRay.direction = result;&#xA;&#xA;    coefficient = reflectColor*(float)((power+1)/(power+2));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;ray&lt;/code&gt; is the incoming ray, &lt;code&gt;normal&lt;/code&gt; is the surface normal, &lt;code&gt;intersection&lt;/code&gt; is where the ray and the surface intersects, &lt;code&gt;coefficient&lt;/code&gt; is light transmit coefficient and &lt;code&gt;power&lt;/code&gt; is the shininess.&lt;br&gt;&#xA;Also, &lt;code&gt;ons&lt;/code&gt; creates two vector perpendicular to the rest from a single vector(the first parameter).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where am I doing wrong?&lt;/p&gt;&#xA;" OwnerUserId="2448" LastEditorUserId="2448" LastEditDate="2016-05-21T15:46:22.470" LastActivityDate="2016-05-21T15:46:22.470" Title="How to properly implement Phong material and importance sample it in a path tracer?" Tags="&lt;brdf&gt;&lt;pathtracing&gt;&lt;sampling&gt;&lt;importance-sampling&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="2474" PostTypeId="2" ParentId="2471" CreationDate="2016-05-21T14:34:32.753" Score="2" Body="&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Prism_%28geometry%29&quot; rel=&quot;nofollow&quot;&gt;prism&lt;/a&gt; is a polyhedron with a polygonal base, another parallel base of the same shape, and joining faces between corresponding edges of the two base faces.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The word &quot;prism&quot; is commonly used to describe a right prism with base faces perpendicular to the joining faces. In this case the joining faces are also rectangles, and so a rectangular right prism is simply a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cuboid&quot; rel=&quot;nofollow&quot;&gt;cuboid&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A cuboid can be described directly, but a simple way to generate one is to start with a cube and use directional &lt;a href=&quot;https://en.wikipedia.org/wiki/Scaling_%28geometry%29&quot; rel=&quot;nofollow&quot;&gt;scaling&lt;/a&gt; to give the edges the required lengths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the base faces are not perpendicular to the joining faces, this oblique prism can be achieved by applying a &lt;a href=&quot;https://en.wikipedia.org/wiki/Shear_mapping&quot; rel=&quot;nofollow&quot;&gt;shear&lt;/a&gt; to a cuboid with the required edge lengths.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So all you need is a parametric description of a cube, and you can go from there to any rectangular prism required.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;As joojaa points out, there isn't an easy way to describe a prism (including a cube) in just a single parametrisation. You will need to describe each face with a separate set of parametric equations. For example, using parameters u and v, you could describe one of the faces as follows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$  x=u  \\  y=v  \\  z=1  \\  0 \leq u \leq 1  \\  0 \leq v \leq 1  $$&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-21T14:50:40.573" LastActivityDate="2016-05-21T14:50:40.573" CommentCount="1" />
  <row Id="2475" PostTypeId="2" ParentId="2455" CreationDate="2016-05-21T17:08:55.753" Score="1" Body="&lt;p&gt;The function in your third code snippet is checking for the winding order to be consistent between two neighboring triangles. It does this by looking at the order in which vertices are listed in their shared edge. If the two triangles have consistent winding (both counterclockwise for instance), they will have opposite orientations for the shared edge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A diagram may help see this:&#xA;&lt;img src=&quot;http://i.stack.imgur.com/JSz6H.png&quot; alt=&quot;diagram of two triangles with vertex winding&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The vertex order is indicated by the blue arrows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;t0: v0, v1, v2&#xA;t1: v1, v3, v2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If you look at the shared edge (v1&amp;ndash;v2), it appears in the first triangle with order &lt;code&gt;v1, v2&lt;/code&gt;, and in the other with order &lt;code&gt;v2, v1&lt;/code&gt; (cycling around the end of the vertex list back to the beginning).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The function in your snippet takes a triangle and looks at one of its neighbors, checks all three possibilities for which edge in the second triangle is the shared one, and checks that the vertices in that (other triangle's) edge appear in the reverse order to the vertices in the current triangle's edge. It returns true if this is the case, and false otherwise.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-21T17:08:55.753" CommentCount="0" />
  <row Id="2476" PostTypeId="2" ParentId="2456" CreationDate="2016-05-21T17:55:41.620" Score="2" Body="&lt;p&gt;One good way you can arrange for a circle (or other shape) to be drawn for each vertex in a mesh is to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Geometry_instancing&quot; rel=&quot;nofollow&quot;&gt;geometry instancing&lt;/a&gt;. This is a GPU feature that allows multiple instances (copies) of a mesh to be drawn at once, with the vertices/indices of the mesh given by one set of buffers, and another vertex buffer that can give additional data per-instance. The vertex shader can be used to combine the data from both buffers in whatever way you choose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be concrete, in your case you could create a mesh representing a circle of the desired radius, with its vertices specified directly in screen-space coordinates and centered at the origin. Then you'd use instancing to make the GPU render a new copy of the circle mesh for every vertex of your original point-mesh. In the vertex shader, you'd calculate the screen-space position of the point (using the world-view-projection transforms as usual), then translate the circle mesh to be centered at that position.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for the second part of your question, on how to have the circles influence each other's color: it depends on what you want to do, specifically. Hardware blending can be used to handle simple cases, such as adding or multiplying the colors, or alpha-blending. If you want something more complicated than that, then it might be possible to do it with a multi-pass algorithm, or (on the latest GPUs) programmable blending.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-21T17:55:41.620" CommentCount="0" />
  <row Id="2477" PostTypeId="2" ParentId="2391" CreationDate="2016-05-21T18:24:35.543" Score="4" Body="&lt;p&gt;This is not a full answer, I would just like to share the knowledge I obtained by studying two of the papers mentioned in the question: &lt;a href=&quot;http://home.eps.hw.ac.uk/~ks400/research.html#SteerableImportance&quot; rel=&quot;nofollow&quot;&gt;Steerable Importance Sampling&lt;/a&gt; and &lt;a href=&quot;http://fileadmin.cs.lth.se/graphics/research/papers/2008/pps/&quot; rel=&quot;nofollow&quot;&gt;Practical Product Importance Sampling for Direct Illumination&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Steerable Importance Sampling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;In this paper they propose a method for sampling the product of the clamped cosine component and environment map lighting:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;L_{EM}\left(\omega_{i}\right)\left(\omega_{i}\cdot n\right)^+&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They make use of the fact that a piece-wise linear approximation of the product function can be relatively well expressed and partially pre-computed using the first nine spherical harmonic bases. They build this approximation on top of an adaptively triangulated EM and use it as an importance function for sampling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;They pre-compute and store approximation coefficients for each triangle vertex and also coefficients for computation of approximation integral over the triangle for each triangle. These coefficients are called vertex and triangle weights. Then they make use of the fact that is it possible to easily compute coefficients for an integral over a set of triangles just by summing the individual triangle weights without incorporating additional spherical harmonic bases. This allows them to build a balanced binary tree over the triangles where each node contains coefficients for computing approximation integral over the node's sub-tree triangles.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The sampling procedure consists of selecting a triangle and sampling its area:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A triangle is chosen by descending down the pre-built binary tree    with probability proportional to the sub-integral approximations. This costs $O\left(\log N_{\triangle}\right)$ on-the-fly computations of sub-integrals, each consisting of one inner product of clamped-cosine spherical harmonic coordinates with the pre-computed coefficients.   &lt;/li&gt;&#xA;&lt;li&gt;The chosen triangle surface is then sampled in $O\left(1\right)$ time in a    bi-linear fashion by a novel stratified sampling strategy proposed in    the paper.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;To me, this looks like a &lt;strong&gt;promising technique&lt;/strong&gt;, but the classical question with papers is how it will behave in the real life. On the one hand, there may be pathological cases when the EM is hard to approximate with triangulated piece-wise linear function, which can lead to an enormous amount of triangles and/or to poor sample quality. On the other hand, it can instantly provide a relatively good approximation of the whole EM contribution, which can be useful when sampling multiple light sources.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Practical Product Importance Sampling for Direct Illumination&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;In this paper they propose a method for sampling the product of environment map lighting and cosine-weighted surface reflectance:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;L_{EM}\left(\omega_{i}\right)f_{r}\left(\omega_{i},\omega_{o},n\right)\left(\omega_{i}\cdot n\right)^+&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only pre-processing in this method is computation of a hierarchical representation of the EM (either mipmap or wavelet based). The rest is done on the fly during the sampling.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The sampling procedure:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Building an on-the-fly BRDF approximation: They first draw several BRDF importance samples and evaluate $f_{r}\left(\omega_{i},\omega_{o},n\right)\left(\omega_{i}\cdot n\right)^+$. From these values they build a quadtree-based piece-wise constant approximation of the BRDF, where each leaf of the tree contains exactly one sample.&lt;/li&gt;&#xA;&lt;li&gt;Computing a product of the BRDF approximation and the EM: Multiplication is done at the BRDF quadtree leaves and averaged values are propagated to parents.&lt;/li&gt;&#xA;&lt;li&gt;Product sampling: uniform samples are fed through the product tree using simple sample warping.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The procedure should generate relatively good samples at the cost of heavy pre-computation &amp;ndash; they show that roughly 100&amp;ndash;200 BRDF samples are needed for BRDF approximation to achieve the best sampling performance. This may make it suitable for purely direct illumination computations, where one generates many samples per shading point, but is most probably &lt;strong&gt;too expensive for global illumination algorithms&lt;/strong&gt; (e.g. uni- or bi-directional path tracers), where you usually generate only a few samples per shading point.&lt;/p&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-05-21T19:29:57.323" LastActivityDate="2016-05-21T19:29:57.323" CommentCount="0" />
  <row Id="2478" PostTypeId="1" AcceptedAnswerId="2479" CreationDate="2016-05-21T21:56:59.703" Score="3" ViewCount="52" Body="&lt;p&gt;First of all, let me explain what I am really trying to achieve. In a post-effect shader acting on a camera-renderer, I want to change the color of each fragment  depending on how many green (RGBA = 0,~1,0,1) fragments there are nearby.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, for the sake of simplicity, let's imagine my screen is all black, with some (~10k) green points spread around. In the fragment shader, for each fragment I have to find how many green fragments there are within a given distance and then change the color of the current fragment being evaluated depending on that number of surrounding green fragments.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I began with the most naïve approach one could think of: for each fragment, I calculated the coordinates of the 10th neighbor fragments to each side: right, left, up and down (which gives a surrounding square, then I discarded the corners from the computation to get a surrounding circle of radius ~10pixels, centered at the current fragment). Then I tested which of the fragments within such &quot;surrounding bounding circle&quot; was green (i.e. color.g &gt; 0.9f). In affirmative case, I increased a counter variable.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my way of thinking, that would be doing a brute force and in order to visualize the result, I then painted each fragment with the RGBA color (counter*0.1f, 0, counter*0.1f, 1). Here is the code snippet:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;uniform sampler2D _MainTex;&#xA;static float2 _Pixels = float2(_ScreenParams.x, _ScreenParams.y); //gets the Screen width and height&#xA;static half sx = 1 / _ScreenParams.x; //defines the measure of a fragment at the X axis of the viewport&#xA;static half sy = 1 / _ScreenParams.y; //defines the measure of a fragment at the Y axis of the viewport&#xA;&#xA;float4 frag(v2f_img input) : COLOR{&#xA;fixed2 uv = round(input.uv * _Pixels) / _Pixels;&#xA;&#xA;float counter = 0.0f;&#xA;float i; float j;&#xA;float thrsh = 0.8f;&#xA;&#xA;[unroll(10)]for (i = zero; i&amp;lt;10;i++) //loop trough 10 fragments horizontally&#xA;{&#xA;    [unroll(10)]for (j = zero; j &amp;lt; 10; j++) //loop trough 10 fragments vertically&#xA;    {&#xA;        //the following lines cut (very roughly) the corners of the surrounding square ixj to approximate a surrounding circle of radius ~10fragments. But this is less important: if such lines are commented, the problem that I will show later will still happen, just with a surrounding square instead of a surrounding circle&#xA;        if ((i == 4 || i == 5) &amp;amp;&amp;amp; j &amp;gt; 9) j = 0; else if ((i == 6) &amp;amp;&amp;amp; j &amp;gt; 8) j = 0; else if ((i == 7) &amp;amp;&amp;amp; j &amp;gt; 7) j = 0;&#xA;        else if ((i == 8) &amp;amp;&amp;amp; j &amp;gt; 6) j = 0; else if ((i == 9) &amp;amp;&amp;amp; j &amp;gt; 5) j = 0; else if (i == 10 &amp;amp;&amp;amp; j &amp;gt; 4) j = 0;&#xA;&#xA;        else if (j &amp;gt; 0) { //so, let's continue only if current iteration does not relate to the corners of the surrounding square, i.e. outside the surrounding circle with radius ~10 fragments&#xA;&#xA;            float tl = tex2D(_MainTex, uv + fixed2(-sx*i, +sy*j)).g; //gets the top-left neighbor times i and j&#xA;            float cl = tex2D(_MainTex, uv + fixed2(-sx*i, 0)).g; //gets the center-left neighbor times i and j&#xA;            float bl = tex2D(_MainTex, uv + fixed2(-sx*i, -sy*j)).g; //gets the bottom-left neighbor times i and j&#xA;            float tc = tex2D(_MainTex, uv + fixed2(0, +sy*j)).g; //gets the top-center neighbor times i and j&#xA;            float cc = tex2D(_MainTex, uv + fixed2(0, 0)).g;  //gets the current fragment bein evaluated (i.e. center-center)&#xA;            float bc = tex2D(_MainTex, uv + fixed2(0, -sy*j)).g;  //gets the bottom-center neighbor times i and j&#xA;            float tr = tex2D(_MainTex, uv + fixed2(+sx*i, +sy*j)).g; //gets the top-right neighbor times i and j&#xA;            float cr = tex2D(_MainTex, uv + fixed2(+sx*i, 0)).g; //gets the center-right neighbor times i and j&#xA;            float br = tex2D(_MainTex, uv + fixed2(+sx*i, -sy*j)).g;  //gets the bottom-right neighbor times i and j&#xA;&#xA;            //Now, for each fragment surrounding that is above the Green threshold, we increase the counter&#xA;                if (tl &amp;gt; thrsh) counter++;&#xA;                if (cl &amp;gt; thrsh) counter++;&#xA;                if (bl &amp;gt; thrsh) counter++;&#xA;                if (tc &amp;gt; thrsh) counter++;&#xA;                if (bc &amp;gt; thrsh) counter++;&#xA;                if (tr &amp;gt; thrsh) counter++;&#xA;                if (cr &amp;gt; thrsh) counter++;&#xA;                if (br &amp;gt; thrsh) counter++;&#xA;&#xA;        }&#xA;    }&#xA;&#xA;}&#xA;&#xA;&#xA;if (counter &amp;gt; 0)&#xA;{&#xA;    return float4(counter *0.1f, 0, counter *0.1f, 1); //in case at least one surrounding fragment was green, paint the fragments pink-ish in proportion to how many green there were in the surrounding circle&#xA;} else {&#xA;    return tex2D(_MainTex, float4(0,0,0,1)); //otherwise, paint black&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, besides &lt;strong&gt;very&lt;/strong&gt; inefficient, the solution above is also not working correctly. Obvioulsy, I was expecting to get, around each green point at the screen, something like a pink-ish circle that is of stronger intensity when close to the center and fades away at the border, i.e. away from the center. Instead, this is what I get (in actual and zoomed sizes):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/Tsef3.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/Tsef3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I tried everything I could think of but I don't get:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) why it is not working properly. What am I doing wrong? Could anyone shed some light?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) also, if there is any obvious more performant way of achieving what I want, please do let me know too (the current solution is taking 12ms in a non HD resolution). Or if you prefer, I can come with a different question for that later, after the operational problem described above is solved.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks anyways for your time.&lt;/p&gt;&#xA;" OwnerUserId="3410" LastActivityDate="2016-05-22T01:01:30.820" Title="Count number of green fragments that are within a given distance from each screen fragment" Tags="&lt;shader&gt;&lt;geometry&gt;&lt;fragment-shader&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2479" PostTypeId="2" ParentId="2478" CreationDate="2016-05-22T01:01:30.820" Score="3" Body="&lt;p&gt;You're getting a cross shape in the output because in your loop, you're counting pixels on the center row and column multiple times. For example the pixel at (0, 0) offset will be counted on every iteration of the inner loop, so 100 times. Similarly, (&amp;pm;i, 0) and (0, &amp;pm;i) will be counted on every iteration of the inner loop, so 10 times for each value of i.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A better way to write the loop is like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[unroll] for (int i = -10; i &amp;lt;= 10; i++)&#xA;{&#xA;    [unroll] for (int j = -10; j &amp;lt;= 10; j++)&#xA;    {&#xA;        float sample = tex2D(_MainTex, input.uv + float2(sx*i, sy*j)).g;&#xA;        if (sample &amp;gt; thrsh) counter++;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will weight all pixels equally within a box. To make a more circular shape you can change the &lt;code&gt;j&lt;/code&gt; iteration bounds based on the value of &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I doubt you need or want to round off &lt;code&gt;uv&lt;/code&gt; to the nearest pixel as you're doing. The UVs coming into the fragment shader should already be correct, and moreover they are not multiples of the pixel size; they are the coordinates of pixel &lt;em&gt;centers&lt;/em&gt;, so they contain a half-pixel offset (which is exactly what you want for sampling a screen-sized texture).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For a more efficient algorithm, you could downsample the original image 2x, then run a 5px-radius neighbor search on that one instead of a 10px-radius search on the original. This will make the circle shape somewhat less precise but depending on what you're doing, it may not matter that much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another approach that's less brute-force but still precise would be to build a spatial hierarchy of the green points. This could be as simple as a uniform grid where each grid square stores a list of the points inside it, or as complicated as a kd-tree. Your fragment shader could then walk this data structure to efficiently search the nodes within a radius of the fragment, and count the points it finds.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-22T01:01:30.820" CommentCount="3" />
  <row Id="2480" PostTypeId="1" AcceptedAnswerId="2481" CreationDate="2016-05-22T15:05:30.210" Score="2" ViewCount="41" Body="&lt;p&gt;So I have a game and I am trying to fake glow inside of it by using the hardwares capability to downsize textures causing them to blur.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The texture I wish to do this with is in a render buffer that is the size of the screen (not power of two)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the quickest way to get these lower resolutions created?&#xA;Tricking the software to make minimaps?&#xA;Another fbo with the texture rendered on it in smaller size?&lt;/p&gt;&#xA;" OwnerUserId="2308" LastActivityDate="2016-05-22T16:37:02.623" Title="Generation of lower resolution textures" Tags="&lt;opengl&gt;&lt;opengl-es&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2481" PostTypeId="2" ParentId="2480" CreationDate="2016-05-22T16:37:02.623" Score="3" Body="&lt;p&gt;I would think rendering to a downsampled FBO would be much faster than generating mipmaps.  glGenerateMipmap will generate the complete mipmap pyramid, which seems wasteful:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.khronos.org/opengles/sdk/docs/man/xhtml/glGenerateMipmap.xml&quot; rel=&quot;nofollow&quot;&gt;https://www.khronos.org/opengles/sdk/docs/man/xhtml/glGenerateMipmap.xml&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Besides, only OpenGL ES 3.0 lets you manually select mip level when texturing:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://www.khronos.org/opengles/sdk/docs/man3/html/textureLod.xhtml&quot; rel=&quot;nofollow&quot;&gt;https://www.khronos.org/opengles/sdk/docs/man3/html/textureLod.xhtml&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Artistically, this might not give you the look you want.  In my experience, simply downsampling gives a grainy look.  I would recommend also doing a pair of one-dimensional blurs, as described in this article from GPU Gems:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems/gpugems_ch21.html&quot; rel=&quot;nofollow&quot;&gt;http://http.developer.nvidia.com/GPUGems/gpugems_ch21.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="3412" LastActivityDate="2016-05-22T16:37:02.623" CommentCount="5" />
  <row Id="2482" PostTypeId="1" AcceptedAnswerId="2483" CreationDate="2016-05-23T02:54:29.953" Score="8" ViewCount="66" Body="&lt;p&gt;I am trying to implement refraction and transmission in my path tracer and I'm a bit unsure on how to implement it. First, some background:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When light hits a surface, a portion of it will reflect, and a portion will be refracted:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/2oBo9.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/2oBo9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How much light reflects vs. refracts is given by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fresnel_equations&quot;&gt;Fresnel Equations&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/cYrKH.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cYrKH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a recursive ray tracer, the simple implementation would be to shoot a ray for reflection and a ray for refraction, then do a weighted sum using the Fresnel.&#xA;$$\begin{align*}&#xA;R &amp;amp;= Fresnel()\\&#xA;T &amp;amp;= 1 - R\\&#xA;L_{\text{o}} &amp;amp;= R \cdot L_{\text{i,reflection}} + T \cdot L_{\text{i,refraction}}&#xA;\end{align*}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in path tracing, we only choose one path. This is my question:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;How do I choose whether to reflect or refract in a non-biased way&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;My first guess would be to randomly choose based on the Fresnel. Aka:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float p = randf();&#xA;float fresnel = Fresnel();&#xA;if (p &amp;lt;= fresnel) {&#xA;    // Reflect&#xA;} else {&#xA;    // Refract&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Would this be correct? Or do I need to have some kind of correction factor? Since I'm not taking both paths.&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2016-05-24T11:25:59.273" Title="Choosing Reflection or Refraction in Path Tracing" Tags="&lt;brdf&gt;&lt;pathtracing&gt;&lt;reflection&gt;&lt;refraction&gt;&lt;btdf&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2483" PostTypeId="2" ParentId="2482" CreationDate="2016-05-23T08:04:39.000" Score="5" Body="&lt;h2&gt;TL;DR&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Yes, you can do it like that, you just have to divide the result by the probability of choosing the direction.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Full Answer&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The topic of sampling in path tracers allowing materials with both reflection and refraction is actually a little bit more complex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's start with some background first. If you allow BSDFs - not just BRDFs - in your path tracer, you have to integrate over the whole sphere instead of just the positive hemisphere. Monte Carlo samples can be generated by various strategies: for the direct illumination you can use BSDF and light sampling, for the indirect illumination the only meaningful strategy usually is the BSDF sampling. The sampling strategies themselves usually contain the decision about which hemisphere to sample (e.g. whether reflection or refraction is computed).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the simplest version, the light sampling usually doesn't take care much about reflection or refraction. It samples the light sources or the environment map (if present) with respect to the light properties. You can improve sampling of environment maps by picking just the hemisphere in which the material has non-zero contribution, but the rest of the material properties is usually ignored. Note that for and ideally smooth Fresnel material the light sampling doesn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For BSDF sampling, the situation is much more interesting. The case you described deals with an ideal Fresnel surface, where there are only two contributing directions (since Fresnel BSDF is in fact just a sum of two delta functions). You can easily split the integral into a sum of two parts - one reflection and one for refraction. Since, as you mentioned, we don’t want to go in both directions in a path tracer, we have to pick one. This means that we want to estimate the sum of numbers by picking just one of them. This can be done by discrete Monte Carlo estimation: pick one of the addends randomly and divide it by the probability of it being picked. In an ideal case you want to have the sampling probability proportional the the addends, but since we don't know their values (we wouldn't have to estimate the sum if we knew them), we just estimate them by neglecting some of the factors. In this case, we ignore the incoming light amount and use just the Fresnel reflectance/transmittance as our estimates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The BSDF sampling routine for the case of smooth Fresnel surface is, therefore, to pick one of the directions randomly with probability proportional to the the Fresnel reflectance and, at some point, divide the result for that direction by probability of picking the direction. The estimator will look like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\frac&#xA;   {L_{i}\left(\omega_{i}\right)F\left(\theta_{i}\right)}&#xA;   {P\left(\omega_{i}\right)} =&#xA;\frac&#xA;   {L_{i}\left(\omega_{i}\right)F\left(\theta_{i}\right)}&#xA;   {F\left(\theta_{i}\right)} =&#xA;L_{i}\left(\omega_{i}\right)&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\omega_{i}=\left( \phi_{i}, \theta_{i} \right)$ is the chosen incident light direction, $L_{i}\left(\omega_{i}\right)$ is the amount of incident radiance, $F\left(\theta_{i}\right)$ is either the Fresnel reflectance for the reflection case or 1 - Fresnel reflectance for the refraction case, $P\left(\omega_{i}\right)$ is the discrete probability of picking the direction and is equal to $F\left(\theta_{i}\right)$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case of more sophisticated BSDF models like those based on microfacet theory, the sampling is slightly more complex, but the idea of splitting the whole integral into a finite sum of sub-integrals and using discrete Monte Carlo afterwards can usually be applied too.&lt;/p&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-05-24T11:25:59.273" LastActivityDate="2016-05-24T11:25:59.273" CommentCount="11" />
  <row Id="2484" PostTypeId="1" AcceptedAnswerId="2485" CreationDate="2016-05-23T11:32:16.083" Score="7" ViewCount="122" Body="&lt;p&gt;In &lt;a href=&quot;http://alfonse.bitbucket.org/oldtut/Basics/Intro%20Graphics%20and%20Rendering.html&quot;&gt;this tutorial&lt;/a&gt;, &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;It is very often the case that triangles are rendered that share&#xA;  edges. OpenGL offers a guarantee that, so long as the shared edge&#xA;  vertex positions are identical, there will be no sample gaps during&#xA;  scan conversion.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;1 What exactly are sample gaps during scan conversion? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;2 Under what circumstances, will it happen? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;3 Does it happen when the line cross the center of the fragment?&lt;/p&gt;&#xA;" OwnerUserId="3416" LastEditorUserId="209" LastEditDate="2016-05-23T12:15:53.193" LastActivityDate="2016-05-23T15:47:18.867" Title="What are sample gaps during scan conversion?" Tags="&lt;opengl&gt;&lt;rasterizer&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2485" PostTypeId="2" ParentId="2484" CreationDate="2016-05-23T11:55:20.703" Score="5" Body="&lt;p&gt;Sample gaps will occur if you don't do things &quot;properly&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In OpenGL or D3D, assuming a consistent winding order, if you have triangles  ABC and BCD, then when a sample point - that is a test during scan conversion to determine if a point (read pixel) is inside a triangle - lies &lt;em&gt;exactly&lt;/em&gt; on the shared edge BC, then that sample will belong to &lt;em&gt;exactly one&lt;/em&gt; of those two triangles. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This avoids both gaps and double filling. The latter would be problematic with, say, translucency or stencil operations.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-05-23T15:47:18.867" LastActivityDate="2016-05-23T15:47:18.867" CommentCount="1" />
  <row Id="2486" PostTypeId="1" AcceptedAnswerId="2487" CreationDate="2016-05-24T04:25:05.057" Score="8" ViewCount="485" Body="&lt;p&gt;I have been studying hardware corporation GPU profilers in recent days (Qualcomm, PowerVR, Intel).  I've noticed that these tools seem to give more low-level details  than the GPU profilers I have used in the past -- XCode's OpenGL ES frame capture and apitrace -- which only listed which OpenGL calls were made and what the state of current resources are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I get started if I want to make a low-level tool that displays things like sampler cache misses and shader assembler code?&lt;/p&gt;&#xA;" OwnerUserId="3412" LastActivityDate="2016-05-24T05:06:18.593" Title="How to get started writing a low-level GPU profiler?" Tags="&lt;gpu&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="2487" PostTypeId="2" ParentId="2486" CreationDate="2016-05-24T05:06:18.593" Score="8" Body="&lt;p&gt;For basic GPU timing data, you can use &lt;a href=&quot;http://www.reedbeta.com/blog/2011/10/12/gpu-profiling-101/&quot;&gt;D3D timestamp queries&lt;/a&gt; or the equivalent &lt;a href=&quot;https://www.opengl.org/wiki/Query_Object#Timer_queries&quot;&gt;OpenGL timer queries&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any low-level hardware data like cache misses is going to be extremely vendor-specific. Each GPU vendor has its own custom API or extension for giving access to low-level performance data on its hardware. The APIs vary in how they work, and they don't necessarily all expose the same details. The available data may also vary between different chip models within the same vendor, so you probably need to know a bit about how the hardware works to make sense of it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are links to the relevant APIs for most of the main GPU vendors.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AMD: &lt;a href=&quot;http://developer.amd.com/tools-and-sdks/graphics-development/gpuperfapi/&quot;&gt;GPUPerfAPI&lt;/a&gt;; see also &lt;a href=&quot;https://www.opengl.org/registry/specs/AMD/performance_monitor.txt&quot;&gt;AMD_performance_monitor&lt;/a&gt; &lt;/li&gt;&#xA;&lt;li&gt;Intel: &lt;a href=&quot;https://software.intel.com/en-us/articles/intel-performance-counter-monitor&quot;&gt;Performance Counter Monitor&lt;/a&gt; (note: it's not clear to me whether this includes access to GPU counters, or only CPU ones); see also  &lt;a href=&quot;https://www.opengl.org/registry/specs/INTEL/performance_query.txt&quot;&gt;INTEL_performance_query&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;NVIDIA: &lt;a href=&quot;https://developer.nvidia.com/nvidia-perfkit&quot;&gt;PerfKit&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;PowerVR: &lt;a href=&quot;https://community.imgtec.com/developers/powervr/tools/pvrscope/&quot;&gt;PVRScope&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Qualcomm: &lt;a href=&quot;https://www.khronos.org/registry/gles/extensions/QCOM/QCOM_performance_monitor_global_mode.txt&quot;&gt;QCOM_performance_monitor_global_mode&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-24T05:06:18.593" CommentCount="2" />
  <row Id="2488" PostTypeId="1" CreationDate="2016-05-24T08:54:32.497" Score="3" ViewCount="65" Body="&lt;p&gt;I am going through an undocumented function that takes in three points and a Z-value that projects three points on the plane defined by Z-value. I want to understand the mathematical theory behind it and I need your help to decode the function:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SlicerSegment project2D(Point3&amp;amp; p0, Point3&amp;amp; p1, Point3&amp;amp; p2, int32_t z) const&#xA;{&#xA;    SlicerSegment seg;&#xA;    seg.start.X = p0.x + int64_t(p1.x - p0.x) * int64_t(z - p0.z) / int64_t(p1.z - p0.z);&#xA;    seg.start.Y = p0.y + int64_t(p1.y - p0.y) * int64_t(z - p0.z) / int64_t(p1.z - p0.z);&#xA;&#xA;seg.end.X = p0.x + int64_t(p2.x - p0.x) * int64_t(z - p0.z) / int64_t(p2.z - p0.z);&#xA;    seg.end.Y = p0.y + int64_t(p2.y - p0.y) * int64_t(z - p0.z) / int64_t(p2.z - p0.z);&#xA;    return seg;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="231" LastEditDate="2016-05-28T15:11:04.637" LastActivityDate="2016-05-28T15:11:04.637" Title="2D projection from some points" Tags="&lt;projections&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2489" PostTypeId="1" AcceptedAnswerId="2519" CreationDate="2016-05-24T10:32:27.893" Score="8" ViewCount="116" Body="&lt;p&gt;I'm trying to implement a microfacet BRDF in my raytracer but I'm running into some issues. A lot of the papers and articles I've read define the partial geometry term as a function of the view and half vectors: G1(v, h). However, when implementing this I got the following result: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/VGIdx.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/VGIdx.jpg&quot; alt=&quot;GGX Geometry term using the half vector&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(Bottom row is dielectric with roughness 1.0 - 0.0, Top row is metallic with roughness 1.0 - 0.0)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There's a weird highlight around the edges and a cut-off around n.l == 0. I couldn't really figure out where this comes from. I'm using Unity as a reference to check my renders so I checked their shader source to see what they use and from what I can tell their geometry term is not parametrized by the half vector at all! So I tried the same code but used to macro surface normal instead of the half vector and got the following result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/bnVxb.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/bnVxb.jpg&quot; alt=&quot;GGX geometry term using the macro surface normal&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To my untrained eye this seems way closer to the desired result. But I have the feeling this is not correct? The majority of the articles I read use the half vector but not all of them. Is there a reason for this difference?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use the following code as my geometry term:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float RayTracer::GeometryGGX(const Vector3&amp;amp; v, const Vector3&amp;amp; l, const Vector3&amp;amp; n, const Vector3&amp;amp; h, float a)&#xA;{&#xA;    return G1GGX(v, h, a) * G1GGX(l, h, a);&#xA;}&#xA;&#xA;float RayTracer::G1GGX(const Vector3&amp;amp; v, const Vector3&amp;amp; h, float a)&#xA;{&#xA;    float NoV = Util::Clamp01(cml::dot(v, h));&#xA;    float a2 = a * a;&#xA;&#xA;    return (2.0f * NoV) / std::max(NoV + sqrt(a2 + (1.0f - a2) * NoV * NoV), 1e-7f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And for reference, this is my normal distribution function:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float RayTracer::DistributionGGX(const Vector3&amp;amp; n, const Vector3&amp;amp; h, float alpha)&#xA;{&#xA;    float alpha2 = alpha * alpha;&#xA;    float NoH = Util::Clamp01(cml::dot(n, h));&#xA;    float denom = (NoH * NoH * (alpha2 - 1.0f)) + 1.0f;&#xA;    return alpha2 / std::max((float)PI * denom * denom, 1e-7f);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3424" LastActivityDate="2016-05-30T21:35:41.830" Title="Correct form of the GGX geometry term" Tags="&lt;shader&gt;&lt;brdf&gt;&lt;pbr&gt;&lt;microfacet&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="2490" PostTypeId="1" CreationDate="2016-05-24T12:40:05.170" Score="3" ViewCount="27" Body="&lt;p&gt;I am going through some ideas that slice a 3D model and fill the area of the resulting polygon contour . Currently they are using the following concepts as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Every resulting line of the polygon after the slicing operation has a parent triangle, as the original model is made of triangle .&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Each of the triangle has a face normal.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;During scan conversion, when a scanline at a particular height intersect any of the polygon contour line, the normal information of the parent triangle is utilized to find out if the intersection is happening while getting into the contour or getting out of the contour. All these information are used to generate the region that are to be filled.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Then there is another way called the even-odd filling rule. I do not think it is necessary for me to describe it here. I believe that all of you are very well aware of it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is which one of it is better option to go with in terms of correctness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-05-24T12:40:05.170" Title="Polygon Filling rule - even odd OR orientation of the normal" Tags="&lt;rasterizer&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2491" PostTypeId="2" ParentId="2488" CreationDate="2016-05-24T19:11:52.377" Score="2" Body="&lt;p&gt;The function calculates the intersection points between two edges of a triangle and a plane that is assumed to cut it. It returns a line segment running along the cut.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or more generally, the function calculates where the intersection points with the two edges would be, if the edges were extended to infinite lines. Given that it's called &quot;project&quot;, that might be the intended use-case; hard to know without seeing how this gets used in context.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first two lines of math are calculating the intersection point of the $p_0 p_1$ edge with the z-plane. It works by linearly interpolating the $x, y$ values along the edge based on the proportion between $z$ values. The second two lines do the same thing for the $p_0 p_2$ edge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;One odd thing is that the function is using integers everywhere. Presumably it's because the program is using fixed-point coordinates (which makes good sense for a lot of geometry processing applications due to its uniform precision). However, the way it's done here seems oblivious to rounding: the integer divisions will truncate any fractional component instead of rounding to nearest.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-24T19:11:52.377" CommentCount="0" />
  <row Id="2492" PostTypeId="2" ParentId="2488" CreationDate="2016-05-25T08:04:54.230" Score="1" Body="&lt;p&gt;Let us look at the first line in detail:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;int64_t(p1.x - p0.x)&lt;/code&gt; is the distance between p1 and p0 in the x (also y later) direction lets call this &lt;code&gt;dx&lt;/code&gt; for later discussion.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;int64_t(z - p0.z)&lt;/code&gt; is the distance from z and p0 for simplicity lets us call this &lt;code&gt;a&lt;/code&gt; later.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;int64_t(p1.z - p0.z)&lt;/code&gt; is the distance from p0 and p1 or vector length simplicity lets us call this &lt;code&gt;len&lt;/code&gt; later.&lt;/li&gt;&#xA;&lt;li&gt;lets look at &lt;code&gt;a/len&lt;/code&gt; well that is simply the length of the ratio of the each point to 0. This equals to how much the vector p1-p0 must be scaled to reach the plane intersection. &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Finally the entire expression is can be read as follows:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;vector p0 + vector p0-1 * length to intersection&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;And visually its as follows:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iYnJb.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iYnJb.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: The vectors in the calculation, (blue lines sow offsets and projections for feeling of 3D)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And the entire function gives the same for the two edges of your triangle. Please note that it is perfectly possible that the result is degenerate as the choice of what edges to use is not checked for so one edge MIGHT be parallel to the plane. Also selection of points to send changes what is being projected.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ZqBRW.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZqBRW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: input blue triangle as 3 points (black), return value projected segment in orange on plane.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-05-26T18:38:36.873" LastActivityDate="2016-05-26T18:38:36.873" CommentCount="2" />
  <row Id="2493" PostTypeId="1" CreationDate="2016-05-25T10:19:16.237" Score="3" ViewCount="18" Body="&lt;p&gt;I have a question about finding the symmetry plane that separate the left and the right halves of a point cloud. We scanned an object, say a human face, with a 3D scanner, such as Kinect, which gives a 3D point cloud. Now we want to find the plane that segments the face into symmetrical left and right halves. I found a paper like &lt;a href=&quot;https://hal.inria.fr/file/index/docid/331758/filename/cvpr2008CombesFinal.pdf&quot; rel=&quot;nofollow&quot;&gt;https://hal.inria.fr/file/index/docid/331758/filename/cvpr2008CombesFinal.pdf&lt;/a&gt;, but could not find any code. Though I found the C++ code &quot;CLARCS&quot; doing this work, but it is no longer available. Thank you.&lt;/p&gt;&#xA;" OwnerUserId="3426" LastEditorUserId="3426" LastEditDate="2016-05-25T10:26:46.230" LastActivityDate="2016-05-25T10:26:46.230" Title="Symmetry plane estimation for 3D point cloud" Tags="&lt;3d&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2494" PostTypeId="1" CreationDate="2016-05-25T13:24:03.887" Score="6" ViewCount="54" Body="&lt;p&gt;The well known &lt;a href=&quot;https://en.wikipedia.org/wiki/Schlick&amp;#39;s_approximation&quot;&gt;Schlick approximation&lt;/a&gt; of the Fresnel coefficient gives the equation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$F=F_0+(1 - F_0)(1 - cos(\theta))^5$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And $cos(\theta)$ is equal to the dot product of the surface normal vector and the view vector.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is still unclear to me though if we should use the actual surface normal $N$ or the half vector $H$. Which should be used in a physically based BRDF and why?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, as far as I understand the Fresnel coefficient gives the probability of a given ray to either get reflected or refracted. So I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This observation would tend to make me think this is where $H$ would come, but it is not obvious to me that the Fresnel of a representative normal is equivalent to integrating the Fresnel of all the actual normals.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-05-25T22:43:42.677" Title="In a physically based BRDF, what vector should be used to compute the Fresnel coefficient?" Tags="&lt;brdf&gt;&lt;pbr&gt;&lt;integral&gt;&lt;fresnel&gt;&lt;schlick&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2495" PostTypeId="2" ParentId="2494" CreationDate="2016-05-25T14:42:49.120" Score="5" Body="&lt;p&gt;In Schlick's 1994 paper, &lt;a href=&quot;http://www.cs.virginia.edu/~jdl/bib/appearance/analytic%20models/schlick94b.pdf&quot; rel=&quot;nofollow&quot;&gt;&quot;An Inexpensive Model for Physically-Based Rendering&quot;&lt;/a&gt;, where they derive the approximation, the formula is:&#xA;$$F_{\lambda}(u) = f_{\lambda} + (1 - f_{\lambda})(1 - u)^{5}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where &lt;a href=&quot;http://i.stack.imgur.com/xxyc2.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/xxyc2.png&quot; alt=&quot;Description of vectors&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, to answer your first question, $\theta$ refers to the angle between the view vector and the half vector. Consider for a minute that the surface is a perfect mirror. So:&#xA;$$V \equiv reflect(V')$$&#xA;In this case:&#xA;$$N \equiv H$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For microfacet-base BRDFs, the $D(h_{r})$ term refers to the statistical percentage of microfacet normals that are oriented towards $H$. Aka, what percentage of the incoming light will bounce in the outgoing direction.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for why we use Fresnel in a BRDF, it has to do with the fact that a BRDF by itself is only a portion of the full BSDF. A BRDF attenuates the reflected portion of light and a BTDF attenuates the refracted. We use the Fresnel to calculate the amount of reflected vs. refracted light, so we can properly attenuate it with the BRDF and BTDF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$BSDF = BRDF + BSDF\\&#xA;$$&#xA;$$\begin{align*}&#xA;L_{\text{o}}(p, \omega_{\text{o}}) &amp;amp;=  L_{e}(p, \omega_{\text{o}}) \ + \  \int_{\Omega} BSDF * L_{\text{i}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}} \\&#xA; &amp;amp;= L_{e}(p, \omega_{\text{o}}) \ + \  \int_{\Omega} BRDF * L_{\text{i, reflected}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}}  \ + \  \int_{\Omega} BTDF * L_{\text{i, refracted}}(p, \omega_{\text{i}}) * \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}}&#xA;\end{align*}$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, in summary, we use $D$ to get the percentage of light that will bounce in the outgoing direction, and $F$, to find out what percentage of the remaining light will reflect/refract. Both these use $H$, because that is the surface orientation that allows a mirror reflection between $V$ and $V'$&lt;/p&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-05-25T16:21:14.240" LastActivityDate="2016-05-25T16:21:14.240" CommentCount="1" />
  <row Id="2496" PostTypeId="1" CreationDate="2016-05-25T15:27:13.490" Score="3" ViewCount="60" Body="&lt;p&gt;I was reading a paper on BRDF. I've come across this formula :&#xA;$$ f(\omega_i, \omega_o ) = \frac{FDG}{4(N.V)(N.L)}$$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The (N.L) term can be cancelled by the cosine term which appears in the rendering equation :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ L_o = \int f(\omega_i, \omega_o) L_i cos(\theta) d\omega_i $$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What about the (N.V) term ? What happens if (N.V) = 0 ?&lt;/p&gt;&#xA;" OwnerUserId="3339" LastActivityDate="2016-05-25T17:08:37.117" Title="What is the effect of the BRDF denominator?" Tags="&lt;raytracing&gt;&lt;color&gt;&lt;brdf&gt;&lt;physics&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2497" PostTypeId="2" ParentId="2424" CreationDate="2016-05-25T15:38:02.133" Score="1" Body="&lt;p&gt;Thanks for your answers. That was helpfull.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is how I understand the 1/r² term for point source (tell me if I'm wrong).&#xA;Let's take the BRDF definition :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ L_o = \int f(\omega, \omega_o) \, dE$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, we have to answer this question : How is the Irradiance E distributed ?&#xA;For one point source, we have : $$ dE = \delta(\omega_i-\omega)E \, d\omega $$&#xA;The irradiance is only comming from one direction (the point source). Therfore, we can simplify the equation :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$ L_o = \int f(\omega, \omega_o) \delta(\omega_i-\omega)E \, d\omega = f(\omega_i, \omega_o) E$$ &#xA;We can use the relation between the intensity I of the point source and E $$ E = cos(\theta_i)I/r^2 $$&#xA;Finally : $$ L_o = f(\omega_i, \omega_o) cos(\theta_i)I/r^2 $$&lt;/p&gt;&#xA;" OwnerUserId="3339" LastEditorUserId="48" LastEditDate="2016-05-25T18:45:30.017" LastActivityDate="2016-05-25T18:45:30.017" CommentCount="0" />
  <row Id="2498" PostTypeId="2" ParentId="2496" CreationDate="2016-05-25T16:05:41.587" Score="0" Body="&lt;p&gt;Don't forget that at the end this is to be integrated in some cone (e.g. pixel footprint).&#xA;Then the visible cross section of differential surface dN is also be $(N\cdot V)$.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-05-25T16:05:41.587" CommentCount="3" />
  <row Id="2499" PostTypeId="2" ParentId="2496" CreationDate="2016-05-25T17:08:37.117" Score="3" Body="&lt;p&gt;Firstly, I highly suggest reading Eric Heitz's paper &quot;&lt;a href=&quot;https://hal.inria.fr/hal-00942452v2/document&quot; rel=&quot;nofollow&quot;&gt;Understanding the Masking-Shadowing Function in Microfacet-Based BRDFs&lt;/a&gt;&quot;, which covers the full derivation of microfacet-based BRDFs. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The $\frac{1}{4(N \cdot V)(N \cdot L)}$ term is a side effect of the derivation of the BRDF for specular microfacets. Specifically, it comes from the Jacobian of the reflection transformation. See the paper and/or &lt;a href=&quot;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&quot; rel=&quot;nofollow&quot;&gt;Walter et. al's 2007 paper&lt;/a&gt; for more details.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As for your concern for a divide by zero, the definition of the rendering equation prevents it. Let me explain:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For $(N \cdot V)$ to equal zero, they must be orthogonal. In this case, the visibility term of the rendering equation will cull the cases at / below the horizon.&lt;/p&gt;&#xA;" OwnerUserId="310" LastActivityDate="2016-05-25T17:08:37.117" CommentCount="5" />
  <row Id="2500" PostTypeId="1" CreationDate="2016-05-25T18:49:13.150" Score="1" ViewCount="32" Body="&lt;p&gt;Ok this is hard to explain but I am going to try, then I will explain it as pure mathematics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Look at this image.&#xA;&lt;a href=&quot;http://i.stack.imgur.com/jBASF.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/jBASF.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;So basically this is a moving line, each frame a new segment is added and the previous frame is faded by an amount decided by fps. (their are dots to make things neat but im not showing them for simplicity).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So based on the fps a number is calculated (0.05 if perfect fps) and as a result the previous frame gets a black plane drawn over it with an opacity of 0.05.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see this causes an issue because of the segments. My solution is to give each vertex a color, however I dont know how to calculate that color.  The line color is defined by hsv, and the fading is decided by rgb.  Furthermore I am pretty sure fading with the transparent plane creates an exponential effect, however HSV.V works linearly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is if I know on a given frame that the fading plane will have an opacity of 0.05 how can I calculate an equivalent HSV color to what the previous frame will be after fading? That way the bottom two vertices can be a darker color and interpolation can fix it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks much!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS The formula that decides the opacity of the black is as follows. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;(dt / (1 / 60)) * 0.5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="2308" LastEditDate="2016-05-25T19:15:22.323" LastActivityDate="2016-05-30T16:47:25.703" Title="Matching a hsv color with a blended color" Tags="&lt;opengl&gt;&lt;color&gt;&lt;color-science&gt;" AnswerCount="2" CommentCount="4" />
  <row Id="2501" PostTypeId="1" CreationDate="2016-05-25T20:40:55.557" Score="2" ViewCount="41" Body="&lt;p&gt;Let's assume I have the following cube.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/zyfHW.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/zyfHW.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume the isovalue = 0. I would like to draw the resulting triangles of the isosurface.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that first I define which values are inside or outside comparing them to the given isovalue and after that we correspond to a particular list of edges.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After that, we have somehow to linearly interpolate the points.&#xA;I have read on an article that using this formula $P = P_1 + \frac{(iso -V_1)(P_2-P_1)}{V_2-V_1}$ we can get the interpolated values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone demonstrate to me how I can interpolate the points in this case ? &lt;/p&gt;&#xA;" OwnerUserId="2214" LastEditorUserId="2214" LastEditDate="2016-05-27T08:09:33.930" LastActivityDate="2016-05-27T08:44:03.607" Title="Linear interpolation on Plane (Marching Cubes)" Tags="&lt;algorithm&gt;&lt;interpolation&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="2502" PostTypeId="2" ParentId="2500" CreationDate="2016-05-25T20:55:50.740" Score="1" Body="&lt;p&gt;The opacity acts linearly through the colors, i.e. it affects R G B similarily, so it should affects the value V the same way.&#xA;You just want to have an intensity factor decay as $0.95^n$.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-05-25T20:55:50.740" CommentCount="2" />
  <row Id="2503" PostTypeId="2" ParentId="2501" CreationDate="2016-05-25T21:01:16.457" Score="1" Body="&lt;p&gt;You can use this formula on each edge where one point is &amp;lt; isovalue and the other is &gt; .&#xA;Then you link together the points belonging to the same square to form the edges of your iso-mesh. &#xA;Then you make polygons by linking edges sharing a vertex. In case of more than 3 sides (could be up to 6), it's up to you to prefer spliting the polygon into triangles.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-05-25T21:01:16.457" CommentCount="4" />
  <row Id="2504" PostTypeId="1" CreationDate="2016-05-25T21:29:14.497" Score="0" ViewCount="24" Body="&lt;p&gt;I am aware about the modern GPUs are not coming with a fixed pipeline processing triangles as DirectX9 fashion.&#xA;However, is still useful to get an idea about how many vertices or triangles can be clipped, rasterized with 1 light (using per vertex lighting) and simple goraud model and 1 texture mapped.&#xA;I dont want to enforce this scenario but some scenario similar can give us a good insight of the performance on similar situations.&#xA;This info is very useful to address performance considerations on software development.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone knows about a website or technical report comparing different models in this way?&lt;/p&gt;&#xA;" OwnerUserId="3430" LastActivityDate="2016-05-25T21:29:14.497" Title="Where to find number about triangles per second?" Tags="&lt;performance&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="2505" PostTypeId="2" ParentId="2494" CreationDate="2016-05-25T22:43:42.677" Score="3" Body="&lt;p&gt;The Fresnel coefficient should be evaluated using $H$, not $N$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You wrote,&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It's not. The BRDF in itself does not approximate the integral over all the hemisphere. The rendering equation does that: you integrate over all incoming light directions, but each time the BRDF inside the integral is evaluated, it's for &lt;em&gt;one specific choice&lt;/em&gt; of incoming and outgoing ray directions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For microfacet BRDFs, the usual simplifying assumption is that individual microfacets are perfect specular reflectors. Then, given the $L$ and $V$ at which to evaluate, the &lt;em&gt;only&lt;/em&gt; microfacets that can contribute are those that are aligned along $H = \text{normalize}(L+V)$, because that's the only way they can reflect light from the incoming to the outgoing ray.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The normal distribution function and the visibility factor in the BRDF together approximate the density of microfacets oriented along $H$ that are visible from both the $L$ and $V$ directions. The Fresnel factor is evaluated &lt;em&gt;for those microfacets&lt;/em&gt;, so the correct angle to use is the one between $L$ and $H$, or equivalently $V$ and $H$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are a couple cases where this argument gets modified. One is if the microfacet model assumes something other than perfect specularity. For instance, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Oren%E2%80%93Nayar_reflectance_model&quot; rel=&quot;nofollow&quot;&gt;Oren-Nayar BRDF&lt;/a&gt; assumes Lambertian microfacets. In this case the BRDF has to incorporate some kind of integral over all the possible microfacet orientations that can scatter light from $L$ to $V$. Then the BRDF won't have a standard Fresnel factor at all; it'll have some other formula that approximates the result of integrating the Fresnel factor over the normal hemisphere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The other case that comes up in real-time graphics is the reflection from an environment map. To be really correct, we should integrate the environment map multiplied by the BRDF over all incoming light directions, but in practice we often sample a prefiltered environment map using the dominant reflection vector $R = \text{reflect}(V, N)$ and then multiply it by some approximate Fresnel formula that depends on the angle between $R$ and $N$ (equivalently $V$ and $N$), as well as the surface roughness. This is very much an approximation, but often good enough for real-time use.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-05-25T22:43:42.677" CommentCount="0" />
  <row Id="2506" PostTypeId="1" CreationDate="2016-05-26T08:59:25.520" Score="3" ViewCount="156" Body="&lt;p&gt;Let's assume that in a triangle we have computed the brightness on the vertices of a triangle and we have found that it is maximum. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can any other point which lies on that triangle have more brightness than the maximum brightness on those vertices ?&lt;/p&gt;&#xA;" OwnerUserId="2214" LastActivityDate="2016-05-28T14:58:48.403" Title="Surface shading on an triangle" Tags="&lt;shading&gt;&lt;brightness&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="2507" PostTypeId="2" ParentId="2506" CreationDate="2016-05-26T09:09:22.663" Score="5" Body="&lt;p&gt;That completely depends on how you are computing the shading. If you are then just linearly interpolating the shaded colours across the triangle, i.e. &lt;a href=&quot;https://en.wikipedia.org/wiki/Gouraud_shading&quot;&gt;Gouraud shading&lt;/a&gt; then, clearly, the answer is &quot;no&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, if you are doing per-pixel shading by, say, interpolating normals and light directions, then you can easily get a brighter area away from the vertices.&lt;/p&gt;&#xA;" OwnerUserId="209" LastActivityDate="2016-05-26T09:09:22.663" CommentCount="4" />
  <row Id="2508" PostTypeId="1" AcceptedAnswerId="2509" CreationDate="2016-05-26T11:39:20.120" Score="4" ViewCount="121" Body="&lt;p&gt;I'm working with &lt;code&gt;DirectCompute&lt;/code&gt;, but this question can be applied to general gpu programming I suppose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As far as I know the threads in a group works in a lockstep. It means that every instruction for every thread executes at the same time, right? But what if one thread out of &lt;code&gt;1024&lt;/code&gt; entered &lt;code&gt;if/else&lt;/code&gt; condition? All other &lt;code&gt;1023&lt;/code&gt; will just wait or lockstep condition will be violated?&lt;/p&gt;&#xA;" OwnerUserId="386" LastActivityDate="2016-05-26T12:02:55.647" Title="Threads lockstep and conditions in compute shader" Tags="&lt;compute-shader&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2509" PostTypeId="2" ParentId="2508" CreationDate="2016-05-26T12:02:55.647" Score="6" Body="&lt;p&gt;Not all threads will execute in lockstep but they are split into groups whose threads are locked to each other.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This means that if only 1 thread out of all threads enters a branch then only 1 group will need to enter that branch while all the others will skip it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In that group that has to execute both branches it will actually execute both branches but threads will throw away the result of the branch that it didn't need to go into.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For more information &lt;a href=&quot;http://computergraphics.stackexchange.com/q/280/137&quot;&gt;see this question&lt;/a&gt;.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-05-26T12:02:55.647" CommentCount="6" />
  <row Id="2510" PostTypeId="1" CreationDate="2016-05-26T13:04:36.520" Score="4" ViewCount="50" Body="&lt;p&gt;I am going through the Vatti Arbitrary Polygon Clipping Algorithm, but got stuck at their very initial explanation. It says the following:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Call an edge of a polygon a left or right edge if the interior of the polygon is to the right or left, respectively. Horizontal edges are considered to be both left and right edges. A key fact that is used by the Vatti algorithm is that polygons can be represented via a set of left and right bounds which are connected lists of left and right edges, respectively, that come in pairs. Each of these bounds starts at a local minimum of the polygon and ends at a local maximum. Consider the &quot;polygon&quot; with vertices $\{p_0, p_1, ..., p_8\}$ shown in Figure 1(a). The two left bounds have vertices $\{p_0, p_8, p_7, p_6\}$ and $\{p_4, p_3, p_2\}$, respectively. The two right bounds have vertices $\{p_0, p_1, p_2\}$ and $\{p_4, p_5, p_6\}$.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The image it refers to is:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://imgur.com/cWpcjyX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://imgur.com/cWpcjyX.png&quot; alt=&quot;Image showing polygon vertices&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand the vertices of the second left bound $\{p_4,p_3,p_2\}$. How are these vertices derived? &lt;/p&gt;&#xA;" OwnerUserId="2712" LastEditorUserId="231" LastEditDate="2016-05-26T13:43:45.090" LastActivityDate="2016-06-01T09:34:21.397" Title="Explanation of the Vatti clipping algorithm" Tags="&lt;algorithm&gt;&lt;polygon&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2512" PostTypeId="2" ParentId="2501" CreationDate="2016-05-27T08:35:44.453" Score="1" Body="&lt;p&gt;P1 and P2 in this case represent the actual &lt;em&gt;coordinates&lt;/em&gt; of the vertices of the cube in your output mesh. These might be the same as the input coordinates of the function you're using, or they might not be, it's entirely up to you how you output them. A simple way to map them would be to just use integer coords, biased by half the resolution of your sample volume - so a mesh spanning 100^3 cubes would have coordinates from -50 to 50 on each axis.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in the formula you describe, (iso - V1) / (V2 - V1) gives you the amount to interpolate by, and (P2 - P1) is a vector representing a single cube lattice edge along the axis you want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's assume we're using unit sized cubes, +Y is up, and +Z is 'into' the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your cube above, you will have crossings on the three edges coming from the left-bottom-front corner. On the X and Z edges you will interpolate by 0.25 : (0 - (-1)) / (3 - (-1)) , and on the Y edge you will interpolate by 0.5.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if the left-bottom-front corner is at the origin, your three mesh vertices will be at &#xA;&amp;lt;0.25, 0, 0&gt; , &#xA;&amp;lt;0, 0.5, 0&gt; and &#xA;&amp;lt;0, 0, 0.25&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Apply the same formula to the right-top-back corner and you get &amp;lt;0.5, 1, 1&gt;, &amp;lt;1, 0.5, 1&gt; and &amp;lt;1, 1, 0.5&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only difference is the left-bottom-front corner values will be P1 and V1 in your formula as it is at the 'negative' end of the edges to be interpolated, and the right-top-back values will be P2 and V2 as it is at the 'positive' end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope that all makes sense.&lt;/p&gt;&#xA;" OwnerUserId="1937" LastEditorUserId="1937" LastEditDate="2016-05-27T08:44:03.607" LastActivityDate="2016-05-27T08:44:03.607" CommentCount="2" />
  <row Id="2514" PostTypeId="2" ParentId="2506" CreationDate="2016-05-27T19:44:48.500" Score="3" Body="&lt;p&gt;Assuming you have almost any lighting model at all:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You cannot bound the view irradiance (light reflected from a surface location towards the eye/camera) by computing the irradiance at the vertices and bounding those, even if the surface is completely diffuse Lambertian and has no view-angle term.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Imagine a large floor triangle, and a point lighting near the center of the triangle, close to the surface.  The center of the triangle is very bright, while the vertices - which are far away from the light source - are dark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Alternatively, if you are just asking if a linearly interpolated quantity can exceed the value at any/all of the end points - then the answer is simply 'no' - your bounds are accurate. The key issue here is that irradiance is a non-linear function over the surface, but interpolation is linear&lt;/p&gt;&#xA;" OwnerUserId="3386" LastActivityDate="2016-05-27T19:44:48.500" CommentCount="2" />
  <row Id="2515" PostTypeId="2" ParentId="2506" CreationDate="2016-05-27T21:04:34.613" Score="1" Body="&lt;p&gt;I am supplementing the answer of others by adding a picture showing a Blinn and Lambert material applied to a triangle (the triangle has normals in different directions at the corners to show the difference in shading). There is one distant directional light in the scene.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/8Ly97.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/8Ly97.png&quot; alt=&quot;Lighting on a triangle lit by directional light&quot;&gt;&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: Image shows lighting on a triangle lit by directional light.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;By comparing the samples in image 1 we can see that the highest color value can be somewhere in the center of the triangle. Which shows that the assumption in your question is generally invalid. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So &lt;strong&gt;no&lt;/strong&gt; you can not generally assume that the corners are the maximum of a triangle. A shader with this property can be constructed but quite a few shaders have this property naturally.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="231" LastEditDate="2016-05-28T14:58:48.403" LastActivityDate="2016-05-28T14:58:48.403" CommentCount="4" />
  <row Id="2516" PostTypeId="1" CreationDate="2016-05-28T07:08:27.267" Score="0" ViewCount="72" Body="&lt;p&gt;I am writing a software and I need to represent a graph in a orthogonal manner from topological data (vector of edges,vertices and their connectivity data)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Graphs&lt;/strong&gt; consist of a set of &lt;strong&gt;vertices&lt;/strong&gt; and a set of &lt;strong&gt;edges&lt;/strong&gt;, each connecting two vertices.A vertex may have any number of connected edges so it makes the problem a lot more complicated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have read some articles and its looks like that the &lt;strong&gt;Kandinsky model&lt;/strong&gt; is the post popular one. However I just don't know the algorithm, any other solution (algorithm) that solve the problem is also very welcome.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Added after edit&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following picture shows a real world example for an electricity network which should be considered as raw data, in order to create a graph from this network, some preliminary tasks must be done. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input Data:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/pJofq.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/pJofq.jpg&quot; alt=&quot;Input Data&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The result I am looking for is something like below, &#xA;there are some characteristics if you take a look much closer : &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Red Polygon in the middle of the above picture(input data) represents an electricity substation which is a node itself and can be connected to more than 4 edges.&#xA;There are more red polygons but only one can be fit into the above picture however as you may see, the following picture could cover much more than a red polygon that means it can map a bigger area, so the following picture is much more denser.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In Schematics diagrams, red polygons (Substations) usually maintain their position against each other relatively so if we manage to see beyond the extents of the above map by zooming out for example, we should almost see a triangle that is obviously can be seen at the below while having the left one at the left, the down one at the down ..... (this is not a rule, but I thought it could be a head start for desired algorithm)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Orthogonal Diagram:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/5gffC.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/5gffC.png&quot; alt=&quot;Orthogonal Schematic&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="537" LastEditorUserId="537" LastEditDate="2016-06-04T21:39:13.460" LastActivityDate="2016-06-05T05:38:24.993" Title="How to represent an orthogonal graph from its edges and vertices data?" Tags="&lt;algorithm&gt;&lt;vector-graphics&gt;&lt;orthogonal&gt;" AnswerCount="1" CommentCount="9" FavoriteCount="1" />
  <row Id="2518" PostTypeId="1" CreationDate="2016-05-28T13:51:32.880" Score="3" ViewCount="62" Body="&lt;p&gt;I would like to rotate a raster image by an arbitrary angle. I don't really care for speed: the rotation should be of highest quality possible. Could someone please suggest a suitable algorithm?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm familiar with the &lt;a href=&quot;http://www.leptonica.com/rotation.html#ROTATION-BY-SHEAR&quot; rel=&quot;nofollow&quot;&gt;rotation by three shears&lt;/a&gt; but I'm not sure if the shears would not cause too much aliasing/blurring.&lt;/p&gt;&#xA;" OwnerUserId="141" LastActivityDate="2016-06-04T11:55:24.110" Title="Image rotation algorithm" Tags="&lt;algorithm&gt;&lt;transformations&gt;&lt;image-processing&gt;&lt;2d&gt;&lt;raster-image&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2519" PostTypeId="2" ParentId="2489" CreationDate="2016-05-28T14:49:53.177" Score="5" Body="&lt;p&gt;TL;DR: Your $G1$ formula is wrong. &lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Just to avoid confusion, I am assuming the isotropic version of the BRDF, the Smith microfacet model (as opposed to the V-cavity model), and the GGX microfacet distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;https://hal.inria.fr/hal-00996995&quot; rel=&quot;nofollow&quot;&gt;Heitz 2014&lt;/a&gt;, the masking/shadowing term $G1$ is&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\chi^{+}\left(\omega_{v}\cdot\omega_{m}\right)&#xA;\frac{2}{1+\sqrt{1+\alpha_{o}^{2}\tan^{2}\theta_{v}}}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and according to &lt;a href=&quot;http://dx.doi.org/10.2312/EGWR/EGSR07/195-206&quot; rel=&quot;nofollow&quot;&gt;Walter 2007&lt;/a&gt;, the formula is&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$&#xA;\chi^{+}\left(\frac{\omega_{v}\cdot\omega_{g}}{\omega_{v}\cdot\omega_{m}}\right)\frac{2}{1+\sqrt{1+\alpha^{2}\tan^{2}\theta_{v}}}&#xA;$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where $\omega_{m}$ is the microfacet normal direction (halfway vector), $\omega_{g}$ is the main (geometric) normal direction (normal), $\omega_{v}$ is the incoming or outgoing direction, $\alpha$ is the isotropic roughness parameter, and $\chi^{+}\left(a\right)$ is the positive characteristic function, or the Heaviside step function (equals one if $a&amp;gt;0$ and zero otherwise).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can notice, the half vector $\omega_{m}$ is used only to make sure the $G1$ is zero if the geometrical configuration is forbidden. More precisely, it makes sure that the back surface of the microsurface is never visible from the $\omega_{v}$ direction on the front side of the macrosurface and vice versa (the latter case is meaningful only when also refractions are supported). If the calling code guarantees this, then you can obviously omit this parameter. That is probably the reason why they did so in Unity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Your implementation, on the other hand, uses the half vector to compute the cosine of the $\omega_{v}$ direction with respect to the microfacet, which leads to computation of something else than the presented formulae.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If it is of any help, then this is my implementation of the $G1$ factor:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float SmithMaskingFunctionGgx(&#xA;    const Vec3f &amp;amp;aDir,  // the direction to compute masking for (either incoming or outgoing)&#xA;    const Vec3f &amp;amp;aMicrofacetNormal,&#xA;    const float  aRoughnessAlpha)&#xA;{&#xA;    PG3_ASSERT_VEC3F_NORMALIZED(aDir);&#xA;    PG3_ASSERT_VEC3F_NORMALIZED(aMicrofacetNormal);&#xA;    PG3_ASSERT_FLOAT_NONNEGATIVE(aRoughnessAlpha);&#xA;&#xA;    if (aMicrofacetNormal.z &amp;lt;= 0)&#xA;        return 0.0f;&#xA;&#xA;    const float cosThetaVM = Dot(aDir, aMicrofacetNormal);&#xA;    if ((aDir.z * cosThetaVM) &amp;lt; 0.0f)&#xA;        return 0.0f; // up direction is below microfacet or vice versa&#xA;&#xA;    const float roughnessAlphaSqr = aRoughnessAlpha * aRoughnessAlpha;&#xA;    const float tanThetaSqr = Geom::TanThetaSqr(aDir);&#xA;    const float root = std::sqrt(1.0f + roughnessAlphaSqr * tanThetaSqr);&#xA;&#xA;    const float result = 2.0f / (1.0f + root);&#xA;&#xA;    PG3_ASSERT_FLOAT_IN_RANGE(result, 0.0f, 1.0f);&#xA;&#xA;    return result;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-05-30T21:35:41.830" LastActivityDate="2016-05-30T21:35:41.830" CommentCount="6" />
  <row Id="2521" PostTypeId="1" AcceptedAnswerId="2524" CreationDate="2016-05-28T22:01:49.243" Score="3" ViewCount="61" Body="&lt;p&gt;I know how to draw lines with DDA but i want to learn more efficient way and google suggest Bresenham's line drawing algorithm is better than DDA&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my implementation :- &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    int x0 = Math.round(m_start.getPosition().getX());&#xA;    int y0 = Math.round(m_start.getPosition().getY());&#xA;    int x1 = Math.round(m_end.getPosition().getX()); &#xA;    int y1 = Math.round(m_end.getPosition().getY()); &#xA;&#xA;    int dx = x1 - x0;&#xA;    int dy = y1 - y0;&#xA;&#xA;    int parameter = 2 * dy -  dx;&#xA;&#xA;     Point k = new Point(new Vector2D(x0, y0), m_display, PixelData.white());&#xA;     k.DrawPoint();&#xA;&#xA;    for(int i = x0 + 1; i &amp;lt; x1; i++){   &#xA;       if(parameter &amp;lt; 0){&#xA;           y0 += 1;&#xA;           Point p = new Point(new Vector2D(i, y0), m_display, PixelData.white());&#xA;           p.DrawPoint();&#xA;&#xA;           parameter += 2 * dy;&#xA;       }else{&#xA;           y0 += 1;&#xA;           Point p = new Point(new Vector2D(i, y0 + 1), m_display, PixelData.white());&#xA;           p.DrawPoint();&#xA;&#xA;           parameter += 2 * dy - 2 * dx;&#xA;       }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and here is the output : -&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/BT5tA.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/BT5tA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;as you can see the line is not ending at the desired coordinate, that small dot is where it should end. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I checked the slope, dx and dy.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    System.out.println(dx + &quot; &quot; + dy + &quot; &quot; + (dy/(float)dx));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output : -&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    100 50 0.5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;the slope is below tan (pi/4).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help me find the bug. &lt;/p&gt;&#xA;" OwnerUserId="3437" LastActivityDate="2016-05-29T07:12:32.357" Title="Bug with the Bresenham's line drawing algorithm?" Tags="&lt;algorithm&gt;&lt;computational-geometry&gt;&lt;line-drawing&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="2522" PostTypeId="2" ParentId="2521" CreationDate="2016-05-28T22:31:51.283" Score="1" Body="&lt;p&gt;Frankly, your implementation doesn't make much sense. It basically iterates x coordinate through &lt;code&gt;i&lt;/code&gt; variable and increments &lt;code&gt;y&lt;/code&gt; variable at each step. Since the &lt;code&gt;parameter&lt;/code&gt; doesn't cross the 0 boundary, it generates the diagonal points you can see on your picture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You also compute and update the &lt;code&gt;parameter&lt;/code&gt; variable somehow, but don't use it anywhere.&lt;/p&gt;&#xA;" OwnerUserId="2479" LastEditorUserId="2479" LastEditDate="2016-05-29T07:12:32.357" LastActivityDate="2016-05-29T07:12:32.357" CommentCount="3" />
  <row Id="2523" PostTypeId="2" ParentId="2461" CreationDate="2016-05-28T23:39:03.033" Score="3" Body="&lt;p&gt;I don't have any experience with Gradient Domain Path Tracing, but here are my thoughts:&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;There seems to be a different problem&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;If you look carefully at the little spikes of distortion in the final image, you will see that they are all lit from the same direction - on their top left side at a consistent 45 degrees. The sphere also appears to be lit from this angle, rather than from above by the light source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is unlikely to be explained by an incorrect probability estimation for a path. I would expect there to be a different problem with the code, that these distortions are hinting at.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I will therefore address these two separate points:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;You want to know how to calculate the Probability Density of a path when using Next Event Estimation.&lt;/li&gt;&#xA;&lt;li&gt;There is evidence of some problem unrelated to this.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I'll also review the code for non-essential points - but I'll leave this until after the essentials.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Probability Density of a path when using Next Event Estimation&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Looking at the &lt;a href=&quot;https://mediatech.aalto.fi/publications/graphics/GPT/kettunen2015siggraph_paper.pdf&quot; rel=&quot;nofollow&quot;&gt;paper on which the code you are following was based&lt;/a&gt; it seems that the novel shift mapping described in section 5.2 is defined in terms of the reflective properties of the surfaces found at the vertices of the path. I must emphasise that I don't have a full understanding of this, but it suggests that Next Event Estimation may not require a change in this approach, as the surfaces encountered will be the same. Hopefully once the other problems are cleared up it will be easier to judge whether the image looks correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that section 5.2 of the paper already mentions (just below Figure 10) that they take into account sampling the emitter &quot;&lt;em&gt;either using BSDF or area sampling&lt;/em&gt;&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The difference with Next Event Estimation is that the area sampling happens at every vertex of the path, but it isn't obvious to me that this should cause a problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The fact that your scene uses only diffuse surfaces means that the offset path should in most cases rejoin the base path at the second vertex, so you would only need to recalculate the area sampling for the first vertex of the offset path.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;The cause of the incorrect lighting direction&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;In reading through the code to familiarise myself with how it works, I noticed that &lt;code&gt;NLdotL&lt;/code&gt; is calculated, but then not used. A text search revealed that the only other occurrence has a different case: &lt;code&gt;nldotl&lt;/code&gt;. Here are the two variables in context (first and ninth lines of this excerpt):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float NLdotL = Math.Abs( Vector3.Dot( NL, -L ) );&#xA;float NdotL = Vector3.Dot( ray.N, L );&#xA;if (NdotL &amp;gt; 0)&#xA;{&#xA;    Ray r = new Ray( I + L * EPSILON, L, dist - 2 * EPSILON ); //make it a tiny bit shorter otherwise I risk to hit my starting and destination point&#xA;    Scene.Intersect( r );&#xA;    if (r.objIdx == -1) //no occlusion towards the light&#xA;    {&#xA;        float solidAngle= (nldotl * light.getArea()) / (dist * dist);&#xA;        E += T * (NdotL) * solidAngle * BRDF * light.emission;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Since &lt;code&gt;nldotl&lt;/code&gt; is not defined, the result of the code is undefined behaviour. In practice, the program is likely to act as if &lt;code&gt;nldotl&lt;/code&gt; is zero, or for some compilers perhaps a constant arbitrary value, or even a different arbitrary value at each iteration. For your particular compiler it appears to be a constant value, and I strongly suspect this is the cause of the distinctive alignment of lighting angle on all the speckles and the sphere. If there is also another contributing problem it will be easier to analyse it in a separate question once this initial problem has been fixed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It may be worth considering using a compiler and/or flag setting that gives an error or at the very least a warning for undefined variables, as this type of mistake is both very easy to make, and very easy to overlook later.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Additional contribution of light source&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;There appears to be another problem that will cause results to be incorrect in a more subtle way, with no obvious distortion. Due to Next Event Estimation, the light source is contributing to each step along the path. This means it should not make a contribution if the path itself hits the light source directly. Otherwise, for that path the light source will be contributing twice. You can correct this by changing:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (material.emissive) //case of a light&#xA;{&#xA;    E += material.diffuse;&#xA;    break;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;to:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (material.emissive) //case of a light&#xA;{&#xA;    break;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This will give a zero contribution from the intersection with the light.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note that since I can only see this one function, I can't guess whether this will also make the light appear black in the image. You may or may not need to adjust for rays that start at the camera and hit the light directly.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;Code review&lt;/h1&gt;&#xA;&#xA;&lt;h3&gt;Doubly finite rays&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;I'm used to defining a ray as a half infinite line segment - having a starting point but no ending point. I notice that this code gives a ray both a starting point and a length. The only place I can see a reason for this is when testing a shadow ray against the light source: the code checks that there are no intersections on the way to the light, so intersections behind the light (or on the light itself) must be excluded. In all other places, the ray is defined with a pseudo-infinite length (&lt;code&gt;1e34f&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following suggestion won't affect the &lt;em&gt;correctness&lt;/em&gt; of your code, but it may be more readable and avoid you having to work around the need for infinity and having to account for epsilon twice.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the ray is simply a starting point and a direction, then shadow rays can simply check that the first intersection is the light, rather than checking that there is no intersection. For example, by replacing:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (r.objIdx == -1) //no occlusion towards the light&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if (r.objIdx == LIGHT) // There is no object intersected before the light&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here I use &lt;code&gt;LIGHT&lt;/code&gt; as a placeholder for the id of the light source, since that part of the code is not included in the question.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This will always give false if the light is occluded by a nearer object.&lt;/li&gt;&#xA;&lt;li&gt;This will always give true if the ray hits the light before any more distant objects.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;This is therefore equivalent to the current code, but does not require the ray to store a length.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Lights that don't reflect&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The code currently models lights and surfaces separately. This means that if an object is a light then it is only emissive, and reflects no light from other objects.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This causes a negligible difference in the example scene in the question, which has a single bright light. However, if used with a number of dimmer lights, it would be more noticeable that they do not light each other. In many cases the difference will not be noticeable, so this is not a problem if it works for the scenes you wish to render.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-29T00:53:36.330" LastActivityDate="2016-05-29T00:53:36.330" CommentCount="0" />
  <row Id="2524" PostTypeId="2" ParentId="2521" CreationDate="2016-05-29T01:45:44.823" Score="2" Body="&lt;p&gt;&lt;em&gt;Note that the approach you appear to be taking will only work for lines with a slope in a single octant. For the other seven octants the algorithm requires changing the approach, for example by changing the roles of x and y. Wikipedia gives a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bresenham&amp;#39;s_line_algorithm#All_cases&quot; rel=&quot;nofollow&quot;&gt;table of cases&lt;/a&gt; that can be used for this conversion.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;For this answer I will assume you are simply trying to get the code working for the first octant.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;int parameter = 2 * dy -  dx;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;parameter&lt;/code&gt; starts off negative only if &lt;code&gt;dx&lt;/code&gt; &gt; &lt;code&gt;2 * dy&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case the total horizontal distance to be covered by the line is more* than twice the vertical distance, which means the first step should be purely horizontal. So if &lt;code&gt;parameter&lt;/code&gt; is negative then &lt;code&gt;y0&lt;/code&gt; should not change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the source of the problem:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(parameter &amp;lt; 0){&#xA;           y0 += 1;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;y0&lt;/code&gt; should only be incremented if &lt;code&gt;parameter&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; negative. This &lt;code&gt;y0 += 1&lt;/code&gt; is causing vertical movement even if the first step should be purely horizontal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The same problem applies on later steps too. The code causes &lt;code&gt;parameter&lt;/code&gt; to switch back and for between negative and non-negative in proportion to the ratio between &lt;code&gt;2 * dx&lt;/code&gt; and &lt;code&gt;dy&lt;/code&gt;. &lt;code&gt;y0&lt;/code&gt; must only be incremented when &lt;code&gt;parameter&lt;/code&gt; is non-negative. That additional occurrence of &lt;code&gt;y0 += 1&lt;/code&gt; prevents &lt;code&gt;y0&lt;/code&gt; from ever staying still, causing a slope of 1 regardless of the inputs. Simply remove that line and it should work correctly.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;*If you are wondering why the algorithm only steps purely horizontally when the parameter is less than zero, rather than less than or equal to zero, bear in mind that the horizontal distance to be covered is one less than the number of pixel columns displayed. If x0 is 0 and x1 is 3, then dx is 3, but 4 columns of pixels will be displayed. So a line 4 pixels across and 2 pixels down actually has xd as 3 and yd as 1. In this case the number of pixels across is exactly twice the number of pixels down, but xd is &lt;em&gt;more&lt;/em&gt; than twice yd, so this still results in the parameter being negative and the line moving purely horizontally on the first step.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-05-29T02:29:31.397" LastActivityDate="2016-05-29T02:29:31.397" CommentCount="1" />
  <row Id="2526" PostTypeId="1" CreationDate="2016-05-29T14:05:33.413" Score="6" ViewCount="52" Body="&lt;p&gt;After updating my graphics driver and trying to run &lt;a href=&quot;http://john-chapman-graphics.blogspot.al/2013/02/pseudo-lens-flare.html&quot;&gt;this&lt;/a&gt; example from John Chapman's blog I got this error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Vertex shader failed to compile with the following errors:&#xA;ERROR: 0:8: error(#105) #version must occur before any other program statement&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The way that the version is defined is by including a file containing this code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#ifndef DEF_GLSL_&#xA;#define DEF_GLSL_&#xA;&#xA;#version 420&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So inside all other shader files one can see that the first line is as follows&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &quot;common/def.glsl&quot;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I remember this working perfectly a few months ago. So what exactly has changed and the &lt;code&gt;#include&lt;/code&gt; command is no longer valid?&lt;/p&gt;&#xA;" OwnerUserId="3452" LastEditorUserId="3452" LastEditDate="2016-05-29T14:53:12.817" LastActivityDate="2016-05-29T16:33:58.057" Title="Why cant I use `#include` after upgrading my driver" Tags="&lt;opengl&gt;&lt;glsl&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="2527" PostTypeId="2" ParentId="2526" CreationDate="2016-05-29T16:33:58.057" Score="5" Body="&lt;p&gt;The error message gives a clue in the wording:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#version must occur before any other program statement&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a reason that the previous 2 lines need to appear before the &lt;code&gt;#version&lt;/code&gt; line? If not, you can avoid the error by moving this line to the beginning, since the actions of the lines are independent of their order in this instance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;#version&lt;/code&gt; needing to be at the beginning of the file is not a new requirement. I don't know why you were able to run this code previously.&lt;/p&gt;&#xA;" OwnerUserId="231" LastActivityDate="2016-05-29T16:33:58.057" CommentCount="4" />
  <row Id="2528" PostTypeId="2" ParentId="2446" CreationDate="2016-05-30T08:37:34.313" Score="3" Body="&lt;p&gt;Are you calling glViewport when switching drawing to FBO ? This should be called when switching into a different resolution render target. I say this because case #1 is a symptom of incorrect glViewport.&lt;/p&gt;&#xA;" OwnerUserId="3073" LastActivityDate="2016-05-30T08:37:34.313" CommentCount="0" />
  <row Id="2529" PostTypeId="1" AcceptedAnswerId="2536" CreationDate="2016-05-30T12:43:29.830" Score="3" ViewCount="24" Body="&lt;p&gt;I'm using a Sobel filter to draw an outline in a shader. How do I control its thickness? Here's my outlining code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float3x3 Kx = {&#xA;-1, 0, 1,&#xA;-2, 0, 2,&#xA;-1, 0, 1};&#xA;&#xA;float3x3 Ky = {&#xA;1, 2, 1,&#xA;0, 0, 0,&#xA;-1,-2,-1};&#xA;&#xA;float Lx = 0;&#xA;float Ly = 0;    &#xA;&#xA;for (int y = -1; y &amp;lt;= 1; ++y) {&#xA;    for (int x = -1; x&amp;lt;= 1; ++x) {&#xA;        float2 offset = float2( x, y );&#xA;        float3 tex = tex2D(sampler, uv + offset).rgb;&#xA;        float luminance = dot(tex, float3(0.3, 0.59, 0.11));&#xA;&#xA;        Lx += luminance * Kx[y+1][x+1];&#xA;        Ly += luminance * Ky[y+1][x+1];&#xA;    }&#xA;}&#xA;float color = sqrt(Lx*Lx + Ly*Ly);&#xA;return float4( color, color, color, color );&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="67" LastActivityDate="2016-05-31T04:50:41.500" Title="Controlling Sobel edge thickness" Tags="&lt;shader&gt;&lt;edge-detection&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2530" PostTypeId="2" ParentId="2500" CreationDate="2016-05-30T16:47:25.703" Score="1" Body="&lt;p&gt;First off you want the start of each line to have the color of an already faded line, ie rgb_color*0.95. You already figured out how to do this so I stop there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the HSV you are interested in the Value component only when fading towards black. Basically you should do hsv_color.value*0.95.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I typed this on phone. Please edit if you get it and may explain better.&lt;/p&gt;&#xA;" OwnerUserId="3041" LastActivityDate="2016-05-30T16:47:25.703" CommentCount="0" />
  <row Id="2531" PostTypeId="1" AcceptedAnswerId="2533" CreationDate="2016-05-30T17:38:15.047" Score="0" ViewCount="38" Body="&lt;p&gt;I am a some-what experienced programer, and I have been working on creating a 3D graphics engine utilizing the OpenGL system, primarily focused on use in games. However, I also want to do some work with functional programming, and wanted to create a clean UI for more functional programs, like a task manager. An example of the kind of effect that I want to create could be somewhat like the Visual Studio Code UI. My question is: is there a specific library that I should use for creating desktop programs, or should I continue to use OpenGL's 3D aspects, but keep everything in a single Z axis plane, effectively creating a 2D program? Or is there a better way to create normal program visuals, that wont be cheating out of a 3D graphical space?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;I am currently using C++/C, but I am also happy to expand my knowledge base to include new languages.&lt;/p&gt;&#xA;" OwnerUserId="3459" LastEditorUserId="3459" LastEditDate="2016-05-30T18:58:14.850" LastActivityDate="2016-05-30T19:55:19.277" Title="Optimal 2D graphics resource" Tags="&lt;opengl&gt;&lt;3d&gt;&lt;2d-graphics&gt;" AnswerCount="1" CommentCount="3" ClosedDate="2016-06-01T09:28:43.650" />
  <row Id="2532" PostTypeId="2" ParentId="295" CreationDate="2016-05-30T17:48:43.137" Score="0" Body="&lt;p&gt;I'll say a few words about rendering without graphics cards.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Niether OpenGL nor DirectX really need a graphics card. An implementation may be fully software accelerated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For OpenGL in Linux there is Mesa software rasterizer. In my experience it works ok as long as you don't push it too hard combining weird features (display list + indexed vertex arrays). I imagine their list of actual bugs is mile high. Make sure to do continous testing!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have less experience using OpenGL in Windows. Edit answer if you know anything!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DirectX SDK is delivered with a software rasterizer called &quot;reference rasterizer&quot;. It is mostly for debugging in my opinion but I know of at least one friend using the reference rasterizer when developing DX11 on a DX10 only capable laptop. As mentioned in other answers DirectX is Microsoft only.&lt;/p&gt;&#xA;" OwnerUserId="3041" LastActivityDate="2016-05-30T17:48:43.137" CommentCount="0" />
  <row Id="2533" PostTypeId="2" ParentId="2531" CreationDate="2016-05-30T19:55:19.277" Score="0" Body="&lt;p&gt;As you said you want to create something like this &lt;a href=&quot;http://i.stack.imgur.com/WOk2z.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/WOk2z.png&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you know these are not graphic intense rendering.&#xA;Thus, (i guess but not 100%) Windows do these rendering on CPU not on GPU.&#xA;GPU is for intense rendering with lighting, textures, 3D models, etc. Where you can take the benefit of algorithm built in GPU circuit, Which is what OpenGL does.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So i would recommend to use built-in libraries that don't use GPU and use CPU for rendering, Although you can use OpenGL or Direct3D as well.     &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some libraries that are built in different programming languages are :-&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Java - Jframe, Graphics, Buffered image,  Canvas. (Note : all these libraries have different functionality and you have to use them together for software rendering &lt;a href=&quot;https://www.youtube.com/playlist?list=PLEETnX-uPtBUbVOok816vTl1K9vV1GgH5&quot; rel=&quot;nofollow&quot;&gt;See this for more detail&lt;/a&gt;) &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;C# - System.drawing,  (i don't have much experience of C#, So i can't name any more libraries), also C# form application.&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;JavaScript - Canvas &lt;a href=&quot;https://www.youtube.com/playlist?list=PLh-MBXZEiyMhulEqYE3gn63idSAKG6Sx1&quot; rel=&quot;nofollow&quot;&gt;check this out&lt;/a&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;C++ - you need third party libraries like QT, GTX etc. &lt;a href=&quot;https://www.youtube.com/watch?v=6KtOzh0StTc&amp;amp;list=PL2D1942A4688E9D63&quot; rel=&quot;nofollow&quot;&gt;Qt learning resource&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=FxCC9Ces1Yg&amp;amp;list=PLSPw4ASQYyymu3PfG9gxywSPghnSMiOAW&quot; rel=&quot;nofollow&quot;&gt;related resource&lt;/a&gt;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Python - Pygame (No personnal eperience) &lt;a href=&quot;https://www.youtube.com/playlist?list=PLQVvvaa0QuDdLkP8MrOXLe_rKuf6r80KO&quot; rel=&quot;nofollow&quot;&gt;See this for detail&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="3437" LastActivityDate="2016-05-30T19:55:19.277" CommentCount="3" />
  <row Id="2534" PostTypeId="1" CreationDate="2016-05-30T21:39:46.060" Score="0" ViewCount="43" Body="&lt;p&gt;I am going through one of the most basic line drawing algorithms and stuck with the following mathematical explanations. The implicit function of line equation is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f(x,y) = ax + by + c&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Book(Computer Graphics , Principles  and practice) mentions that&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;f(x,y) = 0 , when any point m is on line&#xA;f(x,y) &amp;lt; 0, when any point m is above the line, and &#xA;f(x,y) &amp;gt; 0, when any point m is below the line.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It would be great to have some explanation of the claim above. I tried to figure out the first one with the following example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;3x + 2y = 1&#xA;=&amp;gt; 3x + 2y -1 = 0, where a = 3, b = 2 and c = -1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;All I figure out that I need to plug in a (x,y) coordinate so that 3x + 2y - 1 = 0. I am not sure how to choose this coordinate value .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And I am clueless about the next two cases. An example demonstrating all the three cases would be fantastic !&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks &lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-05-31T04:55:52.333" Title="Midpoint algorithm" Tags="&lt;midpoint&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="2535" PostTypeId="2" ParentId="2534" CreationDate="2016-05-31T04:50:19.163" Score="1" Body="&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/TizJk.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/TizJk.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I know i marked the coordinates of the second point wrong)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let $ax + by + c$ be any line such that $b &amp;gt; 0$. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now pick point $(\alpha,\beta)$ on the line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus $\alpha *a + \beta*b + c = 0$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now pick a point $(\alpha, \gamma)$ such that $\gamma$ &gt; $\beta$ &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus $\alpha * a + \gamma * b + c$  &gt; 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thus any point satisfying $ax+by +c$ &gt; 0 line above the line. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;your results can be deduced from the converse of the proof. &lt;/p&gt;&#xA;" OwnerUserId="3437" LastEditorUserId="3437" LastEditDate="2016-05-31T04:55:52.333" LastActivityDate="2016-05-31T04:55:52.333" CommentCount="0" />
  <row Id="2536" PostTypeId="2" ParentId="2529" CreationDate="2016-05-31T04:50:41.500" Score="2" Body="&lt;p&gt;I've done this 2 different ways in the past:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Apply a pre-blur to the image before running the Sobel operator on it. This will have the side effect of getting rid of any noise that's smaller than the blur kernel, but it will also thicken the lines you end up drawing, too.&lt;/li&gt;&#xA;&lt;li&gt;Apply a post-process where you thicken the lines by either stamping a shape at every point that's an edge, or using some sort of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_morphology&quot; rel=&quot;nofollow&quot;&gt;dilate process&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="3003" LastActivityDate="2016-05-31T04:50:41.500" CommentCount="0" />
  <row Id="2537" PostTypeId="2" ParentId="2510" CreationDate="2016-05-31T09:12:46.230" Score="1" Body="&lt;p&gt;(Promoting &quot;comment&quot; to an answer)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It looks to me that it's just finding monotonic runs of edges. Given a defined winding order (in this case anticlockwise), then you can identify {P6, P7, P8, P0} as a decreasing run, as is {P2, P3, P4}. Since the left most vertex, P8, is in (**the middle of) a decreasing run, the decreasing runs define left boundaries and, therefore, increasing runs are right boundaries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;**it just occurred to me that if the left most point is at the beginning/end of a monotonic run then a slightly more involved rule will need to be applied involving the edges entering/leaving that point.&lt;/p&gt;&#xA;" OwnerUserId="209" LastEditorUserId="209" LastEditDate="2016-06-01T09:34:21.397" LastActivityDate="2016-06-01T09:34:21.397" CommentCount="0" />
  <row Id="2538" PostTypeId="1" CreationDate="2016-05-31T10:13:37.447" Score="0" ViewCount="38" Body="&lt;p&gt;I have a bounding area of a contour after the slicing operation. Now to generate the tool-path over the contour, I need to distribute the bounding area into grid space so that each each cell maintain a certain precomputed width.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am doing the math as follows, but I am not sure if the whole area will be covered as there is floating point values concatenated to integer coordinates.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;       short int xDist = std::abs(xMax - xMin);&#xA;       short int yDist = std::abs(yMax - yMin);&#xA;&#xA;       short int xCells = (xDist/scanSpacingStep) + 1;&#xA;       short int yCells = (yDist/scanSpacingStep) + 1;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The indexing starts from (0,0). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-05-31T10:13:37.447" Title="distribute the bounding area into grid space" Tags="&lt;2d-graphics&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="2540" PostTypeId="1" AcceptedAnswerId="2545" CreationDate="2016-05-31T19:43:47.037" Score="3" ViewCount="33" Body="&lt;p&gt;Is there any difference between registers types in hlsl?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example when using register &lt;code&gt;t0&lt;/code&gt; for &lt;code&gt;UAV&lt;/code&gt; will it be somehow precache like textures?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;According to microsoft's presentation about resource in DX12 and hlsl 5.1 this registers mean nothing, but maybe they only mean that there isn't any real registers like on CPU?&lt;/p&gt;&#xA;" OwnerUserId="3123" LastEditorUserId="3123" LastEditDate="2016-06-01T06:51:53.277" LastActivityDate="2016-06-01T18:01:26.300" Title="Hlsl - registers type" Tags="&lt;gpu&gt;&lt;hlsl&gt;&lt;directx12&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2541" PostTypeId="1" AcceptedAnswerId="2543" CreationDate="2016-05-31T19:58:37.443" Score="3" ViewCount="49" Body="&lt;p&gt;Here is my function that does interpolation&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static public PixelData InterpolateColour(Point start, Point end ,float curLength, float totalLength){&#xA;&#xA;    float endPercent = curLength / totalLength;             // Calculates the percent of colour at the ending point of the line to be used in current pixel &#xA;    float startPercent = 1 - (curLength / totalLength);     // Calculates the percent of colour at the starting point of the line to be used in current pixel &#xA;&#xA;    float startHueInRadians[] = PixelData.PixelDataToHsbInRadians(start.getColour());   // Converting the ending point colour into a hsb colour model &#xA;    float endHueInRadians[] = PixelData.PixelDataToHsbInRadians(end.getColour());       // Converting the starting point colour into a hsb colour model&#xA;&#xA;&#xA;    startHueInRadians[0] = (startHueInRadians[0] == 0) ?                                            // if the start colour is red then then choosing the value to be zero or 2*pi&#xA;                           ((endHueInRadians[0] &amp;gt;= (Math.PI * 4)/3) ?  (float)(Math.PI * 2) : 0)    // if the end colour is red then then choosing the value to be zero or 2*pi&#xA;                           : startHueInRadians[0];&#xA;&#xA;    endHueInRadians[0] = (endHueInRadians[0] == 0) ? &#xA;                         ((startHueInRadians[0] &amp;gt;= (Math.PI * 4)/3) ?  (float)(Math.PI * 2) : 0)&#xA;                         : endHueInRadians[0];&#xA;&#xA;&#xA;&#xA;    float a = start.getColour().getAInInt() * endPercent + end.getColour().getAInInt() * startPercent;  //Interpolating alpha, which is not working :(                   &#xA;    float h = endHueInRadians[0] * endPercent + startHueInRadians[0] * startPercent;                    //Interpolating hue.&#xA;    float s = endHueInRadians[1] * endPercent + startHueInRadians[1] * startPercent;                    //Interpolating staturation, which is useless currently. :(&#xA;    float b = endHueInRadians[2] * endPercent + startHueInRadians[2] * startPercent;                    //Interpolating value, which is useless currently. :(&#xA;&#xA;    float f[] = {h,s,b};&#xA;&#xA;    PixelData p = PixelData.HsbinRadiansToPixelData(f);     // Converting interpolated hsb back to rgb&#xA;&#xA;    p.setAInInt((int)a);    //setting the alpha value &#xA;&#xA;    return p;               // returing the intepolated value &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is working properly but it is giant bulky code that runs very slowly because it has all kind of Ternary operator, floating point arithmetic and also it runs once per pixel.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently refactoring my code to be small and less hacky. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are some interpolation algorithm that are used in computer graphics ? because i came up with this code and it is just bad.  &lt;/p&gt;&#xA;" OwnerUserId="3437" LastActivityDate="2016-06-01T00:07:41.430" Title="Is there a way to interpolate color across the line with only integer calculation ?`" Tags="&lt;algorithm&gt;&lt;color&gt;&lt;interpolation&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2542" PostTypeId="1" CreationDate="2016-05-31T21:45:28.007" Score="3" ViewCount="47" Body="&lt;p&gt;I have a post-effect camera-shader in which I want to implement a simple spatial partitioning of the screen between two passes of the fragment shader.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The first pass should divide the screen into cells of size = 30x30 pixels, and then for each green fragment, store &lt;strong&gt;somehow&lt;/strong&gt; which of those screen cells the coordinate of such green fragment  is located. That information should then be available for the second pass to work with.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Well, merely dividing the screen into cells is trivial, considering that it's easy to calculate the row and column indexes of such screen-grid:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static float2 _Pixels = float2(_ScreenParams.x, _ScreenParams.y);&#xA;float2 griduv = round(input.uv * _Pixels) * _invPixels; //pixel-screen space coordinate of the current fragment being evaluated&#xA;half cellsize = 30.0f;&#xA;&#xA;float gridX = griduv .x/cellsize;&#xA;float gridY = griduv .y/cellsize;&#xA;&#xA;int gridrow = gridX  - frac(gridX);&#xA;int gridcol = gridY  - frac(gridY);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If that was not a shader task but rather a standard CPU code, it would be just a matter of saving the &lt;code&gt;i&lt;/code&gt;th &lt;code&gt;griduv&lt;/code&gt; into an array-of-list or array-of-array (e.g. &lt;code&gt;float2[][] grid = new float2[gridrow*gridcol][maxNfloat2]&lt;/code&gt;) at position &lt;code&gt;grid[gridrow*gridcol+gridrow][i]&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in a shader, for most hardwares and most DirectX and OpenGL versions, it seems to me that 1) we can't have multi-dimensional arrays, 2) we can't have resizable containers like lists and 3) we also can't pass variables and containers from one pass to the next.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As one can easily see, this situation is also &lt;strong&gt;not&lt;/strong&gt; well suited for using a simple texture to store values from one pass to the next, since I would need multiple &lt;code&gt;float2&lt;/code&gt; into each grid cell and I can have quite many floats to store.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, how could I proceed in such a situation? In other words, how could I store the pixel-screen coordinates of given fragments in a spatial-partitioning of the screen (in this case a simple fixed size grid of the screen), to be passed from one shader pass to the next?&lt;/p&gt;&#xA;" OwnerUserId="3410" LastEditorUserId="3410" LastEditDate="2016-06-03T00:20:14.557" LastActivityDate="2016-06-03T17:38:05.737" Title="In a shader, how to store fragment coordinates in a spatial partitioning from one pass to the next?" Tags="&lt;shader&gt;&lt;gpu&gt;&lt;fragment-shader&gt;&lt;grid&gt;&lt;space-partitioning&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="2543" PostTypeId="2" ParentId="2541" CreationDate="2016-05-31T22:55:46.933" Score="7" Body="&lt;p&gt;&lt;em&gt;Yes, it is possible to use only integer calculations. I will describe how, but bear in mind that the difference in speed between integer arithmetic and floating point arithmetic is not as great as it was historically. If you want your code to run faster, it is best to profile and identify which parts of the code are taking up most time, before considering whether to convert a particular aspect to use integers.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h1&gt;A straight line through colour space&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Looking at the code, it appears you first convert from RGB to &lt;a href=&quot;https://en.wikipedia.org/wiki/HSL_and_HSV&quot;&gt;HSB&lt;/a&gt; then create a linear gradient, and convert it back to RGB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A linear gradient is a straight line through the colour space, so you can use an integer based straight line algorithm to generate it without the need for floating point arithmetic. For example, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bresenham&amp;#39;s_line_algorithm&quot;&gt;integer based version of Bresenham's algorithm&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;H, S, and B are being treated independently, so you can calculate each one separately. For example, the H component can be calculated by treating the line as a straight line from (x0, H0) to (x1, H1), instead of the usual (x0, y0) to (x1, y1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the range of y values along the line is greater than the range of x values along the line then Bresenham's algorithm will step along integer y values instead of x values. The algorithm can still be applied in exactly the same way, just using (y0, H0) to (y1, H1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You can also combine all 4 calculations into a single loop, keeping track of a parameter each for determining y value, H value, S value and B value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only difference from plotting a line on the screen (using x and y) is that not every value of H, S, and B needs to be taken. If the x range is greater than the y range, the algorithm steps through each of the x values checking whether to increment y. If the y range is greater then it instead steps through the values of y checking whether to increment x. However, if the range of H is greater than the range of x, it makes no sense to step through H values, as many of them will correspond to the same x value, wasting calculations. If the range of H is smaller than the range of x then the algorithm can proceed as usual. If the range of H is often going to be larger than the range of x then it is worth considering a different approach. Similarly for S and B.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Fitting the colour space to your line&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;One way to eliminate this problem is to tailor your colour space to the x (or y if greater) range required for this specific line. Since your input colours and final output colours are in RGB, you are not constrained to a particular range for the HSB colours. You are free to convert from 256 levels of R, G, and B to 1000 levels of H, S, and B (or 1000000, or whatever you choose). If you choose to use a colour space where the range of H values used is the same integer as the range of x values used, then you won't even need to use a line drawing algorithm for the colours. Simply store the required values for converting back to RGB, and each of the H, S, and B values will be x plus a constant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This will require modifying your conversion function, but this will probably be necessary anyway, as it currently uses radians, which are not going to help with keeping things integer.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;More accurate colour spaces&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Note that using a colour space like HSB will not give gradients which look as even as if using a more perceptually uniform color space such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Lab_color_space&quot;&gt;Lab&lt;/a&gt;. However, since such spaces require calculating roots and you are looking to keep to integer arithmetic as much as possible, I'm assuming you are happy with HSB. If you did want to use something like Lab, you could still potentially keep to integers by using look up tables.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;What inputs does this need to work for?&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;You also mention having to make decisions in the code which also takes up time. I would question how much difference this makes since the decisions are outside the loops and therefore only performed once. If you really want to eliminate them, ask yourself which inputs the code needs to work for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently the code checks the start and end values of the hue and decides whether to change the hue clockwise or anticlockwise. If you eliminate this check the code will still work, but it may not always give the colour direction the user intended. Deciding this automatically also may not give the direction the user intended (since different users have different expectations, and the requirement may change from one use to the next). You could simply take the direction as an additional parameter, which would avoid having to make the decision.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Code review&lt;/h1&gt;&#xA;&#xA;&lt;h3&gt;Meaningful variable names&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;The use of &quot;hue&quot; in the naming of variables that store all three of hue, saturation and brightness makes the code less intuitive to read, potentially causing the reader to wonder whether saturation and brightness have been overlooked. It also makes the code difficult to reason about, since &lt;code&gt;startHueInRadians[0]&lt;/code&gt; is a hue, while &lt;code&gt;startHueInRadians[1]&lt;/code&gt; is a saturation, which is not only not a hue, but also presumably not in radians.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In addition to making the code more accessible to others, you may also find it easier to work with it yourself if the naming is consistent, especially if you look back at the code after a few months.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;A potential bug&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;If one end of the colour gradient is at hue zero (red), the code checks whether the other end is greater than or equal to two thirds of the way around the hue circle. This corresponds to blue. If it is less than blue, then the gradient proceeds from zero upwards (anticlockwise), otherwise it proceeds from 2 pi downwards (clockwise). This means that a gradient from red to a colour with hue just above blue will go from red to magenta to blue. A gradient from red to a colour with hue just below blue will go from red to yellow to green to cyan to blue, which may not be what is expected. Intuitively, I would expect the cut off to be half way around the circle rather than two thirds, so that the gradient always includes the fewest colours (taking the shorter, more direct route).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you did intend to have this asymmetry, note that choosing a similar red with hue just above or just below zero will override this behaviour, forcing the long route or the short route respectively.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you want the gradient to consistently take the shorter route, then the code will need to check which route to take for all values, not just zero. For example, currently choosing the start hue just above zero and the finish hue just below zero will give start and finish colours that both look red, but the gradient will pass through red, yellow, green, cyan, blue, magenta and back to red, rather than just fading subtly from one hue of red to another.&lt;/p&gt;&#xA;" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-06-01T00:07:41.430" LastActivityDate="2016-06-01T00:07:41.430" CommentCount="2" />
  <row Id="2544" PostTypeId="1" CreationDate="2016-06-01T17:18:50.150" Score="1" ViewCount="27" Body="&lt;p&gt;I have a scene rendered entirely with objects with additive blend mode.  I have a value that is oscillating from 0, to 1.  When it is 1 the object is supposed to be invisible, 0 should be entirely visible.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I have noticed that something seems off about it, specifically it stays dark for too long.  It is not my oscillating function, I suspect it having something to do with light using a logarithmic scale.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How would I convert my linear 0-&gt;1 value into a properly scaled value between 0-&gt;1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fragment:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;uniform sampler2D s_texture;&#xA;&#xA;varying vec2 v_texcoord;&#xA;varying float alpha;&#xA;&#xA;void main()&#xA;{&#xA;    vec4 col = texture2D(s_texture, v_texcoord);&#xA;    gl_FragColor = vec4( col.rgba  - alpha);&#xA;&#xA;    if (col.a &amp;lt; 0.001)&#xA;    {&#xA;        discard; //11.7-&amp;gt;12.2&#xA;    }&#xA;    //gl_FragColor = vec4(0.4);&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Vertex:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;&#xA;attribute vec2 pos;&#xA;attribute vec2 tc;&#xA;&#xA;uniform mat4 matrix;&#xA;uniform float time;&#xA;uniform float maxLeafs;&#xA;uniform vec2 position;&#xA;uniform float instance;&#xA;uniform vec2 anchorPoint;&#xA;&#xA;&#xA;varying vec2 v_texcoord;&#xA;varying float alpha;&#xA;&#xA;vec4 finalTranslate(vec3 pos)&#xA;{&#xA;    return vec4(pos + vec3(position, 0.0), 1.0);&#xA;}&#xA;vec3 rotate(vec3 pos, float angle)&#xA;{&#xA;    float s = sin(-angle);&#xA;    float c = cos(-angle);&#xA;    return vec3(c * pos.x - s * pos.y, s * pos.x + c * pos.y, 0.0);&#xA;}&#xA;vec3 translate(vec2 pos)&#xA;{&#xA;    return vec3(pos - anchorPoint, 0.0);&#xA;}&#xA;vec3 scale(vec3 pos, float amm)&#xA;{&#xA;    return pos * amm;&#xA;}&#xA;void main()&#xA;{&#xA;&#xA;    float v = log((time * 0.9) + 0.1);&#xA;    alpha = 0.0;&#xA;&#xA;    float leafCoverage = (3.14149 * 2.0);&#xA;    float goal = ((instance) / (maxLeafs + 1.0)) * leafCoverage;&#xA;    float timeFactor = smoothstep(0.0, 1.0, time);&#xA;    float calculation1 = goal * timeFactor;&#xA;    float calculation2 = clamp(timeFactor * leafCoverage, 0.0, goal);&#xA;&#xA;    float scaleFactor = clamp(smoothstep(0.0, 1.0, time * 1.2), 0.0, 1.0);&#xA;&#xA;    gl_Position = matrix * finalTranslate(rotate( scale(translate(pos) ,scaleFactor) ,  calculation1));&#xA;    v_texcoord = tc;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Right now I am just taking the oscillating float time and passing it in directly, but eventually I will put it on a function so it fades in, temporarily is extra bright, then goes to the source texture.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: I solved my problem, to my surprise the GLSL log function is in base e rather then 10.&lt;/p&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="2308" LastEditDate="2016-06-01T18:55:46.843" LastActivityDate="2016-06-02T01:03:53.487" Title="Fading an object in add mode with correct color math" Tags="&lt;color&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="2545" PostTypeId="2" ParentId="2540" CreationDate="2016-06-01T18:01:26.300" Score="4" Body="&lt;p&gt;You have to use the appropriate register type for the resource: &lt;code&gt;b&lt;/code&gt; registers for constant buffers, &lt;code&gt;t&lt;/code&gt; for textures, and &lt;code&gt;u&lt;/code&gt; for UAVs. AFAIK, it is not possible to bind a UAV to a texture slot, or otherwise mismatch registers and resources.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, a resource can have multiple views, so it is possible to have a resource bound as a UAV in one shader and as a regular texture (SRV) in another, which enables all the usual GPU texture sampling features to be used in the latter case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The details of how the DirectX API concepts of slots, registers, and resources map to the actual hardware are vendor-specific. On one GPU they may literally represent registers of some kind, while on another they may represent a small chunk of metadata in memory somewhere, which describes the resource to the hardware.&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-06-01T18:01:26.300" CommentCount="0" />
  <row Id="2546" PostTypeId="2" ParentId="2544" CreationDate="2016-06-02T01:03:53.487" Score="1" Body="&lt;p&gt;Turns this was not an issue with the scale of the values passed in, just that the log function in glsl is base e.  The solution was merely to do log(stuff)/log(10) and that got the result of a base 10 log.&lt;/p&gt;&#xA;" OwnerUserId="2308" LastActivityDate="2016-06-02T01:03:53.487" CommentCount="0" />
  <row Id="2547" PostTypeId="1" AcceptedAnswerId="2550" CreationDate="2016-06-02T01:07:38.570" Score="3" ViewCount="66" Body="&lt;p&gt;So I am trying to make my shader efficient by lowing the amount of texture lookups. If I had a line of code with something like.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;step(1.0, x) * texture2D(pic1, uv);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where x is a value that ranges from 0 to 2 based on the fragment position.  Would the GPU see when step evaluates to 0 and thus not ever calculate texture2D? Or would it still lookup the texture because the 0 is not constant?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;BTW I use the term &quot;short circuit&quot; because I have been told that is what happens when you have &lt;code&gt;false &amp;amp;&amp;amp; (bla)&lt;/code&gt; the computer sees the false and ignores the right.&lt;/p&gt;&#xA;" OwnerUserId="2308" LastEditorUserId="2308" LastEditDate="2016-06-02T01:29:54.797" LastActivityDate="2016-06-02T05:54:21.460" Title="GPU short circuit when multiply by 0?" Tags="&lt;gpu&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2548" PostTypeId="1" AcceptedAnswerId="2549" CreationDate="2016-06-02T05:13:34.250" Score="4" ViewCount="49" Body="&lt;p&gt;I've been reading several sources in the web about transforming world-space points into camera-space ones.&#xA;I am building my view-matrix from the following parameters&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. Camera position&#xA;2. Point the camera is looking at&#xA;3. Up vector&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I perfectly understand the meaning of each of the parameters, however, there are still some doubts.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How is the direction vector built? We have camera position and lookat position, is it camPos-lookatPos or lookatPos-camPos? In other words, does the direction vector aligns with (goes in the same direction of) the world +z-axis or is opposite to it? &lt;em&gt;Direction&lt;/em&gt; is a bit confusing since it makes me think it goes the direction the camera lens is pointing to.&lt;/li&gt;&#xA;&lt;li&gt;What's the reasoning behind translating the camera to the origin? How does this simplifies math?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;My view matrix looks like follows&lt;/p&gt;&#xA;&#xA;&lt;p&gt;$$V = \begin{matrix}&#xA;  Rx &amp;amp; Ry &amp;amp; Rz &amp;amp; 0 \\&#xA;  Ux &amp;amp; Uy &amp;amp; Uz &amp;amp; 0 \\&#xA;  Fx &amp;amp; Fy &amp;amp; Fz &amp;amp; 0 \\&#xA;  -CamPosx &amp;amp; -CamPosy &amp;amp; -CamPosz &amp;amp; 1&#xA; \end{matrix}$$&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;Will this V matrix convert from camera to world or from world to camera coordinates? How can I understand which direction it is going?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="116" LastActivityDate="2016-06-02T05:43:25.093" Title="Understanding view matrix" Tags="&lt;transformations&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2549" PostTypeId="2" ParentId="2548" CreationDate="2016-06-02T05:43:25.093" Score="1" Body="&lt;p&gt;There is no relation between &lt;code&gt;direction&lt;/code&gt;'s direction and the world axis. And that's fortunate otherwise it would mean your camera is not a free view, it's some kind of a axis bound camera, which has its usages but most likely nothing in your mind right now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The default camera matrix is looking at +z when everything is Identity, that's most surely where you got confused. Know that this is purely a convention, but sticking to a widely adopted convention allows for easy compatibility with libraries like glm.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the direction vector goes outward, so its &lt;code&gt;normalize(lookat - campos)&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The view matrix can be constructed directly with TBN vectors in rows 0 1 2 :)&lt;br&gt;&#xA;(that's your RUF)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Point 2. There is no translating of the camera to the origin, that's just how you chose to view (pun) it. There is translation of world objects toward the origin. Because the rasterizer will work in device coordinates (NDC) which is not configurable, so having a roaming camera (that travels) makes it necessary to indicate its translation in world as part of the last row (row 3), and in reverse since it's not a world matrix representing the camera position, it's the matrix that will bring back points into the view space. (by the way, incidentally meaning the actual camera world matrix is the inverse of the view matrix).&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;oh, that was the last phrase of pt 2.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2016-06-02T05:43:25.093" CommentCount="9" />
  <row Id="2550" PostTypeId="2" ParentId="2547" CreationDate="2016-06-02T05:54:21.460" Score="4" Body="&lt;p&gt;There is a difference between being able to determine value at compilation time and at runtime.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;false &amp;amp;&amp;amp; bla&lt;/code&gt;&lt;br&gt;&#xA;Gets eliminated because of C/C++ lazy operand evaluation rule which is standardized with left to right order.&lt;br&gt;&#xA;The compiler generates no code so the machine has no overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;step(1.0, x) * texture2D(pic1, uv);&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That will depend if x is coming from a uniform value, in which case the drivers will compile an adjustment of the shader (and put it in a combinator cache). At some point before starting to render.&lt;br&gt;&#xA;(this can cause hiccups in framerate when new uniform values are encountered that cause massive shader rebuild)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, if &lt;code&gt;x&lt;/code&gt; is &lt;strong&gt;dependant&lt;/strong&gt;, which means it comes from a previous calculation that comes from varying values or system values, the compiler will cut your expression in 2 parts, the &lt;code&gt;lerp&lt;/code&gt; calculus and the &lt;code&gt;sample&lt;/code&gt; call.&lt;br&gt;&#xA;It will move the sampling of the texture in the first instructions of the shader. (provided it's unconditional in the shader flow).&lt;br&gt;&#xA;So you're basically screwed when it's 0, you'll pay the sampling anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In your case (0-2 wrt frag pos), I would recommend using &lt;code&gt;if (inrect(0,0,1,1))&lt;/code&gt; because this is a low frequency &quot;step&quot; function, which means very good branch elimination in 3 quarters of the screen.&lt;/p&gt;&#xA;" OwnerUserId="1614" LastActivityDate="2016-06-02T05:54:21.460" CommentCount="0" />
  <row Id="2551" PostTypeId="1" AcceptedAnswerId="2552" CreationDate="2016-06-02T08:25:26.957" Score="1" ViewCount="292" Body="&lt;p&gt;I am loading a texture in a WebGL fragment shader (GLSL version 1.00).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my working code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;uniform sampler2D u_texture;&#xA;varying float v_coord;&#xA;&#xA;void main(void) {&#xA;  float inv_coord;&#xA;  gl_FragColor = texture2D(u_texture, vec2(v_coord, 0.5));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;v_coord&lt;/code&gt; holds the texture coordinates for each vertex. What I'm trying now, is to invert the texture coordinates. I thought about something like &lt;code&gt;abs(v_coord - 1)&lt;/code&gt;. However, whenever I try to substract &lt;code&gt;- 1&lt;/code&gt; from &lt;code&gt;v_coord&lt;/code&gt;, my shader fails to compile.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;precision highp float;&#xA;uniform sampler2D u_texture;&#xA;varying float v_coord;&#xA;&#xA;void main(void) {&#xA;  float inv_coord = v_coord - 1; // fails here&#xA;  inv_coord = abs(inv_coord);&#xA;  gl_FragColor = texture2D(u_texture, vec2(inv_coord, 0.5));&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Why does substracting minus one in fragment shader fail for a float data type? How to invert the texture coordinate value correctly?&lt;/p&gt;&#xA;" OwnerUserId="361" LastActivityDate="2016-06-02T08:43:46.127" Title="Why does substracting -1 in shader fail for a float data type?" Tags="&lt;texture&gt;&lt;glsl&gt;&lt;maths&gt;&lt;webgl&gt;&lt;fragment-shader&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="2552" PostTypeId="2" ParentId="2551" CreationDate="2016-06-02T08:43:46.127" Score="7" Body="&lt;p&gt;Without seeing the error message I can't be sure but I think it's failing on the 1 being int instead of a float. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;float inv_coord = v_coord - 1.0;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There is a simpler method, you can do &lt;code&gt;1.0 - v_coord&lt;/code&gt; and do away with the abs.&lt;/p&gt;&#xA;" OwnerUserId="137" LastActivityDate="2016-06-02T08:43:46.127" CommentCount="1" />
  <row Id="2554" PostTypeId="1" CreationDate="2016-06-02T13:29:26.403" Score="5" ViewCount="34" Body="&lt;p&gt;I'm developing a simple educational project to study OpenGL ES.&#xA;I'm try to render some simple OBJ models using Blinn-Phong.&#xA;The first version of the of my implementation calculate all the lighting equation in this way:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    #version 300 es&#xA;&#xA;struct material {&#xA;    vec4 ka;&#xA;    vec4 kd;&#xA;    vec4 ks;&#xA;    float sh;&#xA;};&#xA;&#xA;struct pointLight {&#xA;    vec3 position;&#xA;    vec4 color;&#xA;};&#xA;&#xA;uniform mat4 mvMatrix;&#xA;uniform mat4 mvpMatrix;&#xA;uniform mat4 normalMatrix;&#xA;uniform pointLight light;&#xA;uniform material surfaceMaterial;&#xA;&#xA;layout(location = 0) in vec3 inputPosition;&#xA;layout(location = 1) in vec3 inputNormal;&#xA;&#xA;out vec4 v_color;&#xA;&#xA;void main() {&#xA;&#xA;    //Convert vertex and normal to camera space.&#xA;    vec3 normal = vec3(normalize(normalMatrix * vec4(inputNormal, 0.0)));&#xA;    vec4 vertexPositionHomogeneous = mvMatrix * vec4(inputPosition, 1.0);&#xA;    vec3 vertexPosition = vec3(vertexPositionHomogeneous) / vertexPositionHomogeneous.w;&#xA;&#xA;    //Calculate light direction and view direction.&#xA;    vec3 lightDirection = normalize(light.position - vertexPosition);&#xA;    vec3 viewPosition = vec3(0.0, 0.0, 1.0);&#xA;    vec3 viewDirection = normalize(-1.0 *  vertexPosition);&#xA;&#xA;    //Cosine theta diffuse lambertian component.&#xA;    float cosTheta = max(0.0, dot(normal, normalize(lightDirection)));&#xA;&#xA;    //H vector for specular component.&#xA;    vec3 h = normalize(lightDirection + viewDirection);&#xA;&#xA;    vec4 ambient = surfaceMaterial.ka * light.color;&#xA;    vec4 diffuse = surfaceMaterial.kd * light.color * cosTheta;&#xA;    vec4 specular = surfaceMaterial.ks * light.color * pow(max(0.0, dot(h, normal)), surfaceMaterial.sh);&#xA;&#xA;    //Output position and color for fragment shader.&#xA;    v_color = specular;&#xA;    gl_Position = mvpMatrix * vec4(inputPosition, 1.0);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This version doesn't work. This is what I obtain:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/fMfeG.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/fMfeG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I move the calculation to the fragment shader it works. The is the result:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/ZiDxG.png&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/ZiDxG.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This image is obtained using a fragment shader that contains the same lighting calculation that I used in the vertex shader above. So my question is: why the vertex shader only version is not working? Am I missing something about how vertex and fragment shaders works? &lt;/p&gt;&#xA;" OwnerUserId="2237" LastActivityDate="2016-06-02T13:29:26.403" Title="OpenGL ES vertex/fragment shaders - Blinn phong" Tags="&lt;opengl&gt;&lt;lighting&gt;&lt;opengl-es&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="2555" PostTypeId="1" CreationDate="2016-06-02T14:07:28.077" Score="1" ViewCount="38" Body="&lt;p&gt;As a CG assignment with OpenGL and SDL, I want to create a clone of the videogame WipeOut instead of the classical car racing game. Examples of motion in 2D have been given and are rather simple to understand; the complexity in 3D is that the road is not planar - when the vehicle approaches a cliff or a vertical loop, it should be translated on the y-axis and rotated to be mantained parallel to the road. I have no idea on how to get and update such informations (translation and rotation for the road). I thought about creating a bezier curve with informations about its derivative (overlapping the road) and keeping track of the movements of the vehicle with respect to the curve, but I don't know if it's the right (simplest) way of doing this. By the way, being able to track the position of the vehicle relative to the curve would also solve some collision problems by limiting its distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, this feature is not mandatory - I could just keep a planar road - but I'd like to understand how to do it. Suggestions?&lt;/p&gt;&#xA;" OwnerUserId="3472" LastEditorUserId="38" LastEditDate="2016-06-03T04:07:00.677" LastActivityDate="2016-06-03T17:30:58.390" Title="&quot;Anchor&quot; vehicle to 3D path with OpenGL / SDL" Tags="&lt;opengl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2556" PostTypeId="1" CreationDate="2016-06-03T06:48:37.310" Score="12" ViewCount="184" Body="&lt;p&gt;So I know that I'm basically asking to identify the main problem to be solved in creating realistic 3d Graphics, but as someone without much technical experience in the field, I'm curious that if these problems can be identified, what the issue is with implementing them programmatically.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The picture below from Hitman is what I would call &quot;ultra-realistic&quot;. But, it still definitely looks like a 3d model. If you were to put a photograph of the same scene with a real actor and background, there would probably be some differences obvious enough to be able to point out which is which.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is that? &amp;lt; 100% accurate subsurface scattering? Is the lighting ever-so-slightly off? etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/UH2r1.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/UH2r1.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS, sorry for tag choices. I am not familiar enough with this SE to know what to choose. Please edit better ones in, if you know any.&lt;/p&gt;&#xA;" OwnerUserId="3477" LastEditorUserId="2479" LastEditDate="2016-06-03T16:08:51.047" LastActivityDate="2016-06-05T17:05:28.107" Title="What physical properties are “lacking” to keep this 3D scene from looking like a real photograph?" Tags="&lt;rendering&gt;&lt;physics&gt;&lt;photo-realistic&gt;&lt;uncanny-valley&gt;" AnswerCount="4" CommentCount="2" FavoriteCount="2" />
  <row Id="2557" PostTypeId="2" ParentId="2556" CreationDate="2016-06-03T10:02:48.540" Score="4" Body="&lt;p&gt;Hard to say because we can not see the code. Subsurface scattering might be part of that equation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would just point out that human brains are extremely specialized in facial recognition. It has been postulated that the brain has a inbuilt defence mechanism to detect alien impostors/anomalous people. You are right in middle of what is known as &lt;a href=&quot;https://en.m.wikipedia.org/wiki/Uncanny_valley&quot; rel=&quot;nofollow&quot;&gt;uncanny valley&lt;/a&gt; nothing sort of near perfect seems to get it right, it seems&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is also worth noting that we do not really know what it is that is lacking that puts you in uncanny valley. So it seems to me we do not conclusively know what, if anything, is missing. So the entire thing might be because we are neglecting something important but as of yet unknown.&lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-06-03T11:43:21.333" LastActivityDate="2016-06-03T11:43:21.333" CommentCount="0" />
  <row Id="2558" PostTypeId="2" ParentId="2555" CreationDate="2016-06-03T17:30:58.390" Score="1" Body="&lt;p&gt;I think the most common way to have a the &quot;up and down&quot; movement of a vehicle is to actually build a physics simulation.&#xA;It does not have to be a very sophisticated one and how much realism you want is up to you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But just in each frame update the velocity of the car in downward direction (gravity) until it hits the road. Then if the road has an upward slope it will also push the car up (by colliding with the car).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now you have a new problem and that is the problem that the car remains parallel to the ground-plane. And the physical calculation behind this is a bit more complex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When detecting if your car hit the ground you would have to detect the exact position of the hit (which part of the car hit the floor?). Then using the gravity, mass and some material properties (all of which you could just set globally and assume as easy values for simplicity) you can calculate how the reaction force to the collision will be. And you know that the force will be exerted on the collision point.&#xA;Now when you have the center of mass of the car, and the collision point, and the force created by the ground-collision, you can calculate how to rotate the car.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is only a very rough sketch of how I learned to handle such things in a &quot;real&quot; way. I hope this was not too confusing, if you have additional questions I can rework the answer later.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-06-03T17:30:58.390" CommentCount="4" />
  <row Id="2559" PostTypeId="2" ParentId="2542" CreationDate="2016-06-03T17:38:05.737" Score="2" Body="&lt;p&gt;Though I have never used them myself OpenGL in modern versions gives you something called &quot;Shader Storage Buffer Object&quot; These are buffers that you can fill with your data. They are guaranteed to be able to hold up to 16 MB of data and most implementations seem to have no problem with them taking up the whole GRAM.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This feature is core since OpenGL version 4.3.&#xA;The SSBOs are basically just a buffer that you can put any datatype in (a struct for example) and a huge amount of them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I cannot really tell you how to use them, but I think with the name you will find some resource in the web.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-06-03T17:38:05.737" CommentCount="2" />
  <row Id="2560" PostTypeId="5" CreationDate="2016-06-03T18:01:14.717" Score="0" Body="" OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-06-03T18:01:14.717" LastActivityDate="2016-06-03T18:01:14.717" CommentCount="0" />
  <row Id="2561" PostTypeId="4" CreationDate="2016-06-03T18:01:14.717" Score="0" Body="For questions about the path tracing Monte Carlo algorithm for physically accurate global illumination, or its variants." OwnerUserId="231" LastEditorUserId="231" LastEditDate="2016-06-03T18:01:14.717" LastActivityDate="2016-06-03T18:01:14.717" CommentCount="0" />
  <row Id="2562" PostTypeId="2" ParentId="2556" CreationDate="2016-06-03T21:32:35.773" Score="4" Body="&lt;p&gt;I think you identified the problem yourself in your question :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;it still definitely looks like a 3d model&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It's obviously hard to tell and subjective, but while many things are off in this picture, the expression and the proportions of the character model are what I find most unrealistic. It is to the appreciation of the art director, but most of the time, realism is not the main drive in shaping and animating character models. Here we have all the clichés of bland character design, and a significant lack of effort put in the expressiveness of the character. It feels 100% designed as a game persona. As such, no matter how close to reality all visual phenomenons are transcribed, my brain just won't accept it, as no-one looks like that in real life.&lt;/p&gt;&#xA;" OwnerUserId="2828" LastActivityDate="2016-06-03T21:32:35.773" CommentCount="2" />
  <row Id="2563" PostTypeId="1" AcceptedAnswerId="3570" CreationDate="2016-06-04T00:56:18.453" Score="7" ViewCount="78" Body="&lt;p&gt;I would like to add full monte-carlo volumetric scattering to my path tracer, but I'm having a hard time researching how to do it. Let me explain what I would like to do:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/tOp55.jpg&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tOp55.jpg&quot; alt=&quot;Monte Carlo Volumetric Scattering&quot;&gt;&lt;/a&gt;&#xA;A ray enters a material, and we apply the BTDF, then after some distance, a volumetric scattering event happens, after which (in the isotropic case), the ray scatters in any direction in the sphere. This repeats until the ray exits the material with another BTDF. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My questions are as follows:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;How do I choose the distance between scatter events? Intuition tells me there should be some kind of scatter pdf, which gives the probability to scatter after a certain distance? &#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Would this be correct?&lt;/li&gt;&#xA;&lt;li&gt;Would the pdf be a linear function for isotropic materials?&lt;/li&gt;&#xA;&lt;li&gt;Does this function have a name or something I can Google?&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;Would Beer-Lambert still apply between scatter events?&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I would think not. Since Beer-Lambert is a simplification of actual scattering calculations.&lt;/li&gt;&#xA;&lt;li&gt;Then again, perhaps Beer-Lambert is a calculation at the micro scale, and path tracing is on a macro scale.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;What is the volumetric equivalent to a BSDF? It looks like I can use a phase function such as Henyey-Greenstein to determine the new direction, but what do I use for attenuation?&lt;/li&gt;&#xA;&lt;li&gt;Lastly, what are some better Google phrases for Monte-Carlo volumetric scattering? &#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Searching volumetric scattering, or SSS, ends up giving papers, methods, and blog posts about the simplfications of full Monte-Carlo simulation (Dipole, in-scattering, out-scattering, diffusion, etc.)&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="310" LastEditorUserId="310" LastEditDate="2016-06-06T22:50:02.707" LastActivityDate="2016-06-07T03:54:51.840" Title="Full Monte-Carlo Volumetric Scattering" Tags="&lt;pathtracing&gt;&lt;monte-carlo&gt;&lt;volumetric-scattering&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="2564" PostTypeId="2" ParentId="2556" CreationDate="2016-06-04T01:00:33.120" Score="2" Body="&lt;p&gt;I see several things that are off in addition to what others have said. He doesn't appear to be breathing. He's in a very cold place (it appears to be snowing), yet there's no breath in front of his nose or mouth. It's snowing, but none of the snow is in front of him. It's all behind him, or on his clothes, but not falling around him.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I feel like our brains pick up on the subtler things, too. Like the fact that his eyebrows are just a flat texture mapped onto the geometry of his face. They're not separate hairs. Even though you couldn't resolve them very well at this distance, I think we still notice it a little. There are no specular highlights on them, either.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And to me his skin looks rubbery, not alive. I know that in the cold my skin gets a little blotchy - some parts get more pinkish than others. His coloring is very even.&lt;/p&gt;&#xA;" OwnerUserId="3003" LastActivityDate="2016-06-04T01:00:33.120" CommentCount="0" />
  <row Id="2565" PostTypeId="1" CreationDate="2016-06-04T10:25:46.563" Score="0" ViewCount="12" Body="&lt;p&gt;I am trying to understand the algorithmic perspective of Selective Laser Sintering. In one of the article titled - &quot;Software Testbed for Selective Laser Sintering&quot; , it is mentioned within the section of &quot;Toggle Point Generation&quot; that the scan conversion technique from raster display is used here. Then I went to the understand the basic of scan conversion from &quot;Computer Graphics - Principles and Practice&quot;. But got stuck with the following concepts while trying to map them to the concept of SLS:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Filling The Boundary Pixels - The basic idea as depicted in the reference book is can be found in page 92. There is no point of re-iterating it here again. Now When I try to map the concept in SLS, the rightmost cell will never be sintered and each rectangle to be missing its topmost row.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not understand how to deal with the above issue when it comes to sinter accurate model parts, because you cannot have any missing cells unsintered or oversintered.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="2712" LastActivityDate="2016-06-04T10:25:46.563" Title="Scan Line Conversion concept in Additive manufacturing" Tags="&lt;scanline&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2016-06-04T13:30:53.190" />
  <row Id="2566" PostTypeId="2" ParentId="2518" CreationDate="2016-06-04T11:55:24.110" Score="4" Body="&lt;p&gt;It speed does not matter, I suggest to use a truncated sinc or a Lanczos isotropic kernel: to compute a target pixel, you back-rotate the filter and  convolve it with the image. Since it is isotropic, it is separable and you can even use a square filter parallel to the axis of the source image.&lt;/p&gt;&#xA;" OwnerUserId="1810" LastActivityDate="2016-06-04T11:55:24.110" CommentCount="0" />
  <row Id="2567" PostTypeId="1" CreationDate="2016-06-04T17:54:49.740" Score="1" ViewCount="29" Body="&lt;p&gt;I know that OpenGL uses a projection matrix to project the view frustum directly into the canoical volume [-1, 1]^3. But how would I project the frustum into an orthographic cuboid, which would then get projected by an orthographic matrix into the canonical volume? This is purely about the theory behind the workings, I don't intend to implement it. I hope somebody can help.&lt;/p&gt;&#xA;" OwnerUserId="3484" LastActivityDate="2016-06-04T17:54:49.740" Title="How to project a view frustum in an orthographic cuboid?" Tags="&lt;projections&gt;&lt;perspective&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="2568" PostTypeId="2" ParentId="2556" CreationDate="2016-06-05T00:44:00.993" Score="8" Body="&lt;p&gt;Few things, but usually this is what it takes to make the difference:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1- the material reflection at his head, he is bald, yet the diffuse texture shows color difference where the hair is, this means he has a shaved head, not a natural bald, this should translate in reflection, take a look at his head, top right (top left for the image), the reflection is very smooth, no bumps at all, there should be some indication in reflection to the tiny hairs that you can see in the texture map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2- the wrinkles in his face need to be emphasized, he looks like some kind of a worrier, yet he has very smooth skin like a 15 year old has, I'm not talking about deep scars, I am talking about skin texture and bumpiness due to rough way of life, skin bumps, moles, a minor infection that will affect both coloration and bumpiness.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3- his mouth is off, the size and positioning, I know that many people has small mouths, and a relatively large distance between the nose and the mouth, but that drives the attention of the viewers, it feels wrong, even if it was a photo of a real person, you can spare this kind of dragging attention to extreme shapes if reality is what you're after.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4- This is a minor thing, but it had a role to play, the background and outfit, they look fake, this drives the viewers automatically and subconsciously to feel that this is not real, try to put more effort in these, making the outfit less uniform, and the background to have more realistic pattern, with more hue variation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;5- And last, the eyes, there's no reflection, yet the lighting is low and the image is small, so I am not sure if this is due to lighting or to the material itself.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That all been said, all these remarks are minor, the artwork looks awesome, I don't think a CG artwork should look like a photo, a perfectly realistic CG artwork that looks exactly like a photo, will become just another photo to the viewer.&lt;/p&gt;&#xA;" OwnerUserId="3487" LastEditorUserId="3487" LastEditDate="2016-06-05T17:05:28.107" LastActivityDate="2016-06-05T17:05:28.107" CommentCount="4" />
  <row Id="3568" PostTypeId="2" ParentId="2516" CreationDate="2016-06-05T05:33:21.820" Score="0" Body="&lt;p&gt;I can not really suggest a good algorithm, because i really havent found one that would have a acceptable runtime requirements for huge graphs. Granted i havent done much programming in this area as of lately so im not really knowledgeable of the state of the art.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I know there are pretty decent commercial libraries available at &lt;a href=&quot;https://www.tomsawyer.com/products/layout/&quot; rel=&quot;nofollow&quot;&gt;Tom Sawyer Software&lt;/a&gt;, i havent used the newest offering but layout is used in Maya as far as I know. Another source of layout libraries is &lt;a href=&quot;http://www.yworks.com/&quot; rel=&quot;nofollow&quot;&gt;yWorks&lt;/a&gt;, wich makes the free yEd editor that I use when I need to draw ordered graphs. Now yEd might be worth checking because it gives names to graphing algorithms that seem to match some of the literature. &lt;/p&gt;&#xA;" OwnerUserId="38" LastEditorUserId="38" LastEditDate="2016-06-05T05:38:24.993" LastActivityDate="2016-06-05T05:38:24.993" CommentCount="6" />
  <row Id="3569" PostTypeId="1" CreationDate="2016-06-06T18:10:06.110" Score="4" ViewCount="30" Body="&lt;p&gt;For a 3D scene in the World Coordinates, its View Reference Point is at c=(0,3,4), and&#xA;a viewer is looking towards its origin O (0,0,0). Construct a transform matrix which will&#xA;map World Coordinate points to a right-handed (UVN) viewing space, so that c is the&#xA;origin, the line joining c to O is the positive N axis, and the View-Up Vector is (0,0,1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My Attempt&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VRP = (0,3,4)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VUP = (0,0,1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;VPN = (0,0,0,) - c = (0,-3,-4)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;n = VPN / |VPN| = (0,-0.6,-0.8)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Stuck Here&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;u = (n x VUP) / |(n x VUP)| = &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm confused how we would solve u now?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After we have found u, I can just find v easy and then sub those values into the relevant matrices&lt;/p&gt;&#xA;" OwnerUserId="1971" LastActivityDate="2016-06-06T18:10:06.110" Title="View Reference Coordinate System" Tags="&lt;matrices&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="3570" PostTypeId="2" ParentId="2563" CreationDate="2016-06-07T03:47:47.680" Score="5" Body="&lt;p&gt;First of all, a good reference for Monte Carlo path tracing in participating media is &lt;a href=&quot;http://www.cs.cornell.edu/courses/cs6630/2012sp/notes/09volpath.pdf&quot;&gt;these course notes&lt;/a&gt; from Steve Marschner.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way I like to think about volume scattering is that a photon traveling through a medium has a certain probability per unit length of interacting (getting scattered or absorbed). As long as it doesn't interact, it just goes in a straight line unimpeded and without losing energy. The greater the distance, the greater the probability that it interacts somewhere in that distance. The interaction probability per unit length is the coefficient $\sigma$ that you see in the equations. We usually have separate coefficients for scattering and absorption probabilities, so $\sigma = \sigma_s + \sigma_a$.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This probability per unit length is exactly the origin of the Beer-Lambert law. Slice a ray segment into infinitesimal intervals, treat each interval as an independent possible place to interact, then integrate along the ray; you get an &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_distribution&quot;&gt;exponential distribution&lt;/a&gt; (with rate parameter $\sigma$) for the probability of interaction as a function of distance.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, to answer your questions directly:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;You &lt;em&gt;can&lt;/em&gt; technically choose the distance between events however you want, as long as you correctly weight the path for the probability that a photon can make it between two adjacent events without interacting with the medium. In other words, each path segment within the medium contributes a weight factor of $e^{-\sigma x}$, where $x$ is the length of the segment. (This is assuming a homogeneous medium, but see section 4.2 in the Marschner notes linked above for what to do if it's inhomogeneous.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given this, a usually good choice for the distance is to importance-sample it from the exponential distribution. In other words, you set $x = -(\ln \xi)/\sigma$ and then leave out the $e^{-\sigma x}$ factor from the path weight.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then, to account for absorption, you can use Russian roulette to kill off a fraction $\sigma_a/\sigma$ of the paths at each event. This is particularly necessary for very large or infinite media (think atmospheric scattering) where the path could bounce around for an arbitrarily long time if it's not killed. If you're only dealing with small and not-too-dense media, then it might be better to just factor in a weight of $1 - \sigma_a/\sigma$ per event, rather than using Russian roulette.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;No, if you follow the importance-sampling procedure just described, Beer-Lambert is already incorporated implicitly in the sampling, so you don't want to apply it to the path weights.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The volumetric equivalent to a BSDF is the combination of the scattering and absorption coefficients $\sigma_s, \sigma_a$ and the phase function. By convention, the coefficients control the overall balance of transmission, scattering, and absorption, while the phase function is always normalized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;You could do something like this for BSDFs too; you could factor out the overall albedo, and have the directional dependence always be normalized. It's mostly a matter of convention AFAICT.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Try &quot;participating media&quot; (that is, a volumetric &quot;medium&quot;&amp;mdash;plural &quot;media&quot;&amp;mdash;which &quot;participates&quot; in light transport), and &quot;volumetric path tracing&quot;.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="48" LastEditorUserId="48" LastEditDate="2016-06-07T03:54:51.840" LastActivityDate="2016-06-07T03:54:51.840" CommentCount="4" />
  <row Id="3571" PostTypeId="1" AcceptedAnswerId="3572" CreationDate="2016-06-07T04:35:00.773" Score="1" ViewCount="45" Body="&lt;p&gt;I am implementing simple rotation but the object is not rotating around the local center instead it is rotating around world origin.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the code :-  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Matrix2D m = Matrix2D.Rotate((float) (Math.PI));&#xA;&#xA;    Point p =  new Point(new Vector2D(-0.1f,0.7f),this, new PixelData(255, 255, 0, 0));&#xA;    Point p2 = new Point(new Vector2D(0,0.8f),this, new PixelData(255, 255, 0, 0));&#xA;    Point p3 = new Point(new Vector2D(0.1f,0.7f),this, new PixelData(255, 255, 0, 0));&#xA;&#xA;    Vector2D v = Matrix2D.MatrixTimesVector2D(m, p.getPosition());&#xA;    Vector2D v2 = Matrix2D.MatrixTimesVector2D(m, p2.getPosition());&#xA;    Vector2D v3 = Matrix2D.MatrixTimesVector2D(m, p3.getPosition());&#xA;&#xA;    Point z = new Point(v, this, p.getColour());&#xA;    Point z2 = new Point(v2, this, p.getColour());&#xA;    Point z3 = new Point(v3, this, p.getColour());&#xA;&#xA;    new Triangle(z2.Transform(), z.Transform(), z3.Transform(),this).DrawTriangle();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I checked my matrices and matrix multiplication is correct.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Result without rotation :-&#xA;&lt;a href=&quot;http://i.stack.imgur.com/CcoSg.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/CcoSg.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And after rotation :-&#xA;&lt;a href=&quot;http://i.stack.imgur.com/3eTRX.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/3eTRX.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why i am getting this weird rotation ?&lt;/p&gt;&#xA;" OwnerUserId="3437" LastActivityDate="2016-06-07T06:23:58.637" Title="Object rotating around origin instead of object center?" Tags="&lt;transformations&gt;&lt;maths&gt;&lt;matrices&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="3572" PostTypeId="2" ParentId="3571" CreationDate="2016-06-07T06:23:58.637" Score="2" Body="&lt;p&gt;Your triangle's coordinates: &#xA;(-0.1f,0.7f)&#xA;(0.0f,0.8f)&#xA;(0.1f,0.7f)&#xA;are defined with the origin at the center of the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Multiplying a rotation matrix by a vertex position will rotate around the point (0,0,0). In your case, that is the center of the screen.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Possible solution:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Define the vertices as: (-0.1f,-0.5f) (0.0f,0.5f,) and (0.1f,-0.5f)&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Rotate the vertices with your rotation matrix (Like you did):&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vector2D v = Matrix2D.MatrixTimesVector2D(m, p.getPosition());&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vector2D v2 = Matrix2D.MatrixTimesVector2D(m, p2.getPosition());&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vector2D v3 = Matrix2D.MatrixTimesVector2D(m, p3.getPosition());&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If you want your triangle at the top of you screen, use a translation matrix:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/XYXUs.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/XYXUs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I'm not sure what language/library you are using, but create a translation matrix by yourself or using a built in fuction. Above dx, dy and dz are the translations in x,y and z.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Multiply your positions v, v2 and v3 by the translation matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Google tranlsation and rotation matrix and you will understand better! Good luck!&lt;/p&gt;&#xA;" OwnerUserId="3434" LastActivityDate="2016-06-07T06:23:58.637" CommentCount="1" />
  <row Id="3573" PostTypeId="1" AcceptedAnswerId="3574" CreationDate="2016-06-07T13:37:24.170" Score="3" ViewCount="30" Body="&lt;p&gt;I have a program I wrote that allows chromakey (green screen) using a web cam. I wrote it using DirectShow - I created a custom filter to blend the web cam video with a graphic. It works fine but requires huge amounts of CPU. Also, from what I understand, DS is deprecated - or at least on its way out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, I'm thinking about re-doing it with OpenGL and using OpenCV to grab the web cam video. I don't have a lot of experience with OpenGL, so my question is: If I create a chromakey shader will it be executed on the GPU and therefore be faster? Or is this something for which OpenGL is not useful?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like the input of someone with experience before I attempt this.&lt;/p&gt;&#xA;" OwnerUserId="4497" LastActivityDate="2016-06-07T14:35:14.047" Title="Chromakey Conversion" Tags="&lt;opengl&gt;&lt;gpu&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="3574" PostTypeId="2" ParentId="3573" CreationDate="2016-06-07T14:35:14.047" Score="2" Body="&lt;p&gt;It almost certainly depends on implementation. I have used CPU only keyers that had no problems keying full hd in real time. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But back to the question would a shader be faster? Most likely, especially if your chroma key is a classic color transform. Then the GPU would shine as it would get to use it massive multicore architecture to full potential.&lt;/p&gt;&#xA;" OwnerUserId="38" LastActivityDate="2016-06-07T14:35:14.047" CommentCount="0" />
  <row Id="3575" PostTypeId="1" CreationDate="2016-06-07T21:13:56.997" Score="20" ViewCount="5278" Body="&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Vulkan_(API)&quot;&gt;From the wiki&lt;/a&gt;: &quot;the Vulkan API was initially referred to as the 'next generation OpenGL initiative' by Khrono&quot;, &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenGL#Vulkan&quot;&gt;and that it is&lt;/a&gt; &quot;a grounds-up redesign effort to unify OpenGL and OpenGL ES into one common API that will not be backwards compatible with existing OpenGL versions&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So should those now getting into graphics programming be better served to learn Vulkan instead of OpenGL? It seem they will serve the same purpose.&lt;/p&gt;&#xA;" OwnerUserId="3477" LastEditorUserId="2041" LastEditDate="2016-06-11T01:01:18.187" LastActivityDate="2016-06-11T01:01:18.187" Title="Should new graphics programmers be learning Vulkan instead of OpenGL?" Tags="&lt;opengl&gt;&lt;api&gt;&lt;vulkan&gt;" AnswerCount="7" CommentCount="1" FavoriteCount="4" />
  <row Id="3576" PostTypeId="2" ParentId="3575" CreationDate="2016-06-07T21:20:37.550" Score="17" Body="&lt;p&gt;Hardly!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This seems a lot like asking &quot;Should new programmers learn C++ instead of C,&quot; or &quot;Should new artists be learning digital painting instead of physical painting.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Especially because it's NOT backward compatible, graphics programmers would be foolish to exclude the most common graphics API in the industry, simply because there's a new one. Additionally, OpenGL does different things differently. It's entirely possible that a company would choose OpenGL over Vulkan, especially this early in the game, and that company would not be interested in someone who doesn't know OpenGL, regardless of whether they know Vulkan or not.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specialization is rarely a marketable skill.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For those who don't need to market their skills as such, like indie developers, it'd be even MORE foolish to remove a tool from their toolbox. An indie dev is even more dependent on flexibility and being able to choose what works, over what's getting funded. Specializing in Vulkan only limits your options.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Specialization is rarely an efficient paradigm.&lt;/p&gt;&#xA;" OwnerUserId="4494" LastEditorUserId="4494" LastEditDate="2016-06-07T21:25:46.773" LastActivityDate="2016-06-07T21:25:46.773" CommentCount="6" />
  <row Id="3577" PostTypeId="2" ParentId="3575" CreationDate="2016-06-08T01:02:11.473" Score="6" Body="&lt;p&gt;The primary appeal of OpenGL (at least to me) is that it works on many platforms. Currently, Vulkan does not work on OSX, and Apple has a competing API called Metal. It's very possible that it will be some time before Apple supports Vulkan, and when they do, Vulkan support may only come to their latest hardware. OpenGL already supports most hardware, and will continue to do so.&lt;/p&gt;&#xA;" OwnerUserId="4499" LastActivityDate="2016-06-08T01:02:11.473" CommentCount="3" />
  <row Id="3578" PostTypeId="2" ParentId="3575" CreationDate="2016-06-08T07:27:09.513" Score="2" Body="&lt;p&gt;It depends on what you want to do. If you want to learn graphics programming only for yourself it really doesn't matter what you will choose.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you thinking about professional work I recommend Vulkan. It is closer to hardware and I think knowledge about what hardware do is important for graphics programmers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;//EDIT&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Little comment to what i meant:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Vulkan is closer to hardware because OpenGL does a lot of stuff under the hood which in Vulkan you do manually, like execute command queue.&#xA;Also memory management is up to application not driver. You need to worry about allocate memory for &lt;code&gt;uniforms&lt;/code&gt;(variable that you pass from CPU to GPU), about tracking them. You have more things to worry about but also it gives more freedom on usage of memory.&#xA;It can sound scary and overwhelming but it really isn't. It's only next API that you need to learn.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Understanding this things also will help in understanding how GPU work. What will allow you to do some optimization both on Vulkan and OpenGL. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;And one more thing that come to my mind, If you are starting to learn graphics programming probably when you will searching a job as one Vulkan will be more popular (maybe more popular than OpenGL) also as far as I know most of companies that are using some graphics API write own functions around it so there is probability that in job you will not writing in OpenGL neither in Vulkan but knowledge about GPU will be always useful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry for my really bad english.&lt;/p&gt;&#xA;" OwnerUserId="3123" LastEditorUserId="3123" LastEditDate="2016-06-08T18:04:37.543" LastActivityDate="2016-06-08T18:04:37.543" CommentCount="0" />
  <row Id="3579" PostTypeId="2" ParentId="3575" CreationDate="2016-06-08T10:29:07.650" Score="14" Body="&lt;p&gt;If you're getting started now, and you want to do GPU work (as opposed to always using a game engine such as Unity), you should definitely start by learning Vulkan. Maybe you should learn GL later too, but there are a couple of reasons to think Vulkan-first.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;GL and GLES were designed many years ago, when GPUs worked quite differently. (The most obvious difference being immediate-mode draw calls vs tiling and command queues.) GL encourages you to think in an immediate-mode style, and has a lot of legacy cruft. Vulkan offers programming models that are much closer to how contemporary GPUs work, so if you learn Vulkan, you'll have a better understanding of how the technology really works, and of what is efficient and what is inefficient. I see lots of people who've started with GL or GLES and immediately get into bad habits like issuing separate draw calls per-object instead of using VBOs, or even worse, using display lists. It's hard for GL programmers to find out what is no longer encouraged.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;It's much easier to move from Vulkan to GL or GLES than vice-versa. Vulkan makes explicit a lot of things that were hidden or unpredictable in GL, such as concurrency control, sharing, and rendering state. It pushes a lot of complexity up from the driver to the application: but by doing so, it gives control to the application, and makes it simpler to get predictable performance and compatibility between different GPU vendors. If you have some code that works in Vulkan, it's quite easy to port that to GL or GLES instead, and you end up with something that uses good GL/GLES habits. If you have code that works in GL or GLES, you almost have to start again to make it work efficiently in Vulkan: especially if it was written in a legacy style (see point 1).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I was concerned at first that Vulkan is much harder to program against, and that while it would be OK for the experienced developers at larger companies, it would be a huge barrier to indies and hobbyists. I posed this question to some members of the Working Group, and they said they have some data points from people they've spoken to who've already moved to Vulkan. These people range from developers at Epic working on UE4 integration to hobbyist game developers. Their experience was that getting started (i.e. getting to having one triangle on the screen) involved learning more concepts and having longer boilerplate code, but it wasn't too complex, even for the indies. But after getting to that stage, they found it much easier to build up to a real, shippable application, because (a) the behaviour is a lot more predictable between different vendors' implementations, and (b) getting to something that performed well with all the effects turned on didn't involve as much trial-and-error. With these experiences from real developers, they convinced me that programming against Vulkan is viable even for a beginner in graphics, and that the overall complexity is less once you get past the tutorial and starting building demos or applications you can give to other people.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As others have said: GL is available on many platforms, WebGL is a nice delivery mechanism, there's a lot of existing software that uses GL, and there are many employers hiring for that skill. It's going to be around for the next few years while Vulkan ramps up and develops an ecosystem. For these reasons, you'd be foolish to rule out learning GL entirely. Even so, you'll have a much easier time with it, and become a better GL programmer (and a better GPU programmer in general), if you start off with something that helps you to understand the GPU, instead of understanding how they worked 20 years ago.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Of course, there's one option more. I don't know whether this is relevant to you in particular, but I feel I should say it for the other visitors anyway.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To be an indie games developer, or a game designer, or to make VR experiences, you don't need to learn Vulkan or GL. Many people get started with a games engine (Unity or UE4 are popular at the moment). Using an engine like that will let you focus on the experience you want to create, instead of the technology behind it. It will hide the differences between GL and Vulkan from you, and you don't need to worry about which is supported on your platform. It'll let you learn about 3D co-ordinates, transforms, lighting, and animation without having to deal with all the gritty details at once. Some game or VR studios only work in an engine, and they don't have a full-time GL expert at all. Even in larger studios which write their own engines, the people who do the graphics programming are a minority, and most of the developers work on higher-level code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Learning about the details of how to interact with a GPU is certainly a useful skill, and one that many employers will value, but you shouldn't feel like you have to learn that to get into 3D programming; and even if you know it, it won't necessarily be something you use every day.&lt;/p&gt;&#xA;" OwnerUserId="2041" LastEditorUserId="2041" LastEditDate="2016-06-10T16:00:06.960" LastActivityDate="2016-06-10T16:00:06.960" CommentCount="3" />
  <row Id="3580" PostTypeId="1" AcceptedAnswerId="3590" CreationDate="2016-06-08T13:28:33.330" Score="5" ViewCount="58" Body="&lt;p&gt;This question was originally asked on Physics, then moved to &lt;a href=&quot;http://cogsci.stackexchange.com/questions/15186/why-does-checkerboard-pattern-on-a-computer-screen-appear-with-a-yellowish-tint&quot;&gt;Cognitive Sciences&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the following image:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/IxOzp.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/IxOzp.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;br&gt;&#xA;You might want to display the image in a new page, in case it gets resized for mobile displays.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the top half, there is a pixel-sized checkerboard pattern with alternating black and white pixels; on the bottom half, there's a black to white gradient. Now, I don't know if you see the same, but for me, when viewing from distance or defocusing my eyes and the top half blending into one color, I can't find any color in the gradient below which would match it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With simple arithmetics, I would guess at first guess the resulting color be either RGB $(0.5, 0.5, 0.5)$ or, with gamma, $(\sqrt{0.5}, \sqrt{0.5}, \sqrt{0.5})$. I cannot help myself, but the resulting color appears a lot warmer than the metallic gray which would appear if you for example zoomed this page or used a blur filter. I tried to add some yellow to the gradient, and the result looks more similar to the perceived color.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, based on the comments, it appears some people perceive yellowish tint and some don't. And I do on my LCD computer screen but not on my mobile display. Thus I guess it's based on some property of the display.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why don't we perceive the resulting color like real gray? Where does the yellow color come from?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a theory: Based on the &lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/4/4d/Pixel_geometry_01_Pengo.jpg&quot; rel=&quot;nofollow&quot;&gt;color arrangement in a typical LCD pixel&lt;/a&gt;, one white pixel would contain the red and green color together and blue on the right side. Blue color appears darker to human eye than colors with the same physical intensity, so a white pixel is more green than red than blue. Green and red have roughy the same perceptional insensity, and mixing red and green color addivitely gives yellow, thus the yellow tint? Well, shouldn't then all white pixels on the display appear a bit yellow?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this possible? Are there any other explanations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A side question: Do you know any computer image scaling algorithm or blur filter that tries to mimic this, simulating a blurry vision of a human eye correctly?&lt;/p&gt;&#xA;" OwnerUserId="4490" LastEditorUserId="2479" LastEditDate="2016-06-09T15:53:08.310" LastActivityDate="2016-06-09T16:20:01.487" Title="Why does checkerboard pattern on a computer screen appear with a yellowish tint?" Tags="&lt;color&gt;&lt;pixel-graphics&gt;&lt;color-management&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="3581" PostTypeId="2" ParentId="3575" CreationDate="2016-06-08T16:04:31.510" Score="-1" Body="&lt;p&gt;You should be learning OpenGL first, as that is the standard for Graphics, across so many platforms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Even if there is a newer library, understanding the basics, and working with a language that is used all over the industry, is very important... Especially since Vulkan wont be used in Big Companies for awhile, since.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;No one wants to adapt right away and end up screwing themselves over with a non-stable Framework/Library.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;They would need to hire / re-train their current Open-GL programmers, and there is no need to.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Also, I've heard that OpenGL, and Vulkan, aren't exactly the same.  I hear Vulkan is a lower level API, and closer to machine code, than OpenGL.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This answer I just found seems to have a lot of great info&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://gamedev.stackexchange.com/questions/96014/what-is-vulkan-and-how-does-it-differ-from-opengl&quot;&gt;http://gamedev.stackexchange.com/questions/96014/what-is-vulkan-and-how-does-it-differ-from-opengl&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Another thing that is something to consider is the issue that happened back with OpenGL 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OpenGL 1 to OpenGL 2 had MAJOR changes, and since many people were using OpenGL1, and hardware supported it, there is heavy backwards compatibility needed to be implemented in some applications/engines, and that sometimes is really painful for the devs to keep supporting it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Besides the support, the language shifted so much, that you had to learn new stuff.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This has happened with frameworks such as JavaFX (from 1.x to 2.x) and Play! Framework (Also 1.x to 2.x).  Complete changes to the framework, meaning we have to relearn... everything...&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;So essentially what you should do is wait until Vulkan is STABLE.  That means wait until Probably the 2.0 patch.  IT doesn't mean you cannot learn about the basics of Vulkan, but just know this might change.  I would assume that a group like Kronos would provide a very stable API from the start, especially since there are multiple Nvidia and AMD engineers working on Vulkan, including a high up Nvidia person overseeing the project apparently, as well as The base of Mantle; however, like any software, we the coders will see how well it works from the start, but many are wary and waiting to see what happens.&lt;/p&gt;&#xA;" OwnerUserId="3420" LastEditorUserId="3420" LastEditDate="2016-06-09T16:13:28.647" LastActivityDate="2016-06-09T16:13:28.647" CommentCount="7" />
  <row Id="3582" PostTypeId="1" CreationDate="2016-06-08T16:20:40.457" Score="-2" ViewCount="44" Body="&lt;p&gt;I am writing a component that is able to determine how long sun rays hit to a specific surface throughout a year. I need to report the results so that I can make some queries. For example, which buildings do not get enough sun light? or which parts of a building get sun lights less than 3 hours throughout a year? How can I achieve this? Is there an algorithm or component for this purpose? Do I have to code it by myself? You can see the results of analysis in the attaching file&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I mean is I want to convert image into data&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1p31F.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1p31F.jpg&quot; alt=&quot;sun exposure analysis&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="4512" LastEditorUserId="4513" LastEditDate="2016-06-10T15:14:54.183" LastActivityDate="2016-06-10T15:14:54.183" Title="Reporting results of 3d sun exposure analysis" Tags="&lt;3d&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2016-06-10T15:15:43.983" />
  <row Id="3583" PostTypeId="2" ParentId="3575" CreationDate="2016-06-09T04:38:43.670" Score="2" Body="&lt;p&gt;I've been &quot;getting in to&quot; graphics programming for a few months now.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now I'm still in High School, and I can tell you I am almost always looking to develop cross platform applications. This is in fact my number one problem with Vulkan - It isn't developed to work on all platforms. I work with OpenGL in Java and have for over 6 months.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another problem I have is with Vulkan's claims is how it claims to be better. While OpenGL certainly isn't updating their website very often. They continue to constantly release updates and I always look forward to new updates that run faster or work better with new hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That being said, while I mostly work with OpenGL I understand basic principles of DirectX and Vulkan. While I don't work with them extensively, especially Vulkan as it's very hard to learn and not as well structured for programming as OpenGl and DirectX.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lastly, I'd like to add that specializing in a single field like I mostly use Java and OpenGL isn't amazingly marketable standalone. If I wanted to get a job at a company that made games, for the most part OpenGL would only be used to make games for Linux and Android development and possibly OSX. DirectX for Windows and Consoles &amp;lt; Which Vulkan can replace in some cases. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't hold out for OpenGL updates forever, and I wont. But it's my preference for developing.&lt;/p&gt;&#xA;" OwnerUserId="4522" LastActivityDate="2016-06-09T04:38:43.670" CommentCount="3" />
  <row Id="3584" PostTypeId="1" AcceptedAnswerId="3585" CreationDate="2016-06-09T06:35:26.767" Score="0" ViewCount="36" Body="&lt;p&gt;This is my code of the algorithm for making triangles :-&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;private void sort(){&#xA;    int temp = Math.round(m_Vertex1.getPosition().getX());&#xA;    int temp2 = Math.round(m_Vertex2.getPosition().getX());&#xA;    int temp3 = Math.round(m_Vertex3.getPosition().getX());&#xA;&#xA;    m_minX = Math.min(temp3, Math.min(temp, temp2));    //Sorting the X values&#xA;    m_maxX = Math.max(temp3, Math.max(temp, temp2));&#xA;&#xA;    temp = Math.round(m_Vertex1.getPosition().getY()); &#xA;    temp2 = Math.round(m_Vertex2.getPosition().getY());&#xA;    temp3 = Math.round(m_Vertex3.getPosition().getY());&#xA;&#xA;    m_minY = Math.min(temp3, Math.min(temp, temp2));    // Sorting the Y values&#xA;    m_maxY = Math.max(temp3, Math.max(temp, temp2));&#xA;}&#xA;&#xA;public void DrawTriangle(){ &#xA;    new Line(m_Vertex1, m_Vertex2, m_display).DrawLine(Optional.of(EdgeOnePoints));     //Getting all the points on the boundary of the triangle&#xA;    new Line(m_Vertex2, m_Vertex3, m_display).DrawLine(Optional.of(EdgeTwoPoints));&#xA;    new Line(m_Vertex1, m_Vertex3, m_display).DrawLine(Optional.of(EdgeThreePoints));&#xA;&#xA;    Vector2D start = Vector2D.zero();&#xA;    Vector2D end = Vector2D.zero();&#xA;&#xA;    for(float i = m_minY; i &amp;lt;= m_maxY ;i++){        //Iterating from top to bottom of the triangle&#xA;&#xA;        for(float j = m_minX; j &amp;lt;= m_maxX; j++){    //Iterating left to right of the triangle&#xA;            Vector2D temp = new Vector2D(j,i);&#xA;&#xA;            if(EdgeOnePoints.contains(temp) || EdgeTwoPoints.contains(temp) || EdgeThreePoints.contains(temp)){  &#xA;                if(start.equals(Vector2D.zero())){      // Getting the first point of intersection between the triangle and the scanline &#xA;                    start = temp;&#xA;                }else{&#xA;                    if(EdgeOnePoints.contains(start) &amp;amp;&amp;amp; !EdgeOnePoints.contains(temp)){     // Checking if start point and end point are on the same edge or not, if not then it is a valid point else not.&#xA;                        end = temp;&#xA;                        break;&#xA;                    }else if(EdgeTwoPoints.contains(start) &amp;amp;&amp;amp; !EdgeTwoPoints.contains(temp)){&#xA;                        end = temp;&#xA;                        break;&#xA;                    }else if(EdgeThreePoints.contains(start) &amp;amp;&amp;amp; !EdgeThreePoints.contains(temp)){&#xA;                        end = temp;&#xA;                        break;&#xA;                    }&#xA;                }&#xA;            }&#xA;        }&#xA;&#xA;        if(!start.equals(Vector2D.zero()) &amp;amp;&amp;amp; !end.equals(Vector2D.zero())){ //if both start and end points are available then draw then fill all the points in between them.&#xA;            for(float j = start.getX(); j &amp;lt;= end.getX(); j++){&#xA;                new Point(new Vector2D(j, i), PixelData.white(), m_display).DrawPoint();&#xA;            }&#xA;        }&#xA;&#xA;        start = Vector2D.zero(); &#xA;        end = Vector2D.zero();   &#xA;    }&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now i have couple of problems with it :- &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;It is a tortoise, i have tested it on a old machine and just with 2 rotating triangles, the frame rate is below 30.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;i get this weird line in between the triangle at some specific position when rotating the triangle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/6KsEH.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/6KsEH.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;   &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I don't care much about the second problem but can anyone help me making this algorithm fast. should i use something like flood fill or boundary fill algorithm ?&lt;/p&gt;&#xA;" OwnerUserId="3437" LastActivityDate="2016-06-09T07:15:28.370" Title="scan line algorithm is too slow?" Tags="&lt;algorithm&gt;&lt;optimisation&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="3585" PostTypeId="2" ParentId="3584" CreationDate="2016-06-09T07:15:28.370" Score="5" Body="&lt;p&gt;One red flag is the use of &lt;code&gt;new&lt;/code&gt; several times within the loop. Memory allocation doesn't belong in a rasterizer! :) Certainly you should not need to allocate a &lt;code&gt;Point&lt;/code&gt; object (which is used once and then immediately discarded) and do a method call on it, just in order to fill a single pixel! The pixels are hopefully stored as a flat array of bytes, so just write directly to the appropriate address in it. Similarly, don't allocate a &lt;code&gt;Vector2D&lt;/code&gt; every time through, just create one (ideally on the stack, if the language you're using allows for it), and re-use it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Secondly, the algorithm you're using doesn't seem to be a true scanline rasterizer. It iterates over the pixels in each line from left to right, performing edge tests on every single pixel to determine whether it's inside or outside, and recording when that state changes in the &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt; variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way a scanline rasterizer is supposed to work (which is much faster) is to &lt;em&gt;calculate&lt;/em&gt; the intersection points of each edge with the scanline (using good old school algebra line-line intersection math). Ideally you would have already categorized and sorted the three edges as left, right, or horizontal and therefore once you have the intersection points, it's easy to figure out the interval that the triangle covers on the scanline. Then you can iterate over that and shade or fill each pixel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Finally, I'm not sure if this is a real issue or not, but there are a lot of method calls for relatively trivial things like checking whether vectors are equal, or doing an edge-vs-point test. Hopefully the compiler is inlining all those, but you might want to check the generated code to see. If it's not, you could see a fair speedup by manually inlining that stuff, and possibly manually &quot;destructuring&quot; the vectors (i.e. replace a Vector2D variable by two explicit x and y variables).&lt;/p&gt;&#xA;" OwnerUserId="48" LastActivityDate="2016-06-09T07:15:28.370" CommentCount="6" />
  <row Id="3586" PostTypeId="1" CreationDate="2016-06-09T07:40:03.600" Score="3" ViewCount="42" Body="&lt;p&gt;I have been reading a lot of debate on a new feature that will be available in the new version of the game engine Unity 5.4 - called GPU instancing. I understand the importance of instancing in general, both in terms of saving memory and draw calls.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, even after doing some reading, I still couldn't understand what exactly are the differences between GPU instancing and non-GPU instancing - and, more importantly, where the alleged GPU advantages in terms of performance come from in comparison to standard instancing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your knowledge and for any references.&lt;/p&gt;&#xA;" OwnerUserId="2061" LastActivityDate="2016-06-11T15:13:56.427" Title="What are the differences between GPU instancing and standard instancing?" Tags="&lt;gpu&gt;&lt;performance&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="3587" PostTypeId="1" AcceptedAnswerId="3598" CreationDate="2016-06-09T12:46:18.310" Score="1" ViewCount="21" Body="&lt;p&gt;I am trying to devise a GUI system that functions as a virtual patchbay, as seen in something like Logic Studio's environment editor, or seen in virtual synthesizers that use virtual patchcables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm not much of a coder, so I've been experimenting with LiveCode.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The challenge seems to be part code, part graphical. I can't even fathom how a system like that works. What bitmaps should I produce (if any) and how should they be manipulated in code? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any assistance would be greatly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="4528" LastActivityDate="2016-06-10T19:57:34.127" Title="How would I create a virtual cabling system?" Tags="&lt;line-drawing&gt;&lt;physics&gt;&lt;simulation&gt;&lt;edge-detection&gt;&lt;realistic&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="1" />
  <row Id="3588" PostTypeId="1" AcceptedAnswerId="3591" CreationDate="2016-06-09T12:53:14.763" Score="0" ViewCount="43" Body="&lt;p&gt;I have made an 3d solar exposure analysis for a 3d city model that determines how long a surface receive direct sunlight throughout the year. Results are coloured surfaces and legend of coloures that explain how long the colour means. You can see the results in the attaching file. How can I report the results. How can I convert visualization into data? Because I want to query the results of analysis&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/uwKxk.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/uwKxk.jpg&quot; alt=&quot;analysisi results&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="4513" LastActivityDate="2016-06-09T16:33:53.057" Title="How to convert image results into data?" Tags="&lt;raytracing&gt;&lt;3d&gt;&lt;model&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="3589" PostTypeId="2" ParentId="3575" CreationDate="2016-06-09T15:43:51.957" Score="7" Body="&lt;p&gt;Learning graphics programming is about more than just learning APIs. It's about learning how graphics works. Vertex transformations, lighting models, shadow techniques, texture mapping, deferred rendering, and so forth. These have absolutely nothing to do with the API you use to implement them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is this: do you want to learn how to use an API? Or do you want to learn &lt;em&gt;graphics&lt;/em&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In order to do stuff with hardware-accelerated graphics, you have to learn how to use an API to access that hardware. But once you have the ability to interface with the system, your graphics learning stops focusing on what the API does for you and instead focuses on graphics concepts. Lighting, shadows, bump-mapping, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If your goal is to learn graphics concepts, the time you're spending with the API is time you're not spending &lt;em&gt;learning graphics concepts&lt;/em&gt;. How to compile shaders has nothing to do with graphics. Nor does how to send them uniforms, how to upload vertex data into buffers, etc. These are tools, and important tools for doing graphics work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But they aren't actually graphics concepts. They are a means to an end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It takes a &lt;em&gt;lot&lt;/em&gt; of work and learning with Vulkan before you can reach the point where you're ready to start learning graphics concepts. Passing data to shaders requires explicit memory management and explicit synchronization of access. And so forth.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By contrast, getting to that point with OpenGL requires less work. And yes, I'm talking about modern, shader-based core-profile OpenGL.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Just compare what it takes to do something as simple as clearing the screen. In Vulkan, this requires at least some understanding of a large number of concepts: command buffers, device queues, memory objects, images, and the various WSI constructs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In OpenGL... it's three functions: &lt;code&gt;glClearColor&lt;/code&gt;, &lt;code&gt;glClear&lt;/code&gt;, and the platform-specific swap buffers call. If you're using more modern OpenGL, you can get it down to two: &lt;code&gt;glClearBufferuiv&lt;/code&gt; and swap buffers. You don't need to know what a framebuffer is or where its image comes from. You clear it and swap buffers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because OpenGL hides a lot from you, it takes a lot less effort to get to the point where you're actually learning graphics as opposed to learning the interface to graphics hardware.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Furthermore, OpenGL is a (relatively) safe API. It will issue errors when you do something wrong, usually. Vulkan is not. While there are debugging layers that you can use to help, the core Vulkan API will tell you almost nothing unless there is a hardware fault. If you do something wrong, you can get garbage rendering or crash the GPU.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Coupled with Vulkan's complexity, it becomes very easy to accidentally do the wrong thing. Forgetting to set a texture to the right layout may work under one implementation, but not another. Forgetting a sychronization point may work sometimes, but then suddenly fail for seemingly no reason. And so forth.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;All that being said, there is more to learning graphics than learning graphical techniques. There's one area in particular where Vulkan wins.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Graphical &lt;em&gt;performance&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Being a 3D graphics programmer usually requires some idea of how to optimize your code. And it is here where OpenGL's hiding of information and doing things behind your back becomes a problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The OpenGL memory model is synchronous. The implementation is allowed to issue commands asynchronously so long as the user cannot tell the difference. So if you render to some image, then try to read from it, the implementation must issue an explicit synchronization event between these two tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But in order to achieve performance in OpenGL, you have to know that implementations do this, so that you can &lt;em&gt;avoid it&lt;/em&gt;. You have to realize where the implementation is secretly issuing synchronization events, and then rewrite your code to avoid them as much as possible. But the API itself doesn't make this obvious; you have to have gained this knowledge from somewhere.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With Vulkan... &lt;em&gt;you&lt;/em&gt; are the one who has to issue those synchronization events. Therefore, you &lt;em&gt;must&lt;/em&gt; be aware of the fact that the hardware does not execute commands synchronously. You must know when you need to issue those events, and therefore you must be aware that they will probably slow your program down. So you do everything you can to avoid them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An explicit API like Vulkan forces you to make these kinds of performance decisions. And therefore, if you learn the Vulkan API, you already have a good idea about what things are going to be slow and what things are going to be fast.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you have to do some framebuffer work that forces you to create a new renderpass... odds are good that this will be slower than if you could fit it into a separate subpass of a renderpass. That doesn't mean you can't do that, but the API tells you up front that it could cause a performance problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In OpenGL, the API basically invites you to change your framebuffer attachments willy-nilly. There's no guidance on which changes will be fast or slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So in that case, learning Vulkan can help you better learn about how to make graphics faster. And it will certainly help you reduce CPU overhead.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It'll still take much longer before you can get to the point where you can learn graphical rendering techniques.&lt;/p&gt;&#xA;" OwnerUserId="2654" LastActivityDate="2016-06-09T15:43:51.957" CommentCount="5" />
  <row Id="3590" PostTypeId="2" ParentId="3580" CreationDate="2016-06-09T16:20:01.487" Score="7" Body="&lt;p&gt;Because your monitor is not properly calibrated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On my screen at home the top and bottom parts have the same hue. At my office though, the top part tends to looks a bit yellow compared to the bottom part that looks more red.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The difference: my screen at home was from a series that was decently calibrated out of the factory, and on top of that I did calibrate it properly with a color calibration tool. That was quite some years ago and its colors have probably degraded since, but it still makes a difference.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A poorly calibrated monitor will not display exactly the color intensity requested for R, G and B, resulting in differences between the color that should be displayed and the one actually displayed. It can even vary depending the area of the screen (many consumer level LCD screens tend to have more light leaking near the edges for example).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The image you included is a good example to highlight a calibration issue, because it allows to compare directly the average color at 50% intensity (combining two halves at 0% and 100% intensity) with the color displayed when requesting 50% intensity. Moreover, the gradient allows to see if the hue is constant from 0% to 100% intensity.&lt;/p&gt;&#xA;" OwnerUserId="182" LastActivityDate="2016-06-09T16:20:01.487" CommentCount="1" />
  <row Id="3591" PostTypeId="2" ParentId="3588" CreationDate="2016-06-09T16:33:53.057" Score="1" Body="&lt;p&gt;I think getting the real data with high precision is not easy without some API.&#xA;The first idea would be just to manually use some color based selection tool that you can tell to select all areas in the screenshots according to the lookup table.&#xA;But that would not result in some 3D data, but only segmented images where you can annotate the values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another method would be to make a UV-map of the mesh, and then by making screenshots from various regions try to extract the color-coded texture.&#xA;Then you could read the interpolated color-values of the texture and map them back to the actual values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The last but maybe strangest idea would be to make a huge number of screenshots and then use some multiview object reconstruction algorithm (I don't know if there is software or if you have to write it yourself.). There are algorithms that not only reconstruct the geometry, but also the texture.&#xA;This would then give you a model with the color-texture that again you could map back to the values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But ideally there would be some export function in the software you used. Maybe this is a question you should ask the developer of the software. Maybe they have this function somehow but it is a bit hidden.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-06-09T16:33:53.057" CommentCount="0" />
  <row Id="3593" PostTypeId="1" CreationDate="2016-06-10T13:59:01.767" Score="1" ViewCount="20" Body="&lt;p&gt;I know there are tons of questions about it, but I found nothing useful so far so, here I am.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm trying to render a texture which is attached to a wavefront .obj mesh. I continued to fail (somehow, it was like textures were not correctly loaded), so I wrote a simple example code to see where I was wrong. The sample code follows, and the texture is &quot;correctly&quot; loaded and rendered. However, there are some problems in the render itself: the image is incomplete / blurry / repeated / distorted. I will also post the expected result and the actual result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Working on Linux Ubuntu 14.04, OpenGL 3.0 Mesa 10.5.9.&#xA;Any idea of what is happening?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Texture (copyright Angryfly @ Turbosquid) and result on the right:&#xA;&lt;a href=&quot;http://i.stack.imgur.com/9VeLs.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/9VeLs.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#include &amp;lt;math.h&amp;gt;&#xA;#include &amp;lt;GL/gl.h&amp;gt;&#xA;#include &amp;lt;GL/glu.h&amp;gt;&#xA;#include &amp;lt;SDL2/SDL.h&amp;gt;&#xA;#include &amp;lt;SDL2/SDL_image.h&amp;gt;&#xA;#include&amp;lt;iostream&amp;gt;&#xA;&#xA;int main(int argc, char* argv[]) {&#xA;    SDL_Init( SDL_INIT_VIDEO );&#xA;    SDL_GL_SetAttribute( SDL_GL_DEPTH_SIZE, 16 );&#xA;    SDL_GL_SetAttribute( SDL_GL_DOUBLEBUFFER, 1 );&#xA;    // prepare the window&#xA;    SDL_Window *window = SDL_CreateWindow(argv[0], 0, 0,&#xA;            800, 600,&#xA;            SDL_WINDOW_OPENGL|SDL_WINDOW_RESIZABLE);&#xA;    SDL_GLContext context = SDL_GL_CreateContext(window);&#xA;    // enable textures and bind a name (say 50)&#xA;    glEnable(GL_TEXTURE_2D);&#xA;    glBindTexture(GL_TEXTURE_2D, 50);&#xA;&#xA;    glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,&#xA;                     GL_LINEAR_MIPMAP_NEAREST );&#xA;    glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);&#xA;&#xA;    glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT );&#xA;    glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT );&#xA;    // load image with SDL&#xA;    SDL_Surface *s = IMG_Load(&quot;texture.png&quot;);&#xA;    // the following doesn't work (white space)&#xA;    //glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, s-&amp;gt;w, s-&amp;gt;h, 0, GL_RGB, GL_UNSIGNED_BYTE, s-&amp;gt;pixels);&#xA;    gluBuild2DMipmaps(GL_TEXTURE_2D, GL_RGB, s-&amp;gt;w, s-&amp;gt;h, GL_RGB, GL_UNSIGNED_BYTE, s-&amp;gt;pixels);&#xA;&#xA;    glViewport(0, 0, 800, 600);&#xA;    glClearColor(1,1,1,1);&#xA;    glMatrixMode(GL_PROJECTION);&#xA;    glLoadIdentity();&#xA;    gluPerspective(70, 0.75, 0.2, 1000);&#xA;&#xA;    glMatrixMode(GL_MODELVIEW);&#xA;    glLoadIdentity();&#xA;    glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT );&#xA;    gluLookAt(0, 10, 0, 0, 0, 0, 1, 0, 0);&#xA;&#xA;    glActiveTexture(50);&#xA;    // display texture as 2 triangles.&#xA;    glBegin(GL_TRIANGLES);&#xA;    glTexCoord2d(0,0); glVertex3d(-5, 0, -5);&#xA;    glTexCoord2d(0,1); glVertex3d(-5, 0, 5);&#xA;    glTexCoord2d(1,0); glVertex3d(5, 0, -5);&#xA;&#xA;    glTexCoord2d(0,1); glVertex3d(-5, 0, 5);&#xA;    glTexCoord2d(1,0); glVertex3d(5, 0, -5);&#xA;    glTexCoord2d(1,1); glVertex3d(5, 0, 5);&#xA;&#xA;    glEnd();&#xA;    glFinish();&#xA;    SDL_GL_SwapWindow(window);&#xA;    // wait for quit procedure&#xA;    SDL_Quit();&#xA;    return 0;&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3472" LastActivityDate="2016-06-10T19:58:16.703" Title="Texture rendering in OpenGL" Tags="&lt;opengl&gt;&lt;texture&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="3594" PostTypeId="2" ParentId="1725" CreationDate="2016-06-10T14:16:48.143" Score="1" Body="&lt;p&gt;Cone Step Mapping and Relaxed Cone Step Mapping appear to be very decent algorithms. These rely on a bit of preprocessing of the height field to create a 2D map used for more efficient lookups.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.lonesock.net/files/ConeStepMapping.pdf&quot; rel=&quot;nofollow&quot;&gt;http://www.lonesock.net/files/ConeStepMapping.pdf&lt;/a&gt;&#xA;&lt;a href=&quot;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch18.html&quot; rel=&quot;nofollow&quot;&gt;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch18.html&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="4544" LastActivityDate="2016-06-10T14:16:48.143" CommentCount="0" />
  <row Id="3595" PostTypeId="1" CreationDate="2016-06-10T14:19:06.340" Score="2" ViewCount="18" Body="&lt;p&gt;I'm trying to write a mesh deformer that bends a Cube around an axis like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/iWZh7nR.png&quot; alt=&quot;3D Bend&quot;&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The bend degree in figure A is 90 and something like 35 in B. So if I know my desired bend degree how can I get alpha?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know that the distance between the vertices v1 and v2 stays the same and that i basically just have to rotate the edge alpha degrees. I can't figure out where the center of the Circle has to be, so that the bend looks organic.&lt;/p&gt;&#xA;" OwnerUserId="4543" LastActivityDate="2016-06-10T14:19:06.340" Title="Implement own Bend function" Tags="&lt;3d&gt;&lt;mesh&gt;&lt;model&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="3596" PostTypeId="1" CreationDate="2016-06-10T16:17:59.203" Score="0" ViewCount="19" Body="&lt;p&gt;I want to develop a componet for 3d shadow analysis of 3d citymodel. I decided to using ray tracing for this purpose. For everytime step (for sun's dinamic position during the year) rays will be sent from sun (point light source) and with ray intersection tests surfaces which are and how long are in shadow will be determined. Ara there any components for this purpose such as sun's position, ray tracing etc. or Do I have to code everything from begining till to the end? Any answer would be very helpful &lt;/p&gt;&#xA;" OwnerUserId="4513" LastActivityDate="2016-06-10T16:52:28.663" Title="Components that can be used for 3d shadow analysis, sunhours of surfaces throughout the year in a 3d model?" Tags="&lt;raytracing&gt;&lt;shadow&gt;&lt;simulation&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="3597" PostTypeId="2" ParentId="3596" CreationDate="2016-06-10T16:52:28.663" Score="0" Body="&lt;p&gt;I use Nvidia OptiX for Ray casting and tracing purposes. It is based on CUDA and gives you a library that runs on GPU and is capable of building and traversing efficient acceleration structures. You just supply a few programs such as intersection tests and shading and OptiX does the rest.&#xA;I personally think it is really easy to work with it but your milage may vary.&#xA;With this it should be very doable to make your project and even with decent running speed.&lt;/p&gt;&#xA;" OwnerUserId="273" LastActivityDate="2016-06-10T16:52:28.663" CommentCount="2" />
  <row Id="3598" PostTypeId="2" ParentId="3587" CreationDate="2016-06-10T19:52:16.557" Score="3" Body="&lt;p&gt;The shape you’re trying to draw is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Catenary&quot; rel=&quot;nofollow&quot;&gt;catenary&lt;/a&gt;: it’s the shape that a cable/cord of constant density takes when supported at each end. You’ll have to do some research to find a parametric equation for its shape—&lt;a href=&quot;http://mathworld.wolfram.com/Catenary.html&quot; rel=&quot;nofollow&quot;&gt;this page&lt;/a&gt; has a start, though it doesn’t let you substitute in the endpoints so you’ll need some additional work there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once you have an equation that gives you a point along the curve as a function &lt;code&gt;f(t) = (x,y)&lt;/code&gt;, where &lt;code&gt;t&lt;/code&gt; is in the range [0,1], you can draw it the same way you would any other parametric curve: step along the function from 0 to 1 in small increments, drawing individual line segments from the value of &lt;code&gt;f&lt;/code&gt; at the previous increment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For nicer-looking cables, you can follow that process with a wide line in a dark color followed by a narrow line in a lighter one to give the appearance of shading, draw a shadow with the same curve offset vertically in partially-transparent black, etc.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As to the patch ports themselves… you can make grids pretty easily by using an image for a single port and drawing it at regular intervals across the space you want to cover. To figure out which port a given screen position corresponds to (e.g. if the user clicks on a particular one), get that position, divide it by the size of the interval you drew the ports at, and take the ceiling of that value. In other words, if you’re drawing a port every 20 pixels and the user clicks on (42, 27), dividing each coordinate value by 20 gives you (2.1, 1.35), which ceils to (3, 2)—i.e. the third column and second row.&lt;/p&gt;&#xA;" OwnerUserId="506" LastEditorUserId="506" LastEditDate="2016-06-10T19:57:34.127" LastActivityDate="2016-06-10T19:57:34.127" CommentCount="0" />
  <row Id="3599" PostTypeId="2" ParentId="3593" CreationDate="2016-06-10T19:58:16.703" Score="4" Body="&lt;p&gt;This type of artifact is a tell-tale sign that you've messed up your texture image format at the byte level. It's hard to tell what exactly is wrong, but it's something along the lines of bad pitch or a component layout mismatch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have a read of &lt;a href=&quot;https://www.opengl.org/wiki/Image_Format&quot; rel=&quot;nofollow&quot;&gt;this wiki page&lt;/a&gt; and try again. &lt;code&gt;glTexImage2D&lt;/code&gt; should work, keep trying until it does.&lt;/p&gt;&#xA;" OwnerUserId="2817" LastActivityDate="2016-06-10T19:58:16.703" CommentCount="2" />
  <row Id="3600" PostTypeId="1" CreationDate="2016-06-11T04:39:32.547" Score="3" ViewCount="13" Body="&lt;p&gt;When using a Compute Shader, is it possible to keep a buffer in the GPU memory during the whole run-time duration of the application, so whenever the original Compute Shader or other Compute Shaders are executed later they can access data from such buffer and/or update them?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If so, what is the correct way of declaring/referencing buffers when one wants them to be permanent in GPU versus one-time-use only? If that helps, I am using CG in Unity, but an engine/language agnostic answer is fine.&lt;/p&gt;&#xA;" OwnerUserId="4545" LastEditorUserId="4545" LastEditDate="2016-06-11T04:47:32.840" LastActivityDate="2016-06-11T04:47:32.840" Title="Compute shaders: one-time-only versus persistent buffers" Tags="&lt;gpu&gt;&lt;compute-shader&gt;&lt;memory&gt;&lt;buffers&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="3601" PostTypeId="1" CreationDate="2016-06-11T11:59:01.507" Score="0" ViewCount="13" Body="&lt;p&gt;i need a latest banner maker tool for moving the images and other objects.which is the best banner maker tool for this work that available on internet which is downloadeable .please sugges me.&lt;/p&gt;&#xA;" OwnerUserId="4547" LastActivityDate="2016-06-11T11:59:01.507" Title="i need a latest banner maker tool for moving the images and other objects" Tags="&lt;pixel-shader&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2016-06-11T16:56:25.610" />
  <row Id="3602" PostTypeId="2" ParentId="3586" CreationDate="2016-06-11T15:13:56.427" Score="0" Body="&lt;p&gt;My understanding is that in 5.4, instancing will use API calls like glDrawElementsInstanced and shaders need to be modified for instance buffers like in this document: &lt;a href=&quot;https://docs.google.com/document/d/1RS6cVjE8mBVOKqQUuXbaZcPW3kmw3ZWySJwr-5rDwSs/edit&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1RS6cVjE8mBVOKqQUuXbaZcPW3kmw3ZWySJwr-5rDwSs/edit&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;UNITY_INSTANCING_CBUFFER_START (MyProperties)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;UNITY_DEFINE_INSTANCED_PROP (float4, _Color)&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;UNITY_INSTANCING_CBUFFER_END&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="67" LastActivityDate="2016-06-11T15:13:56.427" CommentCount="1" />
</posts>