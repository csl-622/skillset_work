<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="1" PostId="4" Score="2" Text="You could post the relevant shader code here in case of bitrot, no?" CreationDate="2015-08-04T18:17:21.623" UserId="38" />
  <row Id="2" PostId="3" Score="6" Text="Really, it depends on many factors. NVIDIA provides hardware acceleration for vector graphics. Have you seen it? https://developer.nvidia.com/nv-path-rendering-videos" CreationDate="2015-08-04T18:18:45.080" UserId="36" />
  <row Id="3" PostId="4" Score="1" Text="we should have some prettier code markup for shader code, i'll post on meta if someone hasn't beaten me to it!" CreationDate="2015-08-04T18:21:29.027" UserId="56" />
  <row Id="4" PostId="5" Score="0" Text="A crude solution would be to just paint one contrasting color onto another through the stencil, save that buffer out, and count the number of pixels that were altered." CreationDate="2015-08-04T18:28:55.233" UserId="36" />
  <row Id="5" PostId="5" Score="0" Text="Hmm, the specification says *occulsion queries* count the number of fragments that pass the *depth test*, but off the top of my head I'm not sure how that interacts with the stencil test right now." CreationDate="2015-08-04T18:33:09.697" UserId="6" />
  <row Id="6" PostId="6" Score="6" Text="I'd want more than &quot;A recommendation of a solid book&quot; as this question should be given a direct answer by someone who knows about this. We should try to put information on the site instead of pointing to it off-site." CreationDate="2015-08-04T18:34:42.330" UserId="27" />
  <row Id="7" PostId="6" Score="0" Text="@robobenklein Question edited, thought better be careful ;)" CreationDate="2015-08-04T18:37:46.930" UserId="18" />
  <row Id="11" PostId="6" Score="0" Text="@ChristianRau Removing &quot;Thanks&quot; should discussed in a meta, this is different on every stack exchange site..." CreationDate="2015-08-04T18:41:36.920" UserId="18" />
  <row Id="12" PostId="6" Score="1" Text="@poor No, it isn't really. That's something that nowhere ever changes. And as long as we don't have special rules, we employ the general SE ones anyway. But even then, I can hardly imagine anyone for voting this to be allowed here. I've never seen this being good practice on any other SE site." CreationDate="2015-08-04T18:43:15.267" UserId="6" />
  <row Id="18" PostId="10" Score="1" Text="See also http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf" CreationDate="2015-08-04T19:00:48.043" UserId="34" />
  <row Id="19" PostId="10" Score="0" Text="yeah that's a very cool technique, that's the second technique i mentioned, and i link to that same pdf above as well." CreationDate="2015-08-04T19:02:10.423" UserId="56" />
  <row Id="20" PostId="10" Score="0" Text="Woops, missed it, sorry." CreationDate="2015-08-04T19:03:04.390" UserId="34" />
  <row Id="24" PostId="10" Score="1" Text="An adaptive solution for the edge anti-aliasing of distance fields can be found here: http://www.essentialmath.com/blog/?p=151." CreationDate="2015-08-04T19:35:15.710" UserId="1" />
  <row Id="28" PostId="19" Score="2" Text="I'm voting to close this question as off-topic because it doesn't pertain to computer graphics programming specifically." CreationDate="2015-08-04T19:43:58.817" UserId="71" />
  <row Id="29" PostId="19" Score="0" Text="VTC for more visibility; should be discussed. (Although I would love to know the answer either way)." CreationDate="2015-08-04T19:44:27.143" UserId="71" />
  <row Id="30" PostId="19" Score="3" Text="@Qix [Discussion on Meta.](http://meta.computergraphics.stackexchange.com/q/23/16)" CreationDate="2015-08-04T19:46:55.293" UserId="16" />
  <row Id="32" PostId="21" Score="0" Text="It's one character, but `cleaver` -&gt; `clever`. I can't edit it D: Great answer by the way, this makes the most sense so far." CreationDate="2015-08-04T19:54:51.953" UserId="71" />
  <row Id="33" PostId="21" Score="1" Text="@Qix, haha, good catch, thanks!" CreationDate="2015-08-04T19:57:54.950" UserId="54" />
  <row Id="34" PostId="25" Score="0" Text="I will check those right away ! Thanks !" CreationDate="2015-08-04T20:27:28.023" UserId="116" />
  <row Id="35" PostId="25" Score="0" Text="Additionally I also sometimes have a command that will dump a specified fbo out to an image when I press a key. It's the low-tech way." CreationDate="2015-08-04T20:33:29.797" UserId="125" />
  <row Id="36" PostId="27" Score="0" Text="This article is much more &quot;gamer focused&quot; but it might give you some insights... http://www.pcgamer.com/what-directx-12-means-for-gamers-and-developers/" CreationDate="2015-08-04T22:18:49.450" UserId="54" />
  <row Id="37" PostId="5" Score="0" Text="@ChristianRau It seems that only the fragments that pass the depth tests will be counted, but stencil, discard and alpha tests are ignored." CreationDate="2015-08-04T22:54:39.427" UserId="64" />
  <row Id="38" PostId="31" Score="1" Text="Just curious, what is your reason for wanting jpg, is it the smaller file size? In case it helps your specific situation, do you know about dxt compression or distance field textures?" CreationDate="2015-08-05T03:20:34.207" UserId="56" />
  <row Id="39" PostId="15" Score="0" Text="At SIGGRAPH 2014, in advances in real-time rendering, there was talk of subdivision used in call of duty. I don't remember the specifics but there is probably some good info there for you!" CreationDate="2015-08-05T03:21:46.453" UserId="56" />
  <row Id="40" PostId="40" Score="1" Text="Looking at your other answer, it looks like your math background is better than I was working with, but I hope that it still goes into sufficient detail to be helpful. I wanted it to be useful for people of any background coming into this." CreationDate="2015-08-05T04:34:09.823" UserId="174" />
  <row Id="41" PostId="40" Score="1" Text="If you are talking to me, not at all.  Your answer and Bert's are amazingly enlightening.  Thank you so much!  Gotta digest the info a bit now (:" CreationDate="2015-08-05T05:03:31.397" UserId="56" />
  <row Id="42" PostId="38" Score="3" Text="Really interesting question, and though I suspect most of the nitty-gritty details are secret sauce, this might be a good jumping off point for anyone who wants to do the research and write up a summary: http://blog.imgtec.com/powervr/a-look-at-the-powervr-graphics-architecture-tile-based-rendering" CreationDate="2015-08-05T06:05:53.240" UserId="196" />
  <row Id="43" PostId="11" Score="5" Text="I don't know for certain, but I would definitely suspect that glTexSubImage on a texture that has been rendered in the last frame or two will stall the pipeline, since PC drivers often try to buffer up a frame or two, and are not likely to want to make copies of entire textures because of a tiny update. So I would expect double- or triple-buffering the textures (or pixel buffer objects) to be required for maximum performance." CreationDate="2015-08-05T06:14:42.460" UserId="196" />
  <row Id="44" PostId="5" Score="2" Text="@ChristianRau and Maurice, the original [ARB_occlusion_query](https://www.opengl.org/registry/specs/ARB/occlusion_query.txt) spec explicitly says it counts samples passing both depth *and* stencil tests, though. See also [this StackOverflow question](https://stackoverflow.com/questions/26410637/occlusion-queries-with-stencil-test-only)." CreationDate="2015-08-05T06:19:36.760" UserId="48" />
  <row Id="45" PostId="15" Score="4" Text="This sounds like a question best answered by a survey paper on subdivision surfaces, and indeed, searching Google for &quot;subdivison surfaces survey&quot; brings up a number of relevant publications. For example, &quot;Algorithms for direct evaluation [Sta98, ZK02], editing [BKZ01, BMBZ02, BMZB02, BLZ00], texturing [PB00], and conversion&#xA;to other popular representations [Pet00] have been devised and hardware support for rendering of subdivision surfaces has been proposed [BAD+01, BKS00, PS96]&quot; —[Boier-Martin et al., 2005](http://mrl.nyu.edu/~dzorin/papers/boiermartin2005sbt.pdf)." CreationDate="2015-08-05T07:09:06.853" UserId="106" />
  <row Id="46" PostId="15" Score="1" Text="&quot;We also examine the reason for the low adoption of new schemes with theoretical advantages, [and] explain why Catmull–Clark surfaces have become a de facto standard in geometric modelling&quot; —[Cashman, 2011](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02083.x/abstract)." CreationDate="2015-08-05T07:12:13.487" UserId="106" />
  <row Id="48" PostId="31" Score="0" Text="It's unclear to me what your question is. Do you want to know whether it's good to compress using JPEG? Do you want to know what kinds of images compress well with JPEG? Or, are you already using JPEG and you'd like to know how to author your images in order to minimize the artifacts caused by JPEG?" CreationDate="2015-08-05T07:31:18.370" UserId="197" />
  <row Id="49" PostId="28" Score="0" Text="As an extra note OpenGL generally only works on one thread so a graphics intensive app could max out one core. Something like Vulkan allows multiple threads to dispatch commands to a queue which means many graphics calls can be made from multiple threads." CreationDate="2015-08-05T07:46:37.030" UserId="214" />
  <row Id="50" PostId="51" Score="0" Text="Maybe triplanar projection, [as seen in GPU Gems 3](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html)? The question is whether the noise would look blurry or otherwise undesirable when it's blending between different projections." CreationDate="2015-08-05T08:37:33.923" UserId="48" />
  <row Id="51" PostId="6" Score="0" Text="@ChristianRau I'll write a meta in the next days. You are right, but I think it only makes sense to remove it, if there are other reasons to edit the question. That's good practice on blender.se, see: http://meta.blender.stackexchange.com/questions/466/editing-questions-to-remove-thank-you-etc" CreationDate="2015-08-05T08:44:18.463" UserId="18" />
  <row Id="53" PostId="52" Score="0" Text="It's also easy to generate an SDF from a voxel model." CreationDate="2015-08-05T09:10:38.670" UserId="48" />
  <row Id="54" PostId="31" Score="0" Text="@AlanWolfe I have encountered a few occasions that I was only able to use JPEG (mostly in web apps), and that's why I needed it to be in JPEG. Thanks, but I was not familiar with dxt compression nor distance field textures, from what I have seen in [wikipedia](https://en.wikipedia.org/wiki/S3_Texture_Compression), dxt algorithms are different from the ones used in JPEG, do you mean they can be used to create JPEGs?" CreationDate="2015-08-05T10:23:18.237" UserId="157" />
  <row Id="55" PostId="31" Score="0" Text="@Moshoka thanks, it's more related to your last question: how to minimize the &quot;visual impact&quot; brought by artifacts in JPEG images?" CreationDate="2015-08-05T10:24:46.870" UserId="157" />
  <row Id="56" PostId="48" Score="0" Text="Thank you very much for your detailed explanation Nathan +1. I understand the compression algorithms have their limitations for JPEG, but I just wanted to find out if there's a right balance of the amount of compression together with other options (when saving it) that can make the artifacts less noticeable. I edited my question with samples." CreationDate="2015-08-05T10:51:02.013" UserId="157" />
  <row Id="57" PostId="57" Score="0" Text="Are you looking for a more accurate way to sample from a distribution, without montecarlo test i mean? If you don't care much about the computational approach (i.e. you're looking for an accurate approach rather than the computational effort) i could have a solution, but it could be optimized of course." CreationDate="2015-08-05T11:33:16.990" UserId="228" />
  <row Id="58" PostId="57" Score="2" Text="Do you know the analytical function or can you only sample it? Do you know its analytical derivative?" CreationDate="2015-08-05T12:04:10.887" UserId="182" />
  <row Id="59" PostId="18" Score="0" Text="Thanks for your detailed explanation Benedikt! It really helps to dive into it. +5, if I could ;)" CreationDate="2015-08-05T12:25:10.453" UserId="18" />
  <row Id="60" PostId="5" Score="0" Text="@NathanReed Sounds like you're about to write an answer." CreationDate="2015-08-05T14:00:55.260" UserId="6" />
  <row Id="61" PostId="57" Score="0" Text="@JulienGuertault Does my edit clarify?" CreationDate="2015-08-05T14:07:56.533" UserId="231" />
  <row Id="62" PostId="57" Score="0" Text="@Lukkio I'd like accuracy first, then optimisation can come later once the approach is working." CreationDate="2015-08-05T14:08:44.487" UserId="231" />
  <row Id="63" PostId="29" Score="0" Text="Although all cubic splines are, in a sense, equivalent, it's probably conceptually easier to use Catmull-Rom splines. e.g.  https://www.cs.cmu.edu/~462/projects/assn2/assn2/catmullRom.pdf" CreationDate="2015-08-05T14:51:27.773" UserId="209" />
  <row Id="64" PostId="63" Score="0" Text="So, if `dFdx(p) = p(x1) - p(x)`, then `x1` can be either `(x+1)` or `(x-1)`, depending on the position of pixel `x` in the quad. Either way, `x1` has to be in the same warp/wavefront as `x`. Am I correct?" CreationDate="2015-08-05T15:44:31.643" UserId="88" />
  <row Id="65" PostId="63" Score="3" Text="@ApoorvaJ Essentially the same value for `dFdx` is computed for each of the 2 neighbouring pixels in the 2x2 grid. And this value is just computed using the difference between the two neighbor values, if that is `p(x+1)-p(x)` or `p(x)-p(x-1)` just depends on your notion of what `x` is exactly here. The result is the same however. So yeah, you're correct." CreationDate="2015-08-05T15:46:43.313" UserId="6" />
  <row Id="66" PostId="63" Score="0" Text="@ChristianRau That answers my question. Thanks." CreationDate="2015-08-05T15:47:59.020" UserId="88" />
  <row Id="67" PostId="29" Score="0" Text="Do you think the tau parameter helps or hinders in this case?  I might be wrong but i feel like catmull rom is too &quot;user defined&quot; (and must be tuned), whereas the hermite spline attempts to just use information from the data that's there.  It seems like cubic hermite is closer to a &quot;ground truth&quot;, which i guess would be something like an ideal sinc filter.  What do you think though?" CreationDate="2015-08-05T15:57:32.830" UserId="56" />
  <row Id="68" PostId="53" Score="0" Text="So my question is how do we do this on modern GPUs? I don't think there's any way to query the scanout, and it seems to me you can't really submit per-scanline draw calls. Even if you could -- what guarantees do you have that your draws are going to get there before the scanout?" CreationDate="2015-08-05T16:10:53.227" UserId="197" />
  <row Id="70" PostId="68" Score="0" Text="This doesn't sound like Computer Graphics as our scope intended it to be, this site is specifically oriented towards 3d, rendering, simulation, and the process of viewing them. This sounds like it belongs on graphic design." CreationDate="2015-08-05T17:15:14.783" UserId="27" />
  <row Id="71" PostId="68" Score="0" Text="@robobenklein this would most likely get rejected from graphics design, and its borderline. Besides gamma is about viewing your art" CreationDate="2015-08-05T17:41:02.177" UserId="38" />
  <row Id="72" PostId="72" Score="0" Text="Do we have latex yet?" CreationDate="2015-08-05T17:53:28.320" UserId="38" />
  <row Id="73" PostId="67" Score="1" Text="To come up with this question, you must have had some advantages of hexagonal arrangement of pixels. Can you add those to the question?" CreationDate="2015-08-05T17:54:03.760" UserId="180" />
  <row Id="74" PostId="72" Score="2" Text="No, but [hopefully we will in the future](http://meta.computergraphics.stackexchange.com/q/18/6). Feel free to add you answer as an example for its necessity, though." CreationDate="2015-08-05T18:18:01.940" UserId="6" />
  <row Id="75" PostId="68" Score="5" Text="I'd say this post is on topic. It seems to relate heavily to GPU Gems 3: Chapter 24. The Importance of Being Linear&#xA;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-05T18:30:29.643" UserId="56" />
  <row Id="76" PostId="68" Score="0" Text="Ah actually i lied... This question would be welcome to GD.SE because a similar is there. IF i can get to it first, if im away its most likely gets modded to oblivion." CreationDate="2015-08-05T18:32:39.813" UserId="38" />
  <row Id="77" PostId="68" Score="0" Text="@AlanWolfe i used to own that book, somebody at work stole it from my hand reference." CreationDate="2015-08-05T18:37:02.960" UserId="38" />
  <row Id="78" PostId="74" Score="2" Text="You dont need to store each color plane in each pixel even, or have the planes in same configuration.  BUT you loose separable filtering" CreationDate="2015-08-05T19:07:42.360" UserId="38" />
  <row Id="79" PostId="73" Score="1" Text="No latex support yet." CreationDate="2015-08-05T19:13:27.257" UserId="38" />
  <row Id="80" PostId="74" Score="0" Text="Good points! Separable filtering is pretty big.  I wonder if you could do 3 axis filtering for hexagons?" CreationDate="2015-08-05T19:16:29.310" UserId="56" />
  <row Id="81" PostId="68" Score="5" Text="Whether something is on topic elsewhere is not relevant to this decision. We just need to know whether it's on topic *here*." CreationDate="2015-08-05T19:20:25.787" UserId="231" />
  <row Id="82" PostId="68" Score="1" Text="I'm deliberately posting borderline questions to measure where the border is. Please raise as many uncertainties as possible on Meta." CreationDate="2015-08-05T19:21:00.567" UserId="231" />
  <row Id="83" PostId="72" Score="0" Text="If you want to show how your answer *would* look if we had MathJax, to help make the case, you can use [mathurl.com](http://mathurl.com)." CreationDate="2015-08-05T19:36:50.790" UserId="231" />
  <row Id="84" PostId="68" Score="3" Text="Being welcome elsewhere is *never* a proper reason alone to declare something off-topic *here*." CreationDate="2015-08-05T19:43:17.330" UserId="6" />
  <row Id="85" PostId="67" Score="0" Text="@nitishch it was only the one advantage I had in mind. Does my edit get it across better?" CreationDate="2015-08-05T19:57:54.847" UserId="231" />
  <row Id="86" PostId="68" Score="2" Text="@robobenklein nowhere in this stack's scope states this stack is for 3D graphics specifically." CreationDate="2015-08-05T20:03:42.837" UserId="71" />
  <row Id="87" PostId="68" Score="1" Text="Such discussions are important to have - that's what this private beta is all about. So I've raised it on [meta](http://meta.computergraphics.stackexchange.com/questions/35/are-questions-about-gamma-on-topic)" CreationDate="2015-08-05T20:06:19.210" UserId="231" />
  <row Id="88" PostId="53" Score="1" Text="@Mokosha Correct, there's no way to query the scanout directly AFAIK. At best, you can figure out when vsync is (via some OS signal) and estimate where the scanout is by timing relative to that (knowing details of the video mode). For rendering, you can experiment to find out how long it usually takes between glFlush and when the rendering is done, and make some guesses based on that. Ultimately, you have to build in some slack in your timing in case of error (e.g. stay 2-3 ms ahead of scanout), and accept that there will be probably be occasional artifacts." CreationDate="2015-08-05T20:39:58.377" UserId="48" />
  <row Id="89" PostId="70" Score="2" Text="Matt Pettineo's blog posts [How To Fake Bokeh](https://mynameismjp.wordpress.com/2011/02/28/bokeh/) and [Bokeh II: The Sequel](https://mynameismjp.wordpress.com/2011/04/19/bokeh-ii-the-sequel/) are great introductions to how to practically implement post-process DoF and address the typical artifacts you get from it." CreationDate="2015-08-05T20:55:43.830" UserId="48" />
  <row Id="90" PostId="75" Score="0" Text="I remember that answer last time round :) (I was githubphagocyte back then). Interesting to see it expanded into a blog post." CreationDate="2015-08-05T21:01:28.423" UserId="231" />
  <row Id="91" PostId="62" Score="1" Text="I'd never heard of Fibonacci grids before; that's pretty cool!" CreationDate="2015-08-05T21:09:35.380" UserId="48" />
  <row Id="92" PostId="73" Score="0" Text="I was looking for something that could be used with an implicit surface even if it doesn't have a parameterisation. Is it always possible to parameterise an implicit surface if the derivative is known?" CreationDate="2015-08-05T21:10:49.550" UserId="231" />
  <row Id="93" PostId="73" Score="0" Text="Any questions that would benefit from MathJax for formulae can be added to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231) to increase our chances of getting MathJax. (This one has already been added.)" CreationDate="2015-08-05T21:12:23.440" UserId="231" />
  <row Id="94" PostId="73" Score="0" Text="Remember that what you need is the distribution function derived from the curvature, you said you can derive everything (by the way what kind of surface have you got? i.e. the equation). Anyway... what do you mean with &quot;derivative known&quot;? do you know an explicit formula of the derivative? or it is implicit too? (i.e. described by means of differential equation)?" CreationDate="2015-08-05T21:27:45.793" UserId="228" />
  <row Id="95" PostId="73" Score="1" Text="By the way... if the curve/surface is algebric (i mean expressed by polynomial or rational staff) there are computational methods based on bspline/nurbs that explain how to perform parametrization of such curves. I had a glance here http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1827&amp;context=cstech, further method (even advanced) could be found in one of my favourite book on Nurbs (The NURBS book by Tiller)." CreationDate="2015-08-05T21:37:28.987" UserId="228" />
  <row Id="96" PostId="4" Score="0" Text="Is that a specific shader language not available in the list of languages covered by our syntax highlighting?" CreationDate="2015-08-05T22:24:19.397" UserId="231" />
  <row Id="97" PostId="4" Score="0" Text="I'm not sure.  It's just GLSL (from webgl to be exact!).  I just did 4 spaces before each line of code, not sure if there's a better way to mark it up..." CreationDate="2015-08-05T22:37:41.687" UserId="56" />
  <row Id="98" PostId="53" Score="0" Text="The effect of increased latency is due to vsync, which causes the front and backbuffer swaps to synchronize with the vblank of the monitor. Double buffering itself doesn't cause this issue by itself and it is useful to minimize flickering because a pixel can only change once in the front buffer." CreationDate="2015-08-06T00:29:21.403" UserId="64" />
  <row Id="99" PostId="57" Score="0" Text="@trichoplax: yes. Now thinking about how it can be used." CreationDate="2015-08-06T05:01:17.340" UserId="182" />
  <row Id="100" PostId="80" Score="0" Text="Wider gamut monitors will also start to be commonplace in near future. For example I do not calibrate to sRGB on my art worstation, but the profile to profile converter makes images still look the same as on my dev machine." CreationDate="2015-08-06T06:41:08.183" UserId="38" />
  <row Id="101" PostId="81" Score="1" Text="It is indeed a complicated paper (still gives me nightmares from time to time!) but I tried to simplify the whole thing a bit. Let me know if you think the below answer should be adjusted in a way or another :)" CreationDate="2015-08-06T08:29:37.570" UserId="100" />
  <row Id="102" PostId="29" Score="0" Text="I don't see how Catmull-Rom is &quot;user defined&quot;. Once you have a sequence of 4 contiguous points, P[i-1], P[i], P[i+1], P[i+2] (4x4 for 2D case) the curve segment is defined between P[i] and P[i+1] and is C1 continuous with the neighbouring segments. &#xA;&#xA;A sinc filter is fine for audio but not video. See Mitchell &amp; Netravali: https://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf   &#xA;IIRC Catmull-Rom is a special case of the family of filters they propose, but I think that filter is an approximating curve so, unlike C-R, might not go through the original points." CreationDate="2015-08-06T09:24:46.650" UserId="209" />
  <row Id="103" PostId="83" Score="0" Text="Do you mean you have both the pre and post gamma images and you want to find the gamma applied?" CreationDate="2015-08-06T13:55:01.850" UserId="100" />
  <row Id="104" PostId="83" Score="0" Text="@cifz Yes, the original image is from [trichoplax's profile](http://i.stack.imgur.com/iukw5.png?s=328&amp;g=1)." CreationDate="2015-08-06T13:56:52.307" UserId="157" />
  <row Id="105" PostId="29" Score="0" Text="That's how he hermite spline works except that the catmull rom spline has an additional parameter tau (tension) that is user defined.  Also, sinc does apply to video, DSP is DSP :P" CreationDate="2015-08-06T14:04:06.173" UserId="56" />
  <row Id="106" PostId="84" Score="3" Text="Though, I think part of his problem as posed in the question is also that he might not know if it is gamma corrected in any way at all or if the colors were not otherwise (linearly or whatever) modified in contrast to mere gamma correction. But ok, in this case you just take a bigger sample size and try to see if it can be approximated sufficiently well with a gamma transformation." CreationDate="2015-08-06T14:07:57.013" UserId="6" />
  <row Id="107" PostId="67" Score="0" Text="The DSP stack exchange (signal processing) probably would have a more formal answer for you on this BTW." CreationDate="2015-08-06T14:16:31.293" UserId="56" />
  <row Id="108" PostId="84" Score="0" Text="Exactly @ChristianRau, ideally it is to determine the difference even when other color transformations were applied. Thanks cifz, so if you sample several of each image's pixels and the resulting G is approximately 1, then we can conclude that no gamma correction was made?" CreationDate="2015-08-06T14:27:07.320" UserId="157" />
  <row Id="109" PostId="29" Score="0" Text="I must admit, I've never seen a tension parameter associated with Catmull Rom splines before, but then I've really only learned of them via Foley &amp; van Dam (et al) or, say, Watt &amp; Watt which, AFAICR, make no mention of such.&#xA;&#xA;Actually, having said that, given there are four constraints - i.e. the curve has to pass through 2 points and have two defined tangents** at those points and it's a cubic - I'm at a bit of a loss as to how there's any more degrees of freedom to support a tension parameter ....&#xA;&#xA;**Unless you mean the tangents can be scaled?" CreationDate="2015-08-06T14:29:47.557" UserId="209" />
  <row Id="110" PostId="29" Score="0" Text="The tension parameter is shown in that first paper you linked.  And yeah I just mean, the fact that there's a tunable parameter means it must be tuned :p. There is a value of tau you can use such that you end up with a regular cubic hermite spline, and I think it might be &quot;1&quot; but not 100% sure on that." CreationDate="2015-08-06T14:34:42.063" UserId="56" />
  <row Id="111" PostId="84" Score="0" Text="If also the other transformations are unknown, then to my limited knowledge I don't know how and if you can find the gamma. Intuitively I'd say you can't" CreationDate="2015-08-06T14:35:32.827" UserId="100" />
  <row Id="112" PostId="84" Score="1" Text="As @ChristianRau correctly said, you can try and fit the combination of transformations into a gamma function, but that will not tell you what gamma was applied on top of the other unknown transform, but rather an gamma that once applied to the source will give you roughly your destination" CreationDate="2015-08-06T14:45:00.293" UserId="100" />
  <row Id="113" PostId="86" Score="2" Text="There is a authoritatively best filter, its a infinitely wide sinc filter. Its just not possible to use it. untill that time lanczos windowed sinc is a good alternative to michell" CreationDate="2015-08-06T15:38:39.483" UserId="38" />
  <row Id="114" PostId="69" Score="1" Text="This is a really good question that has practical implications as realtime interactive stereo rendering is becoming more prevalent with VR" CreationDate="2015-08-06T16:33:09.960" UserId="56" />
  <row Id="115" PostId="82" Score="0" Text="Thanks! I'm still wondering a couple of things. (1) Even with the diffusion profile approximation, the BSSRDF model still requires integrating over a radius of nearby points on the surface to gather incoming light, correct? How is that accomplished in, say, a path tracer? Do you have to build some data structure so you can sample points on the surface nearby a given point? And (2) Why one positive and one negative light? Is the goal for them to cancel each other in some way?" CreationDate="2015-08-06T18:21:57.887" UserId="48" />
  <row Id="116" PostId="83" Score="1" Text="I don't know for certain that the [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/) licence applies to profile images, but I operate under the assumption that anything that I use as an avatar is automatically licensed that way, and in any case I'm very happy for the image to be reused :)" CreationDate="2015-08-06T18:26:47.613" UserId="231" />
  <row Id="117" PostId="83" Score="1" Text="[Meta Stack Exchange](http://meta.stackexchange.com/questions/176253/profile-images-with-copyright/176256#176256) suggests that profile images are also CC BY-SA 3.0 so as long as you give credit you should be OK using anyone's avatar (provided they complied with the requirement to not post works they don't have the right to...)." CreationDate="2015-08-06T18:42:05.193" UserId="231" />
  <row Id="119" PostId="82" Score="0" Text="1) Indeed, what they propose in the paper with their monte carlo ray tracer is a stochastic sampling with a specific density based on distance and extinction coefficient.  I guess you can dart-throw to find samples and use an appropriate acceptance probability based on the extinction coeff. and distance. (1/2)" CreationDate="2015-08-06T19:37:57.230" UserId="100" />
  <row Id="120" PostId="85" Score="1" Text="These two old articles from *The Inner Product* talk about filters for mipmap generation, which might be relevant to you: [Link1](http://number-none.com/product/Mipmapping,%20Part%201/index.html), [link2](http://number-none.com/product/Mipmapping,%20Part%202/index.html)." CreationDate="2015-08-06T19:38:03.663" UserId="54" />
  <row Id="121" PostId="82" Score="0" Text="I know that Jensen published a hierarchical integration approach in 2002 that unfortunately I just read once and a while ago, so I just remember few bits. The core concept was to decouple the sampling from the diffusion approximation and cluster distant samples. IIRC they used an octree as hierarchical structure. &#xA;&#xA;I never got myself into an offline implementation so I am not of so much help on other details on this I am  afraid.  (2/2)" CreationDate="2015-08-06T19:38:05.870" UserId="100" />
  <row Id="122" PostId="82" Score="0" Text="2) It is this way to satisfy some boundary conditions, you want to let the fluence become zero at a certain extrapolated boundary that has a specific distance from the medium. This distance is computed based on scattering coefficient and scattering albedo." CreationDate="2015-08-06T19:38:19.410" UserId="100" />
  <row Id="123" PostId="86" Score="0" Text="For reference, there's also the [Nvidia image tools](https://code.google.com/p/nvidia-texture-tools/)." CreationDate="2015-08-06T19:48:01.210" UserId="54" />
  <row Id="124" PostId="86" Score="0" Text="If you are using a cubic-esque or lanczos filter, do those guys work equally well for scaling up as they do for scaling down?" CreationDate="2015-08-06T20:14:41.767" UserId="56" />
  <row Id="125" PostId="91" Score="1" Text="A good answer is predicated on knowing how the displays will be connected to the available GPUs (getting 9+ display outputs on a single computer isn't trivial) and the software responsible for your render. Of course it's possible to combine the processing power of multiple GPUs to solve your problem, but the devil is in the details of how the outputs are connected and how many computers are involved." CreationDate="2015-08-06T22:58:52.010" UserId="259" />
  <row Id="126" PostId="90" Score="0" Text="&quot;Laser light is a single frequency. White light however is light made up of the sum of different wavelengths of light.&quot;&#xA;(https://www.physicsforums.com/threads/not-hw-why-are-light-waves-in-the-form-of-the-sine-wave-instead-of-some-other-wave.347805/). One thing you would have to account for is the spectral nature of light." CreationDate="2015-08-07T04:17:34.387" UserId="110" />
  <row Id="127" PostId="82" Score="0" Text="@NathanReed Let me know if this has clarified something, otherwise I can try and expand the thoughts in this comment in the answer itself" CreationDate="2015-08-07T05:38:10.067" UserId="100" />
  <row Id="128" PostId="95" Score="2" Text="You could checkout logarithmic depth buffers. There's an article on [gamasutra](http://www.gamasutra.com/blogs/BranoKemen/20090812/85207/Logarithmic_Depth_Buffer.php)" CreationDate="2015-08-07T08:12:14.850" UserId="214" />
  <row Id="129" PostId="29" Score="0" Text="Re that first paper: That serves me right for pasting in the first link I found that seemed suitable :-)    Again, I think I've only ever seen the tangent at a segment end point set to the (assuming I've done the matrix maths correctly) 1/2 the difference of the two surrounding neighbours." CreationDate="2015-08-07T08:25:01.567" UserId="209" />
  <row Id="132" PostId="97" Score="3" Text="Hm yeah, I'm aware that I can get compiler errors. I was hoping for better runtime debugging. I've also used WebGL inspector in the past, but I believe it only shows you state changes, but you can't look into a shader invocation. I guess this could have been clearer in the question." CreationDate="2015-08-07T09:42:23.417" UserId="16" />
  <row Id="133" PostId="96" Score="0" Text="Have you looked into gDEBugger? Quoting the site: &quot;gDEBugger is an advanced OpenGL and OpenCL Debugger, Profiler and Memory Analyzer. gDEBugger does what no other tool can - lets you trace application activity on top of the OpenGL and OpenCL APIs and see what is happening within the system implementation.&quot; Granted, no VS style debugging/stepping through code, but it might give you some insight in what your shader does (or should do). Crytec released a similar tool for Direct shader &quot;debugging&quot; called RenderDoc (free, but strictly for HLSL shaders, so maybe not relevant for you)." CreationDate="2015-08-07T11:06:15.930" UserId="194" />
  <row Id="134" PostId="96" Score="0" Text="@Bert Hm yeah, I guess gDEBugger is the OpenGL equivalent to WebGL-Inspector? I've used the latter. It's immensely useful, but it's definitely more debugging OpenGL calls and state changes than shader execution." CreationDate="2015-08-07T11:47:56.203" UserId="16" />
  <row Id="135" PostId="96" Score="0" Text="I've never done any WebGL programming and hence I'm not familiar with WebGL- Inspector. With gDEBugger you can at least inspect the entire state of your shader pipeline including texture memory, vertex data, etc. Still, no actual stepping through code afaik." CreationDate="2015-08-07T11:57:21.997" UserId="194" />
  <row Id="136" PostId="96" Score="0" Text="gDEBugger is extremely old and not supported since a while. If you are looking from frame and GPU state analysis you this is other question is strongly related: http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl/25#25" CreationDate="2015-08-07T12:00:52.567" UserId="100" />
  <row Id="137" PostId="100" Score="4" Text="This question may be a bit controversial, because several other SE sites don't want questions about best practices. This is intentional to see how this community stands regarding such questions." CreationDate="2015-08-07T12:39:15.597" UserId="16" />
  <row Id="138" PostId="100" Score="2" Text="Hmm, looks good to me. I'd say we are to a large degree a bit &quot;broader&quot;/&quot;more general&quot; in our questions than, say, StackOverflow." CreationDate="2015-08-07T12:40:15.333" UserId="6" />
  <row Id="139" PostId="95" Score="1" Text="When you say &quot;co-planar&quot; do you mean &quot;nearly&quot;  or &quot;exactly&quot; co-planar and if the latter, are those surfaces ever identical surfaces/triangles? The rendering hardware should be deterministic (assuming you aren't submitting in random order) for the last case and not have fighting.  &#xA;If it's a case of non-identical surfaces but exactly co-planar, could you update the model to split surfaces into overlapping and non-overlapping portions?" CreationDate="2015-08-07T12:55:57.560" UserId="209" />
  <row Id="140" PostId="29" Score="0" Text="Ah OK! Anyways yeah, catmull rom / hermite is really freaking awesome I totally agree. Really neat way to interpolate between data points and have C1 continuity at the edges. It really simplifies things (:" CreationDate="2015-08-07T13:36:17.957" UserId="56" />
  <row Id="141" PostId="100" Score="2" Text="StackOverflow went from being a 'ask us' to a 'dont ask us unless you **have to** please' board." CreationDate="2015-08-07T14:24:01.983" UserId="296" />
  <row Id="142" PostId="52" Score="0" Text="There's an article on GPU Gems 3 about the construction of signed distance fields for arbitrary meshes using the GPU, which is freely available here: http://http.developer.nvidia.com/GPUGems3/gpugems3_ch34.html" CreationDate="2015-08-07T14:32:46.747" UserId="281" />
  <row Id="143" PostId="100" Score="0" Text="If it's meant to determine on-topicness, then how about an associated Meta question?" CreationDate="2015-08-07T15:43:26.963" UserId="21" />
  <row Id="144" PostId="100" Score="0" Text="@S.L.Barth I figured I could still do that once anyone objects, but sure, why not." CreationDate="2015-08-07T15:51:43.950" UserId="16" />
  <row Id="145" PostId="100" Score="2" Text="[Discussion on meta.](http://meta.computergraphics.stackexchange.com/q/53/16)" CreationDate="2015-08-07T16:06:09.930" UserId="16" />
  <row Id="146" PostId="1" Score="3" Text="Please don't call these optical tricks &quot;holograms&quot;. Refer to the &quot;[Things often confused with holograms](https://en.wikipedia.org/wiki/Holography#Things_often_confused_with_holograms)&quot; for an overview. In this case, you're talking about a classic [Pepper's ghost](https://en.wikipedia.org/wiki/Pepper%27s_ghost) illusion. No 3D at all, apart from having 4 different perspectives. **update:** I've proposed an edit to adjust the question accordingly." CreationDate="2015-08-07T16:13:30.893" UserId="30" />
  <row Id="147" PostId="86" Score="0" Text="I've not tried lanczos so I can't speak to that. We chose catmull-rom for upscaling, which is a cubic, and it worked well." CreationDate="2015-08-07T17:13:17.077" UserId="125" />
  <row Id="148" PostId="103" Score="1" Text="I'm a bit confused about your premise ... how is a lowpass filter qualitatively any different than downsampling? I mean, I get that the algorithms are different and all but they both gather samples from neighboring pixels and suppress high frequencies. The big difference is the resolution of the result image, otherwise the two operations are isomorphic. Seems like applying both is redundant." CreationDate="2015-08-07T17:20:12.627" UserId="125" />
  <row Id="149" PostId="103" Score="0" Text="Well here's what confuses me.  I know that you can't just downsample an image without getting aliasing.  Doing bicubic interpolation of pixels when making an image larger works really well and looks nice.  Doing the same when making an image smaller SEEMS to work decently, but I wasn't sure if the result is likely to have much aliasing as a result.  I was wondering if technically, you'd need to do some kind of low pass filter on the image before the doing bicubic sampling, or if the bicubic sampling was good enough in practice?  I could see it being a low pass filter of sorts on its own maybe." CreationDate="2015-08-07T17:24:12.550" UserId="56" />
  <row Id="150" PostId="103" Score="1" Text="That Mitchell-Netravali paper I mentioned in the other question addresses this idea specifically - he generalized cubics and then found the parameters that alias the least. That doesn't mean they don't alias at all, but perhaps it would direct you towards which cubic to use to minimize aliasing." CreationDate="2015-08-07T18:30:49.443" UserId="125" />
  <row Id="151" PostId="82" Score="0" Text="Thanks, if you wouldn't mind putting the info into the answer as well, that would be great!" CreationDate="2015-08-07T18:37:29.810" UserId="48" />
  <row Id="152" PostId="95" Score="0" Text="@SimonF, by &quot;co-planar&quot; I mean &quot;exactly co-planar&quot;.  Soapy's solution only works in the &quot;nearly co-planar&quot; case." CreationDate="2015-08-07T18:38:11.437" UserId="158" />
  <row Id="153" PostId="95" Score="0" Text="Could you give an example of your surfaces? The only thing I can think of off the top of my head is duplicate triangles as @SimonF mentioned." CreationDate="2015-08-07T20:28:32.447" UserId="310" />
  <row Id="154" PostId="95" Score="0" Text="@RichieSams the most common case that I can think of is decals, where you don't need exactly duplicate triangles." CreationDate="2015-08-07T23:15:29.387" UserId="259" />
  <row Id="155" PostId="25" Score="0" Text="It's also probably a good idea to make sure you have the right depth test setup, for the rendering going into your visualisation FBO." CreationDate="2015-08-07T23:18:31.293" UserId="259" />
  <row Id="156" PostId="91" Score="1" Text="I found a blog post where someone was able to do it: http://www.rchoetzlein.com/website/multi-monitor-rendering-in-opengl/  @rys is right though. In order to get a solid answer we would need to know more details of your setup." CreationDate="2015-08-08T00:39:22.360" UserId="310" />
  <row Id="157" PostId="106" Score="2" Text="GL_ARB_shading_language_include is not a core feature. In fact, only NVIDIA supports it. (http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_ARB_shading_language_include)" CreationDate="2015-08-08T01:08:40.533" UserId="311" />
  <row Id="158" PostId="82" Score="0" Text="@NathanReed Done :) I tried not to dwell on too much to avoid an even longer answer, but let me know if you think something needs to be added or clarified further!" CreationDate="2015-08-08T09:00:20.137" UserId="100" />
  <row Id="159" PostId="82" Score="0" Text="Just to add a new reference, there's a new model per the dipole approximation presented this year at siggraph: http://people.compute.dtu.dk/jerf/code/dirsss/" CreationDate="2015-08-08T09:09:35.210" UserId="281" />
  <row Id="162" PostId="110" Score="0" Text="From my understanding, I think this does not solve the problem of sharing struct definitions." CreationDate="2015-08-08T17:35:24.210" UserId="311" />
  <row Id="163" PostId="110" Score="0" Text="@NicolasLouisGuillemot: yes you are right, only instructions code is shared this way, not declarations." CreationDate="2015-08-08T17:39:53.133" UserId="182" />
  <row Id="164" PostId="107" Score="4" Text="Re-rendering the same geometry with the same transforms does reliably generate the same depth values every time. (I.e. it's not a *might*, it's a *will*). That's why multi-pass forward lighting works, for instance." CreationDate="2015-08-08T17:51:51.780" UserId="48" />
  <row Id="166" PostId="118" Score="0" Text="Yes but how do you handle change of basis to dimensions? If i have is a R^3 -&gt; R^4 or R^5 mapping? Anyway yes this will make it work atleast halfway." CreationDate="2015-08-08T20:58:31.147" UserId="38" />
  <row Id="167" PostId="118" Score="1" Text="I've not had to approach this problem before, but I'm skeptical that this answer will work since color spaces aren't always linear." CreationDate="2015-08-08T22:39:28.103" UserId="125" />
  <row Id="168" PostId="100" Score="0" Text="I wrote [this](http://stackoverflow.com/a/20605122/1888983) a while back. Beyond `#include`, I find injecting `#define`s is most useful. One shader, many permutations." CreationDate="2015-08-09T00:36:10.310" UserId="198" />
  <row Id="169" PostId="118" Score="0" Text="Dot product surely will come into play" CreationDate="2015-08-09T02:22:05.913" UserId="56" />
  <row Id="170" PostId="111" Score="0" Text="Yep, I think this was my conclusion too. Do you know offhand if this is the case with DX12 or (the unreleased) Vulkan? It has pretty large applications for deferred rendering as you've mentioned and none of the existing alternatives right now seem satisfactory." CreationDate="2015-08-09T03:37:53.700" UserId="87" />
  <row Id="171" PostId="111" Score="0" Text="@jeremyong Sorry, I don't think there are any changes to blending operations in DX12. Not sure about Vulkan, but I'd be surprised; the blending hardware hasn't changed. FWIW, in games I've worked on we did a variant of bullet #3 for deferred decals, and preprocessed the geometry to separate it into non-overlapping groups." CreationDate="2015-08-09T03:58:19.847" UserId="48" />
  <row Id="172" PostId="118" Score="0" Text="@jorgeRodriguez you can treat the space linear for max values" CreationDate="2015-08-09T04:50:36.953" UserId="38" />
  <row Id="173" PostId="111" Score="0" Text="Gotcha thanks for the recommendation. Deferred decals is precisely what I'm implementing" CreationDate="2015-08-09T05:30:12.440" UserId="87" />
  <row Id="176" PostId="124" Score="0" Text="Thanks! It looks like I'll need two depth buffers for this. I.e. one to store and filter out the last depths, and one to do the depth testing for the current render. Correct me if I'm wrong but I assume I'll need two depth textures for the FBO which I swap between each peeling pass." CreationDate="2015-08-09T09:07:19.353" UserId="198" />
  <row Id="177" PostId="124" Score="0" Text="@jozxyqk Correct, two depth buffers are necessary for this procedure." CreationDate="2015-08-09T09:09:45.863" UserId="170" />
  <row Id="178" PostId="24" Score="1" Text="For more information, see [Pepper's Ghost](https://en.wikipedia.org/wiki/Pepper's_ghost)" CreationDate="2015-08-09T09:12:03.990" UserId="158" />
  <row Id="179" PostId="133" Score="0" Text="Yes something like this." CreationDate="2015-08-09T11:40:33.210" UserId="38" />
  <row Id="180" PostId="122" Score="2" Text="Yes, using an appropriate color space is what you want." CreationDate="2015-08-09T11:55:11.810" UserId="182" />
  <row Id="182" PostId="140" Score="0" Text="I've actually been thinking about this. Ive also toyed on asking what other approaches modeling pipes than matrix and quat hierarchies one could use." CreationDate="2015-08-09T13:17:36.497" UserId="38" />
  <row Id="183" PostId="120" Score="0" Text="Have you heard of anyone using the depth buffer value as a sanity check?" CreationDate="2015-08-09T14:16:34.443" UserId="56" />
  <row Id="187" PostId="120" Score="0" Text="@AlanWolfe No, and I think that is because the motion vector texture is usually 2-component: you'd need a change-in-Z component to know what the depth buffer value should be, and that's not nicely bounded by the screen size. I suspect you could get better rejection strategies than that by adding more per-pixel information." CreationDate="2015-08-09T16:48:45.367" UserId="196" />
  <row Id="188" PostId="120" Score="0" Text="Ah OK. What kind of information do you think would be helpful to add. Shading parameter type stuff to be able to tell if it's the same material?" CreationDate="2015-08-09T17:26:05.140" UserId="56" />
  <row Id="189" PostId="120" Score="1" Text="@AlanWolfe I don't have a lot of concrete ideas. I'm not an expert on when temporal reprojection with neighborhood clamping breaks down and produces artifacts and what information would be useful in those situations. Perhaps transparents (no motion vector information) combined with high-frequency lighting are producing artifacts, and you need some obscurance information. Perhaps geometric aliasing is your problem and you need some other information." CreationDate="2015-08-09T18:11:28.410" UserId="196" />
  <row Id="190" PostId="145" Score="1" Text="Should this be several questions." CreationDate="2015-08-09T18:33:42.517" UserId="38" />
  <row Id="191" PostId="145" Score="0" Text="@joojaa Potentially. The answers to these questions are interrelated, though. I'm looking for an answer of the form &quot;well, a photon can do X, Y, or Z when it interacts with media; X is described by the phase function, Y is described by the Beer-Lambert law, …&quot;" CreationDate="2015-08-09T18:37:55.153" UserId="196" />
  <row Id="192" PostId="107" Score="0" Text="@NathanReed Corrected. Thank you for the clarification" CreationDate="2015-08-09T18:44:49.937" UserId="310" />
  <row Id="193" PostId="130" Score="1" Text="I haven't looked in too much detail at the code, but the image seems to be alright. The Fresnel effect shows up as a red ring. With the roughness so high (0.9), it makes sense that the rest of the image is mostly yellow (ie. mostly diffuse). If you lower the roughness, you may get a red specular highlight" CreationDate="2015-08-09T18:49:36.253" UserId="310" />
  <row Id="194" PostId="131" Score="1" Text="Here are two source code resources for [Tiled](https://software.intel.com/en-us/articles/deferred-rendering-for-current-and-future-rendering-pipelines) and [Clustered](https://software.intel.com/en-us/articles/forward-clustered-shading)" CreationDate="2015-08-09T18:52:48.500" UserId="310" />
  <row Id="195" PostId="130" Score="0" Text="@RichieSams I added new images for different roughness values but can't see red shiny specular highlight yet." CreationDate="2015-08-09T20:24:29.623" UserId="83" />
  <row Id="196" PostId="15" Score="0" Text="These comments sound like the perfect material for someone to write a summarising answer." CreationDate="2015-08-09T21:22:15.260" UserId="231" />
  <row Id="198" PostId="41" Score="0" Text="Could you add a brief note to explain why the two different 5 by 5 kernels have slightly different numbers (one summing to 273, the other summing to 256)? It seems like a potential confusion for someone new to this." CreationDate="2015-08-09T22:14:56.317" UserId="231" />
  <row Id="199" PostId="41" Score="0" Text="Similarly, could you explain why the kernel is flipped in your second diagram? I don't think it's relevant to the explanation, but the fact that it's an apparent extra step may hinder understanding to someone who doesn't know that it isn't necessary." CreationDate="2015-08-09T22:16:34.077" UserId="231" />
  <row Id="200" PostId="62" Score="0" Text="That's a fascinating paper. It looks like you can adjust the parameters to give nearer to a square grid or a hexagonal grid, which would allow different approaches to noise generation." CreationDate="2015-08-09T22:43:53.933" UserId="231" />
  <row Id="201" PostId="151" Score="1" Text="Does &quot;trackball&quot; mean a camera that orbits around an object, like in 3D modeling apps? If so, I think it's usually done by just keeping track of the 2D mouse coordinates and mapping x=yaw, y=pitch for the camera rotation." CreationDate="2015-08-09T23:15:07.823" UserId="48" />
  <row Id="202" PostId="150" Score="0" Text="Say that we don't care very much about efficiency. How would we go about doing coverage calculations for boolean operations on shapes? Is that possible in general, or only for specific shapes?" CreationDate="2015-08-09T23:18:37.830" UserId="196" />
  <row Id="203" PostId="154" Score="1" Text="Are you intentionally restricting the question to bounding volume hierarchies, or are you open to other forms of spatial partitioning?" CreationDate="2015-08-09T23:19:31.850" UserId="196" />
  <row Id="204" PostId="154" Score="1" Text="@JohnCalsbeek I've edited to clarify - thanks for pointing out my inadvertent restriction." CreationDate="2015-08-09T23:32:37.030" UserId="231" />
  <row Id="205" PostId="153" Score="0" Text="This is super interesting! So is the Disney paper you linked." CreationDate="2015-08-09T23:41:26.250" UserId="196" />
  <row Id="206" PostId="151" Score="1" Text="@NathanReed the other option is [axis-angle based](http://codereview.stackexchange.com/q/51205/32061), You project 2 mouse points onto a (virtual) sphere and then find the rotation from one to the other." CreationDate="2015-08-09T23:42:13.803" UserId="137" />
  <row Id="207" PostId="46" Score="0" Text="I am selecting this answer since it gives orders of magnitude, which is the closest so far to what I asked, even though the mentioned source doesn't give much explanation." CreationDate="2015-08-09T23:42:35.030" UserId="182" />
  <row Id="208" PostId="118" Score="0" Text="@JorgeRodriguez would it work if you first translate to a linear colour space? For more than 3 primary colours there will in general be more than one combination that approximates a given colour, but finding one of those combinations should be possible with this approach, provided that the chosen primary colours form a basis." CreationDate="2015-08-10T01:02:36.503" UserId="231" />
  <row Id="209" PostId="15" Score="0" Text="@Rahul Would you mind turning your comments and any other information you might have into an answer?" CreationDate="2015-08-10T01:04:58.047" UserId="70" />
  <row Id="210" PostId="130" Score="1" Text="Your 2nd and 3rd images do appear to have less red in general (in the yellow diffuse area) than your original image. This isn't very apparent because adding a little red to a yellow area leaves it a similar colour (orange-yellow rather than yellow). Do you see any more detail of the red distribution if you reduce the yellow significantly? Omitting the yellow altogether may help in identifying what is going wrong." CreationDate="2015-08-10T01:26:42.160" UserId="231" />
  <row Id="212" PostId="15" Score="0" Text="Sorry, I'm at SIGGRAPH and have absolutely no free time (except to write this comment). Someone else should feel free to write an answer using the links in my comments." CreationDate="2015-08-10T04:17:38.400" UserId="106" />
  <row Id="213" PostId="21" Score="1" Text="If you have an expensive per-pixel shader, the stencil buffer can also be used to batch similar threads, reducing divergence and increasing occupancy as in [this OIT paper, sec 2.6](http://goanna.cs.rmit.edu.au/~pknowles/rbs-preprint.pdf) (disclaimer: I'm an author)." CreationDate="2015-08-10T05:45:33.600" UserId="198" />
  <row Id="214" PostId="162" Score="0" Text="Ha, there you go, taking images from others is more efficient than drawing them yourself." CreationDate="2015-08-10T08:05:00.250" UserId="38" />
  <row Id="215" PostId="15" Score="2" Text="Apologies to @NoviceInDisguise for also not having time, but WRT to Catmull-Clark, perhaps one of the reasons for it still being very much in use was DeRose et al's extensions to it to include, e.g. sharpness factors in the tessellation to allow creases etc. http://www.cs.rutgers.edu/~decarlo/readings/derose98.pdf  &#xA;IIRC those extensions weren't initially free to use (but some commercial tools licensed it from Pixar) however, unless I'm mistaken, it now seems to be free e.g. http://graphics.pixar.com/opensubdiv/docs/subdivision_surfaces.html#arbitrary-topology" CreationDate="2015-08-10T08:12:12.877" UserId="209" />
  <row Id="216" PostId="130" Score="0" Text="@trichoplax i reduced the yellow but again no way to see red specular. I just see red ring(fresnel) effect. Doesnt matter what i set value for roughness I cant see specular effect that focused a point." CreationDate="2015-08-10T08:32:09.737" UserId="83" />
  <row Id="217" PostId="162" Score="0" Text="@joojaa Yes, faster if you remember where they were, but without that gratifying feeling of doing it yourself :P. Also I have this stupid [sub-pixel rendering bug](http://superuser.com/q/930036/264276) in chrome so the text is all colourful." CreationDate="2015-08-10T08:44:38.337" UserId="198" />
  <row Id="218" PostId="162" Score="1" Text="Well that subpixel rendering is something that has not been asked. Yet." CreationDate="2015-08-10T08:59:04.347" UserId="38" />
  <row Id="219" PostId="162" Score="1" Text="[done](http://computergraphics.stackexchange.com/q/165/198)." CreationDate="2015-08-10T09:41:23.373" UserId="198" />
  <row Id="220" PostId="165" Score="0" Text="I believe that some screens have a different layout of the primary colours. Have you viewed your results on different types of screen?" CreationDate="2015-08-10T09:58:22.040" UserId="231" />
  <row Id="221" PostId="165" Score="0" Text="@trichoplax no, but I'm confident both my monitors are RGB. Also here I'm more interested in how subpixel antialiasing techniques are meant to work than a fix for my issue." CreationDate="2015-08-10T10:02:22.253" UserId="198" />
  <row Id="222" PostId="165" Score="0" Text="I didn't mean different primary colours, I just meant that the red, green and blue are arranged in different geometric patterns, so your algorithm would need to know which pattern is being used in order to give good results." CreationDate="2015-08-10T10:06:17.063" UserId="231" />
  <row Id="223" PostId="165" Score="0" Text="[This image](https://en.wikipedia.org/wiki/Subpixel_rendering#/media/File:Pixel_geometry_01_Pengo.jpg) shows how varied the arrangement of subpixels can be." CreationDate="2015-08-10T10:06:48.327" UserId="231" />
  <row Id="224" PostId="165" Score="0" Text="Incidentally, I like the question and I'm interested to see the answers, but I can't upvote until midnight..." CreationDate="2015-08-10T10:09:22.740" UserId="231" />
  <row Id="225" PostId="165" Score="1" Text="@trichoplax yes, sorry I should have clarified, both monitors have pixels split into thirds in R-G-B order from left to right as in [this photo](http://i.stack.imgur.com/SZl3W.png)." CreationDate="2015-08-10T10:09:51.153" UserId="198" />
  <row Id="226" PostId="165" Score="1" Text="To a certain extent, italic text will have less noticeable colour fringes since the sloping lines don't allow the same colour to be present for more than a few consecutive pixels vertically." CreationDate="2015-08-10T10:14:26.257" UserId="231" />
  <row Id="227" PostId="165" Score="0" Text="Hopefully someone with knowledge of this can confirm, but my guess is that the first image has been antialiased at the pixel level first, and then that antialiased image has been antialiased again at the subpixel level. You can see areas where the original antialiasing aligns with whole pixels and has no colour fringes, despite clearly having variations in brightness due to the initial antialiasing, and areas where it does not align resulting in flat colour fringes, rather than the graduated colour fringes in the last image. It appears to be a scaled rasterised font, rather than a vector font." CreationDate="2015-08-10T10:30:46.243" UserId="231" />
  <row Id="228" PostId="107" Score="1" Text="To get that functionality you need to use the invariant qualifier in glsl: https://www.opengl.org/wiki/Type_Qualifier_%28GLSL%29#Invariance_qualifiers" CreationDate="2015-08-10T11:11:30.830" UserId="135" />
  <row Id="229" PostId="107" Score="0" Text="Note that identical shader expressions (and inputs, obviously) when evaluating vertex positions may not be enough to get identical results as some optimisations may depend on the rest of the shader. &#xA;&#xA;GLSL has the &quot;invariant&quot; keyword to declare shader outputs that must be evaluated identically in different shaders." CreationDate="2015-08-10T11:13:07.507" UserId="344" />
  <row Id="231" PostId="130" Score="2" Text="First normalise the Normal vector before using it and second the viewDirection is the outgoing vector from the Position to the camera: uCameraPosition - Position." CreationDate="2015-08-10T11:35:59.267" UserId="290" />
  <row Id="232" PostId="167" Score="0" Text="I would expect problems with pixel geometry, gamma or colour space conversion to show up as colour distortion at arbitrary points in the image, rather than the regular cycle seen in the question's image. The fact that it cycles horizontally between exaggerated colour antialiasing and pure greyscale antialiasing hints that the first application of antialiasing was performed at a different scale." CreationDate="2015-08-10T12:01:41.727" UserId="231" />
  <row Id="233" PostId="167" Score="0" Text="I don't have the full explanation, as the distortion does not seem to be aligned between the different rows of text, but it does seem the problem is related to sub-pixel rendering of already rasterised text rather than vector text." CreationDate="2015-08-10T12:16:59.983" UserId="231" />
  <row Id="234" PostId="167" Score="0" Text="@trichoplax What I try to say is that I doubt there an issue with Anti-Grain's sub-pixel rendering. Instead I would guess the input gets mangled earlier than it enters the rasterizer. Or later, but not in the rasterizer itself." CreationDate="2015-08-10T12:39:48.860" UserId="141" />
  <row Id="235" PostId="167" Score="0" Text="Yes I think the sub-pixel rendering is being applied correctly, but when applied to pre-rasterised text it is not possible to give a good result. I don't think that the renderer is broken, I just think it is being fed the wrong kind of text." CreationDate="2015-08-10T12:46:31.897" UserId="231" />
  <row Id="236" PostId="169" Score="0" Text="You won't find anything relevant just looking at OpenGL, maybe a JavaScript library can do that" CreationDate="2015-08-10T13:18:39.680" UserId="217" />
  <row Id="238" PostId="169" Score="0" Text="I dont think you can, that's why Microsoft asks the user." CreationDate="2015-08-10T13:50:34.907" UserId="38" />
  <row Id="239" PostId="169" Score="0" Text="I wonder if that means there is never a way to determine this, or just that some monitors do not provide that information." CreationDate="2015-08-10T13:58:40.427" UserId="231" />
  <row Id="240" PostId="151" Score="0" Text="@NathanReed yes that is what I meant by trackball, I thought it was a common name in the CG community." CreationDate="2015-08-10T14:11:54.123" UserId="116" />
  <row Id="242" PostId="169" Score="3" Text="well, i think you can get the model and make of your monitor maybe there is some repository that lists this? If there is not what stops anybody of us form building it?" CreationDate="2015-08-10T14:15:14.630" UserId="38" />
  <row Id="243" PostId="151" Score="0" Text="@ratchetfreak yes my approach considers an axis-angle based rotation. My doubt is that if it is needed to map the 2D mouse coords to world-coord or not. I know I can use the (x,y) to compute the `z` value of a sphere of radius `r`, however I'm not sure if that sphere lives in world-space or image-space and what the implications are. Perhaps I'm overthinking the problem." CreationDate="2015-08-10T14:17:15.137" UserId="116" />
  <row Id="245" PostId="24" Score="0" Text="Its an interesting comment about Pepper's ghost. Notice that Pepper's ghost was originally a reflection of a 3D object so the reflection itself was 3D. In the case of these popular device based projections, the projections are only 2D so the effect is only a 2D reflection suspended in space. The most popular Youtube videos meant for doing this don't even have 4 different perspectives, just 1 repeated 4 times." CreationDate="2015-08-10T15:19:01.417" UserId="20" />
  <row Id="246" PostId="164" Score="1" Text="Depth of field may help too." CreationDate="2015-08-10T15:31:29.733" UserId="196" />
  <row Id="247" PostId="175" Score="3" Text="I would take http://www.littlecms.com/ for a spin" CreationDate="2015-08-10T15:44:00.240" UserId="38" />
  <row Id="248" PostId="175" Score="0" Text="@joojaa This is great! They actually say: &quot;lcms is a CMM engine; it implements fast transforms between ICC profiles&quot;. And by their trial of the professional solution, they provide a lot of options (&quot;input for RGB, Gray, and CMYK spaces&quot;, &quot;intents&quot;, &quot;destination color space&quot;, etc.)... I never imagined that it could be so vast." CreationDate="2015-08-10T16:07:35.907" UserId="157" />
  <row Id="249" PostId="175" Score="0" Text="The [international color consortium](http://www.color.org/index.xalter) defines 4 transformation intents for the colors that can not fit the gamut, read the tutorial." CreationDate="2015-08-10T16:08:51.743" UserId="38" />
  <row Id="250" PostId="175" Score="0" Text="@joojaa actually, in their own program they have a small description associated to all of the 7 types of &quot;rendering intents&quot; we can choose from when converting." CreationDate="2015-08-10T16:13:02.087" UserId="157" />
  <row Id="251" PostId="118" Score="0" Text="I expanded my answer to be explicit about how to deal with bases of differing dimensions. Is there any way to use LaTeX for math here? This will probably be a very common feature request." CreationDate="2015-08-10T16:18:55.310" UserId="327" />
  <row Id="252" PostId="118" Score="0" Text="And regarding linearity: afaik most if not all color spaces are linear or have a 1:1 mapping that makes them linear. The CIE XYZ and xyY color spaces (which are supposed to represent all colors visible to the human eye) fit that definition, and are probably what you'd want to use as the &quot;conversion&quot; color space rather than RGB." CreationDate="2015-08-10T16:26:33.650" UserId="327" />
  <row Id="253" PostId="175" Score="0" Text="hey allow for more intents and different white point/ blackpoint combinations. (adobe thus has 3*4 intents)" CreationDate="2015-08-10T16:33:24.357" UserId="38" />
  <row Id="255" PostId="118" Score="2" Text="MathJax request is in works you should add your post as a reference to this [meta post](http://meta.computergraphics.stackexchange.com/a/34/38)" CreationDate="2015-08-10T16:36:56.713" UserId="38" />
  <row Id="256" PostId="164" Score="2" Text="As an addendum: Sebastien Lagarde did a [series of  blog posts](https://seblagarde.wordpress.com/2012/12/10/observe-rainy-world/) that documented how they implemented dynamic rain in the game &quot;Remember Me&quot;" CreationDate="2015-08-10T16:39:34.113" UserId="310" />
  <row Id="257" PostId="175" Score="0" Text="@joojaa I am impressed with the quantity of destination spaces they provide, for the conversions I tested there are shocking differences when ProPhotoRGB is used, [it's very hard to tell the differences when different intents are applied though], EDIT: in some situations intents are also very noticeable." CreationDate="2015-08-10T16:44:22.730" UserId="157" />
  <row Id="260" PostId="170" Score="0" Text="Just noting that I've definitely used extensions via GLES 2.0 on android (and in native code) so you shouldn't have any issues on that part of your solution." CreationDate="2015-08-10T18:37:46.093" UserId="329" />
  <row Id="261" PostId="21" Score="0" Text="@jozxyqk, that's some outside the box thinking right there! Very nice." CreationDate="2015-08-10T19:05:39.823" UserId="54" />
  <row Id="262" PostId="176" Score="0" Text="I'm not sure I'm understanding you. Slightly transparent == translucent. So you're already using translucency. Could you clarify what you're trying to do? Fully transparent background? (Aka, clear/invisible) Or translucent background? (Aka, you can see some stuff behind it, but covered with the background color)" CreationDate="2015-08-10T19:44:53.450" UserId="310" />
  <row Id="263" PostId="130" Score="0" Text="Thanks all of you(RichieSams, trichoplax and xpicox ) for the answers. I lower the roughness, change the color of material and reversed the ViewDirection then i start to see shiny red specular :). Actually i was using right ViewDirection but i could not see the specular because of the color and roughness value. All answers help to fix my problem." CreationDate="2015-08-10T20:05:33.543" UserId="83" />
  <row Id="264" PostId="176" Score="0" Text="@RichieSams Currently I use a simple black background with an applied alpha transparency. So the scene can be seen, but is still sharp, which distracts the readability of the text. Thus I would like to change the simple alpha transparency to translucency. A real-world example maybe is the difference between looking through colored respectively iced glass." CreationDate="2015-08-10T20:07:21.307" UserId="127" />
  <row Id="265" PostId="176" Score="0" Text="@Nero In graphics, &quot;translucent&quot; doesn't usually imply &quot;frosted&quot; or &quot;iced&quot;, so I suggest making what your looking for clearer in your question, since I was also quite confused by it." CreationDate="2015-08-10T20:22:07.777" UserId="327" />
  <row Id="266" PostId="181" Score="0" Text="I reposted this question [from SO where it went unanswered](https://stackoverflow.com/questions/30687682/how-to-render-an-object-that-recieves-shadows-but-does-not-cast-them-in-a-varian). I don't know if it's a good thing to do but it seemed like a good idea since I think it's a good fit here." CreationDate="2015-08-10T20:41:02.740" UserId="349" />
  <row Id="267" PostId="166" Score="2" Text="This GPU Gems article rasterizes quadratic curves by identifying parts of the hull that are curved and analytically computing the coverage in the pixel shader, might be worth a look: https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch25.html" CreationDate="2015-08-10T20:41:47.357" UserId="327" />
  <row Id="268" PostId="183" Score="1" Text="While certainly interesting, I don't think we want to be flooded with plain &quot;How do I achieve effect X?&quot; type questions later on. So: What have you tried?" CreationDate="2015-08-10T22:00:55.653" UserId="16" />
  <row Id="269" PostId="166" Score="2" Text="@yuriks Yeah, Loop &amp; Blinn, totally forgot about it. But isn't it patented?" CreationDate="2015-08-10T22:12:03.490" UserId="141" />
  <row Id="270" PostId="177" Score="0" Text="I think you would also be interested in techniques to simulate Rough Refraction such as those described in http://www-sop.inria.fr/reves/Basilic/2011/DBSHR11/RoughRefr_I3D_Final.pdf. Look at the screenshots and see how it could benefit ice cube rendering !" CreationDate="2015-08-10T22:21:21.550" UserId="110" />
  <row Id="271" PostId="177" Score="1" Text="@wil While those results for rough surfaces are very impressive (especially for a real-time algorithm), ice tends to be very smooth on the surface, and rough inside - almost the opposite effect." CreationDate="2015-08-10T22:31:17.223" UserId="231" />
  <row Id="273" PostId="183" Score="3" Text="I think the question is fine up to the title, but the body is kind of vague. There should be more context like if it's meant for real-time of offline use, level of detail and scale required, etc.  The current answer for example assumes the scale to be able to give a more specific answer taking into account possible tradeoffs, which I think is an important part of a good answerable question." CreationDate="2015-08-11T00:06:56.847" UserId="327" />
  <row Id="274" PostId="170" Score="0" Text="Are we considering very specific programming questions like these? This is a very API-specific programming question and doesn't even involve anything related to rendering techniques or usage of an API to implement them." CreationDate="2015-08-11T00:46:25.163" UserId="327" />
  <row Id="275" PostId="130" Score="0" Text="If the various different suggestions came together into a solution, you could write them up and post it as a self-answer. That's generally encouraged. Then you can include the working final image too..." CreationDate="2015-08-11T01:28:24.907" UserId="231" />
  <row Id="276" PostId="118" Score="0" Text="@trichoplax yuriks and joojaa seem to be confident in this answer so I'll defer to them." CreationDate="2015-08-11T02:47:12.823" UserId="125" />
  <row Id="277" PostId="177" Score="2" Text="Maybe questionslike this would require pictures" CreationDate="2015-08-11T04:44:58.767" UserId="38" />
  <row Id="278" PostId="193" Score="1" Text="I know about the Notification Center and Control Center. But I doubt that these blurs are calculated in real time as the active app is frozen while one of them is shown (or at least it does not update the UI anymore). But the links look promising. Thanks!" CreationDate="2015-08-11T06:40:26.967" UserId="127" />
  <row Id="279" PostId="183" Score="1" Text="@yuriks Yes, the question could use more detail as well, but I think the &quot;What have you tried?&quot; part is important. We should set ourselves similar quality standards as other SE sites like Stack Overflow and expect question authors to show *some* effort on their part. Otherwise, we'll just become a site where people go to have others do their work for them." CreationDate="2015-08-11T08:22:19.557" UserId="16" />
  <row Id="280" PostId="170" Score="0" Text="@yuriks Yes, I think we should definitely except if they are very specific programming questions related to computer graphics. In fact, I think that specific programming questions are what we are lacking the most so far (probably because these are harder to come up with if you don't encounter them right then and there)." CreationDate="2015-08-11T08:30:34.893" UserId="16" />
  <row Id="281" PostId="183" Score="1" Text="[Relevant meta discussion.](http://meta.computergraphics.stackexchange.com/a/69/16)" CreationDate="2015-08-11T08:57:32.987" UserId="16" />
  <row Id="282" PostId="177" Score="2" Text="Do you want to render stills or animation? If the cube is to be animated are you looking for real-time effects? And yes, if this wasn't the private beta, an image of your current results would be nice." CreationDate="2015-08-11T09:00:35.547" UserId="16" />
  <row Id="283" PostId="187" Score="1" Text="&quot;Normally, this kind of data will undergo a lot of processing after being loaded from a file until it is, for instance, displayed on the screen.&quot; like an image gets transformed to a pixel array before getting pushed on screen *regardless* of what the original format was. (hardware compression notwithstanding)" CreationDate="2015-08-11T09:04:51.697" UserId="137" />
  <row Id="284" PostId="199" Score="1" Text="It would be great if you could contain some information about this method within the answer to make it more self-contained." CreationDate="2015-08-11T09:39:32.047" UserId="16" />
  <row Id="285" PostId="196" Score="2" Text="This seems close to being a link-only answer. Could you include an explanation so that this can be understood even without following the link?" CreationDate="2015-08-11T11:21:46.560" UserId="231" />
  <row Id="286" PostId="184" Score="0" Text="Nice explanation and great links... Your video should be visible in this answer :)" CreationDate="2015-08-11T14:04:11.550" UserId="157" />
  <row Id="287" PostId="193" Score="1" Text="@Nero I suspect that's mostly for power reasons. When you swipe down from the Springboard to get to Spotlight, you get a variable-width blur depending on how far you've pulled down, and that's certainly real-time." CreationDate="2015-08-11T14:30:37.093" UserId="196" />
  <row Id="288" PostId="51" Score="0" Text="I added another possible method to my post. It may be worth clarifying your question with more detailed requirements. i.e. why does a 2D slice of 3D noise not satisfy your visual requirements? What are your performance requirements?" CreationDate="2015-08-11T15:02:26.827" UserId="196" />
  <row Id="289" PostId="201" Score="0" Text="It's not your question, but you see commonly on TV that 4:3 images are extended to widescreen by using a blurred and stretched copy in the background. That may work for you." CreationDate="2015-08-11T15:50:17.723" UserId="125" />
  <row Id="290" PostId="196" Score="0" Text="Apologies, you are correct.  I should have given screenshots at least, and it turns out i even linked to the wrong link!" CreationDate="2015-08-11T15:55:09.820" UserId="56" />
  <row Id="291" PostId="121" Score="0" Text="I'm not sure if this should go as an answer or not, but the errors in your image are caused by rendering without depth testing on *all* primitives. You should render the scene in 2 passes: First render normally all solid geometry. Afterwards, disable depth *writes* (not GL_DEPTH_TEST) and render translucent geometry in roughly back-to-front order. This will ensure that transparent geometry will not be drawn in front of solid geometry that's in front of it." CreationDate="2015-08-11T17:45:01.473" UserId="327" />
  <row Id="292" PostId="196" Score="1" Text="Thanks for the images, but it would also be interesting to see an explanation of how this works and why it improves the appearance. A good answer should provide understanding without the need to leave the site - then the links are there for further reading in more detail." CreationDate="2015-08-11T20:43:49.680" UserId="231" />
  <row Id="293" PostId="196" Score="0" Text="I mentioned you apply bump mapping to lighting and refraction calculations. Would you expect my answer to explain bump mapping?" CreationDate="2015-08-11T20:48:29.437" UserId="56" />
  <row Id="294" PostId="196" Score="0" Text="Explain whatever you feel would be helpful. If you are presenting something which answers the question, then you can explain why it helps." CreationDate="2015-08-11T21:41:51.340" UserId="231" />
  <row Id="295" PostId="209" Score="1" Text="A couple questions... are you doing the drawing logic on the CPU or GPU?  Also, are you looking for integer based algorithms or floating point?" CreationDate="2015-08-11T23:54:29.043" UserId="56" />
  <row Id="296" PostId="209" Score="5" Text="@AlanWolfe, integer algorithms on the CPU -- the same environment that Bresenham's algorithm was designed for." CreationDate="2015-08-11T23:56:30.247" UserId="158" />
  <row Id="297" PostId="209" Score="3" Text="https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm is the classic one, though the wikipedia page is pretty half-baked and I don't have access to the paper. This feels like a lazy question though, since it's pretty easy to find this by doing some basic googling." CreationDate="2015-08-12T00:33:52.797" UserId="327" />
  <row Id="298" PostId="209" Score="0" Text="@yuriks, mind turning that into a full-fledged answer?" CreationDate="2015-08-12T00:34:21.103" UserId="158" />
  <row Id="299" PostId="209" Score="0" Text="I fat-fingered the enter key, so I just edited my comment. :)" CreationDate="2015-08-12T00:35:01.630" UserId="327" />
  <row Id="300" PostId="196" Score="1" Text="I had an awesome chance to see ice cubes close up today during dinner, and they seemed to actually be pretty smooth and without bumps. I think the vital part of this is giving them that &quot;wet&quot; look." CreationDate="2015-08-12T00:36:31.713" UserId="327" />
  <row Id="301" PostId="168" Score="0" Text="I guess the division by 300 is just a parameter for the sensitivity of the rotation given the displacement of the mouse?" CreationDate="2015-08-12T00:43:43.130" UserId="116" />
  <row Id="302" PostId="210" Score="1" Text="Are you using path tracing or physically-accurate ray tracing? If so, then the ambient occlusion is already a built-in consequence of the algorithm and does not need to be specifically modelled. Intuitively, to me it seems correct that the shadow is faint: your spheres are mirrors and so diffuse rays shot from the ground near the sphere will tend to be reflected away and towards the skybox instead of back towards the ground as would happen with diffuse spheres." CreationDate="2015-08-12T00:48:52.457" UserId="327" />
  <row Id="303" PostId="211" Score="0" Text="Wouldn't albedo values of 1 produce an image that never converges? Afaik, albedo should always be &lt; 1 for convergence to happen. An albedo of 1 would specifically not cause a contact shadow because rays would be perfectly reflected until they reach a light source such as the sky." CreationDate="2015-08-12T00:52:09.333" UserId="327" />
  <row Id="304" PostId="168" Score="0" Text="Correct. It's what happened to work well with my resolution at the time." CreationDate="2015-08-12T01:02:50.050" UserId="310" />
  <row Id="305" PostId="196" Score="0" Text="Interesting! I wonder what would make it look wet?" CreationDate="2015-08-12T01:35:33.710" UserId="56" />
  <row Id="306" PostId="213" Score="1" Text="It can be more accurate too.  The farther you get away from zero, the less resolution there is between integers, when using floating point numbers." CreationDate="2015-08-12T01:39:21.023" UserId="56" />
  <row Id="307" PostId="203" Score="2" Text="I remembered this paper I read some time ago. The authors compare spectral and RGB-rendered results with different illuminants. Unfortunately the comparison is done on a color chart, so I'm not sure how much the differences affect real life scenes. http://cg.cs.uni-bonn.de/en/publications/paper-details/lyssi-2009-btfspectral/" CreationDate="2015-08-12T02:12:42.263" UserId="327" />
  <row Id="308" PostId="214" Score="1" Text="Your ideas sound a bit like cone tracing and importance sampling, you might give them a read (:" CreationDate="2015-08-12T02:14:08.993" UserId="56" />
  <row Id="309" PostId="151" Score="2" Text="On your edit: Yes. You would need to transform your (x, y, z) values to world space using the View matrix." CreationDate="2015-08-12T02:19:09.637" UserId="310" />
  <row Id="310" PostId="215" Score="1" Text="Do you get a similarly smooth image from only the second light? i.e. it's definitely an artifact of multiple lights, not just the geometry of the second light?" CreationDate="2015-08-12T03:07:56.277" UserId="196" />
  <row Id="311" PostId="214" Score="2" Text="This sounds a bit like this thesis: http://www.graphics.cornell.edu/pubs/2004/Don04.pdf It's effectively importance sampling with an adaptive probability density function. My instinct is that it could work but you'd need to take care to avoid missing and therefore ignoring small features (small distant lights, say)." CreationDate="2015-08-12T03:27:44.193" UserId="196" />
  <row Id="312" PostId="214" Score="2" Text="Here's yet another paper on adaptive importance sampling: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2520" CreationDate="2015-08-12T03:31:37.200" UserId="196" />
  <row Id="313" PostId="212" Score="0" Text="What do you mean by &quot;only compute the transform once&quot;? Meaning you don't have to compute the inverse transform, or you only ever need to transform an object once? Computing the inverse transform is cheap compared to a geometry intersection, and you definitely would need to transform an object multiple times (into the space of each different ray)." CreationDate="2015-08-12T03:35:20.343" UserId="196" />
  <row Id="314" PostId="215" Score="0" Text="Yes, using any kind of single light gives a smooth image. The noise only comes back when there are multiple lights to choose from. (And I suppose that the 3rd example image is somewhat misleading, but when there are 2 lights, the image looks almost as bad as the first one which uses the naive algorithm if I use only 1024 spp.)" CreationDate="2015-08-12T03:36:00.983" UserId="327" />
  <row Id="315" PostId="215" Score="0" Text="To me, that suggests a bug. But it's rather difficult to debug code just by reading it. I'd try disabling pieces of code—for example, in this scene the importance weight of the BRDF sample should be extremely small since light sampling is far more likely to hit the light, so you should be able to set brdf_pdf to 0 and only use light sampling and see if the noise goes away." CreationDate="2015-08-12T03:39:30.193" UserId="196" />
  <row Id="316" PostId="209" Score="2" Text="Just thinking out loud, I figure it should be easy to adapt Bresenham for drawing multi-pixel-thick lines. Then you can do antialiasing by calculating the distance of each pixel center from the mathematical ideal line, and applying some falloff function." CreationDate="2015-08-12T03:57:05.190" UserId="48" />
  <row Id="317" PostId="209" Score="1" Text="Regarding Bresenham being adapted to do AA, this page shows that with some simple code.  From the description of the xaolin wu algorithm it may be similar.  http://members.chello.at/easyfilter/bresenham.html" CreationDate="2015-08-12T04:06:42.050" UserId="56" />
  <row Id="318" PostId="212" Score="3" Text="I think the assumption is that all primitives would be transformed to a flat world-space position during scene preparation. (Which does dramatically increase memory usage.) Then rays would be directly intersected against the primitives without a preceding transformation between spaces, since they're now both in world-space." CreationDate="2015-08-12T04:19:11.740" UserId="327" />
  <row Id="319" PostId="213" Score="2" Text="@AlanWolfe, wouldn't the same resolution issues show up when computing the inverse transformation and applying it to the ray, though?" CreationDate="2015-08-12T04:27:13.073" UserId="158" />
  <row Id="321" PostId="215" Score="0" Text="@JohnCalsbeek Looks like you gave me a good lead! This is how it looks like if I ignore BRDF sampling: i.imgur.com/4c7bRCo.png I'll try to further track down the issue later when I have more time. It seems like the issue is that the impact of the BRDF is overestimated, causing it to overwhelm the light contribution in some cases." CreationDate="2015-08-12T05:54:57.620" UserId="327" />
  <row Id="322" PostId="121" Score="0" Text="@yuriks In this case it's probably a poor example on my part, but everything is meant to be transparent. I wanted something to show how wrong transparency might look when done badly. Also an example where sorting geometry would be amazingly difficult (for example here the floor is one giant polygon and covers the entire depth range)." CreationDate="2015-08-12T06:00:44.490" UserId="198" />
  <row Id="323" PostId="214" Score="1" Text="It's not quite the same thing, but the focus on areas of high gradient reminds me of [gradient-domain path tracing](https://mediatech.aalto.fi/publications/graphics/GPT/). However, before attacking a complex technique like that, I'd start with more basic things like stratified sampling and importance sampling to get the variance down." CreationDate="2015-08-12T06:24:04.383" UserId="48" />
  <row Id="324" PostId="214" Score="1" Text="Here's [a reference on stratified and low-discrepancy sampling](http://cg.informatik.uni-freiburg.de/course_notes/graphics2_04_sampling.pdf) btw." CreationDate="2015-08-12T06:30:08.633" UserId="48" />
  <row Id="325" PostId="211" Score="0" Text="In global illumination, there would be no contact shadow, yes. However, the question is about ambient occlusion, which only performs one bounce. Additionally, as long as the scene is not closed (i.e. the sky is visible), the scene would converge if global illumination was used even with albedo of 1." CreationDate="2015-08-12T08:20:01.360" UserId="79" />
  <row Id="326" PostId="215" Score="2" Text="You should actually add the significant parts of your code into the question instead of linking them elsewhere." CreationDate="2015-08-12T09:26:27.740" UserId="6" />
  <row Id="327" PostId="216" Score="1" Text="This seems to be more of a comment than an answer. Maybe you could elaborate on why fluorescent materials depend on an ultraviolet channel and provide some references?" CreationDate="2015-08-12T09:35:18.963" UserId="16" />
  <row Id="328" PostId="208" Score="2" Text="Ideally, answers should be self-contained and depend vitally on external links. Having links is nice for supplementary material, but an answer shouldn't just consist of a keyword. If you could include some details on what an A-buffer is and how it works that would greatly improve your answer." CreationDate="2015-08-12T09:38:58.897" UserId="16" />
  <row Id="329" PostId="218" Score="1" Text="Have you tried a closed surface with control points, i.e. nurbs or bspline? or free form deformation stuff? both of these are described by using point, but moving a point you deforms the surface described. (In the mean while i try to gather more info on the problem). I was even thinking to convex hull, but i'm not sure of the result, since a liquid deformation could be not convex at all." CreationDate="2015-08-12T10:08:43.980" UserId="228" />
  <row Id="330" PostId="51" Score="0" Text="@JohnCalsbeek Honestly, I don't have any hard requirements at hand. This was just a question I was curious about which I thought I'd give a go for the private beta. Of course, cutting a 2D slice out of 3D noise will be sufficient for many applications, but I'm sure it will have some performance impact and anisotropies (which may or may not be noticeable). &quot;Cutting the sphere out of 3D noise is your best option, because...&quot; is definitely a valid answer." CreationDate="2015-08-12T11:44:37.037" UserId="16" />
  <row Id="331" PostId="217" Score="2" Text="I'm not sure that's the correct takeaway. I would first double-check all the computations in the BRDF sampling branch. I think ShapeSphere::areaPdf looks suspect. It should depend on the distance squared and both the surface normal and the light normal." CreationDate="2015-08-12T13:37:15.447" UserId="196" />
  <row Id="332" PostId="203" Score="0" Text="Beer's law (absorption of color through a transparent object over distance) is hard to model with rgb." CreationDate="2015-08-12T13:50:13.877" UserId="56" />
  <row Id="333" PostId="219" Score="1" Text="Could you perhaps expand on your answer so it's self-contained. Reading it without the link and its contents, the answer isn't all that clear." CreationDate="2015-08-12T15:36:25.090" UserId="4" />
  <row Id="334" PostId="219" Score="0" Text="Actually i think the &quot;idea&quot; (not the implementation) is quite clear. @Cristian said he implemented the simulation on a set of points, basically he should consider the set of points as they belong to a deformable surface (i.e. a continuous surface described by a set of discrete points), then when the points position are updated the surface surface shape too is updated. The effectiveness depends on what he wants specifically to achieve, but i've assumed he wanted to use a kind of &quot;continuous&quot; instead of a discrete point set." CreationDate="2015-08-12T15:48:24.733" UserId="228" />
  <row Id="335" PostId="219" Score="0" Text="Also... i don't know why i can't use formulas here... but of course they would be useful for a more detailed explanation. However physically speaking good reference is any book of computational fluidodynamics, for rendering game physics of course, simplified methods could be found in literature too. For &quot;code&quot; itself depends on the environment he's using he can even find a complete implementation on the CUDA toolkit, i'm not sure about openCL, even the directX SDK presents some example of fluid simulation (espacially the last version, but probably it simulates using tons of points too)." CreationDate="2015-08-12T15:53:13.983" UserId="228" />
  <row Id="336" PostId="219" Score="3" Text="My understanding of the question is that the author has the simulation part covered, but is only asking about visualization. I think it'll prove to be quite difficult to create sets of spline patches that satisfactorily represent the &quot;shape&quot; of the water, and there is little detail on this answer about how you'd do that." CreationDate="2015-08-12T16:39:23.130" UserId="327" />
  <row Id="337" PostId="151" Score="0" Text="Actually, after reading your answer again and thinking about it. If I have the current position of the camera I can transform that into spherical coords and then add to the angles the delta produced by the mouse movement, then go back to cartesian coords and update my view matrix, would that make sense?" CreationDate="2015-08-12T16:47:01.913" UserId="116" />
  <row Id="338" PostId="150" Score="0" Text="@JohnCalsbeek ok im starting to build the analytic answer, its going to take a while" CreationDate="2015-08-12T17:50:18.053" UserId="38" />
  <row Id="339" PostId="225" Score="0" Text="Some others: photon mapping, sphere tracing, cone tracing" CreationDate="2015-08-12T20:11:02.493" UserId="56" />
  <row Id="340" PostId="210" Score="1" Text="do you know about srgb correction? If not that could be a factor.  http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-12T21:57:00.563" UserId="56" />
  <row Id="341" PostId="62" Score="0" Text="I had some not-bad results interpolating near the edges of the tile (edge-wrapped), but it depends on what effect you're trying to achieve and the exact noise parameters. Works great for somewhat blurry noise, not so good with spikey/fine-grained ones." CreationDate="2015-08-13T03:17:25.727" UserDisplayName="user379" />
  <row Id="345" PostId="231" Score="2" Text="Do you happen to have links to any more info about tiled raytracing?  I'm guessing there are multiple ways that tiles can help, and probably different ways that people have exploited those benefits.  Trying to get a toe hold on some info to look a bit deeper at existing techniques." CreationDate="2015-08-13T03:39:39.467" UserId="56" />
  <row Id="346" PostId="231" Score="0" Text="@AlanWolfe, nothing really useful.  The 2x2 tiling was a paper I read several years ago and can't find now, while the larger tiles were just a passing mention on a webpage." CreationDate="2015-08-13T05:20:40.270" UserId="158" />
  <row Id="347" PostId="216" Score="1" Text="I mentioned this in my post just not using the word flourescent. Anyway this can be accomplised at shader level." CreationDate="2015-08-13T05:35:47.267" UserId="38" />
  <row Id="348" PostId="159" Score="3" Text="Subsurface scattering is for sure going to be important in such a close-up. You might look at Jorge Jimenez's work for some inspiration; see his talks at [SIGGRAPH 2012](http://advances.realtimerendering.com/s2012/index.html) and [GDC 2013](http://www.iryoku.com/stare-into-the-future). His work is for real-time but I'm sure some of the ideas can be adapted." CreationDate="2015-08-13T06:13:57.473" UserId="48" />
  <row Id="349" PostId="233" Score="3" Text="Short answer: &quot;light probes&quot; usually means a compact spherical harmonic representation of the environment. They're blurry both because they're very compact (only a few tens of bytes storage) and because they're prefiltered for use as diffuse lighting. I'll expand into a longer answer when I have a chance. :)" CreationDate="2015-08-13T06:18:42.793" UserId="48" />
  <row Id="350" PostId="226" Score="0" Text="related: http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing" CreationDate="2015-08-13T07:36:10.913" UserId="198" />
  <row Id="352" PostId="176" Score="0" Text="@yuriks `translucent: (of a substance) allowing light, but not detailed shapes, to pass through`. Translucent generally has the meaning used in the question, so this seems clear." CreationDate="2015-08-13T10:58:40.523" UserId="231" />
  <row Id="354" PostId="181" Score="0" Text="There is discussion and guidance about SE cross posting [on Meta Stack Exchange](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu). Despite it saying not to cross post, I think it is a good thing that you have posted here and the question clearly fits here. The meta post mentions migrating rather than cross posting, and alternatively deleting the original post (since it is too late to migrate in this case, now that you have posted)." CreationDate="2015-08-13T11:21:38.833" UserId="231" />
  <row Id="355" PostId="233" Score="0" Text="Is there any context for your question? The term 'light probe' is ambigious and used differently in different scenarios, algorithms and engines." CreationDate="2015-08-13T12:07:22.630" UserId="385" />
  <row Id="356" PostId="233" Score="0" Text="Specifically, if you have been to shadertoy.com you'll see two sets of cube maps available for use.  One set is environment maps, the other set looks the same but blurry that are labeled light probes.  Just curious about that and light probes in general." CreationDate="2015-08-13T14:49:46.413" UserId="56" />
  <row Id="357" PostId="234" Score="0" Text="Just want to note that you can do actual integration of a ray path analytically as well, you don't have to use Ray marching if it's undesirable." CreationDate="2015-08-13T15:29:46.127" UserId="56" />
  <row Id="358" PostId="234" Score="0" Text="@AlanWolfe thats what you do in the uniform case, however if the medium participates with geometry then you need to do something more nifty. Anyway i didnt claim this is all methods." CreationDate="2015-08-13T15:53:14.523" UserId="38" />
  <row Id="359" PostId="234" Score="0" Text="For sure, just adding to your answer.  When you say uniform case not sure what you mean exactly but for the case of fog, it doesn't have to be uniform density, just some density function that you can integrate. Is that what you meant by uniform case?" CreationDate="2015-08-13T16:05:33.680" UserId="56" />
  <row Id="360" PostId="219" Score="0" Text="Many of us would like to see the ability to use MathJax for formulas, and plenty of other SE sites already have it. All we need to do is demonstrate a need for it. Any questions that would benefit from MathJax, just add a link to them to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231)." CreationDate="2015-08-13T17:23:20.537" UserId="231" />
  <row Id="361" PostId="219" Score="0" Text="The additional information in your comments would make good additions to the answer." CreationDate="2015-08-13T17:25:43.667" UserId="231" />
  <row Id="363" PostId="241" Score="1" Text="Perhaps because CMYK has 4 components? RGB is a triangle, where each corner represents one of the components. So, it would makes sense that CMYK would be a 4 sided shape, since there are 4 components (Cyan, Magenta, Yellow, Black)" CreationDate="2015-08-13T19:54:53.377" UserId="310" />
  <row Id="366" PostId="241" Score="0" Text="@RichieSams yes, thats what i used to tgink. but black is absense of luminance the luminance is on a axis thats perpendicular on your screen. So black should be away from your viewing direction." CreationDate="2015-08-13T20:37:18.053" UserId="38" />
  <row Id="367" PostId="238" Score="0" Text="I've edited to add syntax highlighting - feel free to change the language if there's a more suitable one." CreationDate="2015-08-13T20:51:25.383" UserId="231" />
  <row Id="368" PostId="241" Score="0" Text="Ok, i think i know why... i need to check this from the media department. But ill leave this question for a while so others have time to answer the question. Its definitely not the K component." CreationDate="2015-08-13T21:06:05.027" UserId="38" />
  <row Id="369" PostId="242" Score="0" Text="Actually i think its clipped that way for the same reason that the cromaticity graphs outer edge abruptly changes direction. The space model is not entirely trivial but yes your right the space shape is complex. And its volume cant be drawn because my fogra measurements in the icc profile dont work all the way.+1" CreationDate="2015-08-13T21:23:10.087" UserId="38" />
  <row Id="370" PostId="241" Score="0" Text="Yea, that makes sense, now that I think about it. Black isn't a color. It's the absence of color." CreationDate="2015-08-13T21:23:33.710" UserId="310" />
  <row Id="371" PostId="243" Score="0" Text="iir is likely &quot;infinite impulse response&quot; which is a type of filtering (the other being finite impulse response). Not sure what rle could stand for. It makes me think of &quot;run length encoding&quot; but I don't think that's what it stands for." CreationDate="2015-08-14T04:03:51.747" UserId="56" />
  <row Id="372" PostId="243" Score="0" Text="I guess I was right about rle but don't understand how it applies.  http://docs.gimp.org/en/plug-in-gauss.html" CreationDate="2015-08-14T04:08:03.363" UserId="56" />
  <row Id="373" PostId="244" Score="0" Text="Great explanation! Just to confirm about the part about using trilinear filtering on the mips... Mips are preferred over volume textures because they use less resolution, and higher Blur levels need less resolution?" CreationDate="2015-08-14T05:37:33.373" UserId="56" />
  <row Id="374" PostId="244" Score="1" Text="I'd say that both these &quot;kinds&quot; of light probes are the same. Light probes are a measure of the radiance (usually pre-convoluted in some way) received at a point in the scene. Cubemaps and SH are just different ways to store/compute a light probe, making different storage/perf/quality trade-offs. (EDIT: Just to make it clear, I agree with the answer, I just think it's counter-productive to think of them as separate things.)" CreationDate="2015-08-14T05:39:21.823" UserId="327" />
  <row Id="375" PostId="216" Score="0" Text="@joojaa: Sorry.. missed that. I'd delete my post if there was an obvious button to do so. Though, having said that, I would say that you'd still need extra channels elsewhere (and not just shaders) to handle it, e.g. on-the-fly generation of environment maps." CreationDate="2015-08-14T08:50:14.530" UserId="209" />
  <row Id="376" PostId="216" Score="2" Text="Delete or don't delete it, same for me. I would rather see you  expand it., there's nothing wrong with supporting evidence and things said differently as long a you contribute with better clarity or new info." CreationDate="2015-08-14T09:19:49.443" UserId="38" />
  <row Id="377" PostId="240" Score="1" Text="Well, there are *quite* a bit more and *quite* a bit more advanced algorithms for mesh parameterization than what you linked to, so I wouldn't really subscribe to the idea that it *&quot;won't be pretty&quot;*, let alone such a short answer that barely touches the tip of the iceberg." CreationDate="2015-08-14T11:02:31.647" UserId="6" />
  <row Id="378" PostId="244" Score="0" Text="@AlanWolfe There is a use and that is to have maps for diffuse ligtning taken from a real environment (called gray ball/white ball), not just specular. Heres a disucssion of both [Mirror/Gray Ball Shaders](http://docs.autodesk.com/MENTALRAY/2015/ENU/mental-ray-help/files/shaders/production/prod_mirrorball.html)" CreationDate="2015-08-14T11:04:40.630" UserId="38" />
  <row Id="379" PostId="240" Score="0" Text="Could you elaborate/link any of those algorithms? I will be happy to add more to the answer." CreationDate="2015-08-14T11:40:49.107" UserId="310" />
  <row Id="380" PostId="240" Score="0" Text="Not at the moment really." CreationDate="2015-08-14T13:37:17.943" UserId="6" />
  <row Id="381" PostId="244" Score="1" Text="@AlanWolfe Correct&amp;mdash;you need less resolution on the more blurred levels. Also, cubemap volume textures aren't a thing that exists in hardware. :)" CreationDate="2015-08-14T18:12:25.503" UserId="48" />
  <row Id="382" PostId="243" Score="0" Text="&quot;RLE Gaussian Blur is best used on computer-generated images or those with large areas of constant intensity.&quot; Makes me think it's some kind of optimization of the blur process, that's somehow data-dependent so it works better on certain kinds of images? Is there any visible difference between the results of the two?" CreationDate="2015-08-14T21:32:57.017" UserId="48" />
  <row Id="383" PostId="243" Score="1" Text="This question seems very poorly formulated... You could have taken the time to do a quick search to at least get the definitions for the acronyms, since the previous commenters don't seem to know what they mean exactly." CreationDate="2015-08-15T04:18:03.440" UserId="54" />
  <row Id="384" PostId="252" Score="1" Text="Side by side images *really* make a difference for getting an intuitive feel for the explanation. I'm going to bear this in mind for my own answers." CreationDate="2015-08-15T09:49:46.923" UserId="231" />
  <row Id="385" PostId="240" Score="2" Text="@RichieSams: A very popular algorithm that quickly gained popularity is the least sqares conformal maps algorithm: https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf" CreationDate="2015-08-15T10:53:50.973" UserId="29" />
  <row Id="387" PostId="258" Score="0" Text="Does this mean that applying a Gaussian blur 4 times would give twice the size, as required?" CreationDate="2015-08-15T20:08:25.447" UserId="231" />
  <row Id="388" PostId="258" Score="0" Text="Yes, absolutely!" CreationDate="2015-08-15T20:12:56.647" UserId="79" />
  <row Id="389" PostId="257" Score="0" Text="Is the directional blurring you are referring to motion blur?" CreationDate="2015-08-15T20:14:08.747" UserId="231" />
  <row Id="390" PostId="257" Score="0" Text="An example before and after image would help confirm what type of blur is being asked about." CreationDate="2015-08-15T20:14:44.000" UserId="231" />
  <row Id="391" PostId="258" Score="0" Text="That covers the question entirely then - would it be worth mentioning how to get twice the size in the question so we can tidy away the comments?" CreationDate="2015-08-15T20:19:31.133" UserId="231" />
  <row Id="392" PostId="260" Score="4" Text="Rendering to a smaller texture and upsampling is a good way to do it. But if for some reason you really need to write to every 16th pixel of the large texture, using a compute shader with one invocation for every 16th pixel plus image load/store to scatter the writes into the render target could be a good option." CreationDate="2015-08-16T00:28:39.233" UserId="48" />
  <row Id="393" PostId="258" Score="0" Text="If it takes 4 to double the size, what size would 3  represent?" CreationDate="2015-08-16T01:01:10.063" UserId="56" />
  <row Id="394" PostId="258" Score="0" Text="I found the answer is N*sqrt(3). It turns out that the total Blur amount of multiple blurs is equal to the square root of the sum of the sizes squared. Wikipedia says that: https://en.wikipedia.org/wiki/Gaussian_blur" CreationDate="2015-08-16T01:08:55.603" UserId="56" />
  <row Id="395" PostId="15" Score="0" Text="Since [the wikipedia page](https://en.wikipedia.org/wiki/Subdivision_surface) lists a number of different improvements over the intervening time (almost 4 decades), and branching into several different types (approximating/interpolating, quads/triangles), I suspect this question may be too broad to be a good fit for this site." CreationDate="2015-08-16T11:58:54.183" UserId="231" />
  <row Id="396" PostId="15" Score="1" Text="I've [raised this on meta](http://meta.computergraphics.stackexchange.com/questions/82/is-this-question-too-broad) to see what people think." CreationDate="2015-08-16T12:06:31.653" UserId="231" />
  <row Id="397" PostId="263" Score="0" Text="I'll give this a shot, thanks" CreationDate="2015-08-16T14:05:42.110" UserId="56" />
  <row Id="398" PostId="266" Score="1" Text="My guess would be that if the UV coords are calculated in VS, the texture unit can start to prefetch them while the PS is starting. If they're calculated in the PS, the texture unit has to wait first." CreationDate="2015-08-16T14:41:33.030" UserId="310" />
  <row Id="399" PostId="266" Score="1" Text="Fwiw this is called a &quot;dependant texture read&quot;, in case it helps your search." CreationDate="2015-08-16T14:51:05.583" UserId="56" />
  <row Id="400" PostId="263" Score="0" Text="BTW just to help future folk. Bicubic interpolation will give higher quality results than bilinear when taking samples, at a higher computational cost." CreationDate="2015-08-16T15:21:07.823" UserId="56" />
  <row Id="401" PostId="271" Score="1" Text="I know there is Instancing in OpenGL, that's why I also posted an answer here. But maybe there is also some other way to achieve the same result." CreationDate="2015-08-16T16:28:04.357" UserId="127" />
  <row Id="402" PostId="266" Score="0" Text="Do you have some measurements showing the perf difference? I actually wouldn't expect there to be much difference at all; texture fetch latency should swamp a few ALU ops. BTW, dependent texture reads are where there are two (or more) texture reads, with the coordinates for the second dependent on the output of the first. Those are slower because of the strict ordering required between the two texture reads." CreationDate="2015-08-16T17:58:36.403" UserId="48" />
  <row Id="403" PostId="271" Score="1" Text="Seems you were right... :)" CreationDate="2015-08-16T21:17:37.130" UserId="231" />
  <row Id="404" PostId="15" Score="0" Text="@trichoplax I'll keep an eye on it, thanks" CreationDate="2015-08-16T21:29:33.483" UserId="70" />
  <row Id="405" PostId="266" Score="0" Text="Well, any operation done in the fragment shader will be more expensive then in the vertex shader. Each triangle takes 3 invocations of a vertex shader, but it might take orders of magnitude more invocations of the fragment shader, depending on its screen size." CreationDate="2015-08-17T01:13:54.647" UserId="54" />
  <row Id="406" PostId="51" Score="0" Text="You might check out this shadertoy which does noise on a sphere:&#xA;https://www.shadertoy.com/view/4sfGzS" CreationDate="2015-08-17T14:58:17.797" UserId="56" />
  <row Id="407" PostId="278" Score="1" Text="Have you tried the normal distribution (Gaussian function)? It seems that it would help here since it is used to figure out where things will be on average with certain characteristics of probabilities.  Dust settling randomly but less often where there's more airflow and more often in crevices seems right in its wheelhouse." CreationDate="2015-08-17T17:23:19.647" UserId="56" />
  <row Id="408" PostId="277" Score="2" Text="Are you sure they were talking about multiple gaussian blurs? Doing several Box blurs is a common way to approximate a gaussian blur." CreationDate="2015-08-17T17:26:18.647" UserId="327" />
  <row Id="409" PostId="277" Score="0" Text="Interesting info.  I believe so, yes, but could be mistaken!" CreationDate="2015-08-17T17:32:44.280" UserId="56" />
  <row Id="410" PostId="278" Score="0" Text="@AlanWolfe thanks for the suggestion - I've added in some more images based on that." CreationDate="2015-08-17T18:22:25.437" UserId="231" />
  <row Id="411" PostId="278" Score="0" Text="exponential looks better to me than linear or the normal distribution based one, but i don't have any non opinion answers to back anything up about correctness :P" CreationDate="2015-08-17T18:38:04.013" UserId="56" />
  <row Id="412" PostId="277" Score="1" Text="It may be simpler just to sample neighboring pixels, its also much more intuitive as a physical model of diffusion, see [12 steps to Navier-Stokes, step 7](http://nbviewer.ipython.org/github/barbagroup/CFDPython/blob/master/lessons/09_Step_7.ipynb)" CreationDate="2015-08-17T18:41:06.873" UserId="38" />
  <row Id="413" PostId="278" Score="0" Text="How about a cellular automata of some kind? Diffusion step and then erode diffuse then erode..." CreationDate="2015-08-17T18:51:53.140" UserId="38" />
  <row Id="414" PostId="280" Score="1" Text="possible duplicate of [Why is this conditional in my fragment shader so slow?](http://computergraphics.stackexchange.com/questions/259/why-is-this-conditional-in-my-fragment-shader-so-slow)" CreationDate="2015-08-18T08:46:09.777" UserId="16" />
  <row Id="415" PostId="280" Score="0" Text="As the answer explains on my question, the fragments get grouped into &quot;warps&quot; or &quot;wavefronts&quot; and if all fragments in such a group use the same branch, only that branch is executed." CreationDate="2015-08-18T08:47:44.803" UserId="16" />
  <row Id="416" PostId="280" Score="0" Text="But what about shaders different from fragment?" CreationDate="2015-08-18T08:58:20.317" UserId="386" />
  <row Id="417" PostId="280" Score="2" Text="I believe [vertices get assembled into warps or wavefronts just the same](https://fgiesen.wordpress.com/2011/10/09/a-trip-through-the-graphics-pipeline-2011-part-13/)." CreationDate="2015-08-18T09:01:05.100" UserId="16" />
  <row Id="418" PostId="280" Score="0" Text="The result being that if all threads follow the same path there will have less &quot;divergence&quot; and the shader will run faster. E.g. I don't think putting `if (true) { ... }` around the entire program would measurably alter the performance (assuming it's not optimized it out, which it would be)." CreationDate="2015-08-18T09:02:36.593" UserId="198" />
  <row Id="419" PostId="280" Score="0" Text="@jozxyqk I think the OP is more interested in wrapping the entire program in `if (false) { ... }` but yes. :)" CreationDate="2015-08-18T09:04:51.867" UserId="16" />
  <row Id="420" PostId="266" Score="0" Text="@NathanReed I don't think you have to limit &quot;dependent texture reads&quot; to just those that come from a previous texture access. I'd probably also include any coordinates computed in the frag shader, as opposed to those that can be determined merely from the linear (well, hyperbolic with perspective) interpolation of vertex attributes." CreationDate="2015-08-18T10:15:51.753" UserId="209" />
  <row Id="422" PostId="280" Score="1" Text="I suspect this is not a duplicate, but it needs to be edited to make it clear what is being asked before that can be determined. Some example code or an explanation of the two options being compared would help a lot." CreationDate="2015-08-18T13:29:55.827" UserId="231" />
  <row Id="423" PostId="280" Score="0" Text="`Is condition anyway will be executed every time?` Are you asking whether the condition is calculated for each pixel/vertex/fragment, or whether the condition is recalculated each time the shader is applied?" CreationDate="2015-08-18T13:31:58.820" UserId="231" />
  <row Id="424" PostId="280" Score="0" Text="Even though there is an accepted answer, the question will still benefit from being edited so that it is clear to future readers." CreationDate="2015-08-18T13:34:47.717" UserId="231" />
  <row Id="430" PostId="275" Score="0" Text="By the way: I experienced that on an iPad 3. So maybe this is actually hardware specific." CreationDate="2015-08-18T17:18:20.950" UserId="127" />
  <row Id="436" PostId="283" Score="0" Text="I think you might get a good answer from the signal processing or math site." CreationDate="2015-08-19T01:01:03.970" UserId="56" />
  <row Id="437" PostId="283" Score="1" Text="I'm hoping that asking on computergraphics.SE will lead to answers that don't just give me signal processing theory or mathematical proofs, but the perspective of people who work with and research computer graphics. There may be something I haven't thought of that makes the question irrelevant, or it may only matter in certain circumstances, and if so I want the computer graphics angle on that." CreationDate="2015-08-19T10:02:58.427" UserId="231" />
  <row Id="438" PostId="283" Score="0" Text="I've no idea how you would efficiently achieve random access to the final constructed data, nor how to extend it to 3D, but could you use something based on aperiodic tiling, e.g. https://en.wikipedia.org/wiki/Penrose_tiling ? i.e. have a random value at the centre of each tile?" CreationDate="2015-08-19T11:47:19.700" UserId="209" />
  <row Id="439" PostId="283" Score="0" Text="(caught by the edit period timeout) ...this was presuming you are concerned with more global alignment of the grid points and not just local effects." CreationDate="2015-08-19T11:53:46.887" UserId="209" />
  <row Id="440" PostId="283" Score="0" Text="@SimonF I'm happy to use any kind of aligned grid (just a square grid would be fine) if a random offset can be applied to each vertex using a probability distribution that results in no grid aligned average frequency variations. I'd be interested to hear about another grid type if that makes the frequency isotropic - or any approaches that haven't occurred to me. My suspicion is that an aperiodic tiling, although globally isotropic, will be locally biased for any practical length scales." CreationDate="2015-08-19T12:08:53.813" UserId="231" />
  <row Id="441" PostId="283" Score="0" Text="@trichoplax Another thought that occurred to me is that the displacements you're suggesting sound like the schemes used to approximate minimum distance Poisson disc distributions using a jittered grid, e.g as used for antialiasing.  I believe some care is needed when choosing how to generate those jittered offsets. I tried quick search in my papers collection and one that sprang up is &quot;Filtered Jitter&quot;, by V. Klassen, (http://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00459/abstract). It's from 2000 so there may be better approaches, but it's surely worth a try." CreationDate="2015-08-19T13:05:35.270" UserId="209" />
  <row Id="443" PostId="284" Score="1" Text="What do you mean by &quot;texture baking&quot;? I'm not familiar with this use of the term." CreationDate="2015-08-19T14:23:54.310" UserId="196" />
  <row Id="444" PostId="283" Score="2" Text="Here's an interesting paper: http://www.cs.utah.edu/~aek/research/noise.pdf (useful keywords: &quot;Fourier spectrum&quot;)" CreationDate="2015-08-19T14:39:14.103" UserId="196" />
  <row Id="445" PostId="284" Score="0" Text="@Calsbeek, its turning 3d calculation on the surface back to a texture in 2d for re-using. Pixar has a paper or technical report where they coin the name. Id search for you but its a bit painfull to do these things on the phone while in transit." CreationDate="2015-08-19T14:39:17.913" UserId="38" />
  <row Id="446" PostId="166" Score="0" Text="See also [Massively-Parallel Vector Graphics](http://w3.impa.br/~diego/projects/GanEtAl14/), published in SIGGRAPH Asia 2014." CreationDate="2015-08-20T01:24:20.347" UserId="192" />
  <row Id="447" PostId="166" Score="0" Text="Between the options you've listed in your question and the Loop and Blinn paper, I think you've pretty much exhausted all the possibilities." CreationDate="2015-08-20T05:41:34.620" UserId="48" />
  <row Id="448" PostId="57" Score="1" Text="You might want to take a look into [finite-element methods](https://en.wikipedia.org/wiki/Finite_element_method), which also use triangulation (or more generally: simplices) and often face the problem of needing a higher sampling density in selected regions. They are bound to have developed algorithms for this." CreationDate="2015-08-20T06:56:31.913" UserId="394" />
  <row Id="449" PostId="293" Score="0" Text="You were like a few seconds faster than I and even have information about Spir-V, which I did not even know." CreationDate="2015-08-20T11:07:42.403" UserId="127" />
  <row Id="450" PostId="166" Score="0" Text="You can tessellate a line, like described [here](http://www.gamedev.net/page/resources/_/technical/directx-and-xna/d3d11-tessellation-in-depth-r3059). Or you can triangulate in a compute shader." CreationDate="2015-08-20T13:28:20.537" UserId="386" />
  <row Id="451" PostId="297" Score="2" Text="You're comparing a cube with a more complex mesh however. Why not replicate the same scenario? The Susan model is easy to get." CreationDate="2015-08-20T15:15:03.147" UserId="4" />
  <row Id="452" PostId="297" Score="0" Text="It's not so easy in a shadertoy implementation! (:" CreationDate="2015-08-20T15:16:15.037" UserId="56" />
  <row Id="453" PostId="297" Score="2" Text="You cube looks correct to me: Get more transparent as it approaches the edges. If you can do full blown Suzanne the a sphere should at least give a better approximation of the look in the other picture." CreationDate="2015-08-20T15:51:06.460" UserId="327" />
  <row Id="454" PostId="297" Score="0" Text="ok fair enough.  i'll try it with better geometry, but i think you are right.  I'll accept it if you make an aswer!" CreationDate="2015-08-20T16:37:42.147" UserId="56" />
  <row Id="455" PostId="299" Score="1" Text="GLSL functions cannot return a value? Hum? From where did you get this idea?" CreationDate="2015-08-20T17:41:34.143" UserId="54" />
  <row Id="456" PostId="280" Score="0" Text="If you add some example code to your question, we can more easily tell you if it is a compile time constant or not (:" CreationDate="2015-08-20T17:59:09.763" UserId="56" />
  <row Id="457" PostId="299" Score="0" Text="I always used to the value-return calling convention as described here https://www.opengl.org/wiki/Core_Language_%28GLSL%29#Functions. If it is possible to define the return type and use &quot;return variable;&quot; syntax I will change it. EDIT: nvm, you're right. The OpenGL 4.5 specification says it should be possible (but I don't know when it was introduced)" CreationDate="2015-08-20T18:14:37.627" UserId="64" />
  <row Id="458" PostId="299" Score="1" Text="Return values have always been supported, AFAIK. If you look at the [1.2 specification, section 6.1](https://www.opengl.org/registry/doc/GLSLangSpec.Full.1.20.8.pdf), you can see the function declaration syntax: `returnType functionName (type0 arg0, type1 arg1, ..., typen argn);`. Plus, a lot of builtin functions return a value... Perhaps the existence of `in/out` parameters has tricked you into thinking it didn't support return values, but both are orthogonal concepts. Apart from that, your answer is pretty good, btw ;)" CreationDate="2015-08-20T18:26:30.610" UserId="54" />
  <row Id="459" PostId="299" Score="1" Text="Yeah, I probably just looked at the wiki instead of the specification itself. Never occurred to me that both methods are possible, although indeed the buildin functions do use it." CreationDate="2015-08-20T18:42:21.180" UserId="64" />
  <row Id="460" PostId="300" Score="0" Text="This question would benefit from a few (artificial) sample pictures" CreationDate="2015-08-20T18:59:56.403" UserId="38" />
  <row Id="462" PostId="304" Score="0" Text="Indeed, the idea was to use complex 3D models, but surround them with rough brushes for portaling. I had brifely considered using voxelization, but dismissed it due to thinking the cost of traversing it would be prohibitive, but I didn't even think of using a tree structure! (Which looks obvious in restrospect...)" CreationDate="2015-08-20T21:26:14.777" UserId="327" />
  <row Id="463" PostId="303" Score="0" Text="Now that you mentioned culling this makes a lot of sense. I just checked and there's a culling mask property in light objects to control which meshes (in a layers) are affected by it." CreationDate="2015-08-20T21:39:29.170" UserId="250" />
  <row Id="464" PostId="306" Score="3" Text="Adam Simmons has done some interesting work in this area. I don't know specifically how he's achieving it, but his SDF-based vector rendering is the sharpest I've seen in practice outside of Valve. https://twitter.com/adamjsimmons/status/611677036545863680" CreationDate="2015-08-20T22:50:02.897" UserId="104" />
  <row Id="465" PostId="281" Score="3" Text="This is true, but it is *not* the only thing you need to consider for performance. GPUs still statically schedule resources per shader, so this may still resources as though you were executing both branches, which may hurt occupancy." CreationDate="2015-08-21T06:16:53.847" UserId="196" />
  <row Id="466" PostId="306" Score="0" Text="@warrenm thanks! This is exactly what I was looking for. Do you want to post this as an answer so I can give you credit?" CreationDate="2015-08-21T13:02:00.990" UserId="250" />
  <row Id="467" PostId="306" Score="0" Text="As a matter of fact Adam already posted source code on shadertoy. Here's the link: https://www.shadertoy.com/view/ltXSDB" CreationDate="2015-08-21T13:02:37.213" UserId="250" />
  <row Id="468" PostId="306" Score="0" Text="You had me excited.  He did post the bezier stuff on shadertoy but not the texture distance field stuff!" CreationDate="2015-08-21T14:33:14.070" UserId="56" />
  <row Id="469" PostId="313" Score="1" Text="Thanks for answering. What does BSDF stand for?" CreationDate="2015-08-21T15:41:28.387" UserId="433" />
  <row Id="470" PostId="298" Score="4" Text="The new explicit graphics APIs aren't really designed for consumption by the average graphics programmer, they're more for the people doing infrastructure work in engines, or for those who *really* need the performance. They give you a much bigger gun and point it right at your foot." CreationDate="2015-08-21T16:07:06.500" UserId="174" />
  <row Id="471" PostId="313" Score="3" Text="BSDF = Biderectional Scattering Distribution Function" CreationDate="2015-08-21T16:41:42.903" UserId="100" />
  <row Id="472" PostId="315" Score="4" Text="There can definitely be differences between IHV implementations. It could be due to a driver bug, or different interpretations of ambiguities in the standard, or even possibly differences in how the compilers treat floating-point arithmetic, etc. Would need some more detailed debugging to understand what's going on." CreationDate="2015-08-21T20:20:52.653" UserId="48" />
  <row Id="473" PostId="306" Score="0" Text="Posted as answer. I don't think it's a very high-quality answer as it doesn't actually describe the technique, but I guess it's a pointer in an interesting direction..." CreationDate="2015-08-21T21:28:04.250" UserId="104" />
  <row Id="474" PostId="306" Score="0" Text="@AlanWolfe I think he has done only for procedurally set bezier curves. I'm not sure the effort required to integrate this into a ttf render lib. When I have some time I'll take a look at it." CreationDate="2015-08-21T22:07:14.407" UserId="250" />
  <row Id="475" PostId="306" Score="0" Text="it looks he has some magic sauce on the side of actually storing and retrieving the distances from a texture.  Without a texture in play, the shadertoy examples are missing that part of the equation." CreationDate="2015-08-21T22:11:30.443" UserId="56" />
  <row Id="476" PostId="315" Score="1" Text="@NathanReed ,what kind of information could be helpful?" CreationDate="2015-08-22T00:20:14.553" UserId="437" />
  <row Id="477" PostId="315" Score="6" Text="Start by debugging it like any other graphics/shader problem. Isolate each pass and see in which pass the error is being introduced, then isolate where in that shader is something going wrong." CreationDate="2015-08-22T00:47:23.243" UserId="48" />
  <row Id="478" PostId="320" Score="0" Text="I dont know but it certainly does make sense not to gamma correct." CreationDate="2015-08-22T12:27:02.963" UserId="38" />
  <row Id="479" PostId="319" Score="1" Text="This is actually a pretty good method. But is it less expensive?" CreationDate="2015-08-22T12:28:39.017" UserId="38" />
  <row Id="480" PostId="311" Score="0" Text="It would be great if you had reference footage for the effect you want. Say, something like this? https://www.youtube.com/watch?v=XH-groCeKbE" CreationDate="2015-08-22T15:20:44.137" UserId="196" />
  <row Id="481" PostId="311" Score="0" Text="@JohnCalsbeek yes that would make it easier to get across what I want. In the video you linked to the individual birds are discernable (just). I'm looking to render a flock a little more distant so that individuals are not visible, but the variations in density are still consistent and realistic." CreationDate="2015-08-22T15:24:42.263" UserId="231" />
  <row Id="482" PostId="26" Score="2" Text="I believe Simplex noise is only patented for 3D and above." CreationDate="2015-08-22T15:49:00.753" UserId="231" />
  <row Id="483" PostId="319" Score="0" Text="It's as expensive as you want your simulation to be. If it's *too* expensive, use fewer samples." CreationDate="2015-08-22T16:02:29.190" UserId="197" />
  <row Id="484" PostId="320" Score="0" Text="I'm not posting this as an answer since I'm not confident in it, but human vision's perception of brightness is *not* linear. In fact, sRGB does a quite good job of compensating for that and giving the most precision in the areas that matter. So you might find that gamma correcting before compressing luma may actually yield worse results." CreationDate="2015-08-22T19:55:49.240" UserId="327" />
  <row Id="489" PostId="324" Score="0" Text="If you are worried about the square root, you can probably square both sides of the equation." CreationDate="2015-08-24T00:26:41.520" UserId="56" />
  <row Id="490" PostId="324" Score="0" Text="@AlanWolfe I can precompute the square root - it's only ever root 2. My main reason for changing the side length is the significant reduction in area. I just wonder if it broke anything..." CreationDate="2015-08-24T00:35:46.710" UserId="231" />
  <row Id="491" PostId="320" Score="0" Text="AFAIK, video standards assume R'G'B', ie. a non-linear colour space, when applying the 3x3 colour transforms to/from YCbCr.  In an application such as video where one wants to maximise quality per bit, it doesn't make sense to use linear.&#xA; I think sections 27 and 29 of Charles Poyton's Color FAQ express it more clearly: http://poynton.com/notes/colour_and_gamma/ColorFAQ.html#RTFToC27" CreationDate="2015-08-24T07:54:04.493" UserId="209" />
  <row Id="492" PostId="320" Score="0" Text="&quot;Video demystified&quot; also says: _&quot;YCbCr is the color space originally defined by BT.601, and now used for all digital component video formats. .... The technically correct notation is&#xA;Y'Cb'Cr' since all three components are derived from R'G'B'.&quot;_" CreationDate="2015-08-24T08:07:05.627" UserId="209" />
  <row Id="493" PostId="247" Score="0" Text="Subdivision surfaces are used a lot more than constructive solid geometry. They still involve triangles (or alternatively splines)." CreationDate="2015-08-24T18:06:28.447" UserDisplayName="user458" />
  <row Id="494" PostId="326" Score="1" Text="Isn't the point of stereoscopic rendering to render the occluded parts of both views for each eye?" CreationDate="2015-08-24T20:55:31.253" UserId="197" />
  <row Id="495" PostId="326" Score="0" Text="IMO, occlusion isn't really the point of it.&#xA;&#xA;The point is the illusion of depth that happens when giving different levels of parallax to each eye based on an object's distance." CreationDate="2015-08-24T20:58:08.737" UserId="56" />
  <row Id="496" PostId="325" Score="0" Text="This is going to be a zero net comment, but... just wanted to mention, if you are unsure of performance, you could profile and see.  But, of course, there might be different characteristics on different hardware that you might not have access to, and you may not be aware of the ways it might be faster or slower.  Like, texture reads are really cheap til you are texture read bound :P" CreationDate="2015-08-24T23:36:28.950" UserId="56" />
  <row Id="497" PostId="328" Score="0" Text="I've watched that presentation before, it is very good. I did not know about what you mention in the first paragraph though, so thanks for that info!" CreationDate="2015-08-25T01:39:07.027" UserId="54" />
  <row Id="498" PostId="318" Score="0" Text="I hadn't heard of bilateral up sampling. Do you have any links by chance? It's different than bicubic or bilinear sampling right?" CreationDate="2015-08-25T01:42:47.320" UserId="56" />
  <row Id="499" PostId="329" Score="0" Text="The question lacks some basic explanation about what is being asked. What exactly are you trying to do? From your video, what seems to be happening is that when the rotation crosses a certain point, the rotation instantly snaps back/forward by 180 degrees. (That is, the colors aren't being swapped, you're seeing the back of the cube instead.)" CreationDate="2015-08-25T04:39:10.677" UserId="327" />
  <row Id="500" PostId="329" Score="0" Text="I assume that the `m_setPan` path isn't used here? Also, why does your rotation code modify position? (And I don't see it setting a rotation anywhere.)" CreationDate="2015-08-25T04:49:56.480" UserId="327" />
  <row Id="501" PostId="329" Score="0" Text="You assume correct. What I'm doing is to orbit the camera over a sphere centered at the origin, therefore given the displacement in (x,y) of the mouse, I compute the spherical coords of the position of the camera on that sphere, once I have placed the camera I update the view-matrix (done inside `setPosition()`)" CreationDate="2015-08-25T04:53:57.070" UserId="116" />
  <row Id="502" PostId="329" Score="1" Text="Just as a note, this is the kind of thing that can be most easily debugged by printing the `m_position`, `theta` and `phi` values to the screen on every update and observing their values as you drag the cube around. This should make it easy to observe any singularities or abnormalities that could be causing the problem." CreationDate="2015-08-25T05:06:13.067" UserId="327" />
  <row Id="503" PostId="330" Score="2" Text="This is usually called &quot;stereo reprojection&quot;, since one way of implementing it is to take the matrix that transforms from the new perspective to the old perspective." CreationDate="2015-08-25T05:15:09.187" UserId="327" />
  <row Id="504" PostId="330" Score="0" Text="@yuriks that makes sense." CreationDate="2015-08-25T05:16:25.527" UserId="38" />
  <row Id="505" PostId="333" Score="0" Text="You could render every pass to a texture and read back the buffers to obtain the output." CreationDate="2015-08-25T06:08:38.767" UserId="127" />
  <row Id="507" PostId="331" Score="1" Text="or use atan2 which is there to deal with all quadrants correctly" CreationDate="2015-08-25T07:53:23.183" UserId="137" />
  <row Id="508" PostId="302" Score="1" Text="FWIW, some monitors come with a built-in motorised calibration unit. It's amusing to watch the &quot;arm&quot; rise out of the bezel so it can sample the screen output during the calibration process." CreationDate="2015-08-25T09:20:40.457" UserId="209" />
  <row Id="509" PostId="331" Score="2" Text="@ratchetfreak Yes, but in general, solution that avoid trigonometry or angles tend to be more robust and elegant." CreationDate="2015-08-25T10:06:14.517" UserId="327" />
  <row Id="510" PostId="335" Score="0" Text="This is a bit too much stuff for one question. I can essentially boil down my introductory lecture on the subject in a few hours. Bit going into detail of use of De casteljanu and the de boor's algorithm would take me too much time." CreationDate="2015-08-25T13:42:10.423" UserId="38" />
  <row Id="511" PostId="335" Score="1" Text="So i would like to see questions 3, 5 and possibly 6 split off as separate questions to make answering and understanding more meal sized." CreationDate="2015-08-25T13:51:16.213" UserId="38" />
  <row Id="512" PostId="335" Score="0" Text="@joojaa Sure, I can split the question, just a sec..." CreationDate="2015-08-25T14:03:19.830" UserId="141" />
  <row Id="513" PostId="335" Score="1" Text="So the question [3](http://computergraphics.stackexchange.com/questions/338), [5](http://computergraphics.stackexchange.com/questions/339/nurbs-curve-drawing) and [6](http://computergraphics.stackexchange.com/questions/340/splitting-of-nurbs-curves) were split off to separate questions." CreationDate="2015-08-25T14:11:09.837" UserId="141" />
  <row Id="514" PostId="340" Score="0" Text="I don't know if it will do the same, but De Boor's algorithm is the equivalent of De Casteljeau.  Interestingly, I know you can use De Boor's algorithm to split a NURBS or b-spline into a piecewise Bezier curve." CreationDate="2015-08-25T14:44:00.587" UserId="56" />
  <row Id="515" PostId="338" Score="0" Text="What do you mean exactly by offset curves?" CreationDate="2015-08-25T14:44:27.810" UserId="56" />
  <row Id="516" PostId="338" Score="0" Text="An &quot;offset&quot; curve has constant distance to the given curve, aka [parallel curve](http://mathworld.wolfram.com/ParallelCurves.html)." CreationDate="2015-08-25T14:47:56.580" UserId="141" />
  <row Id="525" PostId="342" Score="2" Text="I don't have a citation, but if I recall correctly Monster's University was the first Pixar movie to move towards a near-full radiosity solution instead of doing lots of artist controlled &quot;False Radiosity,&quot; so it would certainly seem like it's still used." CreationDate="2015-08-25T18:38:31.753" UserId="174" />
  <row Id="528" PostId="341" Score="0" Text="My understanding is that if your not using the peculiarities of nonuniformness/rationality  then they are using  possibly the same berentein basis formulations. In fact i seem to remember somebody telling me that in fact B-splines started that way. Its only later that we have realized they are the same. Some examples: https://reference.wolfram.com/language/ref/BSplineBasis.html" CreationDate="2015-08-25T19:33:50.240" UserId="38" />
  <row Id="529" PostId="225" Score="3" Text="I vote to close this question, because it is too broad. There are simply too many variants, especially when including *possibly others* like the ones [Alan Wolfe named in his comment](http://computergraphics.stackexchange.com/questions/225/ray-based-rendering-terms#comment339_225). More specific/narrow questions that compare a few techniques e.g. to achieve a specific goal could very well be suited for this format." CreationDate="2015-08-25T20:29:11.020" UserId="127" />
  <row Id="530" PostId="341" Score="0" Text="Right. A bezier curve is a special case of a bspline and a bspline is a special case of a NURBS." CreationDate="2015-08-25T21:47:18.070" UserId="56" />
  <row Id="531" PostId="344" Score="0" Text="The notation for path tracing suggests that it can't handle paths like `ES*L` but of course it can if they are area lights (not punctual lights). Plus, I think that statement in your reference [2] is just plain wrong. Path tracing doesn't ignore caustics; it's just not very efficient at them (photon mapping, Metropolis, VCM etc. are better)." CreationDate="2015-08-25T22:20:52.263" UserId="48" />
  <row Id="532" PostId="344" Score="0" Text="Thanks Ecir for the explanation (specially the regex... I wonder if they ever considered E{2} for both eyes ;). When I mentioned &quot;ray tracing&quot; I was kind of quoting the tutorial of Cornell University, they didn't mention any specific technique, that's why I was doubting if radiosity was a type or partly belonged to ray tracing. So if you were to create a diffuse reflection, would you choose path-tracing over radiosity? Why (which one would be more efficient)?" CreationDate="2015-08-26T01:17:20.417" UserId="157" />
  <row Id="533" PostId="225" Score="1" Text="I would split this question to a few more questions.. We already have a pretty good answer for [ray-marching](http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing)." CreationDate="2015-08-26T10:27:57.190" UserId="38" />
  <row Id="534" PostId="343" Score="0" Text="regarding (*), Bezier curves have the same problem.  The issue there is that the curves (in both cases) are defined as x = f(t), y = f(t). However, you can also define a univariate / explicit / 1 dimensional curve (again, in both cases) as y = f(x), using x in place of t.  In the case of rational curves, instead of being able to represent conic sections, you can represent sine and cosine (and more of course).  NURBS / b-splines aren't special in that regard." CreationDate="2015-08-26T14:04:49.080" UserId="56" />
  <row Id="536" PostId="343" Score="0" Text="I think your statement about length is wrong (only works with linear functions?), and not sure how length calculations is supposed to fit in your explanation (good info you gave, just sayin'!)" CreationDate="2015-08-26T14:16:36.920" UserId="56" />
  <row Id="538" PostId="343" Score="0" Text="@AlanWolfe deleted anyway" CreationDate="2015-08-26T14:21:08.100" UserId="38" />
  <row Id="539" PostId="287" Score="1" Text="Just for future people's curiosity, you might want to make &quot;another question&quot; be a link to that question." CreationDate="2015-08-26T15:22:03.767" UserId="174" />
  <row Id="540" PostId="287" Score="1" Text="@porglezomp that's a good point - done." CreationDate="2015-08-26T15:26:13.020" UserId="231" />
  <row Id="541" PostId="342" Score="2" Text="Games use fake fill lights all the time. One good example I know of is Tomb Raider (2013). There was an awesome presentation about it at GDC 2013. http://www.gdcvault.com/play/1017934/Casting-a-New-Light-on" CreationDate="2015-08-26T15:41:45.873" UserId="310" />
  <row Id="542" PostId="351" Score="4" Text="Also, I've gotten the impression that &quot;albedo&quot; in astronomy is averaged over the whole visible spectrum (and sometimes a wider spectrum, including UV and infrared), while diffuse/specular coefficients are RGB or ideally spectral quantities." CreationDate="2015-08-26T21:00:52.863" UserId="48" />
  <row Id="544" PostId="344" Score="0" Text="@NathanReed I asked about it at [ompf2](http://ompf2.com/viewtopic.php?f=6&amp;t=2057) and ingenious says: &quot;The only type of light paths that a forward path tracer cannot sample is E(D|G)*S+L, where L is a light source whose definition involves a delta distribution, either in the directional emission or the positional. Examples are point lights and directional lights. Such paths can be described using Veach's extended notation for luminaires and sensors, see section 8.3.2 in his thesis.&quot;" CreationDate="2015-08-26T21:10:36.907" UserId="141" />
  <row Id="545" PostId="344" Score="0" Text="@Armfoot I would definitely go with path tracing. Lots of research, books, code to learn from. I don't know which would be faster, though, too many variables (acceleration structure, shading system, ...). Radiosity apparently simulates the heat propagation after splitting the scene into many tiny triangles ([FEM](https://en.wikipedia.org/wiki/Finite_element_method)), I never tried it and the only product to used it I know of was Autodesk Lightscape. Last but not least, are you really sure you will ever need only diffuse reflections?" CreationDate="2015-08-26T21:19:59.523" UserId="141" />
  <row Id="546" PostId="343" Score="0" Text="Most awesome! Thanks a lot, very good explanation!" CreationDate="2015-08-26T21:28:37.290" UserId="141" />
  <row Id="547" PostId="343" Score="0" Text="Typo perhaps? &quot;Instead the underlying surface has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the next.&quot; -&gt; &quot;Instead the underlying **curve** has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the **previous**.&quot; Btw., could you please clarify what you mean by &quot;UV range&quot;? &quot;UV&quot; implies 2D..?" CreationDate="2015-08-26T21:31:50.683" UserId="141" />
  <row Id="548" PostId="344" Score="0" Text="@Armfoot The notation doesn't use E{2} for the same reason that it doesn't use L{n} for multiple lights. This describes a single path, or a single sample. The way that we normally formalise Monte Carlo rendering is to take the Kajiya rendering equation, and then turn it into a random variable, the expected value of which is the solution to the equation. You can then calculate the value of a pixel by taking lots of samples and estimating the mean. Light paths more or less correspond to Feynman diagrams." CreationDate="2015-08-26T21:34:20.893" UserId="159" />
  <row Id="549" PostId="352" Score="0" Text="I'm sorry I'm very new to NURBS: what do you mean by &quot;maximum multiplicity&quot;? I mean, when I do it in the former way, do I end up with multiple overlapping control points?" CreationDate="2015-08-26T22:03:14.283" UserId="141" />
  <row Id="550" PostId="351" Score="3" Text="Interesting. In that case, it doesn't make sense to call something Albedo Map does it?" CreationDate="2015-08-26T22:31:51.383" UserId="250" />
  <row Id="551" PostId="354" Score="0" Text="This seems like several interesting questions in one. At least (4), (5) and (6) sound like independent questions, and maybe (2) and (3) could be a single question. I'd upvote any of those as separate questions, but all in one it seems too broad." CreationDate="2015-08-26T22:31:57.127" UserId="231" />
  <row Id="552" PostId="351" Score="0" Text="@PhilLira that does seem like an unhelpful use of the term... Hopefully it won't catch on and cause more confusion..." CreationDate="2015-08-26T22:34:28.340" UserId="231" />
  <row Id="553" PostId="354" Score="0" Text="Ok! Will do separates and then delete this. thx!" CreationDate="2015-08-26T22:34:30.550" UserId="250" />
  <row Id="554" PostId="354" Score="0" Text="Actually, I just edited this and kept link to others." CreationDate="2015-08-26T22:54:09.513" UserId="250" />
  <row Id="555" PostId="352" Score="0" Text="Let me try to explain in the answer." CreationDate="2015-08-27T01:20:13.860" UserId="159" />
  <row Id="556" PostId="353" Score="0" Text="Hey mjp how are you. I remember you from gamedev.net. I asked a question about actual usages of curves in games and I remember you gave some good info.  Howdy!" CreationDate="2015-08-27T02:17:52.843" UserId="56" />
  <row Id="557" PostId="166" Score="0" Text="You should also look into signed distance fields and signed distance textures." CreationDate="2015-08-27T02:19:42.313" UserId="56" />
  <row Id="558" PostId="352" Score="1" Text="Pseudonym err no not a good knot vector to demonstrate this. I See thet i might need to expand the other post. Altough @EcirHana it might be a good idea to ask what a multilicity is." CreationDate="2015-08-27T09:03:53.187" UserId="38" />
  <row Id="559" PostId="344" Score="0" Text="Thanks again Ecir, I get your point: if we can have a lot more with one method, it's kind of pointless to pursue another if we don't know exactly how better it will perform." CreationDate="2015-08-27T10:14:45.790" UserId="157" />
  <row Id="560" PostId="344" Score="0" Text="I was trying to make a little joke when I mentioned E{2} @Pseudonym but thanks for further explaining why it's a single path and the reason behind it :)" CreationDate="2015-08-27T10:22:20.827" UserId="157" />
  <row Id="561" PostId="343" Score="0" Text="@EcirHana Done do you need the multiplicity explanation here" CreationDate="2015-08-27T11:05:22.887" UserId="38" />
  <row Id="562" PostId="343" Score="0" Text="@joojaa Thanks! Yes, I've just posted another [question](http://computergraphics.stackexchange.com/questions/358/)" CreationDate="2015-08-27T11:20:51.860" UserId="141" />
  <row Id="563" PostId="359" Score="0" Text="You might want to add meta on wether or not this is a relevant qestion?" CreationDate="2015-08-27T12:05:32.300" UserId="38" />
  <row Id="564" PostId="359" Score="0" Text="@joojaa no one has complained about it yet, but feel free to raise it on meta if you like." CreationDate="2015-08-27T12:14:03.480" UserId="231" />
  <row Id="565" PostId="360" Score="0" Text="When you say &quot;knots lie on top of each other&quot;, you mean that their values don't change in the knot vector? So in your example [0, 0, 0, 0], [1, 1 ,1], [2, 2, 2] all &quot;lie on top of each other&quot;?" CreationDate="2015-08-27T12:33:48.817" UserId="141" />
  <row Id="566" PostId="360" Score="0" Text="@EcirHana yes that is what i mean a more normal parametrisation would be [0, 0, 0, 0, 0.5, 1, 1.5, 2, 2, 2, 2] Ill add a animation" CreationDate="2015-08-27T12:45:26.903" UserId="38" />
  <row Id="567" PostId="361" Score="0" Text="The cross sectional area visible dimishes as your incidence angle grows." CreationDate="2015-08-27T13:01:45.767" UserId="38" />
  <row Id="568" PostId="352" Score="0" Text="You're probably right about that @joojaa." CreationDate="2015-08-27T13:03:38.617" UserId="159" />
  <row Id="569" PostId="361" Score="1" Text="@joojaa I follow that bit, but the bit in bold seems to be talking about tilting the surface away from its initial normal vector, which would only make sense for the specific case that the incident light is perpendicular to the surface, or I'm missing something." CreationDate="2015-08-27T13:10:07.617" UserId="231" />
  <row Id="570" PostId="360" Score="0" Text="Just to check: so to split a NURBS curve the main task is to insert knots so that the overall shape remains unchanged, correct?" CreationDate="2015-08-27T13:28:23.723" UserId="141" />
  <row Id="571" PostId="360" Score="0" Text="Yes, once you have enough knots you can delete whats on the other side." CreationDate="2015-08-27T13:31:22.490" UserId="38" />
  <row Id="572" PostId="360" Score="0" Text="There is a different strategy also you can insert one knot and delete the points that nolonger affect the knot you get a slightly different parametrisation though. I prefer this method for most of my own modeling. But nobody else seems to have heard of this." CreationDate="2015-08-27T13:34:43.487" UserId="38" />
  <row Id="573" PostId="360" Score="0" Text="Out of curiosity, what kind of modeling software do you use? Or you mean in your own code?" CreationDate="2015-08-27T13:54:31.220" UserId="141" />
  <row Id="574" PostId="360" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/27460/discussion-between-joojaa-and-ecir-hana)." CreationDate="2015-08-27T13:56:28.700" UserId="38" />
  <row Id="575" PostId="166" Score="0" Text="Something to keep an eye on: https://twitter.com/sheredom/status/636572086211903488" CreationDate="2015-08-27T16:20:06.833" UserId="141" />
  <row Id="577" PostId="356" Score="1" Text="What do you mean by vertical vs horizontal layout of triangles?" CreationDate="2015-08-28T04:04:39.290" UserId="197" />
  <row Id="578" PostId="338" Score="0" Text="I wonder what property of NURBS make them able to do this.  I would think it would be that they are rational, but then rational bezier curves would also have this property." CreationDate="2015-08-28T04:07:40.140" UserId="56" />
  <row Id="579" PostId="315" Score="0" Text="This might be a vsync issue if you're using a variable timestep per-frame." CreationDate="2015-08-28T04:09:40.887" UserId="197" />
  <row Id="580" PostId="323" Score="2" Text="This might be a better question for [Physics.SE](https://physics.stackexchange.com/) or [Astronomy.SE](https://astronomy.stackexchange.com/). I know a point mass does produce lensing effects (see e.g. [this](http://spiro.fisica.unipd.it/~antonell/schwarzschild/)) but no idea if a galaxy can be well-approximated by a point mass for something like this." CreationDate="2015-08-28T05:40:39.697" UserId="48" />
  <row Id="581" PostId="364" Score="2" Text="Great answer! Thank you!" CreationDate="2015-08-28T05:51:19.167" UserId="197" />
  <row Id="582" PostId="333" Score="1" Text="As an improvement to depth peeling, you might look into [Multi-Layer Depth Peeling Via Fragment Sort](http://research-srv.microsoft.com/pubs/70307/tr-2006-81.pdf) which allows you to extract multiple layers in each pass, reducing the total number of passes needed." CreationDate="2015-08-28T06:10:02.113" UserId="48" />
  <row Id="584" PostId="366" Score="3" Text="Besides a source of advancements its also a great way to learn and an even better place to find examples. I know myself have learned shaders over the past year mostly through Shadertoy. I've found it to be such an open source community it's awesome how everyone shares their techniques." CreationDate="2015-08-28T15:32:31.113" UserId="376" />
  <row Id="585" PostId="366" Score="4" Text="Good point! I also have to say, that as a professional game programmer, when I see a demo scene person make something I didn't even think was possible, it makes me want to learn about it and try to bring those techniques into the games I'm working on." CreationDate="2015-08-28T15:46:22.373" UserId="56" />
  <row Id="586" PostId="359" Score="0" Text="It really does seem like a &quot;conversation&quot; and not a question that can be answered.  I know on other stack exchange sites that they prefer questions that can be answered, but not sure what the policy here is." CreationDate="2015-08-28T16:17:22.667" UserId="56" />
  <row Id="587" PostId="365" Score="1" Text="Thanks! That was really helpful. Regading the constants/uniform cache. Are they any tips besides precision (mediump, lowp) I could use to improve uniforms cache hit ratio? Does the order in which I declare uniforms make any difference (as for packing more tightly)?" CreationDate="2015-08-28T18:32:24.263" UserId="250" />
  <row Id="588" PostId="359" Score="0" Text="@AlanWolfe we're in the progress of deciding collectively what our policy will be, so go ahead and mention anything you find relevant on Meta. That way we can have clear guidelines before opening up to a wider community in public beta. I do like to ask questions on the borderline to try and kick start that discussion about policy..." CreationDate="2015-08-28T18:33:44.007" UserId="231" />
  <row Id="589" PostId="365" Score="2" Text="@PhilLira Packing can make a difference, yeah. The compiler will insert padding to prevent vectors from being split across 16-byte boundaries, so try to avoid that. I don't think mediump/lowp actually does anything on uniforms, at least on desktop GPUs (maybe it does on mobile). I wouldn't worry too much about uniform cache hit ratio though. That's extremely rarely, if ever, a bottleneck." CreationDate="2015-08-28T20:23:42.223" UserId="48" />
  <row Id="590" PostId="367" Score="2" Text="GPUs work in a different way. (Some architectures) don't have the concept of a global &quot;program stack&quot;, so recursive function calls are not possible in those. OpenCL probably adopts the lowest common denominator, thus disallowing it completely to remain portable across GPUs. Newer CUDA hardware seems to have introduced support for recursion at some point: http://stackoverflow.com/q/3644809/1198654" CreationDate="2015-08-29T02:04:27.370" UserId="54" />
  <row Id="591" PostId="368" Score="4" Text="I'm reluctant to share this secret sauce, but I've had pretty good luck having a fixed maximum bounce count and having a stack of a fixed size (and a loop with a fixed number of iterations) to handle this. Also (and this is the real secret sauce imo!) I have my materials be either reflective or refractive but never both, which makes it so rays don't split when they bounce. The end result of all this is recursive type raytraced rendering, but through fixed size iteration, not recursion." CreationDate="2015-08-29T03:03:43.217" UserId="56" />
  <row Id="592" PostId="366" Score="0" Text="...and on the other hand, Wolfenstein 3D used ray marching (in 2D)." CreationDate="2015-08-29T11:05:01.710" UserId="159" />
  <row Id="593" PostId="366" Score="0" Text="Oh right totally! John Carmack did some amazing things with ray casting" CreationDate="2015-08-29T14:25:16.227" UserId="56" />
  <row Id="594" PostId="370" Score="1" Text="Have you seen this? Not a whole lot of info but some.  http://www.gamedev.net/topic/573051-lighting-in-object-space/" CreationDate="2015-08-29T18:53:47.497" UserId="56" />
  <row Id="595" PostId="372" Score="1" Text="Can you confirm whether the code shown (with blue removed) also causes the artefacts? If you have narrowed down the code then showing the image from the narrowed down code will help exclude any irrelevant details and highlight the problem." CreationDate="2015-08-30T17:23:05.893" UserId="231" />
  <row Id="596" PostId="369" Score="0" Text="Welcome to Compiter Graphics.SE,  glad to see a familiar face im sure you will be a great addition to our small community. Sorry I cant answer your question though." CreationDate="2015-08-30T18:51:35.957" UserId="38" />
  <row Id="601" PostId="203" Score="0" Text="@trichoplax Sorry for the noise!" CreationDate="2015-08-30T20:41:41.770" UserId="482" />
  <row Id="602" PostId="203" Score="0" Text="@luserdroog thanks for the interest :) Even though this question is only about materials, we could do with new questions related to colour spaces..." CreationDate="2015-08-30T20:43:36.530" UserId="231" />
  <row Id="603" PostId="374" Score="0" Text="Are you looking for a general method that you can apply to an arbitrary Bezier surface, or a way of preparing a fast method for a specific surface? Will your surface shape be fixed before runtime?" CreationDate="2015-08-30T22:05:22.257" UserId="231" />
  <row Id="606" PostId="209" Score="0" Text="@Mark-Both the answers suggested by Alan Wolfe and yuriks are correct" CreationDate="2015-08-31T02:27:59.957" UserDisplayName="user489" />
  <row Id="608" PostId="374" Score="1" Text="Note that you can raymarch bezier surfaces a lot easier than raytracing it. You can also raytrace or raymarch univariate surfaces a lot easier than other kinds!  http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2015-08-31T04:16:10.380" UserId="56" />
  <row Id="609" PostId="372" Score="0" Text="Yes it does, I figured I'd take a screenshot with colour in there otherwise it looks even uglier :)" CreationDate="2015-08-31T04:54:39.483" UserId="193" />
  <row Id="610" PostId="209" Score="2" Text="I can't mark a comment as correct, though." CreationDate="2015-08-31T04:59:46.233" UserId="158" />
  <row Id="611" PostId="379" Score="0" Text="Technically light from the sun is white." CreationDate="2015-08-31T08:09:59.047" UserId="137" />
  <row Id="612" PostId="379" Score="0" Text="@ratchetfreak: True; there's something to say about temperature, but the topic is unrelated to the original question." CreationDate="2015-08-31T09:17:15.757" UserId="182" />
  <row Id="613" PostId="372" Score="0" Text="I can understand that, but since it's the ugliness that you're asking for help with, you might have a better chance of someone seeing the problem if you include the screenshot that matches the code too." CreationDate="2015-08-31T11:04:49.763" UserId="231" />
  <row Id="614" PostId="372" Score="1" Text="Fair enough. I've added an extra screenshot." CreationDate="2015-08-31T13:35:00.863" UserId="193" />
  <row Id="615" PostId="376" Score="0" Text="think about it this way: a deep crease will have shadows in it." CreationDate="2015-08-31T14:21:11.977" UserId="56" />
  <row Id="616" PostId="372" Score="0" Text="Have you tried playing around a bit more with the bias?" CreationDate="2015-08-31T19:36:47.713" UserId="100" />
  <row Id="617" PostId="372" Score="0" Text="This looks like shadow acne https://msdn.microsoft.com/en-us/library/windows/desktop/ee416324(v=vs.85).aspx - this is what the bias factor is supposed to alleviate, as cifz suggests try adjusting the value." CreationDate="2015-08-31T19:49:59.960" UserId="495" />
  <row Id="618" PostId="376" Score="1" Text="The key thing to understand here is that we are trying to calculate occlusion of the ambient light, not occlusion from view." CreationDate="2015-08-31T20:15:57.220" UserId="231" />
  <row Id="619" PostId="368" Score="0" Text="Like tail recursion?" CreationDate="2015-08-31T20:34:32.567" UserId="240" />
  <row Id="621" PostId="325" Score="0" Text="Desktop or mobile? Uniforms can be surprisingly costly on some mobile GPUs." CreationDate="2015-08-31T21:50:42.060" UserId="506" />
  <row Id="622" PostId="306" Score="0" Text="Bit late to the party but this older thread from reddit has a ton of info on various methods of improving the sharpness of SDF based rendering: https://www.reddit.com/r/gamedev/comments/2879jd/just_found_out_about_signed_distance_field_text/" CreationDate="2015-08-31T21:56:04.753" UserId="102" />
  <row Id="623" PostId="388" Score="2" Text="You are probably thinking of occlusion of one point from another. If you think of bodies or light sources with non-zero spatial extent, then the object can be partially occluded (like how the sun may be more or less occluded during a solar eclipse)." CreationDate="2015-08-31T23:26:19.390" UserId="16" />
  <row Id="628" PostId="390" Score="0" Text="&quot;the surface is lit based on how many of those directions are not occluded by objects in the scene. &quot; I unterstand this. But the question is from where comes the light? Where is the origin?&#xA;&#xA;Because if I have two balls, and the ambient lighting has it's origin between those balls, then there won't be nothing occluded, because there is nothing between objects and light source. But if the origin is behind one of the balls, then one ball is occluded by the other. -&gt; http://i.imgur.com/IRDvCzF.png&#xA;So the position of the source is important to determine what will be occluded. But Amb.L. has no pos." CreationDate="2015-09-01T00:28:55.947" UserId="480" />
  <row Id="629" PostId="392" Score="4" Text="Great answer. You might want to add that one way to think about affine transforms is that they keep parallel lines parallel. Hence, scaling, rotation, translation, shear and combinations, count as affine. Perspective projection is an example of a non-affine transformation." CreationDate="2015-09-01T06:08:21.133" UserId="14" />
  <row Id="630" PostId="393" Score="0" Text="I did post an answer, but i don't think looking up Wiktionary/wikipedia for you is a good use of this sites resources. Anyway you may wish to refine your question or we can wait for community opinion. Setting boundaries for what is and what is not in scope, is still valuable for the community." CreationDate="2015-09-01T07:51:17.370" UserId="38" />
  <row Id="631" PostId="392" Score="2" Text="You could add some pictures. If you wont I will :P Also might be good to mention order in matrix and row/column orientation is arbitrary. And that rotations in 3d are not comutative." CreationDate="2015-09-01T07:54:03.350" UserId="38" />
  <row Id="632" PostId="390" Score="2" Text="@Joey With one point light source every point on a surface will be either lit or not. With two point light sources a point on a surface may be lit by zero, one or two lights, giving three different light levels. With many point lights, there is gradual variation in the lighting. Ambient occlusion pretends that there are an infinite number of point light sources in the distance in all directions. This is not physically realistic, but it gives an approximation to the lighting in  a real scene, where light is reflected from the objects in the scene so that they are all lit by second hand light." CreationDate="2015-09-01T09:05:56.367" UserId="231" />
  <row Id="633" PostId="393" Score="0" Text="@joojaa I've added a [meta post](http://meta.computergraphics.stackexchange.com/questions/133/should-we-allow-word-definition-questions) so people can discuss whether word definitions should be on topic." CreationDate="2015-09-01T09:24:47.337" UserId="231" />
  <row Id="634" PostId="393" Score="1" Text="I'm voting to close this question as off-topic because its something you should be able to google in 60 seconds." CreationDate="2015-09-01T11:30:01.913" UserId="38" />
  <row Id="635" PostId="396" Score="0" Text="Oops. the shared models of the red, metal and green balls don't react to ambient light but they do react to my ambient occlusion light. Hope you get the point nonetheless." CreationDate="2015-09-01T11:40:49.123" UserId="38" />
  <row Id="636" PostId="393" Score="0" Text="Thought I marked answer as accepted I'm not 100% satisfied (but that's maybe because I asked wrongly). What is *temporal* in some algorithm? The answers says *temporal* means *depends on time*. But which time? How can I measure something, say 10 seconds ago? And *stochastic*. Of course I googled it before asking and far more than 60 second. But how is it differ from *random*?" CreationDate="2015-09-01T12:19:49.823" UserId="386" />
  <row Id="637" PostId="393" Score="0" Text="The answers answer to what you asked. The time is one :) How to get info from previous frames is a different question (you can reconstruct infos, store them in a buffer etc. etc.). Sthocastic and random are basically synonyms, often you use sthocastic to describe a process and random to describe a variable." CreationDate="2015-09-01T13:13:59.727" UserId="100" />
  <row Id="638" PostId="390" Score="2" Text="It might help a bit for understanding to specify that ambient occlusion is usually simulating light coming from the &quot;sky,&quot; which would clear up questions about what the source of the light is." CreationDate="2015-09-01T13:23:28.933" UserId="174" />
  <row Id="639" PostId="390" Score="2" Text="@porglezomp that's a useful way of thinking about it to gain understanding, but it can also be used in a closed room with no sky, where the ambient occlusion is the occlusion of the light reflected multiple times from the walls." CreationDate="2015-09-01T13:44:08.270" UserId="231" />
  <row Id="640" PostId="390" Score="0" Text="@trichoplax Yeah, that's why I put it in quotes." CreationDate="2015-09-01T13:48:59.960" UserId="174" />
  <row Id="641" PostId="397" Score="0" Text="I feel like temporal coherence could be used to denoise video as well." CreationDate="2015-09-01T16:17:03.930" UserId="56" />
  <row Id="642" PostId="297" Score="1" Text="I can't separate the refraction from the attenuation. Can you render the cube with IOR=1.0 please?" CreationDate="2015-09-01T16:19:11.247" UserId="523" />
  <row Id="643" PostId="297" Score="0" Text="added imallett, also linked to the shadertoy" CreationDate="2015-09-01T16:26:15.773" UserId="56" />
  <row Id="644" PostId="393" Score="0" Text="I think you should ask the question that you want to know. Like i sad its very clear what the words mean but the implementation of those things is a different story." CreationDate="2015-09-01T16:52:55.770" UserId="38" />
  <row Id="648" PostId="386" Score="0" Text="I updated the second item as the edit wasn't what I meant. Otherwise, it looks great!" CreationDate="2015-09-01T23:15:18.120" UserId="511" />
  <row Id="649" PostId="297" Score="1" Text="@AlanWolfe Your IOR=1 render looks exactly as I would expect it, and I skimmed the shadertoy impl and it looks good." CreationDate="2015-09-02T01:26:01.410" UserId="523" />
  <row Id="650" PostId="297" Score="0" Text="If you make that into an answer I'll accept it!" CreationDate="2015-09-02T01:36:13.340" UserId="56" />
  <row Id="651" PostId="382" Score="0" Text="On modern hardware, I'm unaware of hardware-based Phong modes. Also, incidentally, for programmable shaders, which by now are ubiquitous, people almost always use microfacet-based BRDFs." CreationDate="2015-09-02T02:14:37.727" UserId="523" />
  <row Id="652" PostId="359" Score="0" Text="At SIGGRAPH this year, there was a demoscener who showed an old demo. They did texture mapping with two instructions per pixel, by using self-rewriting code. Not exactly a discovery, but pretty neat." CreationDate="2015-09-02T02:21:52.700" UserId="523" />
  <row Id="653" PostId="382" Score="0" Text="My point is that the speed increase of Gourard over Phong isn't enough to justify the loss of quality -- modern computers can (or at least should) be able to do both in realtime." CreationDate="2015-09-02T02:32:31.847" UserId="158" />
  <row Id="654" PostId="382" Score="0" Text="The way you phrased it, it sounded like there is fixed-function Phong functionality, while I don't believe there is. Separately, you say you should use Phong &quot;if quality is important&quot;, but Phong is actually a poor quality BRDF model. By some measures, worse even than Blinn-Phong, which is what hardware Gouraud shading uses to shade vertices." CreationDate="2015-09-02T02:37:40.240" UserId="523" />
  <row Id="655" PostId="392" Score="2" Text="@joojaa I made pictures! [postscript sources](https://groups.google.com/d/topic/comp.lang.postscript/m2QqV4QFFaM/discussion)" CreationDate="2015-09-02T02:45:00.567" UserId="482" />
  <row Id="656" PostId="403" Score="1" Text="I aim to leave regional spelling differences as the author intended, and only standardise in tags (where it matters). However, for my own posts I am interested in the accepted local spelling, so I'll be using matte from now on - thank you :)" CreationDate="2015-09-02T06:17:13.163" UserId="231" />
  <row Id="657" PostId="405" Score="2" Text="I personally see this question more on PostScript than it is on computer graphics." CreationDate="2015-09-02T07:45:08.153" UserId="100" />
  <row Id="658" PostId="387" Score="2" Text="I answered a similar question http://gamedev.stackexchange.com/questions/23/what-is-ambient-occlusion/66638#66638" CreationDate="2015-09-02T09:10:06.683" UserId="8" />
  <row Id="660" PostId="390" Score="0" Text="Thank you guys, now I understand." CreationDate="2015-09-02T09:15:49.943" UserId="480" />
  <row Id="661" PostId="406" Score="0" Text="I dont really see the point of generating the encapsulation with ps2eps cant you just type the eps header in your template. It makes the instruction cleaner (and user needs less dependenies). Instead of converting each file separately do `mogrify -format png *.eps`" CreationDate="2015-09-02T09:43:01.093" UserId="38" />
  <row Id="663" PostId="386" Score="0" Text="Hi Christophe, did you mean &quot;equilateral&quot; triangles rather than &quot;isosceles&quot;?&#xA;&#xA;Rather than &quot;Hilbert&quot; I'd have said &quot;Morton&quot; order as the addressing is *much* easier in hardware." CreationDate="2015-09-02T11:04:42.923" UserId="209" />
  <row Id="664" PostId="407" Score="0" Text="When you say &quot;more colorful&quot; do you mean specifically as if looking through tinted glass?" CreationDate="2015-09-02T11:48:29.010" UserId="231" />
  <row Id="665" PostId="407" Score="0" Text="Yes, that's what i mean" CreationDate="2015-09-02T11:50:53.190" UserId="205" />
  <row Id="666" PostId="407" Score="0" Text="Do you also want to take the tint colour as a separate input, or do you just want an arbitrary tint colour that happens to be cheap to implement?" CreationDate="2015-09-02T11:51:10.013" UserId="231" />
  <row Id="667" PostId="407" Score="0" Text="Sorry, I think I don't understand your question. Could you explain that more?" CreationDate="2015-09-02T11:53:58.373" UserId="205" />
  <row Id="668" PostId="408" Score="0" Text="What do you mean by &quot;composite&quot;? Is there some mathematical operation  that lets me to composite two colours?" CreationDate="2015-09-02T12:00:33.587" UserId="205" />
  <row Id="669" PostId="408" Score="0" Text="That's what the example equations in his post do." CreationDate="2015-09-02T12:51:13.703" UserId="327" />
  <row Id="670" PostId="386" Score="0" Text="@Christophe thanks! This is really helpfull. So, for the border pixels, doesn't texture cache matter? That's was kind of what I was wondering. So, If I have a triangle that intersects tiles (x, y) and (x+1, y) and GPU just rasterized tile (x, y). Assuming tile (x+1, y) will be next, even if a different Execution Units processes it,  won't I benefit from texture cache when sampling texels for this triangle?" CreationDate="2015-09-02T12:57:25.303" UserId="250" />
  <row Id="671" PostId="386" Score="0" Text="Also, I got curious about the Hilbert pattern. I always assumed this was true for block compressed textures. Is this true for all textures? &#xA;&#xA;PS: I also didn't follow the last paragraph." CreationDate="2015-09-02T13:00:59.193" UserId="250" />
  <row Id="672" PostId="356" Score="0" Text="@Mokosha sorry, this somehow got unnoticed to me. I just saw it now. This is more a theoretical than pratical question and I don't even know if this makes sense now. Anyway, what I meant was, say a triangle intersect tiles (x, y) and (x+1, y) and that the these two tiles get processed one after another. Would that be a better for texture cache than If I had a triangle intersecting (x, y) and (x, y+1)? (Because of the border pixels and the layout of triangles not being in the same direction the tiles processing is)" CreationDate="2015-09-02T13:06:14.887" UserId="250" />
  <row Id="673" PostId="408" Score="2" Text="I don't think this is what he wants. If you wear red-tinted glasses, you see red wavelengths at basically full strength, and very little from other wavelengths. That's not a great match for alpha compositing." CreationDate="2015-09-02T14:58:30.927" UserId="196" />
  <row Id="674" PostId="407" Score="0" Text="If [Kostas Anagnostou's answer](http://computergraphics.stackexchange.com/a/410/231) is what you want then ignore my comments. I was just asking whether you wanted that or a hardcoded colour." CreationDate="2015-09-02T15:32:20.770" UserId="231" />
  <row Id="675" PostId="411" Score="0" Text="Pretty much any 3d animation software should be able to do this, so in alphabetic order 3ds Max, Blender, Maya..." CreationDate="2015-09-02T15:33:33.073" UserId="38" />
  <row Id="676" PostId="405" Score="1" Text="@cifz right this would fit [GD.SE](http://graphicdesign.stackexchange.com/) better. But thats not a reason why it can not be here." CreationDate="2015-09-02T19:12:16.817" UserId="38" />
  <row Id="677" PostId="410" Score="0" Text="Also you might want to tint something other than 100% pure one channel color." CreationDate="2015-09-02T19:14:16.117" UserId="38" />
  <row Id="679" PostId="411" Score="0" Text="@trichoplax, the models are things like an upper arm, or a hand." CreationDate="2015-09-02T20:35:11.567" UserId="540" />
  <row Id="682" PostId="406" Score="0" Text="Cool. I didn't know about `mogrify`. For the `ps2eps` part, it automatically calculates (and adds) the bounding-box info. Omitting that part, `convert` will produce full-page-sized images" CreationDate="2015-09-02T20:48:28.243" UserId="482" />
  <row Id="683" PostId="405" Score="0" Text="@citz I think I see what you mean. What if I take it out of the question, and open it up for tikz and asymptote and whatnot." CreationDate="2015-09-02T21:20:08.617" UserId="482" />
  <row Id="684" PostId="412" Score="5" Text="[Vote for MathJax formatting on this site.](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support)" CreationDate="2015-09-02T22:28:52.713" UserId="482" />
  <row Id="685" PostId="412" Score="2" Text="I've added this question to that meta post as an example of further need for MathJax" CreationDate="2015-09-02T23:14:53.637" UserId="231" />
  <row Id="686" PostId="414" Score="1" Text="`Here is what a cube looks like (...) using my own path tracer.`  &#xA;Do you happen to have open-sourced it by any chance ?" CreationDate="2015-09-03T02:53:30.507" UserId="110" />
  <row Id="687" PostId="414" Score="2" Text="No, not yet. I was planning on finishing and releasing this particular variant along with a blog post about glass rendering, but it's been on my backlog for a while." CreationDate="2015-09-03T04:27:27.400" UserId="207" />
  <row Id="689" PostId="410" Score="3" Text="I went ahead and added some example images to the post - hope you don't mind!" CreationDate="2015-09-03T05:29:52.243" UserId="48" />
  <row Id="690" PostId="410" Score="0" Text="Not at all Nathan, thanks!" CreationDate="2015-09-03T07:33:27.863" UserId="270" />
  <row Id="691" PostId="407" Score="0" Text="For more realistic results, you might find this answer interesting: http://photo.stackexchange.com/a/49267/23075" CreationDate="2015-09-03T09:06:49.237" UserId="110" />
  <row Id="692" PostId="349" Score="0" Text="I depends on the amount of calculations.GPU is really fast about math operations.So it is a matter of try and benchmarking." CreationDate="2015-09-03T09:30:42.943" UserId="213" />
  <row Id="693" PostId="405" Score="0" Text="I am answering here to show that this is a bit opinionated as a question, and answers definitely will be. Maybe this entire thread should live in our Meta, like this does on [GD.SE](http://meta.graphicdesign.stackexchange.com/questions/790/how-to-embed-screen-capture-videos-as-animated-gifs-in-answers). Any thoughts, @cifz" CreationDate="2015-09-03T10:15:48.103" UserId="38" />
  <row Id="697" PostId="410" Score="0" Text="That is exactly what I meant. Thanks for effort and examples :)" CreationDate="2015-09-03T16:17:26.723" UserId="205" />
  <row Id="699" PostId="326" Score="1" Text="It is not blue you are thinking of; it is cyan. There are two options: Red/Cyan, and Magenta/Green. In both cases, all three human cone types (color channels) are covered, not just two. I think Magenta/Green is universally recognized as being superior but I could be misremembering; I prefer it anyway." CreationDate="2015-09-03T16:26:25.673" UserId="504" />
  <row Id="700" PostId="369" Score="0" Text="Sorry, but I'm a bit lost as to what you are aiming to do. Are you trying to write an &quot;arbitrary polygon&quot; filling routine that also, say, clips to the sides of the viewing rectangle?" CreationDate="2015-09-03T16:32:18.670" UserId="209" />
  <row Id="703" PostId="424" Score="0" Text="Will your subpixel-rendered images still look okay if viewed on screens in a portrait orientation?" CreationDate="2015-09-03T18:28:39.267" UserId="551" />
  <row Id="704" PostId="428" Score="2" Text="Playing devil's advocate, if possible to do color subpixel rendering (i'm a skeptic!), it seems like it could be useful in VR situations, where they struggle to have sufficient resolution.  Perhaps on (non retina?) phones or tables as well?" CreationDate="2015-09-03T18:30:18.223" UserId="56" />
  <row Id="705" PostId="428" Score="0" Text="@AlanWolfe yes but its also 3 times as expensive to render" CreationDate="2015-09-03T18:36:03.860" UserId="38" />
  <row Id="706" PostId="428" Score="0" Text="yeah for sure, that's true. There are some situations where that isn't a problem though.  For instance, I know of a couple algorithms where you don't need to shoot rays for every pixel every frame.  It seems like this could also have play in rasterized graphics btw.  Again, i am not convinced color subpixel rendering is a real thing, but ya know, IF! :P" CreationDate="2015-09-03T18:39:14.930" UserId="56" />
  <row Id="707" PostId="427" Score="0" Text="With that explanation it totally makes sense, thank you! In case you remember, how was your experience with cone tracing in comparison to normal ray tracing? Of course it's an approximation, but does it achieve a considerable speedup at acceptable quality?" CreationDate="2015-09-03T19:07:57.057" UserId="385" />
  <row Id="708" PostId="428" Score="0" Text="@AlanWolfe I don't understand your doubts. All images are already rendered with sub pixels, but the colours are misaligned by between a third and a half of a pixel. I can't see how correcting that misalignment would fail to produce a higher quality image. Do you have any specific concerns (which might make a good question...)?" CreationDate="2015-09-03T19:10:56.147" UserId="231" />
  <row Id="709" PostId="428" Score="0" Text="@trichoplax Quality would riase at the cost of portability and simplicity. Not a good trade if the quality increase is minuscule. Its more important to know the color profile of the device than this." CreationDate="2015-09-03T20:02:04.447" UserId="38" />
  <row Id="711" PostId="215" Score="0" Text="Try randomly picking a light source *per sample* instead of per pixel." CreationDate="2015-09-03T21:01:10.693" UserId="553" />
  <row Id="714" PostId="428" Score="0" Text="@joojaa I agree that the effort is unlikely to be worth it. I'm just asserting that it is possible. My final section is meant as advice to not work on this unless there is a purely theoretical interest." CreationDate="2015-09-03T21:57:48.737" UserId="231" />
  <row Id="715" PostId="405" Score="1" Text="[Meta question concerning this question.](http://meta.computergraphics.stackexchange.com/questions/147/simple-2d-illustrations-question-main-or-meta)" CreationDate="2015-09-03T22:09:35.070" UserId="482" />
  <row Id="716" PostId="428" Score="3" Text="@joojaa There's no reason it would be 3x as expensive to render. You wouldn't need to shoot 3x the number of rays; you'd just apply 3 different weights when accumulating the rays into the framebuffer. Effectively, you're using a different antialiasing kernel for each color channel." CreationDate="2015-09-03T23:02:28.283" UserId="48" />
  <row Id="717" PostId="429" Score="0" Text="Nathan you are the one that wrote that about depth precision? Wow cool... i read that thing and it helped a lot! small world :P" CreationDate="2015-09-03T23:03:24.317" UserId="56" />
  <row Id="718" PostId="405" Score="0" Text="I wish we could use latex figures like we can latex math markup :P" CreationDate="2015-09-03T23:04:05.170" UserId="56" />
  <row Id="720" PostId="1431" Score="0" Text="By displaying the images magnified, you entirely negate the benefit of using subpixels, so the comparison images aren't representative of the actual results. You should try posting the original-size images if possible." CreationDate="2015-09-04T05:48:17.307" UserId="327" />
  <row Id="721" PostId="1431" Score="1" Text="@yuriks There's another smaller resolution that I can find (http://journals.cambridge.org/fulltext_content/SIP/SIP1/S2048770312000030_fig11p.jpeg) but it's not immediately apparent to me that that version is not minified or magnified. I do think it's educational to see the magnified version, since it lets you see the fringing and doesn't actually get rid of the sharpness relative to the regular downsampling." CreationDate="2015-09-04T05:57:12.960" UserId="196" />
  <row Id="722" PostId="1431" Score="1" Text="@yuriks, problem with these kinds of images are that not all monitors have same subpixel sequence. And on mobile devices the orientation needs to change when the user turns the device." CreationDate="2015-09-04T06:21:43.853" UserId="38" />
  <row Id="723" PostId="405" Score="0" Text="@AlanWolfe that sounds like a Meta Question tagged `[feature-request]` to me, if you want to post one..." CreationDate="2015-09-04T08:49:10.267" UserId="231" />
  <row Id="724" PostId="1436" Score="4" Text="RIP maps also probably aren't used because they don't help on the, rather common, diagonal case.&#xA;&#xA;FWIW, if you can find the code for the Microsoft Refrast, the anistropic filter implementation in that is probably a good reference for how today's HW does it." CreationDate="2015-09-04T09:43:19.890" UserId="209" />
  <row Id="725" PostId="427" Score="0" Text="Oh, gosh, it was a long time ago.  Actually, I only implemented the cone tracing. Whether I actually tried turning off the radius part I simply can't recall but, if I get time, I'll try to remember the pros and cons of going down the cone-tracing route." CreationDate="2015-09-04T09:55:20.113" UserId="209" />
  <row Id="726" PostId="1436" Score="1" Text="&quot;This can be verified by noting that texture usage requirements don't increase when using AF, rather, only bandwidth does.&quot; Killer argument. Good answer!" CreationDate="2015-09-04T10:22:07.030" UserId="385" />
  <row Id="727" PostId="432" Score="1" Text="Illumination does not actually fall of the surface area towards the light is just smaller" CreationDate="2015-09-04T17:11:50.277" UserId="38" />
  <row Id="728" PostId="430" Score="0" Text="Tervetuloa! Yes your point is valid, id say that the system or hardware has to do this as only the system can in future realisticallt be aware of the orientation of screen and the organisation of colors on screen. Preferably the screen itself would do this." CreationDate="2015-09-04T17:17:42.007" UserId="38" />
  <row Id="729" PostId="1434" Score="2" Text="I generally agree with your argumentation. But I think what the author means is, that the approach needs to perform its calculations for all existing patches, not only the visible ones. One could argue that path-tracing in contrast computes radiance only for visible patches/samples. While rays may still go everywhere, there might be parts of the scene that never receive any view-rays/paths; therefore there are no computations at all. Comparing with local GI the &quot;problem of viewpoint independence&quot; its even more apparent. Though, I still agree with you that this should be rephrased." CreationDate="2015-09-04T17:25:00.083" UserId="528" />
  <row Id="732" PostId="420" Score="2" Text="While this link may answer the question, it is better to include the essential parts of the answer here and provide the link for reference.  Link-only answers can become invalid if the linked page changes." CreationDate="2015-09-04T19:07:45.697" UserId="56" />
  <row Id="733" PostId="1432" Score="2" Text="The spec for [`GL_EXT_texture_filter_anisotropic`](https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt) is very detailed. Maybe it might help you better understand the process." CreationDate="2015-09-04T20:39:03.017" UserId="54" />
  <row Id="734" PostId="1434" Score="1" Text="I agree with Wumpf, the view-independent technique does not take visibility into account which results in extra calculations because it needs to compute lighting for *entire scene* no matter where the camera is looking. Furthermore, you cannot reduce resolution of your computation in areas that are far. I think @Wumpf should paraphrase his comment as an answer." CreationDate="2015-09-04T23:02:46.410" UserId="14" />
  <row Id="735" PostId="1438" Score="1" Text="SIGGRAPH 2014 advances in real time rendering has a really interesting talk on call of duty's subdivision surfaces.  You should check it out.  Instead of having a high poly mesh that is made lower poly, they defined shapes analytically and added more triangles as needed" CreationDate="2015-09-05T03:09:18.943" UserId="56" />
  <row Id="736" PostId="1438" Score="0" Text="We can talk about the state of the art in LOD algorithms and data structures here, but if the question is about how modern games do it specifically, you might have more luck asking in gamedev.se: http://gamedev.stackexchange.com" CreationDate="2015-09-05T06:02:37.420" UserId="159" />
  <row Id="737" PostId="1434" Score="0" Text="@ap_ Done. Feel free to edit :)" CreationDate="2015-09-05T12:13:20.047" UserId="528" />
  <row Id="740" PostId="283" Score="0" Text="@JohnCalsbeek I think that paper contains the makings of a great answer if you or anyone else wants to post it." CreationDate="2015-09-05T15:24:12.010" UserId="231" />
  <row Id="741" PostId="432" Score="0" Text="You're right I think, &quot;fall-off&quot; to me is anything that makes the surface area smaller from the lights perspective, so distance from and rotation away from the light have the same effect to me, but my definition of &quot;fall-off&quot; is probably not mathematically correct :P" CreationDate="2015-09-05T15:27:52.980" UserId="554" />
  <row Id="742" PostId="1438" Score="3" Text="I doubt that. Game dev is very graphics light. It's mostly unity and java questions with some path finding and fixed frame rate questions thrown in :p" CreationDate="2015-09-05T16:52:26.470" UserId="56" />
  <row Id="743" PostId="432" Score="0" Text="well yes but that would be hard for a layman to understand. lots of things can fall off." CreationDate="2015-09-05T17:38:40.670" UserId="38" />
  <row Id="744" PostId="283" Score="0" Text="@trichoplax One of the reasons that I haven't tried to write it up yet is that I don't see the anisotropy that you are talking about in any of the frequency domain images in that paper, so I feel like I'm missing something." CreationDate="2015-09-05T17:52:22.860" UserId="196" />
  <row Id="745" PostId="283" Score="0" Text="@JohnCalsbeek The first image shatters my intuition that Perlin noise would be heavily anisotropic, and shows that the problems are due to poor implementation and not really related to being grid based. I still don't have an understanding of why my initial intuition was so wrong." CreationDate="2015-09-05T18:27:23.203" UserId="231" />
  <row Id="746" PostId="283" Score="0" Text="@trichoplax I can see directional artifacts in image 1b in that paper, even though the Fourier transform doesn't seem to show it. Unless that's what the extremely thin lines at right angles are?" CreationDate="2015-09-05T21:45:58.577" UserId="196" />
  <row Id="747" PostId="1438" Score="1" Text="@Alan, Activision used a lot of state of the arts algorithm to create a real 3D strategic game not sn isometric Sprite base one which I am ok with that, but they did a great job in COD however it is still a bit sluggish and lazy even at the early levels with small number of assets (I am talking about their mobile game on an Iphone 5s). I think you need to learn OpenGLES expert features and underlying layers to succeed writing such a game." CreationDate="2015-09-06T06:58:27.263" UserId="537" />
  <row Id="755" PostId="1446" Score="0" Text="Down voter, please consider you vote again, if it is still a -1, Consider leaving a comment for me.Thanks" CreationDate="2015-09-06T13:00:25.020" UserId="537" />
  <row Id="756" PostId="1438" Score="0" Text="I'm talking about their console version of call of duty in case that clears it up." CreationDate="2015-09-06T13:53:12.393" UserId="56" />
  <row Id="758" PostId="405" Score="0" Text="[Another Meta question about this question.](http://meta.computergraphics.stackexchange.com/questions/150/2d-illustrations-question-help-crafting-the-question)" CreationDate="2015-09-06T15:58:21.757" UserId="482" />
  <row Id="759" PostId="1446" Score="0" Text="I didnt downvote but part of me wonders what is the sate of the art here? So i cant up vote either." CreationDate="2015-09-06T15:59:03.380" UserId="38" />
  <row Id="761" PostId="1446" Score="1" Text="I've downvoted because it was hard to read and IMHO it's not relevant to LODs in games. After reconsidering I decided to cancel my donwvote by upwoting and simply add my own answer." CreationDate="2015-09-06T18:02:06.800" UserId="93" />
  <row Id="762" PostId="1446" Score="0" Text="State of the art suppose to mean a very outstanding job but different methods result differently on various cases, for example Call of duty has Layer management and mipmaping, however Dear haunting(DH 2014) uses background with parallax and a mipmaping which has pre rendered generalized textures. Subway surfer is completely a diffrent story, and I say state of the art to all of them, even though Subway Surfer discretely draws buildings and other urban objects or Call of duty is a bit sluggish while zooming or panning. I think they are all best in their case." CreationDate="2015-09-06T18:04:05.847" UserId="537" />
  <row Id="763" PostId="1451" Score="0" Text="Thanks a lot to read" CreationDate="2015-09-06T18:10:10.640" UserId="537" />
  <row Id="764" PostId="1451" Score="1" Text="game engines usually have a set of pre defined methods and you can not get in to the core to actually changing the LOD algorithm. am I right? I was talking about the case you are writing a game yourself with OpenGL or SpriteKit framework, I dont know if one is able to customize LOD's algorithm in Unity or Unreal, is it possible?" CreationDate="2015-09-06T18:19:19.567" UserId="537" />
  <row Id="765" PostId="1450" Score="1" Text="I'm fairly certain that these values are hardware/version specific, but there are minimum values that an implementation must support. You can query them with [`glGet`](https://www.opengl.org/sdk/docs/man/html/glGet.xhtml)." CreationDate="2015-09-06T19:12:20.497" UserId="54" />
  <row Id="766" PostId="1452" Score="0" Text="Also as glampert hinted, you want to find the minimum that must be supported, because that is the amount you can actually rely on, on all hardware." CreationDate="2015-09-06T20:43:18.473" UserId="56" />
  <row Id="767" PostId="1452" Score="0" Text="Well, OpenGL3 enforces 48 as I have mentioned. There might be a higher minimum in practice of course. Or did I misunderstand you?" CreationDate="2015-09-06T20:45:16.870" UserId="528" />
  <row Id="768" PostId="1448" Score="0" Text="thank you so much" CreationDate="2015-09-06T20:46:30.257" UserId="1571" />
  <row Id="769" PostId="1450" Score="2" Text="Note that the textures you've described are 64 MB each, so you may get limited by available VRAM before you hit API limits on the number of textures. 8 textures = 512 MB, so should be fairly safe, but many older cards or mobile cards only have 1 or 2 GB of VRAM, so you won't be able to go much more than 8 of these textures and still have VRAM left over for anything else." CreationDate="2015-09-07T00:01:07.253" UserId="48" />
  <row Id="771" PostId="1453" Score="0" Text="thanks, they even have binary gltf (`.bgltf`) versions available! +1" CreationDate="2015-09-07T08:48:35.527" UserId="361" />
  <row Id="772" PostId="1450" Score="1" Text="What do you mean by 256*256*256? what is the third number?" CreationDate="2015-09-07T09:44:37.250" UserId="537" />
  <row Id="773" PostId="1440" Score="1" Text="Ok, seems the main problem was that I read &quot;viewpoint independent&quot; as &quot;BRDF is isotropic&quot;, not &quot;lighting is calculated, whether you look at the surface or not&quot;. I'll wait another day for another answer and then probably accept this, thanks :)" CreationDate="2015-09-07T10:20:12.323" UserId="385" />
  <row Id="774" PostId="1455" Score="0" Text="I feel this is a very broad question. What exactly do you want to learn that you can't when using one of said low-level graphic APIs?" CreationDate="2015-09-07T11:56:37.447" UserId="385" />
  <row Id="775" PostId="1455" Score="0" Text="@DavidKuri, drawing a 2D geometric primitive (circle or line) in 3D is easy (just need to convert/translate 3D coordinates to 2D coordinates or vice versa). suppose I want to draw a sphere without OpenGL. Where to start from? Just give me a guideline to study." CreationDate="2015-09-07T11:59:45.210" UserId="464" />
  <row Id="776" PostId="1455" Score="0" Text="You mean software rendering?" CreationDate="2015-09-07T12:02:32.213" UserId="137" />
  <row Id="777" PostId="1455" Score="0" Text="@ratchetfreak, yes. I need to draw a sphere/ellipsoid/or whatever it is on the screen. Rasterization in 3D." CreationDate="2015-09-07T12:03:56.820" UserId="464" />
  <row Id="778" PostId="1455" Score="0" Text="@ratchetfreak, I need to demonstrate 3D algorithms like Z-buffer algorithm. So, first, I need to know how to draw a 3D object in 3D." CreationDate="2015-09-07T12:05:46.417" UserId="464" />
  <row Id="780" PostId="1455" Score="0" Text="@trichoplax, I need to learn both." CreationDate="2015-09-07T12:12:21.513" UserId="464" />
  <row Id="782" PostId="1455" Score="0" Text="@trichoplax, line drawing. I will learn shadows and shading later." CreationDate="2015-09-07T12:16:41.050" UserId="464" />
  <row Id="784" PostId="1454" Score="1" Text="It does extend to 3D. For example,  you could consider that, when texturing (a portion of) a triangle, you are, in effect, evaluating a surface cut through the 3D texture.  That shape doesn't have to be isotropic.&#xA;&#xA;Alternatively, just like a 2D anisotropic filtering may approximate with an elliptical footprint, the 3D version could use an ellipsoid." CreationDate="2015-09-07T15:09:05.573" UserId="209" />
  <row Id="785" PostId="1454" Score="0" Text="@SimonF Hmmm, you're right! That is actually I think perhaps a better generalization than the one I gave, and it seems better behaved (as in, it's more obvious what to do)." CreationDate="2015-09-07T16:51:22.520" UserId="523" />
  <row Id="786" PostId="386" Score="0" Text="PVRTC encodes texture blocks in a [morton order](https://en.wikipedia.org/wiki/PVRTC#Data_structure)" CreationDate="2015-09-07T19:09:25.310" UserId="135" />
  <row Id="787" PostId="420" Score="1" Text="This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post." CreationDate="2015-09-07T21:29:21.753" UserId="48" />
  <row Id="789" PostId="369" Score="0" Text="@SimonF Sorry I didn't notice your comment till now. I'm trying to implement a fully-generalized clipping routine for complex self-intersecting shapes AND complex self-intersecting clipping-regions and parameterized with a winding-number rule. But if I could visualize the data structure in concrete terms, that usually works to get me moving." CreationDate="2015-09-07T23:21:32.447" UserId="482" />
  <row Id="790" PostId="412" Score="0" Text="@trichoplax, you used mathurl for pasting images? I'm reading from Android app and i can't check." CreationDate="2015-09-08T01:56:03.440" UserId="316" />
  <row Id="792" PostId="1458" Score="1" Text="I dont really understand why this shopping list question is within scope while asking what graphics api questions are available fo ubuntu is not." CreationDate="2015-09-08T04:18:42.570" UserId="38" />
  <row Id="794" PostId="420" Score="0" Text="I tend to think it does.He can learn from the code inside how the trackball works." CreationDate="2015-09-08T07:59:31.830" UserId="213" />
  <row Id="796" PostId="369" Score="0" Text="*If* you have solved the fill problem for an &quot;unclipped&quot; arbitrary polygon, then you 'only' need to do CSG with an intersection (i.e. AND) operator to then clip it against another arb. poly.  Does that make sense?  In screen/scanline space the CSG is relatively easy. OTOH if you need a vector model, it seems to me that  a scanline model could be extended so that you construct sets of trapezia  (http://mathworld.wolfram.com/Trapezium.html - British  definition (i.e. correct def)) for each that describe the interiors. It should be relatively easy to intersect those to generate the final model." CreationDate="2015-09-08T09:26:11.867" UserId="209" />
  <row Id="798" PostId="386" Score="0" Text="@yuumei No. Although early MBX PVRTC files *were* in morton(ish) order, for later systems (e.g. SGX and Rogue), the data is in raster(ish) order and the driver/GPU loads and rearranges them into whatever is the preferred order for that particular GPU." CreationDate="2015-09-08T12:23:01.563" UserId="209" />
  <row Id="800" PostId="1459" Score="0" Text="`3D computer graphics and geometric modeling` includes a ton of things, ranging from lighting algorithms over mesh processing algorithms to the 'creative' task of creating a 3-dimensional virtual object, complete with textures and possibly animations. Except for being in the same space, these things don't have too much in common. Could you go into detail on what exactly you want to learn?" CreationDate="2015-09-08T12:36:58.927" UserId="385" />
  <row Id="802" PostId="1459" Score="0" Text="@DavidKuri,  http://computergraphics.stackexchange.com/questions/1455/how-to-get-started-with-drawing-3d-primitives-without-using-opengl-or-directx" CreationDate="2015-09-08T13:49:42.797" UserId="464" />
  <row Id="803" PostId="1459" Score="0" Text="The way I read your other questions, you want to learn everything from the ground up. So after software rasterization (in which you are essentially replicating the job of the dedicated GPU), plain renderings APIs would be next." CreationDate="2015-09-08T13:54:27.313" UserId="385" />
  <row Id="804" PostId="1459" Score="0" Text="@DavidKuri, I didn't get you." CreationDate="2015-09-08T13:56:01.723" UserId="464" />
  <row Id="805" PostId="1459" Score="0" Text="You seem to be interested in learning, more so than creating an application. You will probably not learn how, for example, shadow techniques work if you start using a game engine, because they are already there and ready to use. Start with a low-level rendering API and implement a shadowing technique yourself to learn it." CreationDate="2015-09-08T13:58:50.823" UserId="385" />
  <row Id="806" PostId="412" Score="0" Text="@psicomante yes I used mathurl.com and included the links inline - are they readable on Android? It's the next best thing until we get MathJax activated." CreationDate="2015-09-08T14:05:50.627" UserId="231" />
  <row Id="807" PostId="412" Score="0" Text="@psicomante press edit under the question if you want to see the markup (you need to add `.png` to the end of the mathurl.com link to make it show as an image here). Any questions just @mention me in [chat]." CreationDate="2015-09-08T14:07:56.430" UserId="231" />
  <row Id="808" PostId="1460" Score="4" Text="Nitpicking: Blender has also an [integrated Game Engine](https://www.blender.org/manual/game_engine/index.html)." CreationDate="2015-09-08T14:09:02.743" UserId="528" />
  <row Id="810" PostId="412" Score="0" Text="@trichoplax yep, they are almost perfectly readable on Android SE app. It's a good tool until MathJax activated" CreationDate="2015-09-08T15:38:13.600" UserId="316" />
  <row Id="811" PostId="413" Score="0" Text="I agree---my answer is likely not valid any more because I haven't used diagrams in any answers so far. But I expect that to happen soon. So if it's OK for this answer to hang around for a few days, I am sure I will be able to find an answer." CreationDate="2015-09-08T19:04:01.790" UserId="14" />
  <row Id="813" PostId="1455" Score="0" Text="Since this is more than one question, it may help to ask them separately rather than trying to fit them all into one post, which is making this too broad" CreationDate="2015-09-08T20:23:05.353" UserId="231" />
  <row Id="814" PostId="1450" Score="0" Text="@Iman it's a 3d texture rather than a 2d texture - if you want more info you could ask a separate question if you like." CreationDate="2015-09-08T20:44:07.357" UserId="231" />
  <row Id="815" PostId="1447" Score="0" Text="Note that the question is specifically asking about the notation Im() and Re()." CreationDate="2015-09-08T20:46:33.267" UserId="231" />
  <row Id="817" PostId="1444" Score="1" Text="Is there disagreement between this answer and John Calsbeek's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:54:34.700" UserId="231" />
  <row Id="818" PostId="1442" Score="0" Text="Is there disagreement between this answer and Nathan Reed's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:55:10.693" UserId="231" />
  <row Id="819" PostId="1442" Score="1" Text="@trichoplax I think Nathan's assertion that &quot;generating good quality mips for a non-power-of-two texture is a little trickier&quot; makes our answers disagree at least slightly. That alone probably merits more elaboration." CreationDate="2015-09-08T21:09:24.700" UserId="196" />
  <row Id="822" PostId="315" Score="0" Text="Are you using any vendor-dependent GLSL functions like noise*(which as far as I know, most vendors don't implement anyway)?" CreationDate="2015-09-08T22:42:38.267" UserId="1578" />
  <row Id="824" PostId="1461" Score="0" Text="See http://stackoverflow.com/questions/21593786/in-gouraud-shading-what-is-the-t-junction-issure-and-how-to-demonstrate-it-with." CreationDate="2015-09-09T01:35:03.463" UserId="192" />
  <row Id="825" PostId="1462" Score="0" Text="See also http://stackoverflow.com/questions/23530807/glsl-tessellated-environment-gaps-between-patches." CreationDate="2015-09-09T01:48:28.167" UserId="192" />
  <row Id="827" PostId="315" Score="0" Text="@Sam no. I am only using texture(). That shouldn't cause a problem, right?" CreationDate="2015-09-09T10:00:19.927" UserId="437" />
  <row Id="828" PostId="1450" Score="0" Text="I never heard such a thing, I thought textures are 2D (u,v) raster map that draped on 3d objects, however I take a look at tutorials and I become familiar with the concept. Thanks for your tip Terry" CreationDate="2015-09-09T13:19:38.813" UserId="537" />
  <row Id="832" PostId="1467" Score="1" Text="Original question: http://math.stackexchange.com/questions/1428101/an-introduction-to-lane-riesenfeld-algorithms" CreationDate="2015-09-09T15:59:06.333" UserId="192" />
  <row Id="833" PostId="1442" Score="1" Text="I think the problem here is that we're confusing the logical position of texels with their &quot;physical&quot; layout in memory. 1) Pixels are discrete items, i.e. you always need an integer dimension, and so down scaling an odd dimension means that we have to either round up or round down. Since we have to round up once we get to an Nx1 or 1xN texture, it makes sense to always round up.&#xA;2) When laid out in physical addresses, it is not uncommon to pad the texture out to some &quot;convenient&quot;  &quot;stride&quot; size. This may be done for 2 reasons: a) It may make HW cheaper &amp; b) if a P.of.2, Morton order is easy." CreationDate="2015-09-09T16:23:49.693" UserId="209" />
  <row Id="834" PostId="1468" Score="3" Text="As hinted by @BenediktBitterli &quot;Physically Based Rendering&quot; isn't really a yes or a no. In rendering, we always have to balance realism with computational cost. Some renderers will have just a few 'Physically Based' features, for example, Microfacet BRDFs and HDR render targets. Whereas others may have many, for example, full BSDFs, full spectrum render target, light tracing, area lights, etc." CreationDate="2015-09-09T18:19:46.420" UserId="310" />
  <row Id="835" PostId="1436" Score="0" Text="The &quot;High-Performance Software Rasterization on GPUs&quot; link only mentions anisotropic filtering in passing once, and gives no mention of any details. So I'm going to edit it out of the answer because I don't think it's relevant in a helpful way." CreationDate="2015-09-09T18:30:27.923" UserId="327" />
  <row Id="836" PostId="1464" Score="2" Text="When you say &quot;vertex (X,Y) values are nearly always represented by fixed-point numbers&quot;, I'm guessing you mean the screen-space vertex coordinates in the rasterizer, right? Not the original model-space vertices." CreationDate="2015-09-09T20:17:47.533" UserId="48" />
  <row Id="837" PostId="386" Score="1" Text="Updated, thanks @Simon!" CreationDate="2015-09-09T21:14:52.317" UserId="511" />
  <row Id="838" PostId="1456" Score="0" Text="Scratchapixel is definitely the best place to learn about that stuff (2D &amp; 3D).  They explain rasterisation and also ray-tracing and everything there is to know around 3D techniques (texturing, how to store polygonal objects in memory, etc.). Really cool website and it's free content." CreationDate="2015-09-09T21:49:30.003" UserId="1608" />
  <row Id="839" PostId="315" Score="0" Text="@nilspin I don't think so. They're both running on the same OpenGL version, right?" CreationDate="2015-09-09T23:25:30.043" UserId="1578" />
  <row Id="840" PostId="315" Score="0" Text="@Sam yes they are." CreationDate="2015-09-10T04:36:55.543" UserId="437" />
  <row Id="841" PostId="1464" Score="2" Text="@NathanReed Yes. Just the screen-space X&amp;Y (and, perhaps on some GPUs the Z).  I'll edit it to make that clearer." CreationDate="2015-09-10T05:41:56.587" UserId="209" />
  <row Id="842" PostId="1460" Score="1" Text="@Wumpf there is also nothing that states maya can not be used for interactive game like elements. It can, this is how mayas motion capture works, its just not very conductive as a game engine given the software price. Anyway the terms are decidedly diffuse." CreationDate="2015-09-10T06:20:41.840" UserId="38" />
  <row Id="843" PostId="26" Score="1" Text="@trichoplax IANAPL but, as all the claims in the link provided by Benedikt , either explicitly mention either 3 dimensions (i,j,k  or x y z) or a hypercube, it seems you are correct." CreationDate="2015-09-10T08:40:51.893" UserId="209" />
  <row Id="844" PostId="26" Score="0" Text="@SimonF I wasn't as diligent as you - I was basing my opinion on  [this statement on Wikipedia](https://en.wikipedia.org/wiki/Simplex_noise#Legal_status)." CreationDate="2015-09-10T08:44:55.913" UserId="231" />
  <row Id="845" PostId="1460" Score="0" Text="I edited the answer to include your remarks. Thanks." CreationDate="2015-09-10T09:26:04.657" UserId="385" />
  <row Id="846" PostId="1470" Score="0" Text="I don't see yet why the dirac deltas are a problem. While it is impossible to compute that with a computer using sampling (hence the `if`s), the mathematics are clearly defined, right? Looking forward for somebody who can clarify that. Besides, in nature there are no real dirac deltas / infinity values since there are no perfect mirrors; but I guess that is another topic." CreationDate="2015-09-10T10:08:48.700" UserId="528" />
  <row Id="848" PostId="1470" Score="0" Text="As long as BSDF and radiance can be dirac deltas at the same time than the rendering equation(as it is) is not mathematically well defined. Even if only BSDF would be allowed to be Dirac delta and we would formally treat BSDF as distribution than radiance needs to be smooth function in order to be mathematically 100% correct. But radiance under no way can be smooth function e.g.  sharp shadows form discontinuities in radiance." CreationDate="2015-09-10T10:25:34.243" UserId="1613" />
  <row Id="849" PostId="1470" Score="0" Text="Yes in reality you cannot have perfect mirrors, point and directional light sources or pin hole cameras. But we write programs where these things are and we need a theory which underpins them." CreationDate="2015-09-10T10:31:31.473" UserId="1613" />
  <row Id="850" PostId="1471" Score="0" Text="&quot;I would like to point out that correlation is &quot;always&quot; bad. If you can afford to make brand new sample than do it.&quot; Could you elaborate? To me this sounds like any kind of heuristic for sample distribution is bad, which is probably not what you wanted to say." CreationDate="2015-09-10T11:37:55.023" UserId="385" />
  <row Id="851" PostId="1471" Score="0" Text="I edited the answer, I hope that cleared a thing or two." CreationDate="2015-09-10T12:28:23.113" UserId="1613" />
  <row Id="852" PostId="1447" Score="0" Text="Well, Trichoplax, I thing the whole concept is being asked while two parameters are considered unknown,however I thought that it is obvious to everyone that these are complex number and complex values are projected on both real and imaginary axis using Sin and Cos, I have tried to explain what they are for! why down votes? look like the down voter didn't get the theorem fully. By the way, Terry You are everywhere! I love You" CreationDate="2015-09-10T13:43:40.147" UserId="537" />
  <row Id="853" PostId="1472" Score="6" Text="I think you need anisotropic texture sampling" CreationDate="2015-09-10T14:01:59.483" UserId="56" />
  <row Id="854" PostId="1472" Score="1" Text="Please attach your shader completely, it's variable definition I mean, may be you need to define a High precision or mid precision variable instead of lowp" CreationDate="2015-09-10T14:29:46.250" UserId="537" />
  <row Id="855" PostId="309" Score="0" Text="Would you like to specify which texture compression format you prefer? I am guessing but your answer will likely involve a compute-mode texture compression routine." CreationDate="2015-09-10T17:13:32.990" UserId="14" />
  <row Id="856" PostId="1472" Score="3" Text="It migh be an issue with the texture filtering you're using. Which filter is it? Point, bilininear, trilinear? Also, make sure you did compute the correct mipmaps for the texture." CreationDate="2015-09-10T18:22:26.520" UserId="54" />
  <row Id="857" PostId="1470" Score="0" Text="@tom The rigorous mathematics that underlies delta distributions is [measure theory](https://en.wikipedia.org/wiki/Measure_%28mathematics%29). See the [definition of the Dirac delta as a measure](https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_measure). I don't know off the top of my head of a work specifically treating the rendering equation in the context of measure theory, but pretty sure all this stuff is well-founded at the level of mathematical physics." CreationDate="2015-09-10T18:28:11.070" UserId="48" />
  <row Id="859" PostId="1447" Score="0" Text="I'm not sure who Terry is. I understand that you already knew what Im() and Re() mean, but not everyone does. I interpreted the question as asking what the notation means, but it's not my question so I can't be sure of the intention. You could post a comment on the question to ask the question poster to clarify if you like." CreationDate="2015-09-10T23:04:06.557" UserId="231" />
  <row Id="861" PostId="1447" Score="0" Text="Yes you are right I had to do this first, next time I will, and terry is just an abbreviation for your account name which is really hard to write completely." CreationDate="2015-09-10T23:21:27.697" UserId="537" />
  <row Id="862" PostId="1447" Score="0" Text="Oh I see, that bit was to me as well - then thank you :)" CreationDate="2015-09-10T23:28:15.503" UserId="231" />
  <row Id="863" PostId="1447" Score="0" Text="If you use @ before someone's username, then they will get a notification. It also has an autocomplete so you don't need to type out the whole name, if that helps. You should be able to just type `@t` and it will show you `@trichoplax` as an option." CreationDate="2015-09-10T23:31:14.560" UserId="231" />
  <row Id="864" PostId="1473" Score="1" Text="I don't think that there's any meaningful way to make that kind of judgement call at the moment, aside from checking who makes the GPU. Ultimately there's more factors than just &quot;can the hardware execute commands from multiple queues simultaneously&quot;, and D3D12 abstracts away those details. In fact D3D12 doesn't even distinguish between hardware that might execute queues concurrently and those that might do it sequentially, the docs just say that their abstraction *allows* for concurrent execution." CreationDate="2015-09-10T23:46:30.187" UserId="207" />
  <row Id="865" PostId="1473" Score="1" Text="good question ! i also feel it would be special to gain perf to exec compute and shading concurrently. maybe gains can happens thanks to the same facts that makes hyperthreading somehow faster. interleaving operations when some units are busy for the other queue. like shaders clogging the texture units, which are not used by the compute stage, which itself clogs the FPU or DPU." CreationDate="2015-09-11T01:26:37.987" UserId="1614" />
  <row Id="866" PostId="1470" Score="0" Text="I don't understand why the BSDF can have a dirac. BSDF can represents 100% reflectivity using a value of 1, not infinite ? you don't need to make so that the integral is 1 over the hemisphere, you need to make so that it is less than one, strictly. or there is something i don't get. more than 1, for some angle, would mean light from other directions than the perfect reflection also contributes to this particular angle. which would not be a mirror, but some kind of lens !" CreationDate="2015-09-11T01:48:58.433" UserId="1614" />
  <row Id="867" PostId="424" Score="0" Text="as an engineer I would never agree to implement such a... broken idea. EXCEPT if I'm sure the display is totally fixed. like an appli for iPhone 5S. Using this technique generates broken images from screens using reversed patterns, or different arrangements." CreationDate="2015-09-11T02:00:10.627" UserId="1614" />
  <row Id="868" PostId="1436" Score="0" Text="@SimonF also we can add that the additional bandwidth requirement is pretty scary." CreationDate="2015-09-11T02:06:37.223" UserId="1614" />
  <row Id="869" PostId="1455" Score="0" Text="your second image will be much harder to generate than the first." CreationDate="2015-09-11T02:08:03.693" UserId="1614" />
  <row Id="871" PostId="1472" Score="0" Text="@AlanWolfe You were right! I will add some proper answer - hope you don' t mind :)  Lman I am using floats everywhere, but suggestion above solved the problem anyway. Glampert As I sad I think I don't compute mipmaps at all (I think because it might be done by default somewhere, but I don' t know about it :) I used &quot;LinearWrap&quot; sampler state if that's what you mean" CreationDate="2015-09-11T06:11:48.617" UserId="205" />
  <row Id="872" PostId="1470" Score="1" Text="(Disclaimer: I Am Not A Rendering Person.) At any surface point $x$, the role of the BSDF is to act as a linear operator mapping the incident light $L_i$ to the exitant light $L_o$. Now there is no problem with $L_i$ and $L_o$ both being distributions, i.e. linear functions $D(\mathbb S^2)\to\mathbb R$, because addition and scalar multiplication of distributions is well-defined so they form a vector space. When $L_i$ and $L_o$ are functions we can represent the BSDF $\rho$ as a distribution, but if they're not we can still speak of linear transformations." CreationDate="2015-09-11T06:24:04.620" UserId="106" />
  <row Id="875" PostId="1473" Score="0" Text="Hm too bad. Maybe then &quot;aside from checking who makes the GPU, no&quot;  counts already as answer if there is not more to it. After reading all those AMD marketing stuff I'm glad to hear that I'm not alone with my confusion." CreationDate="2015-09-11T07:52:22.560" UserId="528" />
  <row Id="876" PostId="336" Score="0" Text="_&quot;I never really understood FFT, but I saw it being used for JPEG&quot;_.  I'm not sure exactly what you meant by this, but FWIW, JPEG doesn't use FFT. Instead it uses a different transform, the Discrete Cosine Transform (DCT).  The DCT has some advantages over the FFT in that, although they both repeat ad infinitum, the DCT reflects at each repetition which thus implies better continuity." CreationDate="2015-09-11T08:33:51.733" UserId="209" />
  <row Id="877" PostId="336" Score="0" Text="@SimonF You see, I even thought DCT is a special case of FFT. Thanks for clarification!" CreationDate="2015-09-11T09:19:07.820" UserId="141" />
  <row Id="878" PostId="1455" Score="1" Text="Stack Exchange works best when you ask about very specific problems you might encounter in your day to day work/studies. It doesn't work as well for book-length studies. If an answer cannot comfortably fit in the space of a post, it is probably too soon for a Q&amp;A site like this. That is why we close these questions as *too broad.*" CreationDate="2015-09-11T12:10:52.413" UserId="53" />
  <row Id="880" PostId="1447" Score="0" Text="@trichoplax thanks. I am new to SE, i saw its autocomplete but I didn't know that it will notify the guy. but thanks , looks like only one person could be notified and the post owner will always be notified  :) still love you" CreationDate="2015-09-11T14:04:25.563" UserId="537" />
  <row Id="881" PostId="1470" Score="0" Text="@Rahul Good idea to think about BSDF as linear operator taking $L_i$ to $L_o$, but I still wonder if it is somehow advantageous to define radiance as distribution, because it seams to me that measure is sufficient, in which case you can too think about BSDF as linear mapping from one measure $L_i$ to another $L_o$ and in addition it can be represented it as integral of some measure valued function over measure $L_i$. And decomposition of that measure valued function into abs. continuous and singular part with the respect to the solid angle gives you diffusive and specular part of BSDF." CreationDate="2015-09-11T14:22:31.350" UserId="1613" />
  <row Id="882" PostId="1476" Score="1" Text="Glad to help! Anisotropic filtering is more expensive than bilinear. If that becomes a problem for you, you might try a  higher resolution texture, or distance field textures since it looks to be just 2 colors.  http://blog.demofox.org/2014/06/30/distance-field-textures/" CreationDate="2015-09-11T14:33:35.453" UserId="56" />
  <row Id="883" PostId="1478" Score="1" Text="It's probably feasible, but will have to be done on a game-to-game basis. More recent consoles, like the PS3/XB360 also use shaders, so assuming you can reverse engineer the assets, you could modify the shaders to apply additional effects. But having the protected disc complicates things, so you would probably also need a jailbroken device to run the modified software as if it was homebrew." CreationDate="2015-09-11T17:46:16.693" UserId="54" />
  <row Id="884" PostId="1478" Score="0" Text="Thank you for the reply!" CreationDate="2015-09-11T18:58:48.187" UserId="1622" />
  <row Id="885" PostId="1479" Score="1" Text="Try the book [Space-Filling Curves - An Introduction with Applications in Scientific Computing](http://www.space-filling-curves.org/)." CreationDate="2015-09-11T19:14:30.400" UserId="192" />
  <row Id="886" PostId="1479" Score="0" Text="See also section 2.1.1.2 of Samet's *Foundations of Multidimensional and Metric Data Structures*." CreationDate="2015-09-12T01:02:50.127" UserId="192" />
  <row Id="887" PostId="342" Score="1" Text="@porglezomp Mostly marketing speak, but http://www.theverge.com/2013/6/21/4446606/how-pixar-changed-the-way-light-works-for-monsters-university" CreationDate="2015-09-13T07:13:11.773" UserId="457" />
  <row Id="888" PostId="1484" Score="0" Text="What kind of properties are you after for that surface?" CreationDate="2015-09-13T12:35:40.917" UserId="38" />
  <row Id="889" PostId="1486" Score="3" Text="What, [no MathJax](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) ? :-(" CreationDate="2015-09-13T14:24:25.790" UserId="192" />
  <row Id="891" PostId="1485" Score="0" Text="It seems that you think &quot;project this pyramid into a 2D star-shaped object&quot; is a defined operation. It is not, until you do so." CreationDate="2015-09-13T16:47:28.220" UserId="504" />
  <row Id="892" PostId="1488" Score="3" Text="I don't think questions asking for tool recommendations are on topic here. Computer Graphics SE is a Q&amp;A site for computer graphics researchers and programmers." CreationDate="2015-09-13T18:05:31.800" UserId="54" />
  <row Id="893" PostId="1486" Score="0" Text="Yep, looks like it's not working yet :(. Would you mind just using plain code formatting for time being, so all those symbols won't distract from the formulas?" CreationDate="2015-09-13T18:09:00.987" UserId="54" />
  <row Id="894" PostId="413" Score="0" Text="Done! My answer should be valid now." CreationDate="2015-09-13T18:24:50.363" UserId="14" />
  <row Id="895" PostId="413" Score="0" Text="Excellent. Deleting my earlier comments." CreationDate="2015-09-13T19:44:01.363" UserId="482" />
  <row Id="896" PostId="1489" Score="0" Text="Please do not spread the idea of using the .obj format. Please choose a more capable format to promote, whether exporting manually or using the .blend directly, which will probably be best for a beginner's purposes." CreationDate="2015-09-13T22:52:40.237" UserId="504" />
  <row Id="897" PostId="1489" Score="0" Text="Although I do not agree with your comment, this is not the right place for that debate. I have edited my answer to remove the OBJ reference." CreationDate="2015-09-13T23:40:54.133" UserId="14" />
  <row Id="898" PostId="1485" Score="2" Text="In order to UV map in that way, you must think of (5) as four different vertices that happen to have the same XYZ coordinates." CreationDate="2015-09-13T23:43:35.430" UserId="1634" />
  <row Id="899" PostId="1489" Score="0" Text="I'd love to hear your thoughts. Give us a link if you ever post them." CreationDate="2015-09-14T01:54:02.063" UserId="504" />
  <row Id="900" PostId="1473" Score="1" Text="You know just to lift a bit of weight into the importance (actually UNimportance) of this matter. The PS4 SDK has a bug that doesnt allow emitting to any other queue than queue 0. I think if it was so crucial it would have been fixed faster." CreationDate="2015-09-14T02:03:50.897" UserId="1614" />
  <row Id="901" PostId="1489" Score="0" Text="Post a question and watch everyone fire shots :-)" CreationDate="2015-09-14T05:45:09.800" UserId="14" />
  <row Id="902" PostId="1490" Score="0" Text="_&quot;I have also another doubt: are scan converting and rastering the same thing ?&quot;_   &lt;sarcasm&gt;That might depend on who's paying the patent lawyer you meet &lt;/sarcasm&gt;.  I, however, would tend to say that scan converting is probably a subset of the rasterisation process, i.e. Scan conversion being the process that determines which pixels are inside a (or all) each primitive(s). Whether you should also include the shading/texturing in the &quot;scan conversion&quot; is a bit uncertain. I tend to think of those as a 'separate' step." CreationDate="2015-09-14T08:00:20.543" UserId="209" />
  <row Id="903" PostId="1488" Score="2" Text="[Relevant meta post.](http://meta.computergraphics.stackexchange.com/a/143/16)" CreationDate="2015-09-14T09:20:30.070" UserId="16" />
  <row Id="904" PostId="1490" Score="0" Text="See https://en.wikipedia.org/wiki/Bézier_curve#Computer_graphics." CreationDate="2015-09-14T11:03:25.847" UserId="192" />
  <row Id="905" PostId="1486" Score="0" Text="I've added this question to the [list of examples on meta](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) to add to the case for adding MathJax to our site." CreationDate="2015-09-14T13:37:44.347" UserId="231" />
  <row Id="906" PostId="1486" Score="0" Text="&quot;which works for _every quadrilateral_.&quot; Unless I've messed up my &quot;back of the envelope sketch&quot;, I think you need to amend that to &quot;every _convex_ quadrilateral&quot;." CreationDate="2015-09-14T13:45:54.443" UserId="209" />
  <row Id="907" PostId="1486" Score="0" Text="@SimonF, you're right. Fixed. Thanks." CreationDate="2015-09-14T13:56:49.990" UserId="192" />
  <row Id="908" PostId="1477" Score="0" Text="From these details, as I understand it, JPEG expects the input RGB values to be encoded in a way that the display will apply a power function upon display. In order to recreate those specific RGB values, they should *not* be corrected prior to encoding." CreationDate="2015-09-14T23:06:07.037" UserId="197" />
  <row Id="909" PostId="320" Score="0" Text="The gamma exponent is stored in JPEG exif data. most software totally ignore it. but you can assume than after decoding a jpeg its already in gamma space so there is no conversion to do before sending the rgb value on the display buffer." CreationDate="2015-09-15T02:23:33.447" UserId="1614" />
  <row Id="910" PostId="185" Score="1" Text="There has been enough psychology of perception studies that told that we cannot tell what image looks more real. using eyeballing would be a terrible measurement method." CreationDate="2015-09-15T02:26:55.233" UserId="1614" />
  <row Id="911" PostId="1471" Score="0" Text="it feels indeed contradictory, but I would not say stratified sampling reduces the error, it reduces the noise only." CreationDate="2015-09-15T02:30:16.700" UserId="1614" />
  <row Id="912" PostId="1477" Score="1" Text="The trouble with stating it like that is that it's a bit ambiguous. We should probably state that, if your &quot;RGB&quot; data is, in fact, R'G'B' (and let's assume sRGB falls into that category) then you shouldn't modify the values before applying the R'G'B'=&gt;YCbCr matrix.  If, however, the data has, say, been computed with a renderer (so possibly linear), been processed using downscaling (which should be done in linear space) or, say, captured (and cleaned up) with a CCD (which I think is linear), then it has to be remapped prior to JPEG compression." CreationDate="2015-09-15T08:24:24.187" UserId="209" />
  <row Id="913" PostId="1493" Score="2" Text="Assuming a perspective projection, AFAICS the 'boundary' formed by the view-points horizon will be a (truncated) cone and thus most of the projection will be a conic section: https://en.wikipedia.org/wiki/Conic_section.  An ellipse is thus a possibility, but not the only one." CreationDate="2015-09-15T12:27:54.890" UserId="209" />
  <row Id="914" PostId="1493" Score="0" Text="Pardon my naivety, isn't an ellipse a conic section? Could a projected sphere ever result in a parabola or hyperbola?" CreationDate="2015-09-15T12:31:23.007" UserId="1647" />
  <row Id="915" PostId="1493" Score="0" Text="If you look at the wikipedia diagram, https://en.wikipedia.org/wiki/Conic_section#/media/File:Conic_Sections.svg, and consider the plane onto which you are projecting, you can get anything from an ellipse/circle, through to unbounded parabolas or hyperbolas (and I guess if the plane passes through the eye, even degenerate cases)" CreationDate="2015-09-15T12:35:38.403" UserId="209" />
  <row Id="916" PostId="1493" Score="0" Text="Apologies! I omitted a key element of my question, that I was only concerned with *perspective projection*. I'm very rusty in this field and its terminology after many years away from it, yet I remain interested. By the way a [tag:perspective] would be a worthwhile addition to the site for questions such as this." CreationDate="2015-09-15T14:55:59.870" UserId="1647" />
  <row Id="917" PostId="1493" Score="1" Text="In that case I will promote my comments to an answer..." CreationDate="2015-09-15T15:06:46.273" UserId="209" />
  <row Id="918" PostId="1498" Score="0" Text="I'm unable to imagine how the result could be a parabola or hyperbola despite the absolute logic of your argument. Some words clarifying what kind of layout would lead to these would be great. The best I can get my brain around is &quot;something to do with infinities somehow&quot; ..." CreationDate="2015-09-15T15:27:42.237" UserId="1647" />
  <row Id="919" PostId="1497" Score="2" Text="Thanks for your answer. Please see my addenda about perspective projection. Apologies for this oversight in my original wording." CreationDate="2015-09-15T15:28:33.860" UserId="1647" />
  <row Id="921" PostId="1498" Score="3" Text="Maybe something equivalent might help. Imagine you are holding a torch (flashlight for those in North America), which makes a conic beam, and you are in in a dark empty (infinite) warehouse.  Shining the torch at the floor you see an ellipse. Now _gradually_ tilt the axis of the torch back towards the horizontal. The ellipse will get longer and longer until the point when the topmost 'edge' of the beam itself is horizontal, i.e. parallel to the floor. Now the projection is a parabola and it stretches on forever. Tilting it further will form a hyperbola." CreationDate="2015-09-15T15:42:19.340" UserId="209" />
  <row Id="923" PostId="1497" Score="0" Text="Circles are special kind of ellipsis," CreationDate="2015-09-15T16:41:54.680" UserId="537" />
  <row Id="924" PostId="1497" Score="2" Text="Yes I tried to cover that in my original question. Points and line segments are other degenerate ellipses too I believe." CreationDate="2015-09-15T17:27:34.050" UserId="1647" />
  <row Id="925" PostId="1501" Score="1" Text="Very prettily illustrative! What do you think about tackling the parabola and hyperbola cases?" CreationDate="2015-09-15T20:47:34.230" UserId="1647" />
  <row Id="927" PostId="1501" Score="2" Text="@hippietrail Unfortunately, vector art programs don't have parabola and hyperbola tools the way they have ellipse tools, so it would be a bit harder... :)" CreationDate="2015-09-15T22:00:33.020" UserId="48" />
  <row Id="928" PostId="1498" Score="1" Text="@hippietrail: It's perhaps worth noting that, with a view plane in front of the camera, the only way you can end up with a parabola or a hyperbola is if at least part of the sphere is *between* the focal point and the view plane." CreationDate="2015-09-15T22:43:58.657" UserId="525" />
  <row Id="929" PostId="1497" Score="3" Text="@hippietrail: The Earth is actually an excellent example also for perspective projections. If you take an ordinary photograph outdoors, pointing the camera towards the horizon, then (assuming that your lens has no distortion, and that the Earth is approximately a perfect sphere) the image of the Earth in the picture will be (a section of) a very broad hyperbola." CreationDate="2015-09-15T22:53:04.833" UserId="525" />
  <row Id="930" PostId="1493" Score="1" Text="you need to add a constraint. fisheye is also a perspective projection, and you won't get ellipses. the constraint you need is linearity." CreationDate="2015-09-16T01:16:17.283" UserId="1614" />
  <row Id="931" PostId="1506" Score="1" Text="awesome explanation. however a bit fast on 2 points, a bit more details would be loved : 1. how do you jump from dot products to matrix products ? 2. between line 2 and 3 of last quoted section, what happens (n is moved from left to right a bit magically to me)" CreationDate="2015-09-16T01:24:32.717" UserId="1614" />
  <row Id="932" PostId="1506" Score="2" Text="1. (a^T)b is the same as dot(a, b) if a and b are column matrices of the same dimension. Try out the math for yourself!&#xA;2. (AB)^T = (B^T)(A^T), and (A^T)^T = A&#xA;For more matrix identities, check out [The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)" CreationDate="2015-09-16T01:30:29.597" UserId="197" />
  <row Id="933" PostId="1493" Score="0" Text="@v.oddou: Thanks for your help with the terminology. Would that result in something like &quot;projected into linear perspective&quot; or something else?" CreationDate="2015-09-16T05:18:15.043" UserId="1647" />
  <row Id="934" PostId="1497" Score="1" Text="@IlmariKaronen: Wow that makes it super clear and is worthy of an answer of its own! Would there be a version of this that would result in a parabola?" CreationDate="2015-09-16T05:20:12.070" UserId="1647" />
  <row Id="935" PostId="1506" Score="2" Text="@v.oddou Yep, Mokosha is right. Dot product can be expressed as multiplying a 1×n matrix (row vector) with a n​×1 matrix (column vector); the result is a 1×1 matrix whose single component is the dot product. The transpose of a column vector is a row vector, so we can write a·b as a^T b. For the second question, transposing a product of matrices is equivalent to transposing the individual factors and reversing their order." CreationDate="2015-09-16T05:42:33.543" UserId="48" />
  <row Id="936" PostId="1506" Score="0" Text="perfect, its all clear without issue now. thanks both." CreationDate="2015-09-16T06:09:21.337" UserId="1614" />
  <row Id="937" PostId="1493" Score="1" Text="I would rather say something like &quot;where the projection is a linear application&quot;. There might be some shortcut term for this, like &quot;linear epimorphism&quot; or something, but I long forgot that." CreationDate="2015-09-16T06:14:06.193" UserId="1614" />
  <row Id="938" PostId="338" Score="0" Text="Where did you see this claim? It is rather surprising to me (but I don't immediately see that it's impossible)." CreationDate="2015-09-16T06:19:04.597" UserId="1657" />
  <row Id="939" PostId="1493" Score="0" Text="@v.oddou: I've tweaked the wording of the question based on your advice." CreationDate="2015-09-16T06:21:29.973" UserId="1647" />
  <row Id="940" PostId="1509" Score="0" Text="surely you mean illustrator and inkscape (instead of illustrator and illustrator)" CreationDate="2015-09-16T08:10:30.780" UserId="38" />
  <row Id="942" PostId="1509" Score="0" Text="I sure do! Tnx!" CreationDate="2015-09-16T08:30:45.907" UserId="1659" />
  <row Id="943" PostId="1506" Score="0" Text="@NathanReed (Gosh this takes me back to the early PowerVR days where we modelled most things with planes). It might also be worth mentioning that, for optimisation purposes, if you have a matrix _Mr_ that only contain rotations, (i.e. is orthogonal) then Inverse(_Mr_) = Transpose(_Mr_), and so Trans(Inverse(_Mr_)=_Mr_.   You can also take shortcuts with the translation part and if you know the scaling is uniform. FWIW in the SGL PowerVR graphics library, we used to keep booleans to track whether a transformation matrix had these properties to save costs with the normal transformations." CreationDate="2015-09-16T08:32:11.153" UserId="209" />
  <row Id="944" PostId="1500" Score="1" Text="What might be really nice is if your animation could change the shading for the various outcomes: Say white for ellipse, green (for the 'one frame' of parabola), and red for hyperbola. :-)" CreationDate="2015-09-16T08:42:32.223" UserId="209" />
  <row Id="945" PostId="1500" Score="2" Text="@SimonF i thought about this, i was planning something like nathan reed. But i was in a bit of hurry, i was lucky to get this render done. Initially i was a bit sceptical whether hyperbola could exist at all, but yes now it seems obvious." CreationDate="2015-09-16T09:08:58.803" UserId="38" />
  <row Id="946" PostId="1501" Score="0" Text="@NathanReed sure but they do have general graphing tools, (if not you can get one from me) graph a generic parabola and scale/rotate to fit." CreationDate="2015-09-16T09:11:46.980" UserId="38" />
  <row Id="947" PostId="1497" Score="1" Text="@hippietrail I add some explanation at the end of my answer, hope it could answer new aspects of edited question. and thanks for your complement." CreationDate="2015-09-16T10:41:15.820" UserId="537" />
  <row Id="954" PostId="1510" Score="0" Text="nice pic! thanks" CreationDate="2015-09-17T08:50:08.860" UserId="316" />
  <row Id="955" PostId="1510" Score="0" Text="Yeah, complex surfaces leave something to be desired if the textures need to be appraised, as you can not distinguish easily what is texture and what is not. In this case Suzanne is also badly oriented as its predominantly showing the flat angle towards the camera." CreationDate="2015-09-17T09:34:35.957" UserId="38" />
  <row Id="956" PostId="1510" Score="0" Text="@joojaa You're right, the main reason why Suzanne loses here is probably because of the angle. I'll mention that in the answer, or better, render another pic with better orientation." CreationDate="2015-09-17T10:31:02.377" UserId="385" />
  <row Id="957" PostId="1510" Score="0" Text="Looks better, still complex shapes are harder to appraise now i dont know if certain features are because of the normal map or are they there because of Suzannes shape. PS: maybe the literal answer to the question should be &quot;because they are trying to solve same problem&quot;" CreationDate="2015-09-17T11:35:36.283" UserId="38" />
  <row Id="959" PostId="1508" Score="0" Text="What is $G^n$ ? Didn't you mean $C^n$ i.e. $n$-times continuously differentiable? Actually I would be interested if there is subdivision algorithm which gives higher smoothness than Catmul-Clark. Catmul-Clark gives you $C^1$ at extraordinary vertices and $C^2$ everywhere else. People making 3d models for living are actually quite concerned about minimizing number of those extraordinary vertices in their meshes." CreationDate="2015-09-17T21:21:30.903" UserId="1613" />
  <row Id="960" PostId="1515" Score="1" Text="How about rendering a series of comparison swatches and try to fit a conversion curve?" CreationDate="2015-09-18T06:36:53.597" UserId="38" />
  <row Id="961" PostId="1514" Score="1" Text="Why does the incident ray is split after reflection/refraction? If the light is a particle does that means that that this particle recursively split? And if the light is a wave does that means that is splits by frequency (but in this case why it splits after second/third/etc hit)?" CreationDate="2015-09-18T06:59:34.400" UserId="386" />
  <row Id="962" PostId="1514" Score="4" Text="The particle does not split. Rather, the images show the potential paths it could take." CreationDate="2015-09-18T07:03:06.197" UserId="310" />
  <row Id="963" PostId="1514" Score="1" Text="Many particles will hit the (nearly) same spot from the (nearly) same angle. For every particle going out there is (usually) a particle that went in. That means that averaged out the *beam* of particles from a certain angle on a certain spot gets split up in several (a lot) reflections." CreationDate="2015-09-18T09:09:51.863" UserId="137" />
  <row Id="964" PostId="1497" Score="0" Text="I've made some edits which hopefully don't change the intended meaning of your answer - feel free to roll back the changes if you wish." CreationDate="2015-09-18T09:33:41.090" UserId="231" />
  <row Id="965" PostId="1514" Score="1" Text="Great answer shedding light on most of my questions. Why is the specular part of non-metals colorless and not affected by the albedo? How and where does [polarization](http://filmicgames.com/archives/547) come into play?" CreationDate="2015-09-18T10:58:27.123" UserId="385" />
  <row Id="966" PostId="1488" Score="0" Text="A quick search over at [softwarerecs.stackexchange.com](http://softwarerecs.stackexchange.com/) should help you." CreationDate="2015-09-18T11:01:02.690" UserId="385" />
  <row Id="967" PostId="1514" Score="1" Text="_&quot;A material's observed color is the light that is not absorbed.&quot;_ At this point it might be worth referencing the [Are there common materials that aren't represented well by RGB?](https://computergraphics.stackexchange.com/questions/203/are-there-common-materials-that-arent-represented-well-by-rgb) discussion, as fluorescent materials spring to mind." CreationDate="2015-09-18T11:01:51.843" UserId="209" />
  <row Id="968" PostId="1497" Score="0" Text="I changed quote blocks to headings since they weren't quoting anything. Hopefully this is clearer now - I've opened a [meta discussion](http://meta.computergraphics.stackexchange.com/questions/162/should-use-of-quote-blocks-be-restricted-to-quotes) about whether to edit to convert to headings like this." CreationDate="2015-09-18T11:16:52.733" UserId="231" />
  <row Id="969" PostId="1514" Score="0" Text="@DavidKuri I don't know the specific physical reason to *why* metals absorb all diffracted light and non-metals highly attenuate reflected light (IE. black diffuse for metals, and very small, monochrome specular for non-metals). Rather, it's just an observed phenomenon in nature. If anyone has a link or explanation, I would love to see it." CreationDate="2015-09-18T14:55:06.097" UserId="310" />
  <row Id="970" PostId="1514" Score="1" Text="@DavidKuri For polarization, you have to look into the other way of representing light, ie. waves. Polarization filters, for example in cameras, use long polymer strands  to block out certain wave orientations. A similar, but much less pronounced, process happens when the light wave interacts with all materials. This is one of the ways that light 'loses' energy." CreationDate="2015-09-18T15:01:41.967" UserId="310" />
  <row Id="971" PostId="1517" Score="0" Text="I wonder if it will be helpful to plot the various methods on a single plot. For example - I plotted two of your links here: http://i.imgur.com/YJjIMOQ.png" CreationDate="2015-09-18T17:22:46.927" UserId="14" />
  <row Id="972" PostId="1518" Score="0" Text="&quot;you may be able to use a 16-bit RGBA texture and a&quot; AND A WHAT??? :)" CreationDate="2015-09-19T01:12:24.700" UserId="48" />
  <row Id="973" PostId="1518" Score="2" Text="Haha, this is what happens when you leave the tab open to finish later. Edited." CreationDate="2015-09-19T01:13:15.507" UserId="197" />
  <row Id="974" PostId="1498" Score="0" Text="@IlmariKaronen: What would &quot;focal point&quot; mean in this context? The point the eye is focussing on? The vanishing point? (I taught myself 3D perspective rotation and projection as a 12 or 13 year old and never gained fluency in the math and terminology.)" CreationDate="2015-09-19T04:47:42.520" UserId="1647" />
  <row Id="975" PostId="1519" Score="3" Text="Thank you! I only knew the simplifications. These extra details are awesome" CreationDate="2015-09-21T06:07:42.403" UserId="310" />
  <row Id="976" PostId="1498" Score="0" Text="@hippietrail Focal point, in this context, would be the apex of the cone. Effectively the &quot;pinhole&quot; of the perspective, pinhole camera model. (PS Does the name imply meeting &quot;a strange lady. She made me nervous..&quot;?)" CreationDate="2015-09-21T07:49:41.237" UserId="209" />
  <row Id="977" PostId="1521" Score="0" Text="owell it is possible to do this analytically for polygons. Turn each poly into polar coordinates. You can then either analytically render these or sample like any other polygon, except they are now curved. Samplig may be faster though." CreationDate="2015-09-21T19:25:14.143" UserId="38" />
  <row Id="979" PostId="1519" Score="1" Text="This is a fascinating answer. Could you clarify/link the acronym SSS please?" CreationDate="2015-09-21T20:28:47.780" UserId="231" />
  <row Id="982" PostId="1523" Score="0" Text="What's &quot;normal compression&quot; - things like JPEG and PNG? Are you asking about the differences between those and hardware-supported formats like DXT and ASTC?" CreationDate="2015-09-21T21:14:09.627" UserId="48" />
  <row Id="984" PostId="1519" Score="0" Text="@trichoplax Thanks! SSS == sub-surface scattering." CreationDate="2015-09-21T22:06:34.003" UserId="523" />
  <row Id="985" PostId="1523" Score="6" Text="(At last, a subject I know a bit about!)&#xA;&#xA;What makes it different to PNG/JPEG is random access.  Given you want to access Texel(X.Y) you can quickly determine the small footprint of data needed to produce that texel. JPG or PNG might require decompression of up to all of the data!   Sections 1 and 2 of the [Wikipedia article](https://en.wikipedia.org/wiki/Texture_compression) are a good summary." CreationDate="2015-09-21T22:36:22.550" UserId="209" />
  <row Id="987" PostId="1521" Score="0" Text="Thanks for your comment @joojaa, but I cann't get your point. I intend to get the viewing field for each vertex (why do you mention &quot;polygons&quot;?). Is this what you mean: make each vertex as coordinate origin; then express all other vertices in polar coordiates (theta, phi, r) according to this origin; then do some analytical analysis using these coordinates or do sampling?  Could you please make it more clear?" CreationDate="2015-09-22T01:45:42.093" UserId="1692" />
  <row Id="988" PostId="1521" Score="1" Text="Possibly relevant reading: [Interactive Horizon Mapping](http://research.microsoft.com/en-us/um/people/cohen/bs.pdf). The visibility maps you describe are similar to what are called horizon maps in the literature, which store the elevation angle of the &quot;horizon&quot; for a predefined set of directions around each point on a surface. That paper is mainly about using them at runtime, though, with not much detail on generating them." CreationDate="2015-09-22T06:18:14.137" UserId="48" />
  <row Id="990" PostId="1519" Score="0" Text="Thanks :) If you clarify it in the question, it will survive deletion of the comments (which are not guaranteed to be long lived). I've edited in a link and hover text which hopefully leaves your intended presentation intact." CreationDate="2015-09-22T09:48:26.563" UserId="231" />
  <row Id="991" PostId="1525" Score="0" Text="Instead of software to test against yest against known real world measurements and fluid dynamics bechmarks. Otherwise your error is tainted. I saw the same question posted elsewhere on the stackexhange network btw" CreationDate="2015-09-22T13:11:06.700" UserId="38" />
  <row Id="993" PostId="1525" Score="1" Text="I think that testing against real world measurement is good for testing if you have the physics right. If you only want to debug you program, than testing against others code is better idea. Plus in computer simulation you can measure anything without affecting the experiment. For example measuring fluid speed at any point is just impossible in real world experiment, but trivial in computer simulation." CreationDate="2015-09-22T13:32:19.240" UserId="1613" />
  <row Id="994" PostId="1519" Score="2" Text="While i do appreciate the pedantry of this answer. Sub surface scattering is considered a mm scale effect while its true that at molecular ranges everything passes the surface to some degree. But the base constraint is that we are generally counting mm scale effects and trying to abstract lower levels as statistical models. Hence micrometer is equal to immediately as most pixels see much greater area than this. Same applies to color which does not meaningfully exist in physics the same way as our eyes and brain precieve it" CreationDate="2015-09-22T13:38:11.347" UserId="38" />
  <row Id="995" PostId="1525" Score="0" Text="Yes but you also inherit problems of their solvers. I admit i did do this a few times developing a multibody simulator and checking against results form MSC Adams but in hindsight that wasn't really stellarly useful" CreationDate="2015-09-22T14:30:26.947" UserId="38" />
  <row Id="997" PostId="1526" Score="3" Text="Would be interesting to see some results of your detailed recipe if there are any you can share." CreationDate="2015-09-22T14:52:22.670" UserId="457" />
  <row Id="998" PostId="1525" Score="0" Text="Checking against real world experiment was any better? I doubt it, but I might be wrong. The situation with multibody physics is quite different to fluid physics. Even something as simple as billiard has chaotic behavior. Moreover rigid body dynamics with contacts is not even well posed mathematical problem, do you know Painlevé paradox? So doing numerical simulation of multibody physics is doomed to fail in general. Some references: https://plus.maths.org/content/chaos-billiard-table https://en.wikipedia.org/wiki/Painlev%C3%A9_paradox" CreationDate="2015-09-22T15:12:51.850" UserId="1613" />
  <row Id="999" PostId="1525" Score="0" Text="The situation with fluids is quite different. You know that numerical solution converges to weak solution of Navier-Stokes eq. So every solver, if it is consistent, has to converge to the same answer. But you have to do it for small Reynolds numbers, so no turbulence occur." CreationDate="2015-09-22T15:16:46.017" UserId="1613" />
  <row Id="1000" PostId="1525" Score="1" Text="Yes i am aware of how multi body dynamics work, i kind of teach it (and briefly researched it for a year or two). But no checking against known analytical solutions was easier. But a real fluid is similarly chaotic as a multi body dynamic. So one should be able to check against laminar flow situations etc. Friction is a bitch though." CreationDate="2015-09-22T15:18:55.793" UserId="38" />
  <row Id="1001" PostId="1523" Score="0" Text="As SimonF wrote. This is an extremely broad question, and the answer depends on which type you're interested in. Did you look at the specification for e.g. DXT?" CreationDate="2015-09-22T16:08:01.373" UserId="523" />
  <row Id="1002" PostId="1507" Score="0" Text="Also, [LuxRender](http://www.luxrender.net/wiki/images/7/79/Luxball.png)." CreationDate="2015-09-22T16:12:16.720" UserId="523" />
  <row Id="1003" PostId="1524" Score="0" Text="Maybe you can recompute the terms of the N-S equation with numerical differentiation and check how they cancel out." CreationDate="2015-09-23T06:19:43.570" UserId="1703" />
  <row Id="1004" PostId="1527" Score="0" Text="Thanks @joojaa. I think I understand your point now. So, basically, your method is to project the whole polygon sets to a unit sphere that is located at a observer point. Then, the spherical field that is covered by the projected polygons is not visible, right?&#xA;&#xA;This is an analytic method that gives the precise result. I think when apply this to all vertices, at each vertex I need to project to whole polygon set. So, this maybe computational heavy." CreationDate="2015-09-23T06:23:54.490" UserId="1692" />
  <row Id="1005" PostId="1527" Score="0" Text="Depends on how many polygons you have. You can do all the tricks of scanline rendering. And backface cull etc. In fact you can do this with normal hardware all you need to do is duplicate the triangles on the wrapping seams. So all things that apply for normal perspective projection apply here. This may be faster than raytracing extremely many rays though." CreationDate="2015-09-23T06:28:53.833" UserId="38" />
  <row Id="1006" PostId="1491" Score="0" Text="For cubic curves, the scanline approach can also be looked at as a 1D problem, only involving the Y equation, which is a cubic. You can find the two extrema analytically (it's a quadratic equation) to achieve reliable separation of the roots. Full analytical computation of the roots is also possible, but incremental resolution (Newton or other) is less expensive." CreationDate="2015-09-23T06:33:21.050" UserId="1703" />
  <row Id="1007" PostId="1527" Score="0" Text="Strictly from a CS perspective both raytracing and polygon projecting has a O(N) complexity its just that N is dramatically smaller for the polygon projection method. So your raytracer shoots 1-8 rays at the cost of one analytic polygon." CreationDate="2015-09-23T06:37:35.083" UserId="38" />
  <row Id="1009" PostId="1529" Score="1" Text="There are many scanning techniques. Becuase they have a camera, and a accurate position sensor, that is basically a video you could use video tracking techniques to produce point clouds and build polygonmodels out of that.  (for some ideas see https://www.ssontech.com/learning.html great resource even if you dont use their apps)" CreationDate="2015-09-23T08:43:29.903" UserId="38" />
  <row Id="1012" PostId="1526" Score="0" Text="Not at the moment, sorry :/" CreationDate="2015-09-23T13:40:05.343" UserId="1699" />
  <row Id="1013" PostId="1531" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/167/how-should-we-respond-to-homework-questions) about homework questions." CreationDate="2015-09-23T17:23:14.007" UserId="231" />
  <row Id="1014" PostId="1531" Score="4" Text="Could you refine your question? You have a nice introduction, but it's difficult to know what you're really asking. How to use GL_POINTS? How to use points to draw a 2D shape? etc.." CreationDate="2015-09-23T18:34:42.750" UserId="310" />
  <row Id="1017" PostId="1533" Score="0" Text="Bit of nitpicking its not impossible to do exactly. Its just not possible to do exactly for all possible cases. There are lots of cases where this can in fact be done exactly its just a bit hard to generalize this." CreationDate="2015-09-23T23:40:56.507" UserId="38" />
  <row Id="1018" PostId="1533" Score="0" Text="@joojaa IMO possible cases are exceptions rather than the rule. Can you link to a reference ?" CreationDate="2015-09-23T23:50:12.467" UserId="1703" />
  <row Id="1024" PostId="1533" Score="0" Text="Yes but thats enough to refute impossible. Not in front of a desktop right now bit consider this: Since rational splines can do exact circular arcs that means you can do exact offsets of said arc. Since linear splines are possible and their offsets contains lines and circular arcs that too is possible. A big set of realworld uses there." CreationDate="2015-09-23T23:55:40.947" UserId="38" />
  <row Id="1029" PostId="1533" Score="0" Text="So if you say impossible to do exactly in the general case then its ok." CreationDate="2015-09-24T00:04:09.457" UserId="38" />
  <row Id="1032" PostId="1533" Score="0" Text="Yes; I meant impossible in the general case (at least for Bézier). If you actually read the answers I was linking to when I said this, you'll find it says exactly that." CreationDate="2015-09-24T02:16:05.580" UserId="523" />
  <row Id="1034" PostId="1533" Score="1" Text="Point being, what you say should be self contained. You should update your post so that others who dont have the time dont make wrong conclusions. Remember discussions under posts are not permanent. What you dont say on the otherhand you can leave to the link." CreationDate="2015-09-24T06:58:26.377" UserId="38" />
  <row Id="1035" PostId="1533" Score="0" Text="@joojaa: are there any other cases than circles that can be exploited ?" CreationDate="2015-09-24T08:21:08.310" UserId="1703" />
  <row Id="1036" PostId="1514" Score="1" Text="@RichieSams : the explanation is held in Maxwell 4 equations of electromagnetism. Because of conduction there can be no wave intensity at the surface of a metal. This is the reason why grids protects micro-wave doors." CreationDate="2015-09-25T03:02:51.627" UserId="1614" />
  <row Id="1038" PostId="1539" Score="0" Text="Example please ......." CreationDate="2015-09-25T05:47:53.763" UserId="464" />
  <row Id="1039" PostId="1539" Score="0" Text="&quot;Calculations in affine coordinates often require divisions&quot;: I don't see why. In fact you compute exactly the same expressions." CreationDate="2015-09-25T07:07:58.123" UserId="1703" />
  <row Id="1040" PostId="1495" Score="0" Text="This is an image-processing related question, you are not really on the right site. (Maybe Signal Processing.)" CreationDate="2015-09-25T07:14:34.163" UserId="1703" />
  <row Id="1041" PostId="1539" Score="0" Text="@Yves: I'm responding to the more general &quot;use in computer graphics&quot; topic, not the specific &quot;computing matrix transformations&quot; question." CreationDate="2015-09-25T07:54:39.000" UserId="1713" />
  <row Id="1042" PostId="1539" Score="0" Text="@Hurkyl: so do I. When rendering a scene, you compute exactly the same expressions, with the same amount of divisions (the difference lies in dummy terms with a 0 factor)." CreationDate="2015-09-25T08:02:52.633" UserId="1703" />
  <row Id="1043" PostId="1539" Score="0" Text="@Yves: Hrm. I'm used to doing calculations where the conversion back to affine can be deferred to some extent; I'll cede to your expertise if you say that doesn't come up often." CreationDate="2015-09-25T08:05:49.240" UserId="1713" />
  <row Id="1044" PostId="1540" Score="1" Text="In principle, data types can be implemented that don't actually store those entries even though they act like they do." CreationDate="2015-09-25T08:06:53.710" UserId="1713" />
  <row Id="1045" PostId="1540" Score="0" Text="@Hurkyl Obviously. This is rarely done, as general-purpose matrix toolboxes are on hand." CreationDate="2015-09-25T08:09:54.507" UserId="1703" />
  <row Id="1046" PostId="1538" Score="3" Text="I think this question is too broad. In the end your formula will boil down to a sum of *some terms* that depend on *some input* (e.g. normals, light data etc.). The procedure is basically rewriting the calculation code as formulas. There is no special rendering magic there. If you can get any more specific, please add details to the question." CreationDate="2015-09-25T08:51:57.443" UserId="385" />
  <row Id="1048" PostId="1537" Score="0" Text="Example please ....." CreationDate="2015-09-25T14:56:52.050" UserId="464" />
  <row Id="1049" PostId="1537" Score="0" Text="What kind of examples are you looking for? Translation matrices and anything related to perspective projections should be easy enough to look up?" CreationDate="2015-09-25T15:07:33.867" UserId="4" />
  <row Id="1050" PostId="1537" Score="0" Text="@Bart, Analogy needed." CreationDate="2015-09-25T15:20:27.290" UserId="464" />
  <row Id="1051" PostId="1537" Score="2" Text="I'm sorry @anonymous, but that doesn't really tell me anything. You're going to have to use more words to explain what exactly you are looking for." CreationDate="2015-09-25T15:21:27.963" UserId="4" />
  <row Id="1055" PostId="1543" Score="0" Text="This will certainly help. Are you aware of any simpler hextocolor conversions, much narrower? Was thinking more like 8 or 16 different color names" CreationDate="2015-09-26T14:23:44.540" UserId="1723" />
  <row Id="1056" PostId="1543" Score="0" Text="This would work with any number of colors." CreationDate="2015-09-26T14:33:20.363" UserId="38" />
  <row Id="1057" PostId="1543" Score="0" Text="No, no, I understand that. But I want to limit the results to an extremely limited palette consisting of red, blue, green, white, orange, purple, brown and grey, for example" CreationDate="2015-09-26T16:11:13.527" UserId="1723" />
  <row Id="1058" PostId="1543" Score="0" Text="Web or CSS colors have a very high number of possible matches" CreationDate="2015-09-26T16:11:49.663" UserId="1723" />
  <row Id="1059" PostId="1543" Score="0" Text="Yes, whats the problem? Instead of taking the color names form web colors supply your own" CreationDate="2015-09-26T16:52:15.577" UserId="38" />
  <row Id="1060" PostId="1543" Score="0" Text="The problem? Brain not working properly, I think.   Makes perfect sense,  thanks!" CreationDate="2015-09-26T16:55:00.963" UserId="1723" />
  <row Id="1061" PostId="1542" Score="2" Text="Not a full answer, but [color surveys like this one from xkcd](http://blog.xkcd.com/2010/05/03/color-survey-results/) may give some approximation if you're prepared to do a fair bit of preprocessing to exclude meaningless results and narrow down to the number of colors you require." CreationDate="2015-09-26T21:19:58.773" UserId="231" />
  <row Id="1062" PostId="1536" Score="1" Text="There's a very similar question on Gamedev.SE: [What does the graphics card do with the fourth element of a vector as the final position?](https://gamedev.stackexchange.com/questions/17987/what-does-the-graphics-card-do-with-the-fourth-element-of-a-vector-as-the-final)" CreationDate="2015-09-26T22:32:25.247" UserId="48" />
  <row Id="1064" PostId="1535" Score="0" Text="You write &quot;arbitrary kernel with circular symmetry&quot;: Doesn't that mean that you actually only need the convolution with the (Hemispheric) Zonal Harmonics part? If your symmetry axis is different you can still use it by adding rotations before and after the Zonal convolution. How to perform rotations is described in the paper. Integration with the Zonal part (m=0) should be comparatively easy. However, as with Spherical Harmonics, it won't be analytically solvable for arbitrary functions. Simple things like cosine lobes should work fine (haven't tried yet though)." CreationDate="2015-09-27T12:46:29.590" UserId="528" />
  <row Id="1065" PostId="1546" Score="3" Text="The [Graphics Programming Black Book](http://www.jagregory.com/abrash-black-book/) is certainly a classic worth reading. A lot of oldschool black-magic in it ;)" CreationDate="2015-09-27T19:08:12.660" UserId="54" />
  <row Id="1066" PostId="1545" Score="0" Text="excellent answer. also you can add that there are some papers mentioning that in some situations you can do compression yourself and decode it with client code in a pixel shader, rather than depend on dedicated hardware. i know of no real world use of that, that may be only worth for research, but it exists." CreationDate="2015-09-28T03:25:52.790" UserId="1614" />
  <row Id="1067" PostId="1535" Score="0" Text="@Wumpf You're right, that's pretty much what it boils down to. For SH, I'd just scale &quot;each band of f by the corresponding m=0 term from [kernel function] h&quot; (quoting Sloan's Stupid SH Tricks). Question is, can I do the same for HSH?" CreationDate="2015-09-28T07:42:25.580" UserId="385" />
  <row Id="1069" PostId="1541" Score="0" Text="&quot;Sony's PS4 can perform massive matrix multiplications.&quot; You mean the Cell processor of the PS3, right? The PS4 has a rather ordinary x86 processor." CreationDate="2015-09-28T20:40:51.503" UserId="528" />
  <row Id="1070" PostId="1545" Score="1" Text="@Nathan-Reed re transform-based compression, actually, Microsofts's Talisman project used a compression scheme called TREC which (as one of the modes) used DCT but unlike JPEG, allowed random access to blocks (I suspect there must have been a table containing addresses).  This'd then allow variable length data for various blocks, but the indirection is unpleasant for HW - a reason VQ TC fell out of fashion. FWIW I experimented with about a dozen TC ideas B4 PVRTC; some were fixed-rate, transform-based, but &quot;missing&quot; coeffs still use bits. BTC-like fixed coeff locations implies &quot;free&quot; info." CreationDate="2015-09-29T06:08:22.323" UserId="209" />
  <row Id="1073" PostId="1550" Score="1" Text="It might be that he does not need to draw a solid object! Or just draw in a 2d projection screen." CreationDate="2015-09-29T16:34:04.137" UserId="38" />
  <row Id="1074" PostId="1552" Score="0" Text="Welcome to Computer Graphics StackExchange. Your question is very broad and also primarily opinion based. That's why I flagged it for closure. For more information about [How to ask?](http://computergraphics.stackexchange.com/help/how-to-ask), visit our HelpCenter." CreationDate="2015-09-30T07:16:13.547" UserId="127" />
  <row Id="1075" PostId="1552" Score="0" Text="Do you mean 2D or 3D graphics ? They are completely different worlds." CreationDate="2015-09-30T08:08:00.657" UserId="1703" />
  <row Id="1076" PostId="1553" Score="0" Text="IMO, advising Direct3D to a beginner is pure cruelty." CreationDate="2015-09-30T08:09:53.530" UserId="1703" />
  <row Id="1077" PostId="1546" Score="0" Text="Yes, I second the book by Michael Abrash. It is a GREAT read. There are a lot more trick in the sleeve to what is written in this book but the philosophy behind it is important (even to this day !)" CreationDate="2015-09-30T10:06:23.587" UserId="105" />
  <row Id="1079" PostId="1556" Score="4" Text="Not really, it can also mean just that it is redder than what  your monitor can make." CreationDate="2015-09-30T11:42:36.957" UserId="38" />
  <row Id="1080" PostId="1545" Score="2" Text="@Nathan-Reed. From what I have seen, all HW decoders can be implemented with pure logic path (bit decode, some lookup, some math in the data path) but no loop / register necessity. Are you aware of any scheme that add cycle latency to the texture lookup ? (I have for fun implemented a VHDL ETC1 decoder) I was under the impression that each texture unit (TU) had decoders embedded." CreationDate="2015-09-30T14:06:51.400" UserId="105" />
  <row Id="1081" PostId="1553" Score="0" Text="@YvesDaoust, not necessarily true. For a programmer with familiarity with OOP languages, D3D will probably feel much more natural than the procedural ways of OpenGL. At least that's how I felt when I started with D3D9 and C++." CreationDate="2015-09-30T18:54:23.403" UserId="54" />
  <row Id="1082" PostId="1553" Score="0" Text="@glampert: it seems that you never were a beginner." CreationDate="2015-09-30T18:59:29.393" UserId="1703" />
  <row Id="1083" PostId="1553" Score="0" Text="@YvesDaoust, haha, in graphics yes, not in programming, true. My reading is that the OP has some knowledge of programming, but in any case, this is not a point worth discussing, I was just raising another perspective ;)" CreationDate="2015-09-30T19:05:56.603" UserId="54" />
  <row Id="1084" PostId="1557" Score="0" Text="i heard once that we do get the same noise than cameras because noise is actually physical and not only electrical. (i.e there are not so many photons after all). But the brain erases it, using temporal antialiasing I reckon. (i.e we see with lots of motion blur at night)." CreationDate="2015-10-01T01:27:05.613" UserId="1614" />
  <row Id="1085" PostId="1556" Score="1" Text="I don't know about rgb but about energies, it is very possible to get &gt;1 reflectance thanks to fluorescence. Some materials can gather energy from other wavelengths." CreationDate="2015-10-01T01:36:30.790" UserId="1614" />
  <row Id="1086" PostId="1555" Score="0" Text="I'm not too familiar with NVENC, but it looks as if the `dstPitch` for the Y array is being supplied by the NVENC API, so it might conceivably differ from `srcPitch`, thus requiring the 2D copy instead of a straight memory copy. Not sure why the Y channel would require this where Uand V don't, though." CreationDate="2015-10-01T04:45:16.193" UserId="48" />
  <row Id="1087" PostId="1561" Score="0" Text="What a coincidence that we would post at the same time. You save me from having to post an image which is hard to draw when im on mobile." CreationDate="2015-10-01T05:39:51.630" UserId="38" />
  <row Id="1088" PostId="1561" Score="4" Text="@joojaa “You wrote all that on mobile? …You’re braver than I thought.”" CreationDate="2015-10-01T05:51:24.907" UserId="196" />
  <row Id="1089" PostId="1557" Score="0" Text="I don't quite get the idea. If you render an image in low light and simulate a Purkinje effect, it won't look realistic as the human eye will add its own effect, won't it ?" CreationDate="2015-10-01T06:47:31.977" UserId="1703" />
  <row Id="1090" PostId="1557" Score="1" Text="@YvesDaoust Since the image is shown on a LDR monitor under unknown lighting conditions, probably not. Simply put, the image you see on the screen will be brighter so it's easier to perceive. If we were using a HDR monitor and could reproduce the luminance values of a nighttime scene exactly (and have an otherwise dark room), you're right." CreationDate="2015-10-01T06:50:51.667" UserId="385" />
  <row Id="1091" PostId="1561" Score="1" Text="Yes I did. I was thinking of drawing a graph with Mathematica on my mobile over ssh, but that would have been a bit too much... Anyway the spellchecker of my mobile sucks so if you see a lot of misspellings feel free to fix." CreationDate="2015-10-01T07:00:43.170" UserId="38" />
  <row Id="1092" PostId="1557" Score="1" Text="There's nothing wrong with what your striving for, but im afraid that this seems a bit too broad to me as there are so many effects that we need to consider. I could not write this in SE format, because it would indeed be wrong. However if you adjust your scope a bit like &quot;Can you suggest **some** of the effects that i would need to consider&quot; than it would be easier to begin." CreationDate="2015-10-01T07:10:35.407" UserId="38" />
  <row Id="1093" PostId="1557" Score="0" Text="@joojaa I changed the questions as you suggested, thanks." CreationDate="2015-10-01T08:36:03.313" UserId="385" />
  <row Id="1094" PostId="1552" Score="0" Text="Turtle graphics! turtles all the way down" CreationDate="2015-10-01T12:42:44.113" UserId="38" />
  <row Id="1095" PostId="1562" Score="0" Text="Need more context. Are you trying to calculate analytic normals for a parametric surface? An implicit surface? Or do you want to calculate the normals from a generic triangle mesh? Or something else?" CreationDate="2015-10-02T01:23:41.690" UserId="48" />
  <row Id="1096" PostId="1562" Score="0" Text="Thanks, I added more detail. To answer your question I need to calculate normals from a generic triangle mesh. Though to be clear that mesh is different depending on the inputs. My shape is a 3D arrow, as an example here is a screenshot of it 2 different forms (i.e. radial, and linear). The class changes the width, depth, length, arc, and radius of the mesh as requested. http://cl.ly/image/3O0P3X3N3d1d You can see the odd lighting I am getting with my poor attempts at solving this." CreationDate="2015-10-02T01:42:12.217" UserId="1774" />
  <row Id="1097" PostId="1562" Score="3" Text="The short version is: calculate each vertex normal as the normalized sum of the normals of all the triangles that touch it. However, this will make everything smooth, which may not be what you want for this shape. I'll try to expand into a full answer later." CreationDate="2015-10-02T01:58:22.140" UserId="48" />
  <row Id="1098" PostId="1562" Score="0" Text="Smooth is what I am going for!" CreationDate="2015-10-02T02:03:42.117" UserId="1774" />
  <row Id="1099" PostId="1562" Score="0" Text="I implemented my understanding of your short answer and the result is better, but still a bit off. http://cl.ly/image/2T3k2R1Z0V25/o I am going for the smooth look of the cube in that screenshot. On the right is all the shape points along with the surface normals as my code as calculated them." CreationDate="2015-10-02T02:42:44.147" UserId="1774" />
  <row Id="1100" PostId="1562" Score="4" Text="In most cases, if you calculate the vertex positions analytically, you can also calculate the normals analytically. For a parametric surface, the normals are the cross product of the two gradient vectors. Calculating the average of triangle normals is just an approximation, and often results in visually much poorer quality. I would post an answer, but I already posted a detailed example on SO (http://stackoverflow.com/questions/27233820/providing-normals-with-triangle-strips-and-fans-for-opengl-gouraud-shading/27244693#27244693), and I'm not sure if we want replicated content here." CreationDate="2015-10-02T07:03:06.370" UserId="331" />
  <row Id="1101" PostId="1564" Score="0" Text="Related: [Do I need to rebind uniforms or attributes when changing shader programs?](http://computergraphics.stackexchange.com/q/305/127)" CreationDate="2015-10-02T08:36:21.993" UserId="127" />
  <row Id="1102" PostId="1569" Score="0" Text="I understand what PBR is and that it's a vague definition, as you pointed out I should have said BRDF. I get how basic Blinn-Phong shading works, but I'm wondering if (in probably naive terms) going for a (even simplistic) BRDF model is just a rewrite of shader + a slight change in the rendering pipeline, or something more complicated. As for my last question, I mean low end hardware with limited capabilities (think embedded devices)." CreationDate="2015-10-03T19:27:14.637" UserId="34" />
  <row Id="1103" PostId="1569" Score="0" Text="Yes, implementing a BRDF is just a case of re-writing a shader and swapping out which textures are used as imputs. However, as mentioned above, making those textures is the hard part." CreationDate="2015-10-03T21:43:28.917" UserId="310" />
  <row Id="1104" PostId="1569" Score="3" Text="Beyond just rewriting the BRDF, it's also important for the rendering pipeline to be gamma-correct and support HDR to some level. Those are definitely more involved changes if the engine doesn't have them already. Also, PBR tends to require more attention to indirect lighting (particularly specular, to look good with glossy materials), and adding various forms of real-time or semi-real-time indirect lighting to your engine can also be a huge task." CreationDate="2015-10-03T22:25:01.223" UserId="48" />
  <row Id="1108" PostId="343" Score="0" Text="@joojaa Hi, May I ask you a question about `interpolate` a set of points with `closed B-spline curve`. Please see [here](http://scicomp.stackexchange.com/questions/20921/how-to-interpolate-a-set-of-points-with-a-continuous-closed-b-spline-curve)" CreationDate="2015-10-05T07:49:30.763" UserId="1796" />
  <row Id="1109" PostId="1571" Score="0" Text="Thanks for your answer. I consider it to be incomplete though. Your statement from Interpretation 1 &quot;You perceive the spectrum as you would have perceived the rendered spectrum&quot; is arguably wrong. When perceiving the real spectrum, effects kick in that don't when using the conversion you described (e.g. you'd have to use a *scotopic standard observer* in low lighting conditions, as mentioned in Jameson, Hurvich: Visual Psychophysics). What you described is the idea of spectral rendering. Interpretation 2 is what I want to learn more about. The paper will be a good start, thanks for that." CreationDate="2015-10-05T08:30:23.777" UserId="385" />
  <row Id="1110" PostId="1528" Score="1" Text="&quot;Note that lines, ... can be described by an implicit equation with integer coefficients.&quot; Do you mean that it is possible to write Bresenham's algorithm as a Diophantine equation?" CreationDate="2015-10-05T10:14:22.390" UserId="1786" />
  <row Id="1111" PostId="1528" Score="1" Text="@AlexeyPopkov: in a way. In the first quadrant, the line equation is `Y-Y_0=(X-X0)(Y1-Y0)/(X1-X0)`, which gives rational `Y`'s. Setting `Z=Y(X1-X0)`, the equation becomes `Z-Z0=(X-X0)(Y1-Y0)`, of the form `Z=aX+b`." CreationDate="2015-10-05T10:23:54.207" UserId="1703" />
  <row Id="1112" PostId="343" Score="0" Text="@ShutaoTANG [image 1-2 postscript file](http://pastebin.com/Ncz4KxAC) as per request." CreationDate="2015-10-05T17:35:38.827" UserId="38" />
  <row Id="1113" PostId="1575" Score="0" Text="Or you can just download the image files..." CreationDate="2015-10-05T17:46:18.773" UserId="38" />
  <row Id="1114" PostId="1575" Score="0" Text="Thanks. I you hit the money when you said 'render each frame'. That is what I want to capture... the frames output by application. I'd like to 'grab' each rendered frame and pipe them to a client. I'll update my question." CreationDate="2015-10-05T18:14:36.430" UserId="1800" />
  <row Id="1115" PostId="1575" Score="0" Text="Another question: Are you wanting it to be dynamic, or just a static animation? If it's static, @joojaa is right. Just render out all the frames, convert them to a movie file, then just serve the movie file from the server." CreationDate="2015-10-05T18:25:40.463" UserId="310" />
  <row Id="1117" PostId="1569" Score="0" Text="Honestly, this is a bit off topic but to me it seems that focusing on crazy microscopic adjustments on the 5D space curves of BRDF by developping fitting models of microfacets integration, is just nit pick. It is much more important to focus on getting correct global illumination. it doesnt even have to be real time." CreationDate="2015-10-06T00:29:10.157" UserId="1614" />
  <row Id="1118" PostId="1576" Score="0" Text="There is a fourth way, artist supplied normals ;)" CreationDate="2015-10-06T04:19:45.657" UserId="38" />
  <row Id="1119" PostId="1576" Score="0" Text="@joojaa: I assume you are referring to normal maps? I've never heard of manually authored normals otherwise." CreationDate="2015-10-06T05:09:13.187" UserId="182" />
  <row Id="1120" PostId="1576" Score="1" Text="No, manually authored normals. It sometimes happens that your artist knows more about how the normals should behave than the programmers models do. It is sometimes a bit problematic to the calculation engines if they assume normals come from underlying calculations. But certainly it happens and you save lot of time in mathematical modeling." CreationDate="2015-10-06T07:30:19.340" UserId="38" />
  <row Id="1126" PostId="1579" Score="5" Text="Raytracers which render reflections and refractions have been ubiquitous for many many years." CreationDate="2015-10-06T13:59:46.793" UserId="457" />
  <row Id="1128" PostId="1579" Score="0" Text="Path tracing does just that: bounces rays around the scene using the properties of the surfaces it hits to determine bounces.  http://www.thepolygoners.com/tutorials/GIIntro/GIIntro.htm http://www.iquilezles.org/www/articles/simplepathtracing/simplepathtracing.htm" CreationDate="2015-10-06T14:39:52.620" UserId="310" />
  <row Id="1130" PostId="1582" Score="2" Text="The definition order did affect the layout. The relevant part here is the `s_buffer_load_dword` instructions - those are reading the input uniforms, and the last number in hex is the offset to read from. It shows in the first case `xy` is at offset 0 and `zw` at offset 16. In the second case you have `xy` at offset 0, `z` at offset 16, and `zw` at offset 32. It appears all the uniforms are individually 16-byte-aligned, and not packed together or reordered." CreationDate="2015-10-06T17:27:52.623" UserId="48" />
  <row Id="1132" PostId="1585" Score="0" Text="This is the correct beginning for an answer to this question. But it lacks continuation, you should talk of modern unbiased rendering as well to conclude about the recent convergence of techniques toward Kajiya's equation." CreationDate="2015-10-07T01:23:15.187" UserId="1614" />
  <row Id="1133" PostId="1587" Score="4" Text="They're probably referring to the sRGB color space. https://en.m.wikipedia.org/wiki/SRGB" CreationDate="2015-10-07T01:29:55.510" UserId="310" />
  <row Id="1135" PostId="1585" Score="1" Text="@v.oddou: feel free to enter your own. My point is about early techniques to show that this is a pretty old idea." CreationDate="2015-10-07T06:10:41.580" UserId="1703" />
  <row Id="1136" PostId="1575" Score="0" Text="@RichieSams it would be dynamic. I would like to allow for interaction." CreationDate="2015-10-07T08:35:04.333" UserId="1800" />
  <row Id="1137" PostId="1590" Score="0" Text="How would I go about accessing the key frames? I'm not really sure where to start. The game would be streamed from the cloud to a client. Thanks" CreationDate="2015-10-07T11:29:39.523" UserId="1800" />
  <row Id="1138" PostId="1592" Score="0" Text="Have you verified that mipmaps are actually used? The screenshot looks like bilinear filtering, instead of trilinear," CreationDate="2015-10-07T13:47:17.887" UserId="182" />
  <row Id="1140" PostId="1592" Score="0" Text="@JulienGuertault It may just be my poor eyesight but I can't see any distinct discontinuities that'd be indicative of just bilinear + nearest MIP map." CreationDate="2015-10-07T13:57:03.960" UserId="209" />
  <row Id="1141" PostId="1592" Score="0" Text="Actually my sampler state is set to AnisotropicWrap, which, I believe, is the best. Also I am 100% sure, that I use mipmaps." CreationDate="2015-10-07T14:04:13.337" UserId="205" />
  <row Id="1142" PostId="1592" Score="0" Text="@SimonF: sorry for the confusion, I meant linear. But anyway, apparently the problem would be elsewhere." CreationDate="2015-10-07T14:06:34.733" UserId="182" />
  <row Id="1143" PostId="1593" Score="0" Text="Sorry for maybe stupid question, but what is '2x2 box filter'? Does it have any other name?" CreationDate="2015-10-07T14:18:29.363" UserId="205" />
  <row Id="1144" PostId="1593" Score="0" Text="I just meant the naive method of generating the next lower, X*Y MIP map level from the 2X *  2Y level above it. I.e.  that of averaging 2x2 texels in the upper level to produce the corresponding texel in the lower one.  It's cheap, but it's not great.  Even a simple 4x4 tent filter should produce a far better result." CreationDate="2015-10-07T14:25:08.200" UserId="209" />
  <row Id="1145" PostId="1593" Score="0" Text="Just to make sure I fully understand you - now I am creating my mip maps like this: http://pastebin.com/nX1MVvEp  . Instead of that i should generate mipmaps from my original texture with original size yes?" CreationDate="2015-10-07T14:34:54.963" UserId="205" />
  <row Id="1146" PostId="1593" Score="0" Text="Well, I don't know what is inside the &quot;resize&quot; function so it's a little difficult to answer. The chaining, ie. 1024 generates 512, 512 generates 256, etc is not going to be a huge problem, as long as what's going on inside &quot;resize&quot; is ok.&#xA;As an experiment, could you create a 1024^2 texture that's all 0xFFFFFF _except_ for the top left pixel which you set to pure red, (0x0000FF).  What do you get in the 512^2  result?" CreationDate="2015-10-07T14:47:27.457" UserId="209" />
  <row Id="1147" PostId="1593" Score="0" Text="My result: http://postimg.org/image/atd8nc4j3/3636ad65/" CreationDate="2015-10-07T14:55:04.413" UserId="205" />
  <row Id="1149" PostId="1593" Score="0" Text="Sorry, I wasn't very clear. What I meant was &quot;what is in the top left, say, 10x10 texels of 512x512 image&quot;." CreationDate="2015-10-07T15:40:53.390" UserId="209" />
  <row Id="1150" PostId="1584" Score="0" Text="i wouldnt say we can render all effects. We can render most effects we know of. But rarely do we have the time to program in all effects." CreationDate="2015-10-07T16:34:09.477" UserId="38" />
  <row Id="1151" PostId="1591" Score="0" Text="I think a image would do wonders." CreationDate="2015-10-07T16:40:41.067" UserId="38" />
  <row Id="1152" PostId="1595" Score="1" Text="Do you mean that you want to detect areas of an image that are sharp and in-focus, versus blurred and out-of-focus?" CreationDate="2015-10-07T19:46:31.267" UserId="48" />
  <row Id="1156" PostId="1595" Score="1" Text="How did you used edge detection algorithms? Not sure it works and can't test now (so no answer for now :P), but just off the top of my head, you can divide your image in subregions and measure the value of gradients. Based on a threshold you can then decide whether that subregion is &quot;in-focus&quot; or not. Then you can re-use the edge info to refine your first approximation I described before. It makes sense to me know, but it may very well be a brain-fart being it late here :) I'll give it a try tomorrow if I have a minute to." CreationDate="2015-10-07T21:13:59.867" UserId="100" />
  <row Id="1159" PostId="1434" Score="1" Text="Are you going to improve article on Wikipedia? I see question like yours pretty often in Comptuter Graphics SE. Maybe it's worth considering? :)" CreationDate="2015-10-08T05:24:39.500" UserId="205" />
  <row Id="1161" PostId="1591" Score="1" Text="done :-) ......" CreationDate="2015-10-08T07:52:57.993" UserId="1810" />
  <row Id="1162" PostId="1595" Score="0" Text="@cifz - Thanks! Great idea to measure the gradiants, sounds like a simple and fast approach. But this will only work for a rough detection, right?" CreationDate="2015-10-08T10:24:07.547" UserId="18" />
  <row Id="1163" PostId="1595" Score="0" Text="@NathanReed Right. Sorry, not a native :) Do you think I should rephrase the question?" CreationDate="2015-10-08T10:28:27.593" UserId="18" />
  <row Id="1164" PostId="1599" Score="0" Text="Many thanks Fabrice! If possible, can you elaborate *local max frequency*?" CreationDate="2015-10-08T10:44:00.727" UserId="18" />
  <row Id="1165" PostId="1593" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/30017/discussion-between-simon-f-and-bartosz-baczek)." CreationDate="2015-10-08T12:05:38.610" UserId="209" />
  <row Id="1166" PostId="1599" Score="0" Text="I don't remember how these papers were estimating the native focus before refocusing. But I could think of various solutions: poor-man wavelets by calculating the FFT in a grid of subimages (i.e. a grid of windowed FFT), estimating local autocorrelation, poor-man FFT by convolving with a set of Differential of Gaussians (to detect ranges of focus), etc. But this is kind-of instant-hacking, the best is to read what the experts did for real :-)" CreationDate="2015-10-08T12:58:38.223" UserId="1810" />
  <row Id="1167" PostId="1579" Score="0" Text="Youd likely be interested in reading about how the used raytracing techniques to render the black hole in the movie interstellar. They used realistic physics equations and the result was so interesting that they were able to publish a scientific research paper with the results.  http://www.wired.com/2014/10/astrophysics-interstellar-black-hole/" CreationDate="2015-10-08T14:26:19.093" UserId="56" />
  <row Id="1168" PostId="1595" Score="0" Text="@poor Yes, I think it would be helpful to rephrase and edit the title to make it more clear." CreationDate="2015-10-08T17:10:30.513" UserId="48" />
  <row Id="1169" PostId="1551" Score="0" Text="Wow, this should be a blog post somewhere! Great answer!" CreationDate="2015-10-08T18:39:32.033" UserId="54" />
  <row Id="1171" PostId="210" Score="0" Text="I'm with Alan too, gamma can cause all sorts of contrast issues. But a uniformly emissive sphere will tend to produce very faint occlusions. You should try to use a falloff factor in the emission, such that vertical emissions are stronger than grazing emissions. Also your spheres are reflective, make then absorbing. but i guess your tracer does not take that into account otherwise we'd see more energy under your spheres." CreationDate="2015-10-09T01:24:09.873" UserId="1614" />
  <row Id="1172" PostId="1604" Score="0" Text="Interesting - thank you. For the additional light reaching the floor from the spheres, I am expecting this to be less than the light that would reach the floor directly from the sky if the spheres were not there, since they are convex. That is, I can't picture caustics resulting from only convex surfaces when the light is coming evenly from the sky. Even in the extreme case of all light being reflected (no absorption), I would expect simply no shadow, rather than a caustic." CreationDate="2015-10-09T02:47:27.653" UserId="231" />
  <row Id="1173" PostId="361" Score="0" Text="Yes, the wording is quite strange (what is a rotating point for instance ? :-) ). It's not a mistake, it's poor wording. I'm afraid that once a guy quicky made the whole &quot;basic computer graphics&quot; content of wikipedia for some reason, letting a lot of polishing (or more) to be done. Seems like hot topics are well edited and completed (by academics and master/PhD students ?), but not basic topics (I did, for a very few)." CreationDate="2015-10-09T07:44:52.237" UserId="1810" />
  <row Id="1174" PostId="1595" Score="0" Text="@NathanReed Thanks! Something like this? If not, feel free to edit the question :)" CreationDate="2015-10-09T07:52:32.113" UserId="18" />
  <row Id="1175" PostId="1551" Score="1" Text="Glad it's of help. As for a blog, I did write [this a decade ago](http://web.onetel.net.uk/~simonnihal/texcom/texcompcomp.html) but I really don't have time to do them." CreationDate="2015-10-09T08:01:52.133" UserId="209" />
  <row Id="1177" PostId="1592" Score="0" Text="@bartosz.baczek It's great that you found a solution to your problem. But instead of editing the solution into your question, please post it as an answer to your question, as [answering your own question is perfectly fine](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2015-10-09T15:41:33.163" UserId="127" />
  <row Id="1178" PostId="1592" Score="1" Text="@Nero I did so :)" CreationDate="2015-10-09T15:56:13.237" UserId="205" />
  <row Id="1179" PostId="1605" Score="1" Text="Mirror is likely to define what happens when your texture repeats. In the normal mode (just considering horizontal only) if you had a texture that looked like &quot;&lt;&quot;  you'd get &quot;&lt;&lt;&lt;&lt;&lt;&quot;.  With mirroring you get &quot;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&quot;.  Obviously can also apply to vertical direction as well." CreationDate="2015-10-09T16:15:52.590" UserId="209" />
  <row Id="1180" PostId="1605" Score="0" Text="It looks like increasing the resolution is the main helper. Moires patterns are just barely starting to form in the back corner of the rectangle." CreationDate="2015-10-09T16:32:09.603" UserId="310" />
  <row Id="1181" PostId="1605" Score="0" Text="Actually no, mirroring does most of the job :) I tested it with mirroring only and with bigger texture only as well, and turned out that mirror provides great effect. Fortunatelly tile texture is simetric so it' s not a big deal." CreationDate="2015-10-09T16:38:44.877" UserId="205" />
  <row Id="1184" PostId="1607" Score="2" Text="While this answers the question, instead of simply providing the solution to the specific problem it would be much better to describe how the OP could come up with this solution on his own. The question clearly is a homework related question. As of now, this answer does not help to solve the problem with different numbers." CreationDate="2015-10-09T18:51:18.327" UserId="127" />
  <row Id="1185" PostId="1606" Score="1" Text="You can read off the matrix elements from the coefficients of x, y, z in the result. For instance if the result had 2x + 3y in one component, then the corresponding column of the matrix would have values (2, 3, 0). The problem in this case is a bit more complicated as it appears to be relying on homogeneous coordinates (since there's division by x and y in the result), but still you can basically read off the matrix elements from the form of the result." CreationDate="2015-10-09T18:51:22.410" UserId="48" />
  <row Id="1186" PostId="1607" Score="0" Text="Thank you. I just investigated what &quot;perspective divide&quot; is and I found it has to do with finding a w for (x, y, z) in order to get (x, y, z, w). After finding such a value then everything is straight forward. Is this correct? Also, the issue here is to get rid of the variables in denominator." CreationDate="2015-10-09T19:02:02.020" UserId="1836" />
  <row Id="1188" PostId="1607" Score="0" Text="I wonder if I can ask another question related to this same problem. &quot;Assume you have performed the projection on the canonical view volume.  Give the matrix you would use to make the final image of size 5 units wide and 3 units tall.&quot; I think the answer is [5, 0, 0, 0][0, 3, 0, 0][0, 0, 1, 0][0, 0, 0, 1] Am I correct?" CreationDate="2015-10-09T19:23:59.467" UserId="1836" />
  <row Id="1189" PostId="1609" Score="0" Text="I am sorry but I do not get the idea. What do you exactly mean by &quot;reverse order&quot;?" CreationDate="2015-10-09T20:21:31.200" UserId="1836" />
  <row Id="1190" PostId="1609" Score="1" Text="Multiply x by z instead of z by x;" CreationDate="2015-10-09T20:27:31.040" UserId="1826" />
  <row Id="1191" PostId="1609" Score="1" Text="actually order is arbitrary one can model using row vectors and one can model column vectors. The computation yelds same result in both but the multiplication order changes. But yes this is sortof the right answer." CreationDate="2015-10-10T05:11:52.983" UserId="38" />
  <row Id="1192" PostId="1607" Score="0" Text="@JORGE i guess you should update the original question. Please explain this topic or link useful resources. I am interested" CreationDate="2015-10-10T09:05:55.533" UserId="316" />
  <row Id="1194" PostId="1607" Score="0" Text="@JORGE if it's a different question but relating to this one then you can post it separately as a new question but include a link to this question." CreationDate="2015-10-10T15:52:28.570" UserId="231" />
  <row Id="1195" PostId="1609" Score="0" Text="Joojaa, thanks for making that clear! Row matrix means reversed order of multiplication, is that correct?" CreationDate="2015-10-10T19:55:37.973" UserId="1826" />
  <row Id="1197" PostId="211" Score="0" Text="The trick of putting the sphere on the ground and looking at the contact point is a great one. Notice it works for debugging your (spherical) lights too, but in the opposite direction!" CreationDate="2015-10-11T05:04:43.907" UserId="523" />
  <row Id="1200" PostId="210" Score="0" Text="Your sky may be casting a little penumbra instead of a shadow. Reduce it to a small sphere for a check. Also, the spheres are indirectly illuminating the background." CreationDate="2015-10-11T18:45:36.700" UserId="1703" />
  <row Id="1201" PostId="1606" Score="0" Text="[Relevant meta discussion on whether multi part questions should be in one post or separate questions](http://meta.computergraphics.stackexchange.com/questions/181/editing-a-question-to-add-a-second-part)" CreationDate="2015-10-11T20:02:02.933" UserId="231" />
  <row Id="1202" PostId="1606" Score="0" Text="I'd recommend asking the second part as a separate question, since there is already an answer that was based on there only being one part. I've rolled back to the previous edit with only one question, but I've also raised this on Meta to see how others see it." CreationDate="2015-10-11T20:08:12.620" UserId="231" />
  <row Id="1203" PostId="1606" Score="0" Text="I don't know what will be decided on Meta but I've rolled back the edit quickly rather than waiting for the outcome of the Meta discussion, in order to prevent new answers coming in for the second part that would then prevent the edit being rolled back. I don't mean this to seem pushy. If there's anything you'd like to discuss please feel free to join us in [chat]." CreationDate="2015-10-11T20:14:33.037" UserId="231" />
  <row Id="1205" PostId="1590" Score="0" Text="maybe try to record it and than search them by hand. I know this is far away from any kind of automated test or even realtime feedback, but it seemed to be the rather uncomplicated" CreationDate="2015-10-12T07:28:21.890" UserId="1818" />
  <row Id="1210" PostId="1614" Score="1" Text="Can you atomic-add 3 to the counter instead of incrementing it?" CreationDate="2015-10-12T22:38:52.533" UserId="48" />
  <row Id="1211" PostId="1614" Score="0" Text="No, atomic counters can only be queried, incremented (by 1) or decremented (by 1). See [the Article](https://www.opengl.org/wiki/Atomic_Counter#Operations) in the opengl wiki. They are not the same as atomic Add etc on Images. You only have 8 (or so) of them and they are supposedly much faster when accessed extremely often (like when generating (and thus counting) thousands of ... things.)" CreationDate="2015-10-12T23:21:37.290" UserId="1699" />
  <row Id="1212" PostId="1614" Score="0" Text="Yeah, I guess you would have to turn it from an &quot;atomic counter&quot; to just a variable in an SSBO. Would be interesting to see if that's actually any slower (it might not be, depending on HW). Other than that, the only thing I can think of is to do like you said and run a compute shader to multiply the value by 3." CreationDate="2015-10-13T00:35:03.507" UserId="48" />
  <row Id="1213" PostId="1613" Score="0" Text="I would be very carrfull with that associativity comment its easy to misunderstand" CreationDate="2015-10-13T03:55:31.857" UserId="38" />
  <row Id="1214" PostId="1613" Score="0" Text="@joojaa I don't know what exactly you mean, but I've tried to clarify that bit." CreationDate="2015-10-13T07:36:30.703" UserId="16" />
  <row Id="1215" PostId="1613" Score="0" Text="Its hard for a layman to separate betveen order you multiply things and the order in which the elements are in multiplication." CreationDate="2015-10-13T11:25:49.913" UserId="38" />
  <row Id="1216" PostId="1613" Score="0" Text="so they do not understand the difference between assiocative and commutative. so if you talk of the order of multiplication many may think of commutativity" CreationDate="2015-10-13T11:37:31.017" UserId="38" />
  <row Id="1217" PostId="1615" Score="0" Text="I believe the question is seeking embedding of code in the image file, rather than in the image itself. If you're uncertain what a question is looking for you can also comment on the question once you have sufficient reputation." CreationDate="2015-10-13T13:58:22.043" UserId="231" />
  <row Id="1218" PostId="1616" Score="0" Text="I like the second idea because it makes clear that I need two different things: A way to atomically add triangles and a way to get the final index count. Doesn't have to be the same mechanism. And yes, I'll try atomicAdd as well. As Nathan suggested in a comment above, it would be interesting to see if it really is slower. I'll report back with the findings." CreationDate="2015-10-13T14:33:49.207" UserId="1699" />
  <row Id="1219" PostId="1615" Score="1" Text="thanks user1846, I was looking to embed the code inside the image file, as trichoplax mentions." CreationDate="2015-10-13T18:13:03.397" UserId="1806" />
  <row Id="1220" PostId="1616" Score="0" Text="Can confirm that atomicAdd is indeed not slower for this use case on nvidia kepler ( ~1.5k indices: 0.1ms - probably needs larger size for meaningful benchmark). Binding the indirect command buffer as an SSBO and writing to the uint at position 0 works fine. Sometimes one should just try the easy solutions first..." CreationDate="2015-10-13T22:15:00.713" UserId="1699" />
  <row Id="1223" PostId="1581" Score="1" Text="Something slightly unrelated, what about images that are code? Look at the esolang [piet](https://esolangs.org/wiki/Piet) which lets you program with images. And [this video](https://www.youtube.com/watch?v=FvS_DG8yIqQ) where an image is created and then built as an executable." CreationDate="2015-10-14T15:40:37.467" UserId="214" />
  <row Id="1224" PostId="1619" Score="0" Text="Are you sure lerping the forward transformed object is the same as lerping the backward transformed ray? For example, I can renormalize the ray after the lerp (and scale the hit distance accordingly). This doesn't change the result." CreationDate="2015-10-14T19:41:56.740" UserId="523" />
  <row Id="1225" PostId="1620" Score="0" Text="I break it down into a linear approximation, which is quite typical. One can handle more complex transforms with multiple such steps.¶ My problem is not in making the transform nonlinear, but in making the linear approximation to it correct." CreationDate="2015-10-14T19:45:49.290" UserId="523" />
  <row Id="1226" PostId="1619" Score="0" Text="@imallett Lerping the ray should be equivalent to lerping the inverse matrices, but not necessarily to lerping the forward matrices or lerping the object (as inversion isn't a linear operation). And I don't think renormalizing the ray after the lerp fixes things entirely - you can still be in a sheared, nonuniformly-scaled coordinate system that can screw up the math in your intersection routines and suchlike." CreationDate="2015-10-14T20:46:39.937" UserId="48" />
  <row Id="1227" PostId="1619" Score="0" Text="[See edit; better picture] At least, I do think renormalizing should rule out problems with the intersection--but that is what I thought; lerping the ray isn't lerping the object. In your answer you suggested lerping [inverse?] TRS and then recombining. Is this the way production renderers do it?" CreationDate="2015-10-14T21:01:21.383" UserId="523" />
  <row Id="1228" PostId="1622" Score="0" Text="your second `glVertexAttribPointer` doesn't look right, it's missing a *5 in the stride param" CreationDate="2015-10-15T10:31:15.700" UserId="137" />
  <row Id="1229" PostId="1622" Score="0" Text="@ratchetfreak Yes the *5 was missing, But the problem still exists." CreationDate="2015-10-15T12:53:33.407" UserId="1861" />
  <row Id="1230" PostId="1623" Score="1" Text="What is missing for you from the SH wikipedia page ? https://en.wikipedia.org/wiki/Spherical_harmonics" CreationDate="2015-10-15T13:20:06.867" UserId="1810" />
  <row Id="1231" PostId="1623" Score="1" Text="Fourier basis functions are simply sines (or sines.x * sines.y). Like sound can decompose in spectrums (i.e. set of sines), so is it for a 1D drawing. For 2D data, you simply do that in x then y (but I find quite intuitive to extrapolate the notion of wavelength to 2D).  Maybe you should tell what is your background in maths ? (which school level ?)" CreationDate="2015-10-15T13:23:38.570" UserId="1810" />
  <row Id="1233" PostId="1604" Score="1" Text="I understand your intuition. I wanted to dare make an analytical computation for what energy we should expect on a pixel laying just under the vertical tangent of the sphere but I'm so slow at math I gave up lol. Indeed convex surfaces will reflect through only one path to any given singular emission source-point, so caustics seems implausible." CreationDate="2015-10-16T01:19:25.370" UserId="1614" />
  <row Id="1234" PostId="1627" Score="4" Text="I think you're going to have to ask this on a site about electrical engineering..." CreationDate="2015-10-16T01:48:37.990" UserId="54" />
  <row Id="1235" PostId="1623" Score="0" Text="@FabriceNEYRET Thank you for the link. While I don't think this information will benefit other users, my background in maths is prépa(MP*) + engineering school (Master Degree) ; the program did not have in-depth covering of SH. I am indeed looking for material as rigorous as what I was used to study, although I now have less time to do so, so brevity is also important." CreationDate="2015-10-16T03:02:28.307" UserId="110" />
  <row Id="1236" PostId="1623" Score="1" Text="I read the paper you linked, it seems pretty solid." CreationDate="2015-10-16T05:10:24.150" UserId="38" />
  <row Id="1237" PostId="1627" Score="0" Text="Agree with glampert but, assuming all are monochromatic, it's probably just the electronics which control the voltage patterns that, in turn drive the electromagnets (or charged plates??) that deflect the electron beam in order to sweep it across the display phosphor." CreationDate="2015-10-16T06:40:22.217" UserId="209" />
  <row Id="1238" PostId="1605" Score="0" Text="It's hard to judge how much of an improvement this has made since the plane does not extend as far into the distance as in the example in the question." CreationDate="2015-10-16T07:55:13.733" UserId="231" />
  <row Id="1239" PostId="1629" Score="1" Text="Indeed the way Fourier is treated at university or enginneering school can be totally different from place to place, from a pure fancy pure math toy in strange mathematical spaces (but here it's even worse for Finite Element methods) to something really connected to signal, theory and applications. Similarily SH are sometime presented as cool math object dedicated to the physics of electronic orbitals. In all these case it's not easy for (ex)students to transpose to CG or signal. -&gt; that's why it's important to tell where he starts from and where he seeks to." CreationDate="2015-10-16T08:06:35.957" UserId="1810" />
  <row Id="1240" PostId="1627" Score="0" Text="Please not that cathode ray tubes are at the brink of being discontinued as technology. Only very very special uses even can get hold of these things. Even oscilloscopes are using LCD monitors by now. Ancient tech." CreationDate="2015-10-16T16:03:11.870" UserId="38" />
  <row Id="1241" PostId="1635" Score="0" Text="That is indeed a much better test. Actually, the Phong normalization I implemented is indeed based on Global Illumination Compendium 31a. It works out to be a 1D table which you precompute to any desired resolution for each BRDF.&#xA;&#xA;The other normalization term produces much weaker results, as might be expected in this glancing reflection case. I'm also trying to solve it myself--a similar 1D table approach should be possible." CreationDate="2015-10-17T16:53:08.640" UserId="523" />
  <row Id="1242" PostId="1633" Score="0" Text="Is this per-frame? As in a prepass to fill the depth buffer?" CreationDate="2015-10-18T05:49:56.820" UserId="523" />
  <row Id="1244" PostId="1585" Score="0" Text="If you don't count ray casting (which you shouldn't), this was the first ray tracing paper. So reflections/refractions/shadows have all been here since literally the very beginning." CreationDate="2015-10-18T06:20:46.497" UserId="523" />
  <row Id="1245" PostId="1579" Score="0" Text="Because we currently render movies using . . . magic?" CreationDate="2015-10-18T06:21:44.070" UserId="523" />
  <row Id="1246" PostId="1633" Score="0" Text="Nope, done in the loading screen. Not talking about a depth only render!" CreationDate="2015-10-18T14:16:14.810" UserId="56" />
  <row Id="1247" PostId="1637" Score="0" Text="In my understanding, raycasting isnt very good for boundary information. Depthmaps also dont dont benefit from raycasting. Deepshadow maps might benefit from raycasting. Otherwise you would just be better of baking lightning to model uv maps or some kind of voxel tree. Possibly per vertex/face. The question is a bit hard to swallow, could you specify it a bit. Best is not defined, efficient in this case is not terribly defined either" CreationDate="2015-10-19T06:15:54.390" UserId="38" />
  <row Id="1248" PostId="1637" Score="0" Text="Are you asking for something geometric such as Franklin Crow's [Projected Shadow Volumes](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.424.6834)?" CreationDate="2015-10-19T12:19:21.930" UserId="209" />
  <row Id="1249" PostId="1599" Score="0" Text="Any work done on single image, not with Coded Aperture? Thank You." CreationDate="2015-10-19T15:08:50.363" UserId="234" />
  <row Id="1251" PostId="1637" Score="0" Text="Please fill in the &quot;etc&quot; and tell us the purpose of storing the shadow data." CreationDate="2015-10-19T21:07:33.990" UserId="1703" />
  <row Id="1252" PostId="1637" Score="0" Text="I apologize for being so vague.. Its basically for an application where I want to be able to move the shadow around manually to see the change in the 3d objects position. Say I apply a force to the shadow of a ball, I can actually see the ball move and also the change in the shadow. So I need to be able to set up colliders for the shadow and hence need a way to be able to store the boundary of the shadow and information about the object that cast it to carry out the actions." CreationDate="2015-10-20T01:03:27.893" UserId="1879" />
  <row Id="1253" PostId="1637" Score="0" Text="Can I infer that &quot;etc&quot; is void ? What other information is relevant ?" CreationDate="2015-10-20T08:49:06.520" UserId="1703" />
  <row Id="1255" PostId="1639" Score="0" Text="Why not trace on click? There sa really neat trick if there is less than 32-96 objects." CreationDate="2015-10-20T14:48:37.183" UserId="38" />
  <row Id="1256" PostId="375" Score="0" Text="In a word: evil." CreationDate="2015-10-20T15:21:08.137" UserId="523" />
  <row Id="1257" PostId="379" Score="0" Text="@ratchetfreak only if you're in space, which you're typically not. Our glorious atmosphere makes it appear yellowish (by scattering the blue)." CreationDate="2015-10-20T15:23:56.237" UserId="523" />
  <row Id="1259" PostId="1637" Score="0" Text="@ichigo1191 your comments are giving a lot more detail than the question, which makes a big difference. I recommend you [edit] the question to include the extra detail all in one place. Comments are not guaranteed to last forever." CreationDate="2015-10-20T15:40:47.627" UserId="231" />
  <row Id="1260" PostId="1639" Score="0" Text="@joojaa: no idea what you mean." CreationDate="2015-10-20T15:55:13.470" UserId="1703" />
  <row Id="1261" PostId="1641" Score="0" Text="Thank you for the reply. I am going to replicate what I did and post a screen as suggested. Anyway the final intent of this question isn't to debug my code, but to understand if what I am doing is right or there are others steps that I am missing in between!" CreationDate="2015-10-20T15:55:23.550" UserId="1895" />
  <row Id="1262" PostId="1639" Score="0" Text="Why make a buffer of rays? If your only interested in one ray, shoot the ray when your needing it. Its fast enough. Why shoot a shadow buffer with rays just use a separate pass and use depth maps + index color. Again you can do this on demand. If you have 3 color channels you can use a bit mask for surfaces hit tahtway you can store ALL objects in pass of ray if you must etc. Too many open variables in question to nake any meaningful impact." CreationDate="2015-10-20T16:21:27.457" UserId="38" />
  <row Id="1264" PostId="1639" Score="0" Text="@joojaa: I guess this comment belongs to the OP. I am answering the question &quot;how to store ?&quot;." CreationDate="2015-10-20T17:05:27.403" UserId="1703" />
  <row Id="1265" PostId="1642" Score="0" Text="Wow, i want to die now :(&#xA;I even said i wanted gl_GlobalInvocationID to behave like gl_FragCoord, which uses uses window coordinates.&#xA;This was one of the most stupid error ever.&#xA;&#xA;You saved my day and my sanity." CreationDate="2015-10-20T21:56:09.980" UserId="281" />
  <row Id="1266" PostId="1641" Score="0" Text="added examples as suggested!" CreationDate="2015-10-21T11:38:50.710" UserId="1895" />
  <row Id="1267" PostId="1644" Score="0" Text="and what's wrong with a 1024x512 texture that gets blitted to the display buffer by the hardware?" CreationDate="2015-10-21T16:03:15.850" UserId="137" />
  <row Id="1268" PostId="1644" Score="0" Text="Nothing is inherently wrong with that, but what about smaller operations like outputting a single character? Also some smaller systems might not have a GPU, etc." CreationDate="2015-10-21T16:16:38.943" UserId="1902" />
  <row Id="1269" PostId="1644" Score="3" Text="This used to be a thing in VGA days. 0xA0000 anyone?" CreationDate="2015-10-21T16:59:42.720" UserId="48" />
  <row Id="1271" PostId="1641" Score="0" Text="The points from the poisson sampling are right. The algorithm that generates them is fully unit tested and the ones you see in the screens are spheres with the center in the sampled point which i programmatically created before calling Voronoi(points)! I am worried that I am not following the proper path or I am handling the Voronoi result in a wrong way" CreationDate="2015-10-21T20:10:46.213" UserId="1895" />
  <row Id="1272" PostId="1641" Score="0" Text="The images you show have done the voronoi on the 2d function." CreationDate="2015-10-22T03:57:12.260" UserId="38" />
  <row Id="1273" PostId="1641" Score="0" Text="@joojaa From the example images I expected that the Voronoi cell edges on the 2D surface were what was required (to give a collection of line segments connecting points on the sphere surface, rather than the collection of plane sections that would be given in 3D). However, [scipy.spatial.Voronoi](http://scipy.github.io/devdocs/generated/scipy.spatial.Voronoi.html) seems to be designed for N dimensional spaces rather than surfaces embedded in them. I can't immediately see how it would be used for 3D points constrained to a 2D surface." CreationDate="2015-10-22T10:07:57.903" UserId="231" />
  <row Id="1274" PostId="1641" Score="0" Text="Is not a problem to me if I have to change library or to write my own implementation.  If you can suggest any source I need to look at I will be happy to do it!" CreationDate="2015-10-22T10:20:03.673" UserId="1895" />
  <row Id="1275" PostId="1641" Score="0" Text="I'm guessing you would need to implement a distance function for two points on a surface. On the sphere you can get away with just using the 3D Euclidean distance but for arbitrary shapes you'll need something more specific otherwise points that are near to each other in 3D due to folds in the surface will appear nearer to each other than to points between them." CreationDate="2015-10-22T14:29:47.847" UserId="231" />
  <row Id="1276" PostId="1641" Score="0" Text="Do you mean to sample points? I am using isotopic distance to accomplish this. Anyway as you see, my problem is with the Voronoi computation, not the initial sampling procedure, but maybe I didn't understand what you were saying!" CreationDate="2015-10-22T14:36:31.333" UserId="1895" />
  <row Id="1278" PostId="1641" Score="1" Text="Maybe I found the solution to my problems in this paper http://compbio.mit.edu/publications/C01_Amenta_Siggraph_98.pdf . I'll try to implement it." CreationDate="2015-10-22T16:08:11.123" UserId="1895" />
  <row Id="1281" PostId="1622" Score="0" Text="What if you remove the `layout` stuff from the shaders?" CreationDate="2015-10-22T18:47:39.383" UserId="1881" />
  <row Id="1282" PostId="1641" Score="0" Text="No I meant the Voronoi step - converting cell centres (the Poisson sampled points) into cell edges. This also requires a length calculation in order to determine which line is equidistant from a given two points." CreationDate="2015-10-22T20:37:01.267" UserId="231" />
  <row Id="1284" PostId="153" Score="0" Text="+1 So many answers to things I've always wondered about!" CreationDate="2015-10-22T20:53:48.117" UserId="457" />
  <row Id="1287" PostId="1644" Score="0" Text="1024 x 512 x 4 bytes = 2MB. Peanuts in a 2GB address space." CreationDate="2015-10-23T13:37:03.110" UserId="1703" />
  <row Id="1294" PostId="1651" Score="0" Text="Now this appears to be Mac only, which may not work for you with Direct3D, but [Syphon Inject](http://syphon.v002.info) which OBS uses on Mac can capture game windows that use OpenGL _much_ faster than standard video capture.  Syphon &quot;allows applications to share frames - full frame rate video or stills - with one another in realtime&quot; to quote the website." CreationDate="2015-10-24T22:45:32.967" UserId="1922" />
  <row Id="1296" PostId="1652" Score="0" Text="Thanks for the answer. I think that ultimately, I will instead of saving images, stream them to a remote machine, so copying the framebuffer may be the way to go. You seem to have a fair bit of experience with this stuff. Are you able to expand on any of the questions that I posed? Thanks for the help!" CreationDate="2015-10-25T10:10:03.303" UserId="1800" />
  <row Id="1297" PostId="1651" Score="0" Text="@Gliderman Thanks, I will check that out!" CreationDate="2015-10-25T10:10:32.563" UserId="1800" />
  <row Id="1298" PostId="1652" Score="1" Text="@pookie Hey! Sure, I can expand this. Is there anything specifically you'd like me to comment on? For your questions, I think 1&amp;2 can be done with the DLL hooks, so be sure to read the links I've included (specially the one about starcraft). 3&amp;4 will be API specific, but on OpenGL, you can start with glReadPixels and work from there if you find it to be too slow. My experience is from recording frames from my own games plus some DLL hacking done in the past, but I've never combined both in the attempt of recording frames from a third party app ;)" CreationDate="2015-10-25T17:13:52.243" UserId="54" />
  <row Id="1299" PostId="1652" Score="0" Text="Thank you! I've checked the links - windows DLL injection and starcraft link (pretty awesome) seem to be good starting points. I will take a deapre look into glReadPixels, too. In your opinion, which did you find easier to work with and which did you find faster: DirectX or OpenGL?" CreationDate="2015-10-25T17:23:39.877" UserId="1800" />
  <row Id="1300" PostId="1652" Score="0" Text="@pookie, if you have some programming experience, I think Direct3D will be much easier to use, the libraries are better designed. OpenGL has too may &quot;rough edges&quot; in my opinion (this coming from someone that uses OpenGL more than D3D). D3D is Windows only, so that might be a negative for you, don't know. Performance-wise, I don't think there's going to make much of a difference which one you use. The best advice I can probably give you is: first figure out how to get it working, then worry about optimizations." CreationDate="2015-10-25T18:08:27.037" UserId="54" />
  <row Id="1301" PostId="1652" Score="0" Text="Thanks, I've a fair bit of c# knowledge - not sure how I will take to D3D, but I will give it a stab." CreationDate="2015-10-25T18:11:45.210" UserId="1800" />
  <row Id="1302" PostId="1652" Score="1" Text="Ah, ok, if you're familiar with OOP programming, then D3D will definitely be easier. OpenGL is all procedural and uses a bunch of global state, that's mostly what I meant about rough edges. Best of luck to you!" CreationDate="2015-10-25T18:30:15.723" UserId="54" />
  <row Id="1306" PostId="1656" Score="2" Text="Code looks right on a first glance. Are you checking for GL errors? Do you have [KHR_debug](http://renderingpipeline.com/2013/09/opengl-debugging-with-khr_debug/) output set up?" CreationDate="2015-10-26T02:41:25.983" UserId="48" />
  <row Id="1308" PostId="1641" Score="0" Text="These articles look relevant to what you are trying to achieve:&#xA;http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html&#xA;http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html" CreationDate="2015-10-26T03:19:08.577" UserId="182" />
  <row Id="1311" PostId="1657" Score="5" Text="I'm voting to close this question as off-topic because [current consensus](http://meta.computergraphics.stackexchange.com/q/146/16) seems to be that asking for literature (or similar) recommendations is not a good fit for the site." CreationDate="2015-10-26T08:51:24.840" UserId="16" />
  <row Id="1312" PostId="1656" Score="0" Text="Updated my question." CreationDate="2015-10-26T09:11:02.463" UserId="214" />
  <row Id="1314" PostId="1658" Score="0" Text="It's the book. :-)" CreationDate="2015-10-27T00:30:18.227" UserId="1886" />
  <row Id="1316" PostId="1662" Score="0" Text="If it's a read-only lookup table, you can just use a buffer/texture. You could either pack it into one of the normal texture formats, or you can use some of the newer features of DX11 / OpenGL to have a custom format. UAV in DX11 land, or a texture / shader_image_load_store in OpenGL land." CreationDate="2015-10-27T17:33:49.673" UserId="310" />
  <row Id="1318" PostId="1662" Score="0" Text="In addition, give this presentation a look: https://www.cvg.ethz.ch/teaching/2011spring/gpgpu/cuda_memory.pdf It's for CUDA, but it should give you a better idea of what is happening on the underlying hardware" CreationDate="2015-10-27T17:41:58.977" UserId="310" />
  <row Id="1319" PostId="1663" Score="0" Text="In your code sample the instance buffer is empty, is this intended? Also there is no vertex buffer binding (either glBindBufferRange​ or the newer glBindVertexBuffer)." CreationDate="2015-10-27T22:21:27.677" UserId="528" />
  <row Id="1320" PostId="1663" Score="0" Text="I'm filling the data into the second paragraph, also, why would I need to call glBindBufferRange?&#xA;&#xA;Isn't that just needed for UBOs / SSBOs?" CreationDate="2015-10-28T11:32:11.837" UserId="1938" />
  <row Id="1321" PostId="1665" Score="0" Text="Amazing, just finished implementing it and Element Buffers are the way to go." CreationDate="2015-10-28T16:46:49.410" UserId="116" />
  <row Id="1322" PostId="1663" Score="0" Text="Oh sorry, overlooked the buffer filling. And I also remembered the glBindBufferRange wrong since I was by now too much used to the newer glBindVertexBuffer semantics. But there should be a glBindBuffer for the per-vertex data somewhere between your VertexAttrib calls, right?" CreationDate="2015-10-28T19:43:50.567" UserId="528" />
  <row Id="1323" PostId="1663" Score="0" Text="Yes. As you can see, my per-instance's data's vertex attrib locations start at 3, that's because, previously, from 0 to 2, I've defined my per-vertex data. I just posted the instancing related parts of the code." CreationDate="2015-10-29T04:38:49.910" UserId="1938" />
  <row Id="1324" PostId="1663" Score="0" Text="Sorry for double posting, however, strangely enough, [this](http://imgur.com/KgVN4L1) is what happens when I turn on the &quot;wireframe mode&quot;. The middle quad gets cut." CreationDate="2015-10-29T05:01:15.347" UserId="1938" />
  <row Id="1326" PostId="1669" Score="2" Text="Any context? Or reference where you have read that? Because it does not make sense to me. Plus, if I'm not mistaken, Gn continuity is defined only for piece-wise polynomial surfaces, there is no reason for a surface to be polynomial and in practice most of the surfaces are piece-wise linear." CreationDate="2015-10-30T14:29:20.230" UserId="1613" />
  <row Id="1327" PostId="1669" Score="2" Text="G2 just mentions the geometric n-derivability, independently of any parameterisation." CreationDate="2015-10-30T20:06:05.620" UserId="1810" />
  <row Id="1329" PostId="1671" Score="0" Text="I would be more modest on general claims about &quot;textures can be modeled as&quot;. I would rather say &quot;some (restricted) famillies of textures can&quot;." CreationDate="2015-10-31T09:20:29.293" UserId="1810" />
  <row Id="1330" PostId="1672" Score="0" Text="[Skeletal Animation](https://en.wikipedia.org/wiki/Skeletal_animation)" CreationDate="2015-10-31T17:48:22.650" UserId="54" />
  <row Id="1331" PostId="1671" Score="0" Text="It would help if you could add some links to explain the concepts you are referring to, like Markov Random Fields and texture neighborhoods. Otherwise, we're just guessing what you're talking about." CreationDate="2015-10-31T21:08:47.123" UserId="48" />
  <row Id="1332" PostId="1669" Score="0" Text="@tom He is talking of general surfece design like in CAD. No they dont need to be polynomials, but in practice they often are (except for circular arcs and conics)" CreationDate="2015-11-01T08:00:08.843" UserId="38" />
  <row Id="1333" PostId="1672" Score="0" Text="I assume the question is really how are transformation matrices interpolated. http://stackoverflow.com/questions/3093455/3d-geometry-how-to-interpolate-a-matrix" CreationDate="2015-11-01T08:50:02.107" UserId="457" />
  <row Id="1334" PostId="1673" Score="1" Text="I edited the question to include the image, code and link to the other question. However, in my opinion, it would have been better to edit the other question and include the new things you found out there. It is also encouraged to update the question to include what you have tried to solve the problem even after you posted it. [See our Help Center](http://computergraphics.stackexchange.com/help/no-one-answers)." CreationDate="2015-11-01T11:38:06.240" UserId="127" />
  <row Id="1336" PostId="1673" Score="0" Text="Oh, I did not know about this. Thank you for both editing my questions and providing advice." CreationDate="2015-11-01T14:26:15.407" UserId="1938" />
  <row Id="1337" PostId="1669" Score="0" Text="@joojaa Than I'm still puzzled why the use of special notation Gn. In mathematics there is standard notion of Cn differentiable manifold. So is Gn and Cn the same? I thought that Gn manifold is piece-wise polynomial, so it is C-infty manifold except at the patch seams." CreationDate="2015-11-01T16:48:00.427" UserId="1613" />
  <row Id="1339" PostId="1672" Score="0" Text="Or have a look at screw theory." CreationDate="2015-11-01T22:06:49.970" UserId="1613" />
  <row Id="1340" PostId="1675" Score="4" Text="your question is too vague. &quot;design&quot;, &quot;implement&quot; = the rendering only ? now there are several aspects in rendering: intersecting the shape, surface materials, volume materials, light transport... the strange thing here is that the caustics on the floor seems very accurate, but the glass aspect seem very non-physical. Anyway, yes, ray-based algorithms are involved at many place here.  Anyway people would more consider design = shape and motion. (+ textures, when present)." CreationDate="2015-11-02T13:11:45.747" UserId="1810" />
  <row Id="1341" PostId="1671" Score="0" Text="Do you mean &quot;Texture Synthesis&quot; in which a large texture can be generated from a small exemplar?" CreationDate="2015-11-02T15:14:52.437" UserId="209" />
  <row Id="1342" PostId="1675" Score="0" Text="Do you mean the mathematical shape?" CreationDate="2015-11-02T18:36:39.023" UserId="38" />
  <row Id="1344" PostId="1529" Score="0" Text="I have actually designed and implemented (with help of my colleague) an algorithm to create 3D mesh of the city from the data that is collected (for a competitor company). However, what you're asking would require breaking NDA and I don't believe any sane person would be willing to do that, so your question can only be truly answered by developers who have worked on something like this in their spare time and thus are not bound by legal paperwork (regardless of whether they still work for that company or not)." CreationDate="2015-10-07T18:35:40.373" UserId="1820" />
  <row Id="1345" PostId="1562" Score="0" Text="Since you are using SceneKit, would `SCNShape` be useful to you? It can create a 3D mesh with normals and texture coordinates by extruding a Bézier path." CreationDate="2015-10-13T22:17:45.723" UserId="1852" />
  <row Id="1347" PostId="1669" Score="0" Text="@tom C continuity is the parametric continuity and G is the geonetric continuity and in this case the continuity over 2 separate geometries." CreationDate="2015-11-03T05:19:53.993" UserId="38" />
  <row Id="1348" PostId="420" Score="1" Text="@MichaelIV Nevertheless, Alan Wolfe has a point. You could greatly improve your answer by including relevant code in the answer itself to make it self-contained and future-proof against the link going dead some day." CreationDate="2015-11-03T08:45:30.887" UserId="16" />
  <row Id="1349" PostId="1589" Score="0" Text="This seems to have a couple of problems. a) It's not entirely clear whether you're the developer or just a player of the game (in the latter case, this is clearly off-topic, as this site is about computer graphics *programming* and *research*). b) If you are the developer this doesn't seem to be actually about graphics but more about the network code of your game. In that case, you might want to try [GameDev.SE](http://gamedev.stackexchange.com/) or [so] but be sure to read their respective help centres to make sure your question is of high-quality and on-topic." CreationDate="2015-11-03T08:50:00.717" UserId="16" />
  <row Id="1350" PostId="1624" Score="0" Text="(A belated) Welcome to Computer Graphics SE! I'm afraid questions about 3D printing are not on topic here, as this site is about computer graphics programming and research. Unfortunately, I can't think of any better Stack Exchange to redirect you to either. There *is* [a proposal for a 3D Printing SE](http://area51.stackexchange.com/proposals/82438/3d-printing-and-rapid-prototyping) in the commitment phase though, so you might want to keep an eye on that." CreationDate="2015-11-03T08:54:32.170" UserId="16" />
  <row Id="1351" PostId="1672" Score="0" Text="Welcome to Computer Graphics SE! I think there's a good and interesting question there, but currently it's not clear what exactly is being asked. Please consider editing some more detail into your question and also show what you've tried/found out yourself." CreationDate="2015-11-03T08:56:22.190" UserId="16" />
  <row Id="1352" PostId="1675" Score="0" Text="Welcome to Computer Graphics SE! As Fabrice's and joojaa's comments show it's not entirely clear what you're asking. Please include some more detail whether your question is about the rendering (refraction and caustics), or the geometry or something else. It might also help to include the source of the image." CreationDate="2015-11-03T08:59:23.493" UserId="16" />
  <row Id="1353" PostId="1621" Score="1" Text="The original vector is  $(2/(x+y),(5y+z)/(2x+2y),3)$ while your expansion assumes  $(2/(x+y),5(y+z)/(2x+2y),3)$" CreationDate="2015-11-03T13:47:17.367" UserId="137" />
  <row Id="1354" PostId="1486" Score="0" Text="@ratchet, MathJax, yeah!" CreationDate="2015-11-03T15:13:15.963" UserId="192" />
  <row Id="1355" PostId="1621" Score="0" Text="@ratchetfreak Thanks for your comment. I corrected my mistake." CreationDate="2015-11-03T15:36:09.700" UserId="127" />
  <row Id="1356" PostId="258" Score="0" Text="N.B. this fact is also used when deriving theoretical bounds for [anisotropic diffusion](https://en.wikipedia.org/wiki/Anisotropic_diffusion)." CreationDate="2015-11-03T17:12:34.340" UserId="523" />
  <row Id="1357" PostId="1679" Score="0" Text="thank you.. I shall look into that too :)" CreationDate="2015-11-03T19:02:25.193" UserId="1879" />
  <row Id="1358" PostId="317" Score="0" Text="Direct3D is also what you would use if you're programming for any of the Xbox consoles." CreationDate="2015-11-03T23:37:08.393" UserId="1989" />
  <row Id="1359" PostId="295" Score="1" Text="ratchet freak's answer is a good one.  If you want to learn Direct3D you can read my free book [&quot;The Direct3D Graphics Pipeline&quot;](https://legalizeadulthood.wordpress.com/the-direct3d-graphics-pipeline/).  The API details are mostly Direct3D9, but the concepts are the same.  The thing is, the concepts for programming 3D graphics haven't really changed at all since the late 1960s.  The APIs for expressing those concepts have gotten much smarter." CreationDate="2015-11-03T23:38:58.073" UserId="1989" />
  <row Id="1360" PostId="1687" Score="0" Text="obj is simple enough to write a parser for, collada also has free libs to parse it." CreationDate="2015-11-04T22:16:51.503" UserId="137" />
  <row Id="1362" PostId="1691" Score="1" Text="Yes, it makes a difference. As an extreme example, suppose the incident light spectrum is a delta function at 600 nm and the reflectance of the surface is a delta function at 610 nm. Then the exitant light is zero, but if you convert both spectra to XYZ or RGB that's not what you will get." CreationDate="2015-11-05T11:01:38.407" UserId="106" />
  <row Id="1363" PostId="1691" Score="0" Text="@Rahul True. I changed the question to reflect that I'm less interested in these kind of contrived cases and more in the usual, real-world (somewhat) continuous spectra." CreationDate="2015-11-05T11:37:42.180" UserId="385" />
  <row Id="1364" PostId="1691" Score="0" Text="I think this previous question covers a lot of similar ground: [Are there common materials that aren't represented well by RGB?](http://computergraphics.stackexchange.com/q/203/106) Most of the answers apply equally well to any three-component colour space, like XYZ." CreationDate="2015-11-05T12:22:43.103" UserId="106" />
  <row Id="1365" PostId="1691" Score="1" Text="I understand that there are &quot;Effects for which the path of a ray is dependent on its wavelength&quot;, and I've explicitly excluded those. I also understand that there are &quot;Colours that the human eye can detect that cannot be displayed in RGB&quot;. Since I will end up in sRGB anyway, I can't change that. The question holds: do the intermediate calculations yield the same results _in common cases_ (no sodium lamps, no delta functions, just wide-range-of-frequencies stuff)?" CreationDate="2015-11-05T12:43:04.497" UserId="385" />
  <row Id="1366" PostId="1691" Score="1" Text="There will almost always be a difference in intermediate results when rendering using a three-component color space rather than the full spectrum. The pathological cases presented here just take that difference to an extreme. You say you don't care about sodium lamps, but you do care about wide-range-of-frequencies stuff - this makes this question very vague. Unless we know where your threshold is, we won't be able to answer this question." CreationDate="2015-11-05T13:08:26.880" UserId="79" />
  <row Id="1367" PostId="1691" Score="0" Text="@BenediktBitterli I'd be happy about some mathematical formulation of how to quantify the error, or a detailed description of the sources of error. Is early integration the only source, or does the inclusion of a reference illuminant pose a problem itself?" CreationDate="2015-11-05T13:18:27.120" UserId="385" />
  <row Id="1370" PostId="1693" Score="0" Text="It's entirely possible to run the pixel shader for overlapping triangles at the same time. All that matters is that the *blending* happens sequentially. Pixel shaders can kick off a lot of work to the blending unit in any order, which can then reorder all those blend operations depending on the triangle that it came from." CreationDate="2015-11-05T14:44:47.070" UserId="196" />
  <row Id="1371" PostId="1693" Score="0" Text="I've seen situations where flickering does occur due to multiple triangles lying in the same plane, but I don't remember if that happened while the camera/world/etc. transforms were static or if it required movement and recalculation. That's an edge case, though." CreationDate="2015-11-05T16:07:18.187" UserId="2000" />
  <row Id="1372" PostId="1693" Score="0" Text="@JAB It sounds like you're referring to [z-fighting](https://en.wikipedia.org/wiki/Z-fighting).  This usually occurs due to the fact that even if two primitives are analytically co-planar, quantization in the interpolator produces a pattern where some pixels from both primitives are visible.  The pattern will shift, causing flickering, only if the vertices (or camera) are moved, otherwise the interference pattern should be consistent between frames." CreationDate="2015-11-05T17:46:51.760" UserId="1992" />
  <row Id="1373" PostId="1693" Score="0" Text="@MooseBoys That's exactly it, yeah. It's been a few years since I did much with 3D graphics." CreationDate="2015-11-05T18:43:21.977" UserId="2000" />
  <row Id="1374" PostId="1682" Score="0" Text="Thanks for the advice, I chose the lazy solution for now. I use a function in the vertex shader that determines the wave attenuation from the distance from tha camera." CreationDate="2015-11-05T18:52:22.637" UserId="1987" />
  <row Id="1375" PostId="1693" Score="0" Text="Besides just running multiple vertices and pixels in parallel, it's also possible for multiple draw calls to be in flight at the same time, if enough GPU resources are available and there is no dependency between the draw calls." CreationDate="2015-11-05T21:15:30.600" UserId="48" />
  <row Id="1376" PostId="1694" Score="0" Text="There are many possibilities, so you should describe more:&#xA;&#xA;- What kind of material you aim at (basic Lambertian, or mirror specular  + glossy specular + multi-transparent layers + diffuse + subsurface scattering ?)&#xA;&#xA;- What paradigm of &quot;light particule&quot; you want to stick with (monochromatic photons, polychromatic photons, chuck of light ray) &#xA;&#xA;- What paradigm of &quot;light transport algorithm&quot; you want to stick with (from pure stochastic with each basic behavior in 0/1, to full valuated)." CreationDate="2015-11-06T15:45:31.220" UserId="1810" />
  <row Id="1377" PostId="1694" Score="0" Text="@FabriceNEYRET I updated the post. But what do you mean under monochromatic photons, polychromatic photons, full valuated light transport algorithm?" CreationDate="2015-11-06T16:29:05.437" UserId="386" />
  <row Id="1378" PostId="1694" Score="4" Text="a monochromatic photon (i.e. a &quot;real photon&quot;) die or not, but cannot fade. a polychromatic photon is a bag of synchronized photon, each can have his own fate.  &quot;full valued&quot; means that you transport the faded amount of energy (i.e. the optical geometry flux) rather than throwing coins to either absorb or reflect unfaded." CreationDate="2015-11-06T16:37:54.630" UserId="1810" />
  <row Id="1392" PostId="1701" Score="0" Text="Can you give an example with a 3d modeling software ?" CreationDate="2015-11-09T00:41:21.270" UserId="1636" />
  <row Id="1393" PostId="1700" Score="3" Text="Following from your comment on the answer, could you clarify whether you want to know about a theoretical approach, how to achieve this programmatically or how to achieve this in modelling software? This Stack Exchange is specifically about computer graphics programming and theory. If your question is specifically about how to achieve the effect in modelling software, I'm afraid your question is off topic. It makes a good question as a general CG problem though." CreationDate="2015-11-09T09:14:42.337" UserId="16" />
  <row Id="1397" PostId="1701" Score="3" Text="@Valerio this is not a forum for how to use modeling software." CreationDate="2015-11-09T14:03:21.607" UserId="38" />
  <row Id="1399" PostId="138" Score="0" Text="[Also see this related question on GameDev SE.](http://gamedev.stackexchange.com/q/111051/19876)" CreationDate="2015-11-10T09:05:28.537" UserId="16" />
  <row Id="1403" PostId="1709" Score="0" Text="Thanks, the docs recommended a fuzz factor. Combined with shaving border artifacts it worked perfectly. `convert original.png -shave 5x5 -fuzz 1% -trim +repage trimmed.png`" CreationDate="2015-11-12T19:13:30.373" UserId="2040" />
  <row Id="1407" PostId="1711" Score="3" Text="You could improve this answer by elaborating a bit, or even just linking to something that explains further." CreationDate="2015-11-12T20:41:33.173" UserId="48" />
  <row Id="1409" PostId="1711" Score="1" Text="One way of expanding this answer would be to address the drawbacks the author mentions, and explain how your approach helps with those." CreationDate="2015-11-12T20:46:54.930" UserId="231" />
  <row Id="1410" PostId="1714" Score="1" Text="Note that I might have answered a question that is slightly different from yours, but I did that on purpose as yours is clearly an homework question and I preferred to give you the tools to come up with an answer by yourself rather than giving you one directly :)" CreationDate="2015-11-13T09:35:22.210" UserId="100" />
  <row Id="1412" PostId="1716" Score="0" Text="Yes, algorithms are on topic, software programs aren't, and so I guess libraries are in-between? I was hoping for an answer along the lines of &quot;Oh, you need the &lt;maximal euclidian distance in XYZ colorspace&gt; algorithm. There's an implementation in xyz.js; go an study it there.&quot;" CreationDate="2015-11-13T12:50:10.797" UserId="2040" />
  <row Id="1414" PostId="1716" Score="0" Text="For your particular question, I suspect your main task will be choosing which [colour space](https://en.wikipedia.org/wiki/Color_space) best suits your purpose, which should simplify the remainder of the task." CreationDate="2015-11-13T14:18:27.197" UserId="231" />
  <row Id="1415" PostId="1720" Score="0" Text="Not super experienced here either but I suppose you could be using a different shader in one. Lambert's would look like the one on the right. But gourands would average them and make it appear smoother. http://prosjekt.ffi.no/unik-4660/lectures04/chapters/jpgfiles/3-shadings.jpg" CreationDate="2015-11-14T04:07:38.430" UserId="113" />
  <row Id="1416" PostId="1720" Score="1" Text="Thanks for your comment! Quite frankly, the only thing I did was to apply the Smoother modifier. There was no added shader before and I did not add any shader after." CreationDate="2015-11-14T04:09:48.693" UserId="2061" />
  <row Id="1417" PostId="1722" Score="0" Text="Great answer! That's exactly what I was looking for: a conceptual explanation that didn't leave crucial details out but was still clear and direct to the point. Many thanks" CreationDate="2015-11-16T00:47:13.523" UserId="2061" />
  <row Id="1418" PostId="1708" Score="0" Text="I'm closing this question as off-topic because it appears to be about using image processing software, not about computer graphics programming and research. [This might be on topic on Super User](http://superuser.com/questions/tagged/imagemagick)." CreationDate="2015-11-16T09:49:11.447" UserId="16" />
  <row Id="1419" PostId="1722" Score="2" Text="_&quot; The eye however is extremely sensitive to abrupt changes in color, and interprets that as a hard crease.&quot;_   Actually the human visual system is very good at detecting changes in the _derivative_ of the shading. The shading can be continuous but if there are discontinuities in the rate of change [as in this image](http://www.cs.ru.ac.za/research/Groups/vrsig/pastprojects/041machbands/image03.png) these can be surprisingly noticeable. Search for Mach band effect." CreationDate="2015-11-16T10:57:28.713" UserId="209" />
  <row Id="1420" PostId="1722" Score="1" Text="@SimonF thats why they see a crease, because they derive it edge detection and all that. But the human brain does not really eveluate the gradient flow 2 smooths are allmost equal to most humans (theres no second derviate sensing for example). So having  smooth sphere is smooth, even if the normals do not behave entirely spherically just as long as they are smooth. Thets why we get away with the trick. Very few surfaces actually behave this way." CreationDate="2015-11-16T11:59:40.677" UserId="38" />
  <row Id="1421" PostId="1722" Score="0" Text="_&quot;(theres no second derviate sensing for example)&quot;_ I just checked in  Glassner's &quot;Principles of Digital Image Synthesis&quot; (Volume 1 page 29)  ... and now I am more confused than ever." CreationDate="2015-11-16T12:41:54.293" UserId="209" />
  <row Id="1422" PostId="1722" Score="0" Text="@SimonF You may be confused about the reflections that are naturally one derivative lower than what the surface is. Thus a human can under certain conditions sense the second derivative. But the point is rather that humans can see creases, but they don't make meaningful difference between all different changes, the fact that a reflection is slightly off or in wrong direction isnt automatically apparent to a human. Without deeper analysis. Just as long as there's no abrupt change is for the most part good enough in many circumstances. We should continue this in the chat room though" CreationDate="2015-11-16T12:49:24.150" UserId="38" />
  <row Id="1424" PostId="1728" Score="0" Text="The algorithm in your link is very simple. It looks less sophisticated than some of the papers I found from the nineties. It looks like a good starting point, but I'm hoping for the highest-performance solution for a production system, not just a &quot;my first raytracer&quot;." CreationDate="2015-11-17T08:35:18.037" UserId="2041" />
  <row Id="1425" PostId="1725" Score="0" Text="Screen-space reflections: create a height-field using the depth and frame buffer, ray trace it to get crude reflections. I don't know about the details, but I'd imagine Crysis, Killzone, lately Frostbite etc. will have used some sophisticated technique to get it fast. Have you looked into this?" CreationDate="2015-11-17T10:15:27.807" UserId="385" />
  <row Id="1426" PostId="13" Score="0" Text="&quot;Whereas, in Monte Carlo ray tracing or simply path tracing, you sample only one ray in a direction preferred by the BRDF.&quot; Per se, you don't know how the ray is selected. Naive approaches use random rays. Taking the BRDF into account is importance sampling and not inherent to Monte Carlo ray tracing or path tracing." CreationDate="2015-11-17T10:20:32.100" UserId="385" />
  <row Id="1427" PostId="1729" Score="0" Text="dynamic indexing can cause issues in some drivers" CreationDate="2015-11-17T10:21:21.460" UserId="137" />
  <row Id="1428" PostId="1725" Score="1" Text="@DavidKuri Thanks, that's a good pointer for how to get the core ray-marching fast. There should be a lot of optimizations possible for a more static height-field that don't work so well on screen-space tracing, such as pre-computing mipmaps or a min-max quadtree, so I'm still hoping for an answer that covers that." CreationDate="2015-11-17T10:38:27.833" UserId="2041" />
  <row Id="1429" PostId="1728" Score="0" Text="This stuff is used in demoscene code and screenspace reflections in the most advanced modern games. The fastest code is sometimes the simplest. I wouldn't dismiss it due to its simplicity. It'll be interesting to see if you get any other responses though." CreationDate="2015-11-17T14:18:41.427" UserId="56" />
  <row Id="1430" PostId="1725" Score="0" Text="Hey Dan BTW are you looking for CPU or GPU solutions? And real time or non real time rendering?" CreationDate="2015-11-17T14:27:43.217" UserId="56" />
  <row Id="1431" PostId="1725" Score="0" Text="@AlanWolfe My use is GPU and non-real-time (i.e. max throughput rather than best image quality you can manage in 16 ms), but I'll still upvote interesting answers that are fast on the CPU or primarily for interactive renderers." CreationDate="2015-11-17T14:46:44.920" UserId="2041" />
  <row Id="1432" PostId="1725" Score="0" Text="You could try to create a signed distance field from the height map. Thats basically a 3d texture that stores the distance to the next surface. This allows to &quot;travel the ray faster&quot;. Unreal Engine 4 uses this for mid-range ambient occlusion, soft shadows and terrain shadows in general" CreationDate="2015-11-17T15:34:12.997" UserId="1888" />
  <row Id="1433" PostId="1730" Score="0" Text="is it possible for bode the modes to be set to 1? and if it shouldn't be are you guarding against it?" CreationDate="2015-11-17T16:25:10.333" UserId="137" />
  <row Id="1434" PostId="1730" Score="0" Text="for now I am assuming that the user is either in editMode or deleteMode. he doesn't press 'c' and 'd' together. if 'c' is pressed, it is made sure he exits from editMode before pressing ' d'." CreationDate="2015-11-17T16:27:28.313" UserId="1588" />
  <row Id="1435" PostId="1730" Score="0" Text="is that pseudo code or the actual code in your app (if so what language)?" CreationDate="2015-11-17T16:30:30.950" UserId="137" />
  <row Id="1436" PostId="1730" Score="0" Text="its c++. and this is the pseudo code giving the general idea of what I want to achieve. The actual code has some other flags for calling drawBeizer fn etc." CreationDate="2015-11-17T16:51:53.837" UserId="1588" />
  <row Id="1437" PostId="1721" Score="0" Text="Both answers are really good, it was hard for me to pick. Since I had asked about the simplest way, I think Nathan takes the cake." CreationDate="2015-11-17T17:28:29.103" UserId="14" />
  <row Id="1438" PostId="1730" Score="1" Text="It may help to try to remove everything from your code that you can, without removing the problem. This may highlight the cause for you, but even if it doesn't you can then edit the minimal code into your question, which will increase the chance of someone being able to spot the underlying cause." CreationDate="2015-11-17T17:44:41.283" UserId="231" />
  <row Id="1439" PostId="1730" Score="0" Text="Relevant discussion on Meta about [whether we should require a minimal working example](http://meta.computergraphics.stackexchange.com/questions/126/should-we-require-minimal-working-examples-mwes). This needs more attention to make clear the community consensus." CreationDate="2015-11-17T17:47:03.237" UserId="231" />
  <row Id="1440" PostId="1728" Score="2" Text="What's missing in your response is that IQ uses a standard heightfield mesh as an initial guess to kickstart raymarching the actual terrain. He first renders a low-poly version of the terrain using standard rasterization, and then runs a pixel shader over the image that starts raymarching at the rasterized depth minus some conservative threshold. This is the only way to actually make this realtime." CreationDate="2015-11-17T19:23:46.443" UserId="79" />
  <row Id="1441" PostId="1728" Score="0" Text="I believe that only part of what you are saying is true.  he does use heuristics based on terrain height (along with distance from camera) to come up with how far the ray can march, but as far as i have heard, he doesn't use rasterization.  Here is an example of his work, which does not use rasterization, but that isn't to say that there aren't implementations that DO use rasterization:  https://www.shadertoy.com/view/MdX3Rr" CreationDate="2015-11-17T19:27:12.690" UserId="56" />
  <row Id="1442" PostId="41" Score="0" Text="don't forget to work in linear color space for correct results." CreationDate="2015-11-18T00:57:19.487" UserId="1614" />
  <row Id="1443" PostId="1731" Score="1" Text="I was considering a similar solution to Nero's answer, but you've tagged this [tag:bezier-curve]. Are you limited to using Bezier curves with thickness, or is drawing circular arcs actually an option?" CreationDate="2015-11-18T09:05:37.100" UserId="16" />
  <row Id="1444" PostId="1732" Score="2" Text="+1 for the not at all intended pun" CreationDate="2015-11-18T09:44:50.190" UserId="2041" />
  <row Id="1445" PostId="1581" Score="0" Text="Besides vulnerabilities in image decoders (e.g. GDI+) it is possible to embed code e.g. AFTER the EOF marker on a JPEG. In this case the custom code is in same image file but technically not part of the image. See example in this document re: &quot;hammertoss&quot; malware: https://www2.fireeye.com/rs/848-DID-242/images/rpt-apt29-hammertoss.pdf" CreationDate="2015-11-18T17:12:26.290" UserId="2081" />
  <row Id="1446" PostId="1581" Score="0" Text="Related: http://security.stackexchange.com/questions/55061/can-malware-be-attached-to-an-image" CreationDate="2015-11-18T17:12:55.610" UserId="2081" />
  <row Id="1447" PostId="1732" Score="0" Text="Yes, that's what I'm after, but there's still an overlap. If it helps, I'm using Adobe After Effects’ shape layers and expressions to create this. Rectangle 1 is driving Rectangle 2's round corner value.&#xA;&#xA;e.g.&#xA;&#xA;Rectangle 1&#xA;Round Corners = 30&#xA;&#xA;Rectangle 2&#xA;Stroke width = 40&#xA;Round Corners = 60 (30*2)&#xA;&#xA;produces this:&#xA;![rectangles2](https://dl.dropboxusercontent.com/u/1414976/rectangles2.png)&#xA;&#xA;I've set the opacity of Rectangle 1 to 50% so you can see the overlap. The darker purple is the edge of Rectangle 2's stroke.&#xA;&#xA;I can supply by After Effects file if it will help." CreationDate="2015-11-18T20:37:37.240" UserId="2076" />
  <row Id="1448" PostId="1732" Score="0" Text="@GregGunn If your inner/blue box has a border radius of $30px$ and the red stroke's width is $40px$, then the radius in the middle of the red stroke needs to be $30px + (40px/2) = 50px$." CreationDate="2015-11-18T20:46:16.047" UserId="127" />
  <row Id="1449" PostId="1732" Score="0" Text="@Nero Nailed it. That equation works perfectly—thank you." CreationDate="2015-11-18T20:58:43.250" UserId="2076" />
  <row Id="1454" PostId="1737" Score="1" Text="Thanks Nathan that's exactly what I was after, where did you get this diagram may I ask, is it in any public documentation from oculus ?" CreationDate="2015-11-19T05:34:35.090" UserId="288" />
  <row Id="1455" PostId="1737" Score="1" Text="@GarryWallis Cass [tweeted it](https://twitter.com/casseveritt/status/608677561674149889) a few months ago." CreationDate="2015-11-19T05:37:24.127" UserId="48" />
  <row Id="1457" PostId="1734" Score="4" Text="Pro tip: `x * x` is much faster than `pow(x, 2.0f)`." CreationDate="2015-11-19T09:49:11.077" UserId="2041" />
  <row Id="1459" PostId="1728" Score="0" Text="I'm a little confused that the question is about ray tracing, and this answer is about ray marching. There is a fundamental difference between the two and what they can achieve." CreationDate="2015-11-19T12:35:05.170" UserId="182" />
  <row Id="1460" PostId="1674" Score="0" Text="What does, &quot;The change has to not only be locally satisfied but also satisfied on the retina,&quot; mean?" CreationDate="2015-11-19T13:05:03.053" UserId="2041" />
  <row Id="1461" PostId="1674" Score="0" Text="The eye is not recording a continious signal.  Its discrete, so even if your surface might technically meet the condition presented on a mathematical level. It might not be enough if the dicrete sample spacing does not see the change. So the slope still has to be big enough for human eye to notice." CreationDate="2015-11-19T13:38:05.733" UserId="38" />
  <row Id="1462" PostId="1674" Score="0" Text="It sounds like you're saying the derivative (of the normal) doesn't just have to be continuous, but its derivative has to be below some limit. If that's what you mean, I think that last paragraph of your answer could be clearer." CreationDate="2015-11-19T13:45:04.373" UserId="2041" />
  <row Id="1464" PostId="1674" Score="0" Text="@DanHulme its not a limit the derivate, its not a question of slope, only, but the interwall of the slope. So it is about a discrete sampling. So a very sharp angle but small difference in slope might seem continious. Likewise continious changes under a short interwall might seem sharp. Its not about mathematics its about sampling. Its just hard to qantify as its a biological system." CreationDate="2015-11-19T14:41:26.223" UserId="38" />
  <row Id="1465" PostId="1728" Score="0" Text="If you notice, the question mentions ray marching (through a grid) as an example algorithm that he has read about in the past." CreationDate="2015-11-19T14:46:45.220" UserId="56" />
  <row Id="1466" PostId="1738" Score="0" Text="Thank you very much for your help, I really appreciate you taking the time to write it.  I have implemented your suggestions.  Unfortunately, my shadows are still missing.  My most recent output is in the original qustion.  I'm assuming this means the problem is somewhere upstream in my code.  Of course, that's about 200-300 lines of setting my stage.  If you've got the stomach for it, I've put it on [pastebin](http://pastebin.com/03PHrUWJ).  If you don't, I totally understand.  I really appreciate your assistance; I've been so close to this for so long I think I've become blind to my mistakes" CreationDate="2015-11-19T23:55:15.083" UserId="2084" />
  <row Id="1467" PostId="1743" Score="4" Text="did you check the numbers with a simple triangle? 1,0,0; 0,1,0; 0,0,1 and ray from origin to 1,1,1" CreationDate="2015-11-20T08:54:33.727" UserId="137" />
  <row Id="1468" PostId="1741" Score="0" Text="Could you maybe add an example mesh and how it's being transformed? If you're actually moving the *vertices*, not the individual polygons, I'm not sure why they would pass through each other from a simple bending operation. Shouldn't the polygons on the concave side just shrink a bit?" CreationDate="2015-11-20T09:12:56.430" UserId="16" />
  <row Id="1469" PostId="1738" Score="1" Text="If you're still having trouble, just apply the usual debugging techniques. Keep simplifying your code to cut the problem space in half. Try removing all the shading computation and just set the colour to `intersections / 999999`, so your unshadowed intersections are just white. Draw an object at the light position to make sure it's correct: in both your renders, it looks like it might be in the bottom-right of the image, between the two objects. Put a breakpoint conditional on the shading co-ordinates and step through the shading of one particular point that you know ought to be in shadow." CreationDate="2015-11-20T09:19:35.913" UserId="2041" />
  <row Id="1470" PostId="1741" Score="0" Text="@MartinBüttner By the sound of it, he/she will get a similar problem to that of doing offset curves when the offset exceeds the radius of curvature. e.g. [look at the inner set of  green curves](https://en.wikipedia.org/wiki/Parallel_curve#/media/File:Evolute_and_parallel.gif) which have been displaced too far from the red." CreationDate="2015-11-20T10:44:10.333" UserId="209" />
  <row Id="1471" PostId="1741" Score="0" Text="Seems to me the subject lines ask about recalculating normals while the body  asks about self intersection." CreationDate="2015-11-20T12:50:07.063" UserId="38" />
  <row Id="1472" PostId="1744" Score="1" Text="This is a sensoring problem. The damage is done in the scanner. Photographees know a lot about these things. Its not really a computer graphics problem as a image capture problem. How to use software is out of scope." CreationDate="2015-11-20T12:52:50.583" UserId="38" />
  <row Id="1473" PostId="1744" Score="0" Text="@joojaa Maybe you have idea where to look out for help? Tried to find desktop publishing and scanning and graphic design forums, but no luck so far." CreationDate="2015-11-20T14:52:54.637" UserId="2100" />
  <row Id="1474" PostId="1744" Score="0" Text="Well there is [GD.Se](http://graphicdesign.stackexchange.com/)" CreationDate="2015-11-20T15:25:07.923" UserId="38" />
  <row Id="1475" PostId="1741" Score="1" Text="I am sorry, my subject and body are not coherent. This is because I am not sure of the exact terminology to use.  I think Simon F has interpreted my question as I intended though; I need to figure out how to handle the situation where the offset exceeds the radius of curvature.  I will upload a sketch momentarily." CreationDate="2015-11-20T16:16:57.443" UserId="2091" />
  <row Id="1478" PostId="1741" Score="0" Text="Ah yes, theres really nothing you can do about this kind of things except not bend too much." CreationDate="2015-11-20T20:53:45.187" UserId="38" />
  <row Id="1479" PostId="1745" Score="1" Text="I think you should add more details about how you actually created your color palette/table so users might be able to help you.  You might also consider one of the computer science-based Stack Exchanges." CreationDate="2015-11-19T20:29:48.510" UserDisplayName="honeste_vivere" />
  <row Id="1482" PostId="1730" Score="0" Text="This question is not exactly about OpenGL and Graphics but about GLUT and Input. Might be better suited for http://gamedev.stackexchange.com/" CreationDate="2015-11-21T23:31:38.017" UserId="528" />
  <row Id="1483" PostId="351" Score="0" Text="I don't think I get this. Isn't it basically just a minor difference in this case? E.g. what you said implies that the only difference would be that with albedo, the diffuse reflection is `(albedo * (1 - specular))` and specular is `albedo * specular`, instead of flat diffuse and specular numbers? I really don't get it :(" CreationDate="2015-11-22T04:11:00.777" UserId="2111" />
  <row Id="1484" PostId="1747" Score="0" Text="1080p is the new pixel art." CreationDate="2015-11-22T06:56:28.460" UserId="504" />
  <row Id="1485" PostId="351" Score="0" Text="@Llamageddon there are a number of differences covered in the answer but as a simple example: the albedo of a surface could be 0.8, but the RGB value of it's diffuse component could be (0.6, 0.5, 0.9). The albedo is generally just a single scalar value, whereas the diffuse component may have multiple values to give colour rather than just brightness." CreationDate="2015-11-22T12:44:58.633" UserId="231" />
  <row Id="1487" PostId="1729" Score="0" Text="Well, I guess that's the case, can't find a better explanation." CreationDate="2015-11-23T15:18:30.047" UserId="2064" />
  <row Id="1489" PostId="1751" Score="0" Text="What about when vertex shaders are used to deform meshes? (which I assume is possible... I'm a newbie to graphics programming)" CreationDate="2015-11-24T03:21:18.113" UserId="2111" />
  <row Id="1490" PostId="1751" Score="0" Text="Also, do you have any papers on GPU-backed occlusion culling? Is it a built-in feature of modern GPUs, or..?" CreationDate="2015-11-24T04:54:26.810" UserId="2111" />
  <row Id="1491" PostId="1751" Score="0" Text="@Llamageddon On modern GPU architectures, the vertex shader is always run, and is perfectly capable of deforming meshes—that's just transforming vertices non-rigidly. A more expensive vertex shader that does more work is, of course, more work that will be skipped by culling. I don't have any papers on GPU occlusion culling, academia seems to not be very enthralled with it. It is not a built-in feature of GPUs, just a creative use of compute." CreationDate="2015-11-24T08:02:07.733" UserId="196" />
  <row Id="1492" PostId="1751" Score="0" Text="Oh, I meant what about culling meshes that will be deformed? Do you cull them after running the vertex shaders? That sounds convoluted." CreationDate="2015-11-24T10:01:09.083" UserId="2111" />
  <row Id="1493" PostId="1751" Score="0" Text="@Llamageddon Generally you just cull them against a conservative volume. Or several smaller volumes that you deform with the mesh (for example, when skinning you can attach culling volumes to joints)." CreationDate="2015-11-24T16:43:05.913" UserId="196" />
  <row Id="1494" PostId="1751" Score="0" Text="Ah, I see. Would you mind getting in touch via some sort of IM/mail/direct messaging? I would really appreciate a mentor/tutor in graphics programmings stuff, since it's a topic that fascinates me greatly, but... various reasons, curb my ability to pursue it on my own." CreationDate="2015-11-25T04:58:56.910" UserId="2111" />
  <row Id="1496" PostId="1751" Score="0" Text="@Llamageddon The chat room for this site http://chat.stackexchange.com/rooms/26589/the-cornell-box is probably a good venue." CreationDate="2015-11-25T06:57:46.010" UserId="196" />
  <row Id="1497" PostId="1751" Score="0" Text="I know but I just am not good at chatrooms, I much prefer having someone to talk to and get to know x.x It's fine if you don't want to do anything such." CreationDate="2015-11-25T07:26:33.150" UserId="2111" />
  <row Id="1498" PostId="1757" Score="2" Text="I found this while searching around: http://neil-strickland.staff.shef.ac.uk/courses/algtop/pictures/sphere/ I think the animations help picture the homeomorphism between a sphere without poles and the plane/cylinder, so I thought you might want to include it" CreationDate="2015-11-26T09:02:46.857" UserId="16" />
  <row Id="1499" PostId="1757" Score="0" Text="Yeah i was planning on drawing a picture once in front of a computer." CreationDate="2015-11-26T09:10:07.183" UserId="38" />
  <row Id="1500" PostId="1754" Score="0" Text="If you solve it for still images, it ought to be the same solution for video frames right?  Also just to make sure, you are just trying to avoid the distortion at the poles when naively putting a texture on a sphere?" CreationDate="2015-11-26T18:12:40.187" UserId="56" />
  <row Id="1501" PostId="1759" Score="6" Text="+1 for showing that homework questions can be high-quality questions. :)" CreationDate="2015-11-27T12:55:58.820" UserId="16" />
  <row Id="1502" PostId="1754" Score="0" Text="I believe you are correct with the still images. However, I wasnt able to find any good material about that either.&#xA;&#xA;And yes, I'm trying mainly to solve the distortion at poles problem, since the fact that left side has to be identical to right side of the video for a good transition is a trivial problem with easy solution." CreationDate="2015-11-27T14:32:42.717" UserId="2138" />
  <row Id="1503" PostId="1754" Score="0" Text="Does this info help any? http://blender.stackexchange.com/questions/10741/what-is-the-best-way-to-unwrap-a-sphere" CreationDate="2015-11-27T15:54:58.697" UserId="56" />
  <row Id="1506" PostId="1761" Score="0" Text="Lines or line segments?" CreationDate="2015-11-29T18:33:25.253" UserId="197" />
  <row Id="1507" PostId="1762" Score="0" Text="Can we assume that the corners are 90 degree angles" CreationDate="2015-11-29T18:42:42.237" UserId="38" />
  <row Id="1508" PostId="1763" Score="0" Text="How would one calculate the aspect ratio if we weren't so close to the center?" CreationDate="2015-11-29T21:15:03.770" UserId="2162" />
  <row Id="1509" PostId="1763" Score="0" Text="@succubus there is a lengthy explanation [here](http://www.handprint.com/HP/WCL/perspect3.html) but you can do this with matrix calculation. Just didnt have time to outline the math." CreationDate="2015-11-29T22:20:47.103" UserId="38" />
  <row Id="1510" PostId="1761" Score="0" Text="It is only important, that the two new points are connected, therefore segments of curves are also OK. Question edited." CreationDate="2015-11-29T22:33:43.360" UserId="2161" />
  <row Id="1511" PostId="1756" Score="0" Text="&quot;..of a plane, cylinder or torus&quot; Klein bottle and real-projective plane feels left out :( Have a look at https://en.wikipedia.org/wiki/Fundamental_polygon" CreationDate="2015-11-29T22:58:50.367" UserId="1613" />
  <row Id="1512" PostId="1756" Score="0" Text="Mobius band is now crying..." CreationDate="2015-11-29T23:06:49.073" UserId="1613" />
  <row Id="1513" PostId="1757" Score="0" Text="Well your answer is not very consistent. If the sphere is not a sphere but a cylinder, because you are missing those two points, than the cylinder is not a cylinder but it is a plane, because you are missing the whole edge. And there are more than 3 topological families, by gluing different edges you can get sphere, cylinder, torus, mobius band, klein bottle, real-projective plane. Have a look at wiki page about Fundamental polygon." CreationDate="2015-11-29T23:13:41.173" UserId="1613" />
  <row Id="1514" PostId="1763" Score="0" Text="Thanks for your help, unfortunately I can't accept your answer because I messed up with my stackexchange account." CreationDate="2015-11-29T23:20:19.373" UserId="2162" />
  <row Id="1515" PostId="1761" Score="2" Text="You might look into path-finding algorithms for this. Use existing segments as obstacles and find a path between the two new endpoints. Maybe apply some smoothing to the resulting path to make it a nicer-looking curve." CreationDate="2015-11-29T23:36:38.467" UserId="48" />
  <row Id="1516" PostId="1764" Score="0" Text="Other tools include yEd (Free to use but no free licese), gephi... this is a NP Hard problem." CreationDate="2015-11-30T03:45:31.187" UserId="38" />
  <row Id="1517" PostId="1763" Score="0" Text="@succcubbus please refer to [official help page](http://computergraphics.stackexchange.com/help/merging-accounts) about merging your account to regain the ownership of the question." CreationDate="2015-11-30T10:38:01.683" UserId="2170" />
  <row Id="1518" PostId="1741" Score="0" Text="Well I'm pretty sure there is something that can be done, I just don't want to re-invent the wheel.  I'm pretty sure that professional applications like Maya have this solved.  Right now I'm thinking of something along the lines of checking normal directions before and after the transform to identify problem polys.  Perhaps the blender source code has something." CreationDate="2015-11-30T20:29:36.923" UserId="2091" />
  <row Id="1519" PostId="1767" Score="0" Text="Isn't this slightly under-constrained?   If you only have 3  coplanar(?),  non-colinear points, won't that allow you to have a circle? (Or do you also have the centre?)" CreationDate="2015-12-01T13:20:31.557" UserId="209" />
  <row Id="1520" PostId="1767" Score="0" Text="Are the three points any particular points on the ellipse? e.g. if one is guaranteed to be one end of the major axis, and another is one end of the minor axis, then the problem is trivial." CreationDate="2015-12-01T15:16:01.610" UserId="2041" />
  <row Id="1521" PostId="1767" Score="0" Text="Yeah, someone elsewhere pointed out it's underconstrained, and the Keplerian elements are probably more usable. That said, I can arrange for the points to be the end of the major and minor axes if that trivializes the problem." CreationDate="2015-12-01T16:43:47.290" UserId="1634" />
  <row Id="1522" PostId="1767" Score="3" Text="If you have the center and major/minor axis vectors, then the corners of the quad will just be center ± major ± minor." CreationDate="2015-12-02T01:10:19.490" UserId="48" />
  <row Id="1523" PostId="1766" Score="0" Text="[Please explain why you downvote](http://meta.stackexchange.com/questions/135/encouraging-people-to-explain-downvotes) when you do." CreationDate="2015-12-02T09:58:10.617" UserId="2173" />
  <row Id="1524" PostId="1770" Score="0" Text="I'm not, I'm referring to virtual texturing, most well known as idTech 5's [MegaTexture](https://en.wikipedia.org/wiki/MegaTexture) technology. Also see [this](http://holger.dammertz.org/stuff/notes_VirtualTexturing.html) and [this](http://silverspaceship.com/src/svt/). I've seen it mentioned in overview of many modern engines' rendering pipelines, and in a few papers that use a similar approach for shadowmaps. It does have a lot in common with texture atlases, yes, it uses them, in a way, but I'm not confusing it with texture atlases." CreationDate="2015-12-02T18:18:08.880" UserId="2111" />
  <row Id="1525" PostId="1770" Score="0" Text="Ahh. Thanks for the links. Can you add them to the question. I will update my answer accordingly" CreationDate="2015-12-02T18:49:41.500" UserId="310" />
  <row Id="1526" PostId="1770" Score="3" Text="IMO, the main drawback of simple texture atlases (not virtual textures) is you lose wrap modes like repeat and clamp, and bleeding occurs due to filtering/mipmapping - not floating-point precision. I'd be surprised to see float precision becoming a problem for ordinary (non-virtual) textures; even a 16K texture (the max allowed by current APIs) isn't big enough to really strain float precision." CreationDate="2015-12-02T20:58:17.627" UserId="48" />
  <row Id="1527" PostId="1765" Score="1" Text="&quot;What is the best solution&quot; is mostly a matter of opinion. Does your current solution work well enough, or are there problems with it that you're looking to solve? If so, what are those problems specifically?" CreationDate="2015-12-02T21:50:53.767" UserId="48" />
  <row Id="1528" PostId="1765" Score="0" Text="I wanted to make sure my method is fine. But I have a problem to project my 3D direction vector on the screen, so I asked [this question](http://computergraphics.stackexchange.com/questions/1766/how-to-use-getviewprojmatrix-transformvectorlinedirection-in-ue4)." CreationDate="2015-12-02T22:28:05.527" UserId="2173" />
  <row Id="1529" PostId="1766" Score="0" Text="I dont know why it was downvoted. but this seems like a close dupllicate to your other question." CreationDate="2015-12-02T23:48:17.757" UserId="38" />
  <row Id="1530" PostId="5" Score="0" Text="Hey OP, if possible, could you post stats of the results you got out of the accepted answer, if you did?" CreationDate="2015-12-03T09:00:19.020" UserId="2111" />
  <row Id="1531" PostId="5" Score="0" Text="@Llamageddon To be honest, I had this actual problem maybe two years ago or so and just dug it up from a past CG projects to come up with decent questions during the private beta. I haven't yet revisited said project, so I didn't get around to trying out the answer." CreationDate="2015-12-03T09:02:28.797" UserId="16" />
  <row Id="1532" PostId="1770" Score="0" Text="@RichieSams Btw, I think your answer is a good one, even if to a different question. You should make a Q&amp;A post." CreationDate="2015-12-03T09:29:11.247" UserId="2111" />
  <row Id="1533" PostId="1773" Score="0" Text="Hey, thank you for the excellent answer. I know this is typically frowned upon, but I have various issues, so I mostly just skim through things - to get an intuitive overview of topics for the future(I'm afraid properly learning and implementing things is out of my reach for the moment) - anyway, if possible, could you post a pseudocode example outlining the process itself, ideally, but not necessarily, illustrated?" CreationDate="2015-12-03T09:33:14.113" UserId="2111" />
  <row Id="1534" PostId="1774" Score="1" Text="about saving processing power: http://computergraphics.stackexchange.com/q/259/137" CreationDate="2015-12-03T10:22:03.977" UserId="137" />
  <row Id="1535" PostId="1772" Score="0" Text="Ok, thanks for this answer! I used `TransformVector` after reading the first paragraph of [this page about homogenous coordinates](http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/). I understood I should not worry about W when projecting a direction, but maybe I'm wrong." CreationDate="2015-12-03T11:30:33.810" UserId="2173" />
  <row Id="1536" PostId="1772" Score="0" Text="I thought about projecting both points on my screen, but I changed my mind because I thought it was not necessary." CreationDate="2015-12-03T11:34:06.963" UserId="2173" />
  <row Id="1537" PostId="1774" Score="0" Text="@ratchetfreak Interesting post. Stenciling isn't conditionals, though." CreationDate="2015-12-03T11:57:17.630" UserId="2111" />
  <row Id="1538" PostId="1774" Score="1" Text="But the GPU still groups (at least) 2x2 pixels together even if 3 out of 4 will fail the stencil test." CreationDate="2015-12-03T12:02:05.197" UserId="137" />
  <row Id="1539" PostId="1774" Score="0" Text="Are you certain about that? I'd imagine that that's what happens for conditionals, while with stenciles, the GPU is a bit smarter about it." CreationDate="2015-12-03T12:36:46.807" UserId="2111" />
  <row Id="1540" PostId="1774" Score="0" Text="Given that it will calculate pixels just off the triangle I doubt that it will cull the singular stencil-failing pixels." CreationDate="2015-12-03T14:40:19.997" UserId="137" />
  <row Id="1541" PostId="1774" Score="2" Text="@Llamageddon Pixel shaders are always packed together in 2x2 quads, because a pixel shader that doesn't sample at least one texture is a rare thing indeed. But if you are doing depth-only rendering, that matters less. Also, for what it's worth, shadow rendering is traditionally bound by vertices, so in traditional high-vertex-count situations this probably won't help. Also don't forget that you'd be writing to every pixel's stencil just to skip writing to that same pixel's depth (unless your pixel shader is complex)." CreationDate="2015-12-03T15:37:16.540" UserId="196" />
  <row Id="1542" PostId="1774" Score="0" Text="@JohnCalsbeek I see. What about other hypothetical scenarios, such as ones involving more complex shader code, or a more regular stencil, that discards, for example, the left half of the screen? What situations can stencils be used in to accelerate things?" CreationDate="2015-12-03T18:11:52.060" UserId="2111" />
  <row Id="1543" PostId="1772" Score="0" Text="@arthur.sw That article is OK as far as affine transforms go, but when projections get involved, things are more complicated." CreationDate="2015-12-03T18:40:23.763" UserId="48" />
  <row Id="1544" PostId="1773" Score="1" Text="@Llamageddon, it just so happens that I still had a diagram at hand ;) I'm afraid pseudo-code is going to bit a bit hard to provide, since there's quite a bit of real code to it. But I hope the expanded answer helps giving a general idea of the technique." CreationDate="2015-12-03T18:47:15.340" UserId="54" />
  <row Id="1545" PostId="1773" Score="0" Text="Amazing answer, though I still find some details unclear: If the pre-pass is low-res, isn't it possible to miss some textures altogether? What happens then? How do the shaders for the pre-pass and final render look? Is the pre-pass used only for fetching the textures, or...?" CreationDate="2015-12-03T22:03:09.053" UserId="2111" />
  <row Id="1546" PostId="1773" Score="3" Text="It's worth noting that most modern hardware now exposes programmable page tables, eliminating the need for a redirection texture.  This is exposed through e.g. [tag:directx12] [reserved resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn899181(v=vs.85).aspx), which builds on [tag:directx11] [tiled resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn786477(v=vs.85).aspx), or [tag:opengl] [sparse textures](https://www.opengl.org/registry/specs/ARB/sparse_texture.txt)." CreationDate="2015-12-04T02:32:18.077" UserId="1992" />
  <row Id="1547" PostId="1773" Score="1" Text="@Llamageddon, the feedback pre-pass can be done at a lower res to save as much computing and memory as possible, since pixels for a page will generally repeat (you can notice the big colored squares in my demo). You're correct that it might eventually miss a visible page like that, but that's not usually going to have a big visual impact because the system should always keep at least the lowest mipmap of the whole VT available in cache. That second paper I linked has all the shader examples in the appendix, you can also refer to the repo for my own project, they are similar." CreationDate="2015-12-04T03:15:23.287" UserId="54" />
  <row Id="1548" PostId="1773" Score="0" Text="The feedback pre-pass is only useful for determining the visible set of pages needed for a view/frame, but you could also combine something like depth pre-pass to it." CreationDate="2015-12-04T03:16:55.707" UserId="54" />
  <row Id="1549" PostId="1770" Score="0" Text="Hmm, this explains it quite well, though I don't really understand how it works with mip levels. I wish I could write down my specific problem with understanding it down, but it kinda eludes me..." CreationDate="2015-12-04T10:29:13.167" UserId="2111" />
  <row Id="1550" PostId="1775" Score="2" Text="cos and sin in the std lib take the angle in radians" CreationDate="2015-12-04T12:40:35.183" UserId="137" />
  <row Id="1551" PostId="1774" Score="0" Text="@Llamageddon The canonical example for stencil these days is discarding all visible sky when doing a lighting full-screen pass. Lighting is expensive, and the sky often large." CreationDate="2015-12-04T15:48:26.510" UserId="196" />
  <row Id="1552" PostId="1773" Score="0" Text="BTW, why do you say that &quot;Virtual Texturing also doesn't handle transparency in an easy way&quot;? Transparency has nothing to do with how textures are stored in memory..." CreationDate="2015-12-04T19:52:20.630" UserId="48" />
  <row Id="1553" PostId="1773" Score="0" Text="@NathanReed, it's because of the page id pre-pass. It's impossible to handle transparency there, because blending 2 or more colors would produce a different page number. What I mean is, suppose object A gets assigned page X which when encoded into a color makes a red tone, now object B gets assigned page Y, which results on green when encoded in the feedback pass. If A and B were to get blended, the color output in the pre-pass for those pixels would be a shade of yellow, translating to the wrong page id/number." CreationDate="2015-12-05T00:00:57.850" UserId="54" />
  <row Id="1554" PostId="1773" Score="1" Text="@glampert Ahh, I see; that makes sense. Still, I think there are lots of options for handling transparencies; in the page ID pass, you could dither (so histogramming would see all the pages, unless there were a huge number of transparent layers), or use a [k-buffer approach](http://www.sci.utah.edu/~csilva/papers/i3d2007.pdf), or even just base transparent texture residency on which objects are near the camera (as opposed to rendering them in a feedback pass)." CreationDate="2015-12-05T02:12:20.280" UserId="48" />
  <row Id="1555" PostId="1772" Score="0" Text="Yes, I would like to have more details about this, maybe I should ask this question on [math.stackexchange](http://math.stackexchange.com/)?" CreationDate="2015-12-05T15:34:24.700" UserId="2173" />
  <row Id="1557" PostId="1772" Score="0" Text="@arthur.sw What details are you interested in? I could add a quick example showing how the divide-by-W affects vectors, if that would be helpful." CreationDate="2015-12-05T21:12:09.347" UserId="48" />
  <row Id="1558" PostId="1772" Score="0" Text="Well I just wanted to make sure that projecting two points is the only or best way to do it." CreationDate="2015-12-06T19:26:40.830" UserId="2173" />
  <row Id="1563" PostId="1786" Score="4" Text="This sounds more like a forum discussion than a specific question. This site works best with questions that can be answered with a single, factual answer. The &quot;what is your wishlist&quot; part is definitely not suitable for an SE site." CreationDate="2015-12-08T15:09:24.190" UserId="2041" />
  <row Id="1565" PostId="1786" Score="1" Text="You can come to [the cornell box](http://chat.stackexchange.com/rooms/26589/the-cornell-box) our chatroom and fire up a discussion" CreationDate="2015-12-08T18:45:12.097" UserId="137" />
  <row Id="1567" PostId="1785" Score="0" Text="Scrolling through your output quickly, it looks reasonable. Although it's hard to tell since it's not plotted as an image. Can you clarify what the problem is?" CreationDate="2015-12-08T19:02:08.877" UserId="48" />
  <row Id="1568" PostId="1789" Score="0" Text="Ok this does answer the question. Hovewer, the explanation probably reqjires one to knpw the answer before understanding. Yould you drop down the abstraction a bit." CreationDate="2015-12-09T05:59:02.037" UserId="38" />
  <row Id="1569" PostId="1789" Score="0" Text="The theory is clear, but how would one go about implementing it? The 2 rotations I used in my example could be done without releasing the mouse inbetween, so there isn't really a clear cut line between them. The user may even go crazy and rotate up left down left which could yield something like cameraRotation.X = 0 cameraRotation.Y = 200, which says nothing about the actual ordner in which the transformations happened." CreationDate="2015-12-09T07:00:28.523" UserId="2219" />
  <row Id="1570" PostId="1791" Score="0" Text="Nice ! Your explanation gives me the intuition about it. Do you happen to have some sources where I can read a bit more about rasterization? I suppose this n-separating stuff comes from 2D where is easier to understand and then I can do more thinking to grasp the 6- and 26-separating in 3D." CreationDate="2015-12-09T14:47:13.873" UserId="116" />
  <row Id="1571" PostId="1785" Score="0" Text="Hope the addition make it more clear.." CreationDate="2015-12-09T16:44:38.587" UserId="2226" />
  <row Id="1572" PostId="1785" Score="0" Text="Hmmm...it's still not very clear to me. Is the problem that e.g. at a 45 degree angle, the line only goes out to (7, 7) and stops, instead of going all the way out to the corner at e.g. (10, 10)?" CreationDate="2015-12-09T18:07:36.510" UserId="48" />
  <row Id="1573" PostId="1785" Score="0" Text="yes exactly..   but also that the method only works for slopes within 0-1" CreationDate="2015-12-09T18:11:14.893" UserId="2226" />
  <row Id="1574" PostId="1791" Score="0" Text="@BRabbit27 I don't think the &quot;n-separating&quot; terminology is used much in 2D rasterization; I've only seen it when discussing voxelization. It just refers to the number of neighbors. I'll add a bit to the answer about that." CreationDate="2015-12-09T18:11:56.777" UserId="48" />
  <row Id="1575" PostId="1795" Score="0" Text="Nice..  it works.. :)" CreationDate="2015-12-09T19:37:20.910" UserId="2226" />
  <row Id="1576" PostId="1789" Score="0" Text="@Patrick Moving the mouse back without releasing the mouse button while rotating an object usually leads to the same result as if the mouse had not been moved e.g. up and back down at all. I added a few words about this (pretend the mouse moved instantly to the current position)." CreationDate="2015-12-09T21:50:46.807" UserId="127" />
  <row Id="1577" PostId="1795" Score="0" Text="well.. partly seem to have an issue with negative values.. Some end position contains negative values..&#xA;&#xA;the center is here 10,10.. thereby making the matrix sized 20X20.." CreationDate="2015-12-09T22:12:46.037" UserId="2226" />
  <row Id="1578" PostId="1795" Score="0" Text="endPos: (15,-2) Angle: 293" CreationDate="2015-12-09T22:17:29.263" UserId="2226" />
  <row Id="1579" PostId="1798" Score="0" Text="Thanks @NathanReed. 1 more question. Say I want to traverse the tree breadth first, I am at level 0 (i.e at the root of the tree, I haven't divided the tree yet, I have a bounding box including all the scene primitives). I divide the bounding box along an axis such that the left child node is the one closer to the camera. Then I find that a certain ray R intersects the left node. R should be early terminated with regard to the right child node and to all of its future children nodes. But I still have to test the intersection of R with the future children nodes of this left node, am I?" CreationDate="2015-12-10T07:29:19.003" UserId="2233" />
  <row Id="1580" PostId="1793" Score="0" Text="I don't think the question's necessarily too open-ended, but any numeric answer is going to be wrong within 12 months." CreationDate="2015-12-10T09:51:04.117" UserId="2041" />
  <row Id="1581" PostId="1799" Score="0" Text="I already asked this question here http://gamedev.stackexchange.com/questions/112165/brdf-and-spherical-coordinate-in-ray-tracing. Nobody seems to have an answer. I think that the arguments in my &quot;UPDATE 2&quot; could be the way to follow (because it seems that the project of vector in this way described could be the correct way to calculate the azimuth angle). Anyone could help me with an answer and maybe a canonical reference to be used as study reference?" CreationDate="2015-12-10T13:10:30.087" UserId="2237" />
  <row Id="1582" PostId="1796" Score="0" Text="Could you provide more detail on what the exact problem is? The only problem you describe is about negative coordinates. A circle of radius 10 around $(10, 10)$ barely reaches zero. Even if there are numerical errors leading to values slightly below zero, after rounding these should be gone." CreationDate="2015-12-10T16:24:12.387" UserId="127" />
  <row Id="1583" PostId="1793" Score="0" Text="@DanHulme Yeah, but the approaches used to reach that kind of efficiency stay the same. And when not, I've seen questions that require updating answers periodically on other stackexchange sites, so I think that's fine." CreationDate="2015-12-10T18:47:56.763" UserId="2111" />
  <row Id="1584" PostId="1798" Score="1" Text="@user2651062 Normally you would have built the whole tree before you start traversing it. Your comment makes it sound like you're trying to build and traverse at the same time? Or else what do you mean by &quot;future child nodes&quot;? In any case, you have to traverse all child nodes that intersect the ray or segment. If it intersects the left child, you descend into the left child and repeat the process for its children. You might need to descend into both children if they both intersect the ray/segment." CreationDate="2015-12-10T20:01:27.557" UserId="48" />
  <row Id="1585" PostId="1796" Score="0" Text="It would also be useful to see the output for a variety of different input values." CreationDate="2015-12-11T00:22:00.550" UserId="231" />
  <row Id="1586" PostId="1775" Score="0" Text="This appears to be the same user and question as [Draw angles lines in raster graphics using bresenham line algorithm](http://computergraphics.stackexchange.com/questions/1785/draw-angles-lines-in-raster-graphics-using-bresenham-line-algorithm)" CreationDate="2015-12-11T06:29:34.403" UserId="48" />
  <row Id="1588" PostId="1793" Score="5" Text="This is really impossible to answer. First of all, what is &quot;realtime&quot;—60fps? 30? Less? Second, the answer will vary hugely based on what GPU you have and what resolution you're rendering at. Third, the answer will vary hugely depending on the details of how the rendering works. Limits on scene complexity are more complicated than just the number of polygons per se, but involve such things as the number of draw calls, state changes, render passes and so on—which are affected by how the engine works, how the artists constructed the scene, and so on..." CreationDate="2015-12-11T06:39:54.973" UserId="48" />
  <row Id="1589" PostId="1800" Score="0" Text="Thank you so much for your help @NathanReed. One last question: do you have any good reference material on how to calculate the tangent space and convert my vectors wi and wo to the coordinate space for ray tracing/BRDF? At the moment I didn't find any useful one. In this way I would be able to do some comparison between the way with tangent space and the way using vectors math." CreationDate="2015-12-11T12:19:49.197" UserId="2237" />
  <row Id="1590" PostId="1793" Score="0" Text="@NathanReed And those are precisely what I'm asking about, the number itself isn't important, and likewise, realtime is loosely defined here as well, although personally I'd go with being able to maintain a framerate above 30. The question itself is about techniques and approaches used to minimize costs per frame, I just couldn't think of a better way to title and phrase it." CreationDate="2015-12-11T12:31:49.687" UserId="2111" />
  <row Id="1591" PostId="1800" Score="0" Text="@FabrizioDuroni I assume you're familiar with how to convert between coordinate systems in general? For tangent space you just have to set up coordinates using the surface normal plus some two vectors perpendicular to it as the axes. For normal mapping, the two vectors are often chosen to match the texture space U and V axes (as mapped to the particular surface). For isotropic BRDFs without normal mapping, it doesn't really matter." CreationDate="2015-12-11T19:11:04.707" UserId="48" />
  <row Id="1593" PostId="1793" Score="1" Text="@Llamageddon Considering your comments, I'm not quite sure what you actually ask for. On one hand, your question title is quite clear (max out geometry and how to do so), but as Nathan pointed out, this is kind of impossible to answer. On the other hand, in your comments you say you want to know how to minimize cost per frame. This is an extremely broad question, because you could improve/optimize your shaders, scene graph, models, textures, API usage, simply everything that does some part of your rendering. You could probably write entire books about this (if not done by someone already)." CreationDate="2015-12-12T00:39:01.593" UserId="127" />
  <row Id="1596" PostId="1800" Score="0" Text="yes I'm familiar with how to convert between coordinate system (so I know that a particular matrix that use the component of the basis vector must be used) but I'm not so familiar with tangent space and texture mapping. Searching the web it seems that the calculation vary between type of objects (sphere, triangle...). You say &quot; For isotropic BRDFs without normal mapping, it doesn't really matter.&quot;, what do you mean? So how do I choose them? Thank you again, I accepted your answer because your the only one that really give me some good hints." CreationDate="2015-12-12T17:43:10.420" UserId="2237" />
  <row Id="1597" PostId="1793" Score="0" Text="@Nero As I said, it's a broad question, I'm interested in all kinds of techniques utilized to push hardware to the limits - occlusion culling, scene graph management, batching, instantiation, virtual texturing, deferred rendering, etc. Setting aside implementing more than just the baseline of geometry, shadows, basic shading, but inclusive of everything leading up to that. I'm sorry if it's a bad question." CreationDate="2015-12-12T23:32:10.617" UserId="2111" />
  <row Id="1598" PostId="1788" Score="0" Text="This seemed a good question - did you delete it because you found the solution? If so you could undelete it and post an answer with your solution. Answering your own question is encouraged and you gain reputation for both the question and the answer. Plus it may help someone else who has a similar problem in future..." CreationDate="2015-12-13T14:11:21.907" UserId="231" />
  <row Id="1599" PostId="1793" Score="0" Text="The more you stuff every technique in one program (what usually an engine does) you are not pushing the hardware to the limits, on the contrary you give it many opportunities to breathe. The way to use the hardware at max capacity is what OCCT does. only a benchmark can do this." CreationDate="2015-12-14T02:19:57.877" UserId="1614" />
  <row Id="1600" PostId="1801" Score="0" Text="if c is a union of the projected pixels, when s1 or s2 completely obstructs the other sphere, it does not mean c gets empty. please clarify." CreationDate="2015-12-14T02:24:53.623" UserId="1614" />
  <row Id="1601" PostId="1793" Score="0" Text="@v.oddou I'd say it's still pushing it to its limits, just in a smarter way. I'm not necessarily interested in super-advanced techniques, just how to... ugh, honestly, I thought this question would be self-explanatory, I even gave examples of what I mean. I just give up, honestly. It's like you guys are literally trying to interpret the question word-by-word instead of actually considering it. Should I just write a wiki answer and let others correct me where I'm wrong or miss something? :/" CreationDate="2015-12-14T02:56:24.510" UserId="2111" />
  <row Id="1602" PostId="88" Score="1" Text="I'm going to give you a straightforward answer: GPU are turing complete. what do you conclude from this ? second answer: lux render http://www.luxrender.net/wiki/SLG" CreationDate="2015-12-14T05:13:53.773" UserId="1614" />
  <row Id="1603" PostId="1806" Score="1" Text="A million seems a bit low to me." CreationDate="2015-12-14T09:00:57.703" UserId="38" />
  <row Id="1604" PostId="1806" Score="0" Text="just take how many MPoly/s the card is capable of, and that's the FPS at which it will render 1 million. I just recalled an experiment for a terrain renderer on an ATI4800HD. If you take this list https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units they don't give Vertices/s info starting from the era of unified architecture. but 10 year old hardware seems to advertise about 40 FPS for 1 million triangles. + c.f. edit in my answer" CreationDate="2015-12-14T09:30:38.697" UserId="1614" />
  <row Id="1605" PostId="1803" Score="1" Text="Great answer and nice illustrations, thanks!" CreationDate="2015-12-14T10:44:02.737" UserId="2244" />
  <row Id="1606" PostId="1806" Score="0" Text="@v.oddou Yeah, but to get near that number you need to do batching of geometry, or instancing, in case of dynamic scenes, and **that** is what I'm asking about. How to not bottleneck yourself 2% of the way towards what hardware can do." CreationDate="2015-12-14T13:49:23.330" UserId="2111" />
  <row Id="1607" PostId="1788" Score="1" Text="Hello @trichoplax actually I found the solution, I'll share the answer with everyone answering my own question. Honestly I deleted my question because I thought none cares about this issue." CreationDate="2015-12-14T21:56:19.720" UserId="2228" />
  <row Id="1608" PostId="1788" Score="1" Text="BTW, instead of editing the question with &quot;SOLVED&quot; in the title, it's preferable to just accept your own answer. (The site might make you wait a day after posting to do that; I don't remember.)" CreationDate="2015-12-15T00:37:03.437" UserId="48" />
  <row Id="1609" PostId="1788" Score="0" Text="Hey! @NathanReed  I'll change the title, thanks about that :)" CreationDate="2015-12-15T00:38:57.447" UserId="2228" />
  <row Id="1610" PostId="1797" Score="0" Text="I'm curious - what languages like this are out there? I know there are engine-specific ones like Unity's or UE4's shader systems, plus some academic researchy things like [Spark](https://graphics.stanford.edu/papers/spark/), but I'm not aware of anything else in current use in this space." CreationDate="2015-12-15T01:22:18.650" UserId="48" />
  <row Id="1611" PostId="1806" Score="0" Text="@Llamageddon aaah, I see, THAT is a question indeed. Let me see what I can say about it. (EDIT2)" CreationDate="2015-12-15T01:47:59.120" UserId="1614" />
  <row Id="1612" PostId="1797" Score="0" Text="@NathanReed, probably Apple's [Metal](https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/Introduction/Introduction.html) is one of the most notable, as of now, but I haven't looked into much detail..." CreationDate="2015-12-15T02:42:46.927" UserId="54" />
  <row Id="1613" PostId="1797" Score="0" Text="Oh, OK. Metal isn't on top of HLSL or GLSL though; it's a primary shading language for Apple GPUs that compiles directly to HW microcode (via a proprietary LLVM backend I believe)." CreationDate="2015-12-15T04:42:36.130" UserId="48" />
  <row Id="1617" PostId="1762" Score="0" Text="The table's corners are, yes." CreationDate="2015-12-16T11:56:41.987" UserId="2162" />
  <row Id="1619" PostId="1806" Score="0" Text="Great in depth answer! I've made a few minor edits, as a user rather than a moderator. Feel free to roll back any/all if they don't match your intention." CreationDate="2015-12-17T12:50:50.273" UserId="231" />
  <row Id="1623" PostId="1816" Score="1" Text="wow, nice animation!" CreationDate="2015-12-18T20:46:59.857" UserId="2173" />
  <row Id="1624" PostId="1816" Score="0" Text="You can answer [here](http://stackoverflow.com/questions/34357110/how-to-triangulate-from-a-vorono%C3%AF-diagram) as well or I will delete my other question." CreationDate="2015-12-18T20:48:24.100" UserId="2173" />
  <row Id="1625" PostId="1816" Score="2" Text="@arthur.sw Cross-posting is generally discouraged on SE, so I suppose deleting it there would be the better option." CreationDate="2015-12-18T20:49:20.453" UserId="16" />
  <row Id="1626" PostId="1816" Score="0" Text="an interactive voronoï diagram creator: http://alexbeutel.com/webgl/voronoi.html" CreationDate="2015-12-19T00:47:21.030" UserId="2173" />
  <row Id="1633" PostId="1822" Score="1" Text="Sorry, but that's plain wrong. OpenGL and DirectX use approximations which are inherently faster than precise raytracing. The whole point of accelerated 3D graphics is having algorithms which balance between realism and speed, looking good enough for most practical uses: gaming, CAD, etc." CreationDate="2015-12-21T15:16:53.507" UserId="2312" />
  <row Id="1634" PostId="1822" Score="1" Text="@IMil OpenGL can be used for raytracing. Its faster because it is optimized for the hardware in question. But Maya does NOT have to ray trace. Maya and Max can use openGL and directX just as much as your game. Mayas (and 3ds) viewport is opengl or directX (your choice). The fact that your processor is slower in certain parallel processing loads is another thing. So the answer stands. The standard settings of maya is no more realistic than a standard scanline." CreationDate="2015-12-21T15:36:03.813" UserId="38" />
  <row Id="1635" PostId="1818" Score="1" Text="The short answer is that OpenGL takes shortcuts." CreationDate="2015-12-21T18:59:57.240" UserId="2316" />
  <row Id="1636" PostId="1821" Score="5" Text="This answer would be even better if you explained what the problem turned out to be and what you changed to fix it..." CreationDate="2015-12-21T20:25:45.710" UserId="231" />
  <row Id="1637" PostId="1820" Score="0" Text="The problem seems to be that all variables are `int`. In particular, `dx` and `dy` will probably get 0." CreationDate="2015-12-22T01:50:08.923" UserId="192" />
  <row Id="1645" PostId="1824" Score="4" Text="Others common renderers have this conflation problem too. See http://w3.impa.br/~diego/projects/GanEtAl14/sample.html?contour." CreationDate="2015-12-22T10:15:25.367" UserId="192" />
  <row Id="1649" PostId="1825" Score="0" Text="Thank you for this detailed answer!" CreationDate="2015-12-22T15:37:12.760" UserId="2319" />
  <row Id="1650" PostId="1821" Score="1" Text="Ill update the question and answer now with more info." CreationDate="2015-12-22T21:22:27.333" UserId="2248" />
  <row Id="1651" PostId="1827" Score="0" Text="Its also a question of time. Even if you could render at say 60 fps and get acceptable results it rarely pans out to optimize for it. Say it takes 3 minutes per frame and you have 200 frames to render. You might be able to get the 60 fps by hiring a shader writer and by optimizing but then that takes atleast a day or two of your time. But  200 frames at 3 mins only takes 10 hours so you save that cost. In practice its cheaper to buy more hardware and not worry too much about it. Games simply can not take this approach." CreationDate="2015-12-23T05:02:36.703" UserId="38" />
  <row Id="1652" PostId="1827" Score="0" Text="@joojaa It's also a little bit more complex though. Just doing really good real-time shaders for Maya might take a year or so at the very, very least, even from an experienced shader developer (with lesser gains), because the flexibility of the nodal system there is targeted towards production rendering. It would take a reverse engineering mindset and kind of new kind of GLSL/HLSL code generation technique (like a meta programming system) to translate these general-purpose shader nodes into a real-time shading system that captures the range of effects of UE 4, e.g." CreationDate="2015-12-23T05:06:44.513" UserId="2247" />
  <row Id="1653" PostId="1827" Score="0" Text="@joojaa UE 4's shader engine is directly targeted towards an heavily-approximated PBR mindset (a very small subset of Disney's PBR shader). They designed even their material system for a fast, real-time purpose, instead of starting with something like Maya's material system which isn't at all (designed for raytracing). Even if the brightest of the UE 4 worked on VP 2.0, they'd have to work night and day for possibly years to achieve the same results against a design not intended to do this sort of stuff." CreationDate="2015-12-23T05:08:30.277" UserId="2247" />
  <row Id="1654" PostId="1827" Score="0" Text="but thats a onetime cost even if youd have that pipeline in a VFX app each scene might need that extra optimization. Theres no reason why a maya user could't render in UDK for example for same shader dev platform." CreationDate="2015-12-23T05:15:20.407" UserId="38" />
  <row Id="1655" PostId="1827" Score="0" Text="They dont have to allways be top notch just good enough for the job at hand. But yes i agree that my time factor is off by a magnitude. But even if it would take days its still not worth it. Games do not have the option." CreationDate="2015-12-23T05:17:28.650" UserId="38" />
  <row Id="1656" PostId="1827" Score="0" Text="@joojaa Yeah, tricky part is that we're often talking about production renderers that are 10+ years old, Maya which is 17+ years old -- huge legacy attached ranging from content to studio pipelines to plugins and so on. It's hard to modernize it -- game engines are always cutting-edge, always purging their previous generation engines and kind of starting a new canvas with each engine generation (though they might reuse or continue a lot of previous work). VFX companies just keep poking at their old codebase." CreationDate="2015-12-23T05:19:29.040" UserId="2247" />
  <row Id="1657" PostId="1827" Score="1" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/33352/discussion-between-joojaa-and-ike)." CreationDate="2015-12-23T05:20:41.790" UserId="38" />
  <row Id="1659" PostId="1828" Score="0" Text="Does this do the same thing except with an increase on the Z height on each iteration of the for loop?" CreationDate="2015-12-24T09:37:43.297" UserId="2248" />
  <row Id="1660" PostId="1828" Score="0" Text="Pretty much, except its always symmetrical (istead of the original a and b, I used a uniform `size` param)." CreationDate="2015-12-24T09:54:29.600" UserId="2317" />
  <row Id="1661" PostId="1830" Score="0" Text="Are you aiming for a solution with a specific programming language / library?" CreationDate="2015-12-24T13:13:16.813" UserId="2317" />
  <row Id="1662" PostId="1830" Score="0" Text="solution  can be in any language .... what is more important is algorithm" CreationDate="2015-12-24T13:23:49.630" UserId="2335" />
  <row Id="1663" PostId="1793" Score="0" Text="@AlanWolfe Do you have a fetish for missing the point, or...?" CreationDate="2015-12-24T14:12:00.003" UserId="2111" />
  <row Id="1664" PostId="1831" Score="2" Text="Something looks wrong with your first image pair...the result is mostly complete black, which doesn't seem correct. Also, it would be good to handle divide-by-zero so rows that are complete black in the input don't flip out. Maybe just replace the row by all 128s in that case?" CreationDate="2015-12-24T21:30:50.837" UserId="48" />
  <row Id="1665" PostId="1825" Score="1" Text="BTW, for a more detailed look at why alpha blending doesn't &quot;do the right thing&quot; in cases like this, check out this paper: [Interpreting Alpha](http://jcgt.org/published/0004/02/03/) by Andrew Glassner." CreationDate="2015-12-24T21:34:26.007" UserId="48" />
  <row Id="1666" PostId="1825" Score="0" Text="@NathanReed Will look thnx, but here it is simply that even if alpha worked right coverage wont know which parts cover the pixels and which not. So  Two layers with 50% alpha could mean fully opaque or only one layer visible because objects fill identical region we just dont know." CreationDate="2015-12-24T22:06:05.467" UserId="38" />
  <row Id="1667" PostId="1831" Score="0" Text="Same algorithm for 1st and 2nd img. Will check though, makes sense about the flipping, thanks." CreationDate="2015-12-24T22:20:57.027" UserId="2317" />
  <row Id="1668" PostId="1825" Score="2" Text="@joojaa Yes that's basically the point the paper is making: alpha can represent either opacity, coverage, or a combination of both. :)" CreationDate="2015-12-25T00:15:09.077" UserId="48" />
  <row Id="1669" PostId="1833" Score="3" Text="Very easy way to achieve a similar look is to just add a small value to the color or each pixel, then clamp to max (255 or 1). It will make the image look more &quot;washed out&quot;." CreationDate="2015-12-25T22:39:48.917" UserId="54" />
  <row Id="1672" PostId="1834" Score="0" Text="I edited my question so that it is more clear. I am not working in 3d btw." CreationDate="2015-12-26T18:54:49.437" UserId="113" />
  <row Id="1674" PostId="1840" Score="0" Text="I tried `float dirX = -(a + b * angle) * sin(angle) + b * cos(angle);` and `float dirZ = (a + b * angle) * cos(angle) + b * sin(angle);&#xA;` then `float newAngle = atan2(dirZ, dirX);` but this does not set the angle correctly." CreationDate="2015-12-27T14:38:35.250" UserId="2248" />
  <row Id="1676" PostId="1840" Score="0" Text="I'm multiplying the result by 180 * Pi to convert to radians" CreationDate="2015-12-27T14:52:42.093" UserId="2248" />
  <row Id="1678" PostId="1840" Score="0" Text="Programming language is c++" CreationDate="2015-12-27T14:57:16.397" UserId="2248" />
  <row Id="1679" PostId="1840" Score="0" Text="Atan2 c++ http://www.cplusplus.com/reference/cmath/atan2/" CreationDate="2015-12-27T15:02:54.920" UserId="2248" />
  <row Id="1681" PostId="1840" Score="0" Text="ill update the question with what I have so far can you have a look?" CreationDate="2015-12-27T15:13:48.623" UserId="2248" />
  <row Id="1682" PostId="1839" Score="0" Text="It would be nice if you could include in your post what libraries you use. Your using Unity right?" CreationDate="2015-12-27T15:35:39.203" UserId="38" />
  <row Id="1683" PostId="1839" Score="0" Text="Using Bullet physics and freeglut" CreationDate="2015-12-27T15:39:25.653" UserId="2248" />
  <row Id="1684" PostId="1840" Score="0" Text="Ive just noticed that your circle is spiraling in the opposite direction to mine" CreationDate="2015-12-27T16:33:02.170" UserId="2248" />
  <row Id="1685" PostId="1840" Score="0" Text="@damorton It depends on how your csys is defined and from which side of the spiral you look from. These are arbitrary definitions. I just have a right handed coordinate system in a 2d space." CreationDate="2015-12-27T17:51:48.493" UserId="38" />
  <row Id="1686" PostId="1839" Score="0" Text="Not Unity https://github.com/damorton/bullet-dominos" CreationDate="2015-12-27T22:06:55.920" UserId="2248" />
  <row Id="1688" PostId="1843" Score="1" Text="Kinda looks like you changed it to `3.2`." CreationDate="2015-12-28T07:35:46.130" UserId="457" />
  <row Id="1689" PostId="1845" Score="1" Text="'Processes' is very vague. Is that vertex shader ops? Raterizer? Shading? All of the above? None of these are meaningful, because they have a massive scene dependence. FLOPS is kind of better, but still not great because it doesnt take into account register pressure, memory latency, etc." CreationDate="2015-12-28T16:52:43.583" UserId="310" />
  <row Id="1690" PostId="1846" Score="1" Text="Hello and welcome. Are you trying to circumvent the [future MIT license](http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-stack-exchange?cb=1) by posting a image or what up with that?" CreationDate="2015-12-28T17:10:03.210" UserId="38" />
  <row Id="1691" PostId="1840" Score="0" Text="Turned out that the above was perfect, the problem was with the dimensions of each domino. Swapping the width and height values fix it :/ thanks @joojaa" CreationDate="2015-12-28T17:52:47.107" UserId="2248" />
  <row Id="1693" PostId="1843" Score="0" Text="Well, in my code its 4.1 haha" CreationDate="2015-12-28T18:32:20.553" UserId="2355" />
  <row Id="1694" PostId="1849" Score="0" Text="That's interesting, though I see from the photon-mapping link that what I'm asking about is how to approach Spectral Rendering.  Do you think it's reasonable to change the question so far as to update it with this term?  I still think I'm asking the same thing." CreationDate="2015-12-28T23:14:10.250" UserId="2360" />
  <row Id="1695" PostId="1849" Score="0" Text="@NewAlexandria Hmm...I interpreted it as being primarily about caustics. If you're really asking about spectral rendering, that's a different question than the one I answered. :) I think it wouldn't be a bad idea to post a new question, if you want to. Maybe edit this one to be specifically about caustics?" CreationDate="2015-12-29T04:13:59.447" UserId="48" />
  <row Id="1704" PostId="1845" Score="0" Text="I understand that there are all of these factors. Nonetheless, I'd be interested to know about how many triangles per second can be drawn assuming modest/reasonable/typical choices for the various factors (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets)." CreationDate="2015-12-29T18:32:58.440" UserId="2358" />
  <row Id="1706" PostId="1849" Score="0" Text="Thanks, [I did, here](http://computergraphics.stackexchange.com/questions/1854/how-is-spectral-rendering-handled)" CreationDate="2015-12-29T20:25:53.253" UserId="2360" />
  <row Id="1708" PostId="1852" Score="0" Text="Thanyou so much! Now I know the how alpha blending works and fixed the particle stuff!  The solution I found worked best was making sure the function got called 60 times a second regardless of the framerate.  Thankyou much!" CreationDate="2015-12-29T23:51:03.110" UserId="2308" />
  <row Id="1714" PostId="1842" Score="0" Text="you can use glGetString(GL_SHADING_LANGUAGE_VERSION) to see which glsl versions are available to you. printf(&quot;Supported GLSL version is %s.\n&quot;, (char *)glGetString(GL_SHADING_LANGUAGE_VERSION));" CreationDate="2015-12-29T15:10:54.010" UserId="2366" />
  <row Id="1722" PostId="1831" Score="0" Text="The above solution seems correct at first glance but what if user enter the output image as input then it shows distorted output. The solution should be adaptive and learn when not to perform calculation as the output of first input is the solution so when output is again input no changes should be seen." CreationDate="2016-01-01T07:45:53.923" UserId="2383" />
  <row Id="1723" PostId="1831" Score="0" Text="@SandeepNeupane that makes sense, although that's not what the author of the question asked. I'm sure this answer can serve as the basis for a more adaptive, complete solution." CreationDate="2016-01-01T18:16:25.310" UserId="2317" />
  <row Id="1724" PostId="67" Score="1" Text="[Related question on SuperUser.](http://superuser.com/q/1019825/215723)" CreationDate="2016-01-01T19:27:16.227" UserId="16" />
  <row Id="1725" PostId="1857" Score="0" Text="Welcome to Computer Graphics Stack Exchange. You could improve your question by adding what information about the polygon and the red box you have. Do you e.g. know the precise coordinates of the vertices, or do you only have the image? Also, please describe what you already tried to do. These information will help us to write good answers and especially also one that helps you best. Some more tips can be found on our [ask] page in our Help Center." CreationDate="2016-01-01T22:09:06.727" UserId="127" />
  <row Id="1726" PostId="1859" Score="0" Text="Note that the question also requires the specified edge to be at the bottom." CreationDate="2016-01-02T23:40:30.977" UserId="231" />
  <row Id="1727" PostId="1859" Score="0" Text="@trichoplax, right. Added that bit of code." CreationDate="2016-01-03T05:34:53.337" UserId="2317" />
  <row Id="1728" PostId="1862" Score="4" Text="There are scaling algorithms like bicubic scaling which use splines to approximate the color of pixels when scaled to any size." CreationDate="2016-01-03T13:09:54.743" UserId="1683" />
  <row Id="1729" PostId="1861" Score="3" Text="These &quot;textures&quot; are called light cookies, which essentially is a texture which shows the strength of light on the projected area. This is actually really simple to implement. When the light attenuation is calculated in the shader, it looks up the cookie texture and returns the strength (which can be stored in the R, G, B, or A channels)." CreationDate="2016-01-03T13:13:00.867" UserId="1683" />
  <row Id="1730" PostId="1859" Score="0" Text="From the example in the question, I believe &quot;at the bottom&quot; means that the horizontal edge becomes the base of the shape, rather than moving it to the bottom of the screen." CreationDate="2016-01-03T14:14:13.190" UserId="231" />
  <row Id="1731" PostId="1859" Score="0" Text="For instance, your side AD is at the top of the shape, so that even when it is moved to the bottom of the screen, the rest of the shape is still below it, off screen." CreationDate="2016-01-03T14:15:43.820" UserId="231" />
  <row Id="1732" PostId="1835" Score="1" Text="You may have to use culling and only draw visible cubes if you can't do a million cubes.  Another option is to merge cubes into larger rectangular shapes." CreationDate="2016-01-05T01:16:56.063" UserId="56" />
  <row Id="1733" PostId="1865" Score="0" Text="Interesting. Could you point us to any sources, examples or results of this approach?" CreationDate="2016-01-05T09:11:27.457" UserId="385" />
  <row Id="1734" PostId="1865" Score="0" Text="I'm on my phone so can't take a screenshot, but this shadertoy uses the method and looks pretty decent: https://www.shadertoy.com/view/ltfXDM" CreationDate="2016-01-05T12:30:40.763" UserId="56" />
  <row Id="1735" PostId="1865" Score="2" Text="POV-Ray is an open-source ray-tracer that uses a similar method to simulate dispersion. It's not a ray per channel: you can configure how many rays are used, spread equally across the spectrum." CreationDate="2016-01-05T15:19:28.693" UserId="2041" />
  <row Id="1736" PostId="1854" Score="0" Text="I feel like this question is way too broad as it stands. Whole books have been written on the subject. Perhaps you could narrow it down to a specific question that's not covered by existing resources?" CreationDate="2016-01-05T15:20:30.733" UserId="2041" />
  <row Id="1737" PostId="1854" Score="0" Text="I can see this being answered along the lines of &quot;There are hundreds of ways, each of which falls into one of the following N broad categories. If you want to know specific detail about one of these categories you can ask a new question.&quot;" CreationDate="2016-01-05T16:17:38.973" UserId="231" />
  <row Id="1738" PostId="1862" Score="0" Text="@EvilTak, can you expand your comment into an small answer?" CreationDate="2016-01-05T16:37:23.030" UserId="54" />
  <row Id="1739" PostId="1866" Score="0" Text="There is a affine transform that will map each corner to its texture coordinate, you can use that to map P to its uv." CreationDate="2016-01-05T16:59:28.540" UserId="137" />
  <row Id="1740" PostId="1866" Score="0" Text="@ratchetfreak could you provide me a link plz ?" CreationDate="2016-01-05T17:06:38.397" UserId="2214" />
  <row Id="1742" PostId="1867" Score="1" Text="@Nero Thanks for Jaxing it up." CreationDate="2016-01-05T20:35:35.477" UserId="457" />
  <row Id="1745" PostId="1866" Score="0" Text="There is a good write up on how to do the intersection point calculation as well as barycentric cord calculation in one go [in this paper](http://www.cs.virginia.edu/~gfx/courses/2003/ImageSynthesis/papers/Acceleration/Fast%20MinimumStorage%20RayTriangle%20Intersection.pdf). This essentially amounts to transforming the triangle." CreationDate="2016-01-06T07:26:36.763" UserId="38" />
  <row Id="1746" PostId="1867" Score="0" Text="i sthere a error in $Bary_B$? Should the first term be $(A_y-C_y)$ or am i wrong?" CreationDate="2016-01-06T07:35:25.327" UserId="38" />
  <row Id="1747" PostId="1867" Score="0" Text="@joojaa I don't think so. It's the same in the Wikipedia article, and it seems correct from a test calculation I did." CreationDate="2016-01-06T07:52:02.270" UserId="457" />
  <row Id="1748" PostId="1862" Score="0" Text="@glampert did that. Do you want me to remove my comment?" CreationDate="2016-01-06T08:00:26.423" UserId="1683" />
  <row Id="1749" PostId="1867" Score="0" Text="ah, so it is $-(A_y-C_y)$, might be good to point out as you would pre calculate $AC_y = (A_y-C_y)$." CreationDate="2016-01-06T08:11:20.477" UserId="38" />
  <row Id="1750" PostId="1867" Score="1" Text="@joojaa The entire denominator and some of the terms in the nominator can be precalculated for each triangle, only few of the terms depend on $P$. I've added a link to a question dealing with methods of calculation. In this formula I thought it would be better to keep the notation simple and uniform rather than efficient." CreationDate="2016-01-06T08:15:03.043" UserId="457" />
  <row Id="1753" PostId="1861" Score="0" Text="I'm not really sure what part of this you don't understand. It's a relatively simple lighting environment and doesn't present any problems for a real-time renderer. Is it the way lights overlap that seems hard, or the shapes of the lights, or something else completely?" CreationDate="2016-01-06T10:13:16.560" UserId="2041" />
  <row Id="1754" PostId="1862" Score="0" Text="@EvilTak, I think you can leave it. Nice answer btw, thanks!" CreationDate="2016-01-06T14:27:11.127" UserId="54" />
  <row Id="1755" PostId="1874" Score="2" Text="The question is about software (read: CPU) rasterization. Some of the information you gave are about rasterization in general, some techniques - in my book - have nothing to do with rasterization at all. Could you please clarify in your answer how the techniques use or benefit from software rasterization?" CreationDate="2016-01-07T12:42:47.580" UserId="385" />
  <row Id="1762" PostId="1878" Score="0" Text="show us what your output is and describe what you expect it to be" CreationDate="2016-01-08T14:33:11.280" UserId="137" />
  <row Id="1763" PostId="1878" Score="0" Text="It just wont compile. It is a version of a basic pixel ilumination shader. The previous version just supports 1 light. I am trying to adapt it so that it can process 8 lights." CreationDate="2016-01-08T14:44:49.503" UserId="2425" />
  <row Id="1764" PostId="1878" Score="0" Text="we'll need to see the info log from the failed compilation/linking. It's a good practice to always log it while developing." CreationDate="2016-01-08T14:55:00.770" UserId="137" />
  <row Id="1765" PostId="1878" Score="0" Text="I am pretty much just starting with shaders...  I do get :&#xA;&#xA;&#xA;	&#xA;Cannot compile frgament shader: 0(24) : error C1101: ambiguous overloaded function reference &quot;mul(mat4, vec3&quot;) (0): mat3x4 mul(mat3x1, mat1x4) (0): mat3 mul(mat3x1, mat1x3) (0): mat3x2 mul(mat3x1, mat1x2) ..." CreationDate="2016-01-08T15:01:46.820" UserId="2425" />
  <row Id="1766" PostId="1879" Score="0" Text="I do not get matrix errors anymore but It just wont compile. I have posted the original working code for a single light. How should I approach it?" CreationDate="2016-01-08T15:36:28.617" UserId="2425" />
  <row Id="1767" PostId="1880" Score="1" Text="This question is about image / signal processing, which is not within the scope of this site. However, there is a SE for signal processing here: http://dsp.stackexchange.com/" CreationDate="2016-01-08T20:41:33.267" UserId="310" />
  <row Id="1768" PostId="1880" Score="0" Text="increase contrast add more frames from video feed. Most of it remains guesswork though" CreationDate="2016-01-08T21:19:06.850" UserId="137" />
  <row Id="1769" PostId="1880" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/201/is-this-graphics-or-image-processing-is-there-an-overlap) on whether image processing is on topic." CreationDate="2016-01-08T23:15:37.607" UserId="231" />
  <row Id="1770" PostId="1881" Score="0" Text="I haven't heard of anyone doing it yet, but I really think temporal techniques could be used to get sub pixel accuracy from video streams." CreationDate="2016-01-09T02:18:06.283" UserId="56" />
  <row Id="1771" PostId="1881" Score="0" Text="@AlanWolfe Definitely. Googling &quot;video super-resolution&quot; turns up a number of papers on that idea." CreationDate="2016-01-09T04:53:26.357" UserId="48" />
  <row Id="1772" PostId="1880" Score="0" Text="Is there a way to move this question to the other SE or should I just cut/paste it over there ?" CreationDate="2016-01-09T07:13:44.533" UserId="2427" />
  <row Id="1774" PostId="1883" Score="0" Text="Very interesting answer! I thought about this, but did not take the time to explain it. &#xA;&#xA;In my opinion, the best solution is to combine both methods: use the first method when possible and handy (when the selected axis does not point to the camera), and your method otherwise. There should be an angle threshold (the angle between the axis and the camera forward vector) to switch between those two methods. In both cases, the GUI should give a good feedback of what's going on (direction and amount of the extrusion, maybe with graduation)." CreationDate="2016-01-09T13:11:37.553" UserId="2173" />
  <row Id="1775" PostId="1883" Score="0" Text="I feel like I must accept the answer which would explain both solutions, do you want to write it?" CreationDate="2016-01-09T13:12:29.870" UserId="2173" />
  <row Id="1776" PostId="1883" Score="0" Text="Dont recommend dual approach. This kind of twitching between the modes makes worst UXp" CreationDate="2016-01-09T13:15:55.557" UserId="2433" />
  <row Id="1777" PostId="1880" Score="0" Text="Per these comments and @trichoplax 's meta discussion I went ahead and cross-posted this in two other SE forums. Here are the links to those discussions: http://dsp.stackexchange.com/questions/28168/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos?noredirect=1#comment52603_28168 and http://video.stackexchange.com/questions/17363/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos" CreationDate="2016-01-09T15:55:40.803" UserId="2427" />
  <row Id="1778" PostId="1880" Score="2" Text="Although this was with good intentions, note that [cross posting on several SE sites is not recommended](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu)." CreationDate="2016-01-09T16:00:30.437" UserId="231" />
  <row Id="1779" PostId="1880" Score="0" Text="@O.M.Y. note that despite the comment about being off topic, the only response so far on meta is to say that this question is definitely on topic here. It will take time to see what other responses come in, but I don't see any reason to move this question at present unless you have your own reasons to." CreationDate="2016-01-09T16:02:31.760" UserId="231" />
  <row Id="1780" PostId="1880" Score="0" Text="Also cross posting is considered rude." CreationDate="2016-01-09T16:16:52.227" UserId="38" />
  <row Id="1781" PostId="1880" Score="0" Text="@trichoplax While it is likely that some of the people in the three SE communities will be joined in all three, there is no guarantee that experts in VIDEO will be part of DSP or CG and vice-versa. As the question seems to be on topic for all three I would like to *gather wool* from each community, give credit where credit is due to each expert, then merge the information into a really good answer for all three SE communities. I am a futurist and it seems reasonable to me that these three fields will simultaneously become both more *integrated* and more *specialized* as we progress." CreationDate="2016-01-09T16:17:25.070" UserId="2427" />
  <row Id="1782" PostId="1880" Score="0" Text="@joojaa I asked about that in a meta discussion and was told to go ahead and post it in the other forum. If I was given bad information then I apologize, but others have not complained and one even helped me to add the cross post links into the comments." CreationDate="2016-01-09T16:19:46.647" UserId="2427" />
  <row Id="1783" PostId="1880" Score="1" Text="You were not asked to post you were told there are other avenues by one user. Cross posting is not the stackexhange way never. If you have a question you need to make it worthy of the site your asking. So you cant ask the same question on video for example." CreationDate="2016-01-09T16:22:05.063" UserId="38" />
  <row Id="1784" PostId="1880" Score="1" Text="@O.M.Y. the meta post I linked to about cross posting shows community consensus is against cross posting, but there are also answers in favour of cross posting in certain rare circumstances, which are worth reading. Even then they recommend tailoring your question to different sites rather than just copy and pasting." CreationDate="2016-01-09T16:23:34.663" UserId="231" />
  <row Id="1785" PostId="1880" Score="1" Text="The reason that meta post exists is because it is not immediately obvious why cross posting would be a bad thing. It's well worth reading through. If there is going to be ill feeling on our site due to this it might be worth having a local meta discussion to see how our particular community feels about when and whether to cross post." CreationDate="2016-01-09T16:25:40.903" UserId="231" />
  <row Id="1786" PostId="1880" Score="0" Text="@joojaa This is where I was told to &quot;go ahead and post&quot; in VIDEO ... http://meta.video.stackexchange.com/questions/1467/about-asking-a-question-in-two-different-se-communities/1468#1468" CreationDate="2016-01-09T16:28:00.320" UserId="2427" />
  <row Id="1787" PostId="1880" Score="1" Text="Still i think you should re word your question anyway. Video will get you a different answer." CreationDate="2016-01-09T16:28:56.570" UserId="38" />
  <row Id="1788" PostId="1880" Score="0" Text="Also user @OlliNiemitalo (2000+ rep) helped me add the cross links in DSP without complain... again if I was provided bad information I apologize but I did ask and I did feel it was okay based on those responses. Now I feel confused and wonder if I should just delete it all and shut up." CreationDate="2016-01-09T16:31:39.993" UserId="2427" />
  <row Id="1789" PostId="1880" Score="1" Text="@O.M.Y. you certainly shouldn't feel bad about posting as you have sought advice in advance. There will always be conflicting opinions about things. If you want to talk about it further I recommend [chat] rather than trying to fit it into comments here." CreationDate="2016-01-09T16:35:06.137" UserId="231" />
  <row Id="1790" PostId="1881" Score="0" Text="I did toyed with SISR a while ago, but my mind cleverly decided to erase that information from the brain. If I can recall though one important bit (mentioned in the paper) that I would explicit in your excellent answer is that the patterns or substructures are to be searched not only in the original image but also in multiple scales of it. That IIRC lead to significantly better results, although as I said I don't remember that much details :(" CreationDate="2016-01-09T17:40:34.587" UserId="100" />
  <row Id="1792" PostId="1888" Score="1" Text="Interesting question. If there is a specific reason that you don't want a shape with planar quads, like a [rhombic dodecahedron](https://en.wikipedia.org/wiki/Rhombic_dodecahedron), mentioning that reason and explaining your motivation for using non-planar quads might help give insight into what is required." CreationDate="2016-01-10T16:12:58.640" UserId="231" />
  <row Id="1793" PostId="1889" Score="0" Text="Thanks for the answer! But I am not looking for a simple 2D shadows algorithm (there are lots on the web), I am looking for a shadow/light algorithm that can handle volumetric fog and colored transparent surfaces (as seen in the Youtube video)." CreationDate="2016-01-10T17:57:52.223" UserId="2437" />
  <row Id="1794" PostId="1888" Score="1" Text="@trichoplax Ok, updated the question :)." CreationDate="2016-01-10T17:57:59.480" UserId="2440" />
  <row Id="1795" PostId="1889" Score="0" Text="@sydd that is just a shader on the shadow volume. Easy to extend." CreationDate="2016-01-10T18:17:25.673" UserId="38" />
  <row Id="1797" PostId="1892" Score="1" Text="It's unfortunately not uncommon for drivers to contain bugs (or &quot;features&quot; to let Game X play optimally even though it doesn't use the API correctly)" CreationDate="2016-01-11T12:01:05.807" UserId="137" />
  <row Id="1799" PostId="1892" Score="0" Text="I hope it's not driver bug, but my bug with padding in that struct." CreationDate="2016-01-11T13:46:39.523" UserId="2413" />
  <row Id="1800" PostId="1892" Score="0" Text="and if you use std430 layout?" CreationDate="2016-01-11T13:50:07.660" UserId="137" />
  <row Id="1801" PostId="1892" Score="0" Text="@ratchetfreak OS X only support OpenGL [4.1](https://developer.apple.com/opengl/capabilities/) ( also, ARB_gpu_shader_fp64 is not supported on some 4.1 Mac Radeons, but it should be on my Intel ), and std430 was introduced in OpenGL 4.3" CreationDate="2016-01-11T13:59:30.073" UserId="2413" />
  <row Id="1802" PostId="1892" Score="0" Text="I've tested it now on one iMac, and it's working there ( but super slow )." CreationDate="2016-01-11T14:39:47.893" UserId="2413" />
  <row Id="1803" PostId="1892" Score="0" Text="So it looks like this shader is not compatible with Intel cards or it's bug in OS X Intel driver." CreationDate="2016-01-11T14:43:31.763" UserId="2413" />
  <row Id="1806" PostId="1889" Score="0" Text="@sydd added colored example" CreationDate="2016-01-11T17:43:40.670" UserId="38" />
  <row Id="1807" PostId="1893" Score="1" Text="What units is the radius measured in? It doesn't make a lot of sense to have a radius of 0.12 pixels (it would just be the one center pixel). It's probably 0.12 centimeters, or 0.12 ems, or something like that." CreationDate="2016-01-11T19:05:36.990" UserId="48" />
  <row Id="1808" PostId="1893" Score="0" Text="Also what does it mean to 'pick a pixel'? Given a pixel coordinate, do you want to know if it is inside the circle?" CreationDate="2016-01-11T20:44:44.263" UserId="457" />
  <row Id="1809" PostId="1893" Score="0" Text="By &quot;to pick a pixel&quot; I meant to randomly choose a pixel inside the circle. The units are not mentioned. The circle has a center C(Cx, Cy), which is a pixel. My guess is, a pixel inside the circle has coordinates P(Px, Py) such that sqrt((Px-Cx)^2+(Py-Cy)^2) &lt; 0.12 (the distance between P and the center C is less than the radius) . That's the most logical way to think about it in my opinion." CreationDate="2016-01-11T21:59:37.363" UserId="2233" />
  <row Id="1810" PostId="1896" Score="0" Text="So what's the problem, specifically? &quot;I cannot make it work&quot; doesn't give us much to go on. Are you getting compiler errors? Post them. Bad output? Show us a screenshot." CreationDate="2016-01-11T22:11:03.257" UserId="48" />
  <row Id="1811" PostId="1896" Score="0" Text="I have just started with shaders. Sorry about that, I am a little bit desperate right now...  It compiles just fine. But won't  affect the lighting in any way. The objects just stays there unaffected by the shader, same as it it was not even loaded." CreationDate="2016-01-11T22:13:12.750" UserId="2425" />
  <row Id="1812" PostId="1896" Score="0" Text="as a test, you might make the sphere have a color based on some value that you are using to calculate your lighting.  This is a really primitive way of getting an idea of the value of your variables.  Doing this, you might notice that something is constant across the sphere which shouldn't be, or a similar problem, which will then help point you in the direction of what is going wrong specifically." CreationDate="2016-01-12T01:16:59.820" UserId="56" />
  <row Id="1813" PostId="1895" Score="0" Text="Is there a reason nobody responds? Should I reformulate the question?" CreationDate="2016-01-12T08:36:11.910" UserId="2447" />
  <row Id="1815" PostId="1895" Score="0" Text="Its early days there are only so many people who have time to answer and you've only so far reached 5 views. This stackexchange is still an infant and has not got many users give it time." CreationDate="2016-01-12T10:52:36.993" UserId="38" />
  <row Id="1816" PostId="1895" Score="0" Text="OK. Thanks. I did not realize so little people were here." CreationDate="2016-01-12T12:10:08.863" UserId="2447" />
  <row Id="1817" PostId="1899" Score="1" Text="Thanks! A, B, C are vectors right? Also, I am using the scanline method because it allows me to get the exact number of points I need. Could you take a look at the code and guess why it is not working? Just the formulas. Also I would upvote the response, but I don't have 15 reputation." CreationDate="2016-01-12T13:04:15.660" UserId="2447" />
  <row Id="1818" PostId="1899" Score="0" Text="Yes, A B &amp;C are vectors; 2D in your case but it applies equally well to N dimensions. As for going through the code... as I said I don't know Lua and even getting a standard polygon scanline renderer correct can be tricky - e.g. You have to be very careful when counting crossings which lie exactly on vertex positions. When you extend that to handling Beziers directly (which I did about 20+ years ago) it's more difficult still. I'm sorry I don't have the time." CreationDate="2016-01-12T14:09:28.923" UserId="209" />
  <row Id="1819" PostId="1899" Score="1" Text="Thanks for the help. Just found the issue. The a and c in the quadratic equation were inverted." CreationDate="2016-01-12T16:22:31.000" UserId="2447" />
  <row Id="1820" PostId="1889" Score="0" Text="thanks, it looks nice! But as I wrote my post is about porting the algorithm I described to the GPU. Actually someone gave an answer on SO at http://stackoverflow.com/questions/34708021/how-to-implement-2d-raycasting-light-effect-in-glsl . This answer is what im looking for, but its slow, thus I dont think that this algorithm is feasible for a real game." CreationDate="2016-01-12T17:23:18.417" UserId="2437" />
  <row Id="1821" PostId="1889" Score="0" Text="@sydd raycasting can be accelerated by precomputing the data into a better datastructure." CreationDate="2016-01-12T18:19:55.377" UserId="38" />
  <row Id="1822" PostId="1903" Score="0" Text="in graph (A), y range is   -1.0 ~ 1.5," CreationDate="2016-01-13T08:25:40.240" UserId="2459" />
  <row Id="1825" PostId="1903" Score="2" Text="Welcome to Computer Graphics SE! I tried to improve the readability of your post a bit. However, I'm not sure it's a great fit for this site. You could probably just ask this on [so] instead. I've [started a discussion on meta](http://meta.computergraphics.stackexchange.com/q/205/16) whether questions about plotting simple graphs are considered on topic or not." CreationDate="2016-01-13T12:42:17.800" UserId="16" />
  <row Id="1828" PostId="1904" Score="0" Text="I've made some minor edits that hopefully do not change your original intention. Feel free to roll back anything that changes your question." CreationDate="2016-01-13T21:59:34.110" UserId="231" />
  <row Id="1829" PostId="1903" Score="2" Text="Given that this is specifically about R, it's likely a better fit for [Cross Validated](https://stats.stackexchange.com/)." CreationDate="2016-01-14T00:02:58.473" UserId="48" />
  <row Id="1830" PostId="1901" Score="0" Text="What's the problem with doing a dynamic loop? That seems like the natural way to solve this kind of problem. If the board has a fixed 8x8 size, the longest ray would only pass through 16 squares at most, so it's not a crazy iteration count. And if you restrict the camera angles like you said, so that people can't look straight across the board, you can cut down the longest ray length considerably." CreationDate="2016-01-14T00:09:16.400" UserId="48" />
  <row Id="1831" PostId="1901" Score="0" Text="I've been going down that route without having a better solution, so it's nice to hear your support of it, thanks Nathan." CreationDate="2016-01-14T00:17:58.143" UserId="56" />
  <row Id="1835" PostId="1909" Score="1" Text="Hello, it could be a particular issue so, can you edit your post and add your GPU code (vertex/fragment shaders) as well as your specific CPU's code on which you are dispatching your shadow map to GPU? this is in order to do a fast debug." CreationDate="2016-01-15T04:07:54.017" UserId="2228" />
  <row Id="1836" PostId="1909" Score="0" Text="I added the code but as you can see, there's nothing really advanced here. I also precise that I am using Unity." CreationDate="2016-01-15T07:27:49.133" UserId="2372" />
  <row Id="1838" PostId="1901" Score="0" Text="Have you tried using raymarching instead of raytracing? Raytracing is more of a CPU/software rendering oriented algorithm, whereas raymarching with signed distance fields is much more easy to implement on a GPU, IMO." CreationDate="2016-01-16T06:24:50.037" UserId="1683" />
  <row Id="1839" PostId="1901" Score="0" Text="For sphere and axis aligned box, directly finding the intersection with raytracing is the better way to go.  For more complex shapes, sure, ray marching is definitely a nice option.  Ray marching exists to get around having to analytically solve line segment vs arbitrary shape." CreationDate="2016-01-16T06:35:37.547" UserId="56" />
  <row Id="1840" PostId="1909" Score="0" Text="This should be the same issue as in http://stackoverflow.com/questions/1513383/texturing-error-on-a-sphere" CreationDate="2016-01-16T13:53:15.863" UserId="2476" />
  <row Id="1841" PostId="1912" Score="0" Text="transform feedback buffers" CreationDate="2016-01-16T17:02:05.970" UserId="137" />
  <row Id="1842" PostId="1912" Score="0" Text="Sounds very good. The constraints of the cloth simulation make it necessary that I can access all neighbours of a particle (to calculate the spring forces). Can this be done with such buffers? In a geometry shader maybe? to pervent the feedback buffer to print each vertex multiple times I would like to use GL_POINTS. After a first look it seems that this makes it harder to calculate the springs." CreationDate="2016-01-16T19:18:54.543" UserId="273" />
  <row Id="1845" PostId="1907" Score="0" Text="This seems to be helpful .... Thanks cheers!!!!!!!1" CreationDate="2016-01-18T03:34:44.603" UserId="2383" />
  <row Id="1846" PostId="1911" Score="1" Text="Your question could probably be improved by including some more information about the actual algorithm instead of just linking to it. Posts should generally be self-contained so as not to be susceptible to link rot (even though that's unlikely for Wikipedia) and so that people can understand the full question without having to read an external link first." CreationDate="2016-01-18T09:10:28.977" UserId="16" />
  <row Id="1849" PostId="1916" Score="1" Text="Depends heavily on what you trace, but this isnt a good fit for the site in my opinion written in this form. Edit the post to be about backface culling in raytracing." CreationDate="2016-01-18T16:08:59.780" UserId="38" />
  <row Id="1856" PostId="1921" Score="1" Text="I think even in the silhouette case, using *signed* projected area (as you noted) means that assumption 3 isn't violated, so long as the microsurface's boundaries match the macrosurface's. Even if there are overhangs beyond the silhouette, the signed projected area of facets on the front and back sides of the overhang will cancel out." CreationDate="2016-01-19T18:30:58.430" UserId="48" />
  <row Id="1857" PostId="1921" Score="0" Text="(Also, maybe this goes without saying, but I think the assumptions also guarantee that the microsurface is a nice, 2-manifold surface without any holes or other weird stuff.)" CreationDate="2016-01-19T18:51:22.653" UserId="48" />
  <row Id="1858" PostId="1924" Score="2" Text="Could you elaborate on what you mean by sampling? The data is just a LUT. The simple way to use it would be to look up the closests input / output vectors and LERP the spectral data between them. Or do you mean how to importance sample?" CreationDate="2016-01-19T23:32:20.063" UserId="310" />
  <row Id="1859" PostId="1924" Score="0" Text="Hi @RichieSams i update my question with more specific problems. Ok for the linear interpolation (I want to keep thing simple at the moment :))" CreationDate="2016-01-19T23:55:21.643" UserId="2237" />
  <row Id="1860" PostId="1925" Score="2" Text="Modern cards differ from previous generation cards as they nolonger have a fixed pipeline for triangles and thus its harder to say what the rate is as it waries with conditions outside the card." CreationDate="2016-01-20T06:22:05.103" UserId="38" />
  <row Id="1861" PostId="1925" Score="0" Text="I was worried that might be the case for NV. AMD did have a fixed triangle setup engine in Hawaii, but I wouldn't be surprised if it went away in the next architecture revision." CreationDate="2016-01-20T06:39:17.713" UserId="2500" />
  <row Id="1862" PostId="1926" Score="0" Text="Thank you @RichieSams. So what about that transformation matrix? Is it some kind of change of coordinate matrix, for example like the one used for transform input data to a camera coordiante system?" CreationDate="2016-01-20T07:23:31.213" UserId="2237" />
  <row Id="1863" PostId="1921" Score="0" Text="@NathanReed That's true, I should have been more precise about that. As for what the assumptions guarantee, I think of it the other way round: the fact that a surface, however faceted, has to be the whole of a boundary between some &quot;inside&quot; and some &quot;outside&quot; forces it to have the three properties." CreationDate="2016-01-20T09:58:42.097" UserId="2041" />
  <row Id="1864" PostId="1926" Score="1" Text="Exactly. It's a change of coordinate matrix. The data is in world coordinate space. And specifically, the input vector is a small portion of the space. So transform to world, then rotate to the input vector domain" CreationDate="2016-01-20T14:24:10.900" UserId="310" />
  <row Id="1865" PostId="1927" Score="2" Text="How are you representing the rectangle, and how are you moving it? Can you share the relevant sections of your code?" CreationDate="2016-01-20T18:53:01.593" UserId="48" />
  <row Id="1866" PostId="1927" Score="1" Text="We will need more detail on what you are trying to achieve, and what you have tried so far so we don't duplicate effort." CreationDate="2016-01-20T20:20:24.237" UserId="231" />
  <row Id="1867" PostId="1927" Score="0" Text="I'm new to this code base so I'm still figuring things out. I'll update the question with some more information when I get time." CreationDate="2016-01-20T22:27:34.393" UserId="2506" />
  <row Id="1868" PostId="1925" Score="0" Text="Again it seems to me that you can make certain default assumptions that take a lot of the uncertainly out of what's &quot;outside the card&quot; for the purpose of comparing card speeds (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets). But I'll take this fixed-pipeline estimate of 2 billion per second as being similar to what you'd get with those assumptions. Thanks." CreationDate="2016-01-21T01:26:13.003" UserId="2358" />
  <row Id="1869" PostId="1925" Score="0" Text="I'm going to study up on NV's white papers - I need to catch up on their architecture (I worked for AMD a few years back). Ideally, there would be be synthetic benchmarks which would help everybody understand where the bottlenecks are, but there was a long history of cheating on them..." CreationDate="2016-01-21T03:31:10.087" UserId="2500" />
  <row Id="1870" PostId="1927" Score="1" Text="There is no such thing as a straight line on a sphere. Thus no rectangle. But most likely also your coordinate system is cylindrical causing the cordinates not to be uniform." CreationDate="2016-01-21T05:25:11.487" UserId="38" />
  <row Id="1871" PostId="1925" Score="0" Text="If it's an operation that always (or nearly always) needs to be done (and triangle set up is one such thing) then there can be large power/area/efficiency reasons for using dedicated hardware." CreationDate="2016-01-21T10:31:56.017" UserId="209" />
  <row Id="1872" PostId="1927" Score="0" Text="@joojaa: A rectangle is mapped on to a sphere. The coordinates on the globe are LatLon but from what I can tell, the X and Y used aren't related." CreationDate="2016-01-21T15:26:45.447" UserId="2506" />
  <row Id="1873" PostId="1931" Score="0" Text="Is the dipole approximation at all related to the Beer–Lambert law?" CreationDate="2016-01-22T03:16:56.483" UserId="2457" />
  <row Id="1874" PostId="1931" Score="0" Text="@maogenc The dipole approximation concerns homogeneous materials with a very high scattering coefficient, such that they're dominated by multiple scattering. The Beer–Lambert law would also apply to such materials." CreationDate="2016-01-22T06:38:14.047" UserId="48" />
  <row Id="1875" PostId="1937" Score="0" Text="How sure are you that the light source is parameterized in the same way?" CreationDate="2016-01-22T09:27:04.150" UserId="385" />
  <row Id="1876" PostId="1937" Score="0" Text="Pretty sure, because and Lux uses diffuse IES data on light sources by default and I only support diffuse material. Also, both light sources have the same strength/color of 6.0." CreationDate="2016-01-22T09:49:04.440" UserId="2448" />
  <row Id="1877" PostId="1937" Score="1" Text="Still it might be that you are using a different [photometric quantity](http://mentalraytips.blogspot.de/2007/03/understanding-photometric-and.html) somewhere, that your tone mapping is still different despite Gamma 2.2, that you're [missing a pi](https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/) somewhere, or any other reason. It's hard to tell, please give some more details about your shading, show some code etc. Besides, the camera is not the same in both images, you might wanna adjust that too :)" CreationDate="2016-01-22T10:20:14.420" UserId="385" />
  <row Id="1879" PostId="1937" Score="0" Text="I have edited in some code. Hopefully it helps. Also, here is the whole repo. More code could be found here. (Please use the &quot;dev&quot; branch. It's where all the new codes are.) https://bitbucket.org/Seanstone5923/simpleray5cpu. OK, I'll fix the camera as soon as possible." CreationDate="2016-01-22T15:03:49.677" UserId="2448" />
  <row Id="1880" PostId="101" Score="0" Text="When they are available you can use SSBOs to get a more flexible output format where you don't need to encode in colors. However, the big drawback of this approach is that it alters the code which can hide/alter bugs, especially when UB is involved. +1 Nevertheless, for it is the most direct method that is available." CreationDate="2016-01-22T15:10:21.610" UserId="2521" />
  <row Id="1882" PostId="1940" Score="2" Text="You may need to add relevant code but this should be pretty easy to paralellisize and use the GPU for calculation. How many objects are we talking about." CreationDate="2016-01-22T16:39:56.777" UserId="38" />
  <row Id="1885" PostId="1939" Score="1" Text="Before/after screenshots would be helpful, otherwise we're all trying to imagine what you mean. Also, is the question about how to enlarge a known region (i.e. you have already identified where the eyes are), or about how to find/track the objects to enlarge? Because those are very different questions. :)" CreationDate="2016-01-22T19:13:21.767" UserId="48" />
  <row Id="1886" PostId="1939" Score="0" Text="Welcome to Computer Graphics Stack Exchange. Asking for off-site resources is off-topic for this site, that's why I removed that part of your question." CreationDate="2016-01-22T19:18:26.767" UserId="127" />
  <row Id="1887" PostId="1940" Score="0" Text="Welcome to Computer Graphics Stack Exchange. As joojaa already said, it's quite hard to help you to improve your code without seeing it. Please edit your question and add relevant parts of your code. Additionally, a screenshot of the result then also helps understanding what your code does." CreationDate="2016-01-22T19:26:54.350" UserId="127" />
  <row Id="1888" PostId="1940" Score="0" Text="Off the cuff, I'd say render the objects in a floating-point texture using additive blending to accumulate their [gravitational potential](https://en.wikipedia.org/wiki/Gravitational_potential). Then use a second pass to convert the texture into a color map." CreationDate="2016-01-22T19:39:39.613" UserId="48" />
  <row Id="1889" PostId="1946" Score="0" Text="Hi @Dragonseel thank you for your answer. I read somewhere on the web that you could also need the texture coordinate to calculate the tangent space. Are there other procedure that could be used to calculate the tangent space?" CreationDate="2016-01-22T20:49:11.507" UserId="2237" />
  <row Id="1890" PostId="1946" Score="1" Text="Nice answer. I edited it to apply MathJax. A minor note: You can't use $DIR$ for the pixel that looks directly at the sphere, because of linear dependency to $N$. But actually you could use any vector instead of $DIR$ that is linearly independent. Or am I missing something?" CreationDate="2016-01-22T20:51:29.750" UserId="127" />
  <row Id="1891" PostId="1946" Score="0" Text="I don't know about the texture coordinate, but given that you already have the intersection-point and the normal calculating two cross-products seems to be very cost effective way. Yes, any vector that is linearly independent can be used. I wrote $DIR$ because it is an easily usable vector. You could just invent a way to generate a non-linear dependent vector (switch two components comes to mind, but I'm not that sure)." CreationDate="2016-01-22T21:18:56.590" UserId="273" />
  <row Id="1892" PostId="1946" Score="0" Text="If the tangent space is to be used for texture mapping (e.g. filtering, normal mapping etc) then it's convenient for the tangent space axes to be aligned with the texture UV axes. But if any arbitrary tangent space is fine, then [an arbitary vector orthogonal to the normal](http://lolengine.net/blog/2013/09/21/picking-orthogonal-vector-combing-coconuts) can be used." CreationDate="2016-01-22T23:15:56.423" UserId="48" />
  <row Id="1893" PostId="1939" Score="0" Text="@Nero no worries." CreationDate="2016-01-23T04:56:08.257" UserId="2520" />
  <row Id="1894" PostId="1939" Score="0" Text="@NathanReed I have already tracked the region of the video for enlargement. I am only looking at enlarging the regions are blend well with the rest of the video content." CreationDate="2016-01-23T04:58:07.287" UserId="2520" />
  <row Id="1895" PostId="1940" Score="0" Text="@NathanReed Right my thoughts exactly..." CreationDate="2016-01-23T07:57:50.153" UserId="38" />
  <row Id="1896" PostId="1948" Score="1" Text="Your point is well taken as a matter of practice: when I used to work at AMD, to test architectural limits, I would do things like clock down the GPU so I could simulate &quot;infinitely&quot; fast memory. However, such architectural limits are all part of a well designed, balanced GPU; understanding gives insight into GPU design, implementation and programming." CreationDate="2016-01-23T15:58:04.627" UserId="2500" />
  <row Id="1897" PostId="1951" Score="1" Text="Just out of curiosity didnt we agree that [software recommendations are offtopic](http://computergraphics.stackexchange.com/help/on-topic)? I dont nesseserily mind but..." CreationDate="2016-01-24T01:21:46.423" UserId="38" />
  <row Id="1898" PostId="1951" Score="1" Text="@joojaa The question is about using scripting to automate image operations, which seems on-topic and generally useful to me. Moreover I don't think I should hold back my answer just because it refers to some external software." CreationDate="2016-01-24T03:04:22.223" UserId="48" />
  <row Id="1899" PostId="1951" Score="0" Text="Its a great answer! Thanks so much!" CreationDate="2016-01-24T07:51:48.717" UserId="2537" />
  <row Id="1900" PostId="1951" Score="0" Text="Love you man. You saved so much of my time :) Hope you have a great day!" CreationDate="2016-01-24T08:27:08.883" UserId="2537" />
  <row Id="1901" PostId="1951" Score="1" Text="No thats not nesseserily what im saying, im just pointing out that have closed questions of simililar nature due to them being software recommendations." CreationDate="2016-01-24T08:40:07.400" UserId="38" />
  <row Id="1904" PostId="1953" Score="0" Text="I read that in a raster display a picture is kind-of jaggy but in random we get a smooth line. why and how does this happen when both work on pixel level anyhow?" CreationDate="2016-01-24T14:13:13.327" UserId="2540" />
  <row Id="1905" PostId="1953" Score="0" Text="Both systems produce some aliasing (I think this is what you mean with 'jaggy') on pixel-level since there is only a finite discrete resolution available. I don't know how the amount of aliasing compares, since in the background of a rasterizer geometric primitives also get converted into pixel-grid, and a random scan has to do the same conversion. I can only assume that because a random scan can directly follow the line it is easier to anti-alias." CreationDate="2016-01-24T14:36:38.283" UserId="273" />
  <row Id="1906" PostId="1957" Score="1" Text="I dont think the memory type matters, however you may want to get a new chipset so that you can do most of the state of the art stuff. Since the memory type may a indicator of the chipset... Memory per see should not matter." CreationDate="2016-01-25T07:32:58.423" UserId="38" />
  <row Id="1911" PostId="1959" Score="0" Text="Statistical tests for random number generators should be useful. Computing the expected number of in order (reverse order) pairs might be a good place to start with a test. This paper has lots of references: http://csrc.nist.gov/groups/ST/toolkit/rng/documents/nissc-paper.pdf." CreationDate="2016-01-25T10:30:09.687" UserId="2500" />
  <row Id="1912" PostId="1960" Score="0" Text="Welcome to Computer Graphics SE! Re your second point, I looked at the cyclic decomposition of the permutation table used by Perlin. It consists of multiple cycles of lengths `{4, 121, 89, 12, 4, 15, 4, 6}`, so apparently that's good enough? (Or maybe it isn't and a different permutation table would be even &quot;better&quot;? Although I'm not sure a human could perceive the difference. Or is actually better to have multiple cycles?)  I'm not following your third point. A uniform random distribution of what? And what step distance do you mean?" CreationDate="2016-01-25T11:29:40.773" UserId="16" />
  <row Id="1913" PostId="1960" Score="0" Text="Thanks! Yeah, that was quite cryptic I guess. It was years ago I experimented with this, so I don't recall the exact implementation, but when you have the implementation for the optimal path generation, it should become evident that you pick random positions of what is left of unused indices - it is that random step length I refer to. I updated the answer" CreationDate="2016-01-25T11:53:09.523" UserId="1790" />
  <row Id="1914" PostId="1957" Score="0" Text="&quot;Maxwell&quot; is NV's current architecture - it has some new graphics features which are described in NVs GeForce GTX 980 Whitepaper. I believe the lowest end Maxwell is the 950." CreationDate="2016-01-25T12:23:23.247" UserId="2500" />
  <row Id="1915" PostId="1936" Score="0" Text="Note that there is a fundamental difference: supersampling (with jittering or any other technique) requires _additional samples_, while a post-filter (like the suggested low-pass filter) works on the _rendered image_. Nevertheless, this is valuable additional information and both techniques go hand in hand." CreationDate="2016-01-25T12:47:23.523" UserId="385" />
  <row Id="1916" PostId="1936" Score="0" Text="I answered initially without seeing yours (antialias with some kind of stochastic supersampling), but your answer was better. I still thought the older paper was still worth referencing, so I changed my answer instead of deleting it. I hope that's reasonable etiquette?" CreationDate="2016-01-25T17:34:39.693" UserId="2500" />
  <row Id="1917" PostId="1957" Score="0" Text="It's really very simple now, especially for Nvidia GPUs: the more you pay, the more you get. Check out this [Price/performance ratio table](http://www.videocardbenchmark.net/gpu_value.html). It roughly shows that if you are low on budget, GeForce GTX 950 is the best you can get. Also it's based on the latest architecture, so you get DirectX 12 and CUDA compute capability 5.2. Alternatively you may look into laptops. Low-end discrete GPUs are almost free there." CreationDate="2016-01-25T17:45:15.380" UserId="2115" />
  <row Id="1918" PostId="1937" Score="0" Text="I'm afraid I dont know enough about it to be really helpful, however, I would probably try to adjust the light intensity so the images look equal, then see with which factor you needed, maybe it can give a clue as what went wrong. Or maybe you cant get it to look the same, like the whole image gets to light to get the centre the right brightness, then maybe some angle calculation is not the same.. just some ideas to try out" CreationDate="2016-01-25T19:11:02.740" UserId="2422" />
  <row Id="1919" PostId="1958" Score="0" Text="So there're no architectural differences whatsoever?" CreationDate="2016-01-26T01:31:12.990" UserId="1870" />
  <row Id="1920" PostId="1958" Score="1" Text="It's just memory. Of course there are some hardware-level differences in how the chip talks to the memory, but it makes no difference for programming it. In CPU programming, you don't write different code for DDR3 vs DDR4 memory either." CreationDate="2016-01-26T02:15:08.723" UserId="48" />
  <row Id="1921" PostId="1958" Score="0" Text="Thanks, you've answered my question fantastically :D" CreationDate="2016-01-26T02:30:11.817" UserId="1870" />
  <row Id="1922" PostId="1963" Score="3" Text="Is your actual texture also just black dots on white? If so, you might be able to generate the texture procedurally, e.g. with [Poisson disc sampling](http://devmag.org.za/2009/05/03/poisson-disk-sampling/) with varying radii. Otherwise, if your texture is actually something more continuous, blending might be an option (that wouldn't work well for crisp textures like your example though). So if you could clarify what your actual pattern/texture looks like that might help getting more helpful answers." CreationDate="2016-01-26T07:37:53.980" UserId="16" />
  <row Id="1923" PostId="1936" Score="0" Text="Totally, your answer is a fine addition :) I just wanted to make sure nobody gets confused." CreationDate="2016-01-26T08:43:32.147" UserId="385" />
  <row Id="1924" PostId="1951" Score="0" Text="The question is about a scripting tool, perhaps making it an &quot;API recommendation&quot; question? It would probably be at home in game development. Maybe I've got a meta meta question here - can questions be recommended for transfer to or crossposted on other stack exchange sites?" CreationDate="2016-01-26T12:53:28.380" UserId="2500" />
  <row Id="1925" PostId="1937" Score="0" Text="Have you tried setting `FACTOR_CUT_OFF` to 0 (introduces bias, LuxRender is unbiased)? Have you tried excluding techniques like Russian Roulette, to see where the culprit may be?" CreationDate="2016-01-26T13:31:53.990" UserId="385" />
  <row Id="1926" PostId="1936" Score="0" Text="Understood. It really sinks in that aliasing is a result of the initial samples being at regular intervals: if every sample hits the pickets of a fence, you're stuck with a white wall. Applying a low pass filter to a regularly sampled image just isn't universally effective (the high frequency picket fence aliases down to a low frequency artifact - a white wall)." CreationDate="2016-01-26T14:00:57.493" UserId="2500" />
  <row Id="1927" PostId="1936" Score="0" Text="The post filtering approach also seems like a really interesting modern CPU (with integrated GPU) load balancing question. Ray trace using CPU cores, which walk such data structures efficiently, post process the images on the GPU. Fun project!" CreationDate="2016-01-26T14:02:20.203" UserId="2500" />
  <row Id="1928" PostId="1937" Score="0" Text="Yes, I did test that. In fact, the image I shown is rendered under a unbiased condition. Also, Russian Roulette is enabled in the image that I shown. I have set `BOUNCE_DEPTH` to -1(which makes my renderer to ignore bounce limits)." CreationDate="2016-01-26T15:10:53.090" UserId="2448" />
  <row Id="1929" PostId="1967" Score="0" Text="So essentially, you want to remove the hand and pencil from the drawing? So you can have just a time-lapse of the drawing over time? Did I understand you correctly?" CreationDate="2016-01-26T16:37:49.393" UserId="310" />
  <row Id="1930" PostId="1967" Score="0" Text="Actually, I want to know how the painter draw. That is, the sequence of its drawing stroke. The painter will draw by hands on paper. Like that http://imgur.com/FeN1IPy" CreationDate="2016-01-26T16:40:09.933" UserId="2551" />
  <row Id="1931" PostId="1967" Score="0" Text="Remove the hand and pencil may benefit to analyze and it is good also." CreationDate="2016-01-26T16:40:52.333" UserId="2551" />
  <row Id="1932" PostId="1963" Score="1" Text="Post process? Something like a variant on edge detection: write out the distance to the nearest dot center then, in screen space, search each pixel's dot sized neighborhood for any (close to) zero values. This gives screen space dots, so you'd have to write and mess with UV derivatives to get back to texture space... Note that you still have a continuity problem - as the p high/low boundary moves across the surface, dots appear and disappear. If p can be continuous (a soft boundary) you might be able to shrink/grow the dots along the boundary..." CreationDate="2016-01-26T16:50:03.830" UserId="2500" />
  <row Id="1933" PostId="1967" Score="0" Text="So, sort of a vector field drawing showing the path the pencil takes on the paper? For example, each stroke would be an individual curve/arrow." CreationDate="2016-01-26T19:17:59.540" UserId="310" />
  <row Id="1935" PostId="1963" Score="1" Text="Another option is to look into Wang tiling.  [introduction](http://procworld.blogspot.com/2013/01/introduction-to-wang-tiles.html), [tile genetics](http://procworld.blogspot.com/2013/01/tile-genetics.html), [GPU gems](http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter12.html)" CreationDate="2016-01-27T02:06:31.343" UserId="2457" />
  <row Id="1936" PostId="1963" Score="0" Text="@MartinBüttner good point, I've added a line about the type of texture" CreationDate="2016-01-27T04:13:21.460" UserId="31" />
  <row Id="1937" PostId="1967" Score="0" Text="Yes. I just record how painters they paint in daily life actually." CreationDate="2016-01-27T07:02:16.060" UserId="2551" />
  <row Id="1938" PostId="1963" Score="1" Text="Have you looked at texture bombing? http://http.developer.nvidia.com/GPUGems/gpugems_ch20.html" CreationDate="2016-01-27T08:24:37.303" UserId="2463" />
  <row Id="1939" PostId="1967" Score="0" Text="Any demo videos so we can test this? You could simply track the ne dof the pencil. But if it gets occluded then no dice. You could also track the evolution of the shape over frames by difference keying. Or just put the paper on a wacom tablet and change to a wacom ink pen." CreationDate="2016-01-27T08:33:28.030" UserId="38" />
  <row Id="1940" PostId="1971" Score="0" Text="One could paint on a semitransparent surface and capture image from behind, use wacom tablet trough paper and so on... Reverse difference matte should also give the evolution." CreationDate="2016-01-27T08:42:03.433" UserId="38" />
  <row Id="1941" PostId="1967" Score="0" Text="Its my test video, a very simple one. No overlap. http://1drv.ms/1UpF4x3" CreationDate="2016-01-27T09:06:40.550" UserId="2551" />
  <row Id="1942" PostId="1971" Score="0" Text="I understood he had the finished video from somewhere and now wants to reconstruct the process." CreationDate="2016-01-27T10:14:24.280" UserId="273" />
  <row Id="1943" PostId="1971" Score="0" Text="Sure but sometimes, one really can not get a good measurement in hindsight. Anyway for the test video tracing works well, and since he has a test video maybe he also hasnt set up the system yet." CreationDate="2016-01-27T10:21:52.227" UserId="38" />
  <row Id="1944" PostId="1973" Score="1" Text="Thanks for the reply Nathan, I did expect that MSDN page was incorrect from my initial experiments. I might give the clip planes solution another try when I get some time, i'll post another question when I have more information on what's going wrong. Thanks again for your help." CreationDate="2016-01-27T11:40:47.010" UserId="288" />
  <row Id="1945" PostId="1974" Score="0" Text="Im not sure it exists as such, determining wetehr or theres a possibility for 0-1 or 2 or more is pretty trivial but the formulation does not really make it ieasy to make sure its 0 or 1 without actually checking." CreationDate="2016-01-28T13:44:13.640" UserId="38" />
  <row Id="1946" PostId="1974" Score="0" Text="What is the runtime requirements? An solution that should be able to produce pretty accurate results would be to approximate both curves by a large number of short straight segments and then intersecting them in a pairwise fashion. But that costs much time and memory." CreationDate="2016-01-28T14:05:10.967" UserId="273" />
  <row Id="1947" PostId="1974" Score="0" Text="@Dragonseel Well, I would be happy for any solution, really, but since you asked O(1) would be nice. But approximating the curves with line segments leads to the same problems as the test for bounding box overlap..." CreationDate="2016-01-28T15:00:20.193" UserId="141" />
  <row Id="1948" PostId="1974" Score="0" Text="Interesting problem. I don't think there's an easy answer but I'd like to be wrong. Do you have a link for the Sederberg and Meyers paper?" CreationDate="2016-01-28T15:21:11.820" UserId="2500" />
  <row Id="1949" PostId="1974" Score="0" Text="@DanielMGessel Yes, see the edit above." CreationDate="2016-01-28T15:25:10.350" UserId="141" />
  <row Id="1950" PostId="1977" Score="0" Text="Using the identity matrix is equivalent to placing the object at the origin, with no scaling, and no rotation." CreationDate="2016-01-28T19:29:07.043" UserId="310" />
  <row Id="1951" PostId="1977" Score="0" Text="@RichieSams yes, same as putting object to world coordinates (or other parent), except theres a matrix in between to manipulate." CreationDate="2016-01-28T19:46:41.473" UserId="38" />
  <row Id="1952" PostId="1967" Score="1" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/210/is-reconstruction-of-pen-strokes-from-real-life-video-on-topic) on whether this is on topic." CreationDate="2016-01-28T21:42:55.493" UserId="231" />
  <row Id="1953" PostId="1974" Score="0" Text="Are the control points of your curves 2D or 3D (or, for that matter, greater)?" CreationDate="2016-01-29T09:41:23.870" UserId="209" />
  <row Id="1954" PostId="1974" Score="0" Text="@SimonF 2D, sorry, see the edit." CreationDate="2016-01-29T09:48:31.333" UserId="141" />
  <row Id="1955" PostId="1974" Score="0" Text="(Sigh. Caught by the comment edit timeout). You also say _&quot;I also would like to use floating-point numbers in the implementation&quot;_  I suspect that you might be limited on how &quot;reliable&quot; the test will be by whatever rounding goes on in floating point. For example, if there is a huge dynamic range in the control point values, you may lose too much precision." CreationDate="2016-01-29T09:50:10.977" UserId="209" />
  <row Id="1956" PostId="1974" Score="0" Text="@Eric: Being pedantic here:  By planar do you mean the _same_ plane?" CreationDate="2016-01-29T09:55:32.927" UserId="209" />
  <row Id="1957" PostId="1974" Score="0" Text="@SimonF Yes, same plane." CreationDate="2016-01-29T09:58:28.413" UserId="141" />
  <row Id="1958" PostId="1974" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/35018/discussion-between-simon-f-and-ecir-hana)." CreationDate="2016-01-29T10:03:58.910" UserId="209" />
  <row Id="1959" PostId="1963" Score="1" Text="Can you give an idea of the range of shapes/patterns you need to cover and how much flexibility there can be in the approach? @DanielMGessel's suggestion of changing the circle radii instead of their density might make things simpler and give smoother animation but it's not yet clear how much your approach is constrained." CreationDate="2016-01-29T12:19:35.967" UserId="231" />
  <row Id="1963" PostId="1979" Score="0" Text="Do you understand how frame buffers work? https://en.wikipedia.org/wiki/Framebuffer (Caveat, it's got alot of history.) Is that some of what you are looking for?" CreationDate="2016-01-29T14:11:53.177" UserId="2500" />
  <row Id="1964" PostId="1982" Score="0" Text="Yes, but im personally more interested in the case where  about the case where the Bm and/or B0 are Both within the volume of A's max and min bound but does not pierce it then you need to subdivide and in worst case scenario calculate the intersection point. Better ways would be to use the minimum bounding box also known as thick line approximation." CreationDate="2016-01-29T15:23:46.557" UserId="38" />
  <row Id="1965" PostId="1982" Score="0" Text="Given that, with every binary subdivision, the difference between the curve and the segment connecting the end points goes down by reasonable factor (and, off the top of my head, I think it might have been 4x for quadratics) surely the bounds are going to converge to a &quot;thin&quot; ribbon fairly rapidly." CreationDate="2016-01-29T15:44:00.973" UserId="209" />
  <row Id="1966" PostId="1982" Score="0" Text="Yes but worst case scenario is that the other bezier starts at the other." CreationDate="2016-01-29T15:54:48.087" UserId="38" />
  <row Id="1967" PostId="1982" Score="0" Text="You mean, for example, _An_ == _B0_.  Do you define that as an intersection or not?" CreationDate="2016-01-29T16:01:40.370" UserId="209" />
  <row Id="1968" PostId="1982" Score="0" Text="No more like B0 is At somewhere on the curve. Or even a just minimally crossing" CreationDate="2016-01-29T16:08:39.820" UserId="38" />
  <row Id="1969" PostId="1982" Score="0" Text="Thanks a lot for the effort but I think the problem is the 6th case. If you propose to subdivide the curves further then this is not better than the overlapping bboxes because it could happen than after many subdivisions we still wont be able to answer &quot;yes&quot; or &quot;no&quot; definitely. But in ordered not to end up in infinite loop or floating point rounding errors we would have to stop at some point, basically guessing." CreationDate="2016-01-29T16:54:07.137" UserId="141" />
  <row Id="1971" PostId="1984" Score="0" Text="Blender supports Nurbs surfaces and a &quot;spin&quot; operator for curves. Is that the operation you want? If so you could maybe lookup how Blender implemented the operation." CreationDate="2016-01-29T23:53:59.457" UserId="273" />
  <row Id="1972" PostId="1973" Score="0" Text="Finally figured out why the VR prototype I did wasn't working. Turns out my prototype was a bit too simple, I was just doing the squish and move code to all vertex shaders and also doing all draws with an instance count of 2. This meant that when the last BLIT ran to copy the render target to the back buffer happened it also squish, moved and rendered twice causing the 4 views :) figured it out pretty quick once I'd captured in RenderDoc." CreationDate="2016-01-30T00:40:49.583" UserId="288" />
  <row Id="1973" PostId="1973" Score="0" Text="BTW: Thanks again for the information on the SV_RenderTargetArrayIndex in the pixel shader now I've thought about it it makes sense that the RT output would need to be consistent across all pixels." CreationDate="2016-01-30T00:41:03.577" UserId="288" />
  <row Id="1975" PostId="1984" Score="0" Text="@Dragonseel No, i know how to make a revolved surface, im only interested in how to make a silhouette curve in my own 2d software. I can do a discrete solution for this like blender would in maya, max, creo, solidworks. The only app that ive ever seen to do this acceptably is Rhino. But since this is a special case seems to me there should be a analytic solution." CreationDate="2016-01-30T06:56:11.890" UserId="38" />
  <row Id="1976" PostId="1984" Score="0" Text="@Dragonseel updated the question to refelect what i said" CreationDate="2016-01-30T08:29:24.650" UserId="38" />
  <row Id="1978" PostId="1984" Score="0" Text="If you figure out what happens in your first image when rotating around a vertical axis, the answer will generalize to all cases (others are rotations of that case in the image plane) - is that right?" CreationDate="2016-01-31T11:10:45.940" UserId="2500" />
  <row Id="1979" PostId="1984" Score="0" Text="@DanielMGessel Most likely yes, at least in the case where your shape gets thinner towards you. When the situation gets thinker towards you you get all kinds of other possible self occlusion problems. Basically this case is most likely some evolution of the swept ovals shape. But im not really certain how to find the intersection point other than by binary search which isn't really good for analytical stuff ;)" CreationDate="2016-01-31T11:16:24.907" UserId="38" />
  <row Id="1995" PostId="1984" Score="0" Text="It's a good problem. I don't know the answer - obviously ;) - just feeling it out as a problem to solve. Self occlusion - of course. That'll introduce discontinuities." CreationDate="2016-01-31T17:10:12.063" UserId="2500" />
  <row Id="1996" PostId="1986" Score="0" Text="Why is it 6th degree polynomial, and not 3rd, if we are talking about cubic Beziers? And the two methods you linked to, are they amenable to finding solutions only in $[0,1]^2$, as opposed to whole $R^2$?" CreationDate="2016-01-31T19:51:51.933" UserId="141" />
  <row Id="1997" PostId="1940" Score="0" Text="I'd still have to calculate for each point gravity for each object. This seems to be the expensive part" CreationDate="2016-01-31T20:33:44.037" UserId="2524" />
  <row Id="1998" PostId="1986" Score="1" Text="@EcirHana It's 6th degree because it's the squared distance. (You could square-root it, but then it's no longer polynomial, and will be non-smooth at the zeroes.) Note that the $[0,1]$ is the parameter space, not the space the splines live in, i.e. these are splines with endpoints. In any case, the methods will work fine in $\mathbb{R}^2$, but they only &quot;travel downhill&quot; from an initial guess and find a local minimum; something more is needed to examine the whole parameter region and find the _global_ minimum. Constraining the parameter space is probably helpful there." CreationDate="2016-01-31T22:00:52.463" UserId="48" />
  <row Id="2000" PostId="1986" Score="1" Text="Nathan - nice formulation! I'm rusty, but: I think you can divide each bezier curve into at most 5 segments, by where $x$ or $y$ change direction in the curve. $x$, as a function of $c_i$ changes direction at most twice (roots of the derivative) breaking the curve into 3 segments, 2 of which may be divided again by changes in direction of $y$. Now you have, not straight segments, but segments that &quot;don't curve too much&quot;. I think if you start your search at 25 points, chosen by segment pairs, you could be always find the global minima, but I can't quite see how to prove (or disprove) it." CreationDate="2016-02-01T03:47:09.483" UserId="2500" />
  <row Id="2001" PostId="1982" Score="0" Text="Oops! I'd renumbered the initial list but not the solution cases!" CreationDate="2016-02-01T08:50:38.250" UserId="209" />
  <row Id="2002" PostId="1986" Score="0" Text="@Nathan: I had considered that but, having spent much time writing code to find minima in texture compression formats, it all seemed a bit hideous." CreationDate="2016-02-01T13:30:27.697" UserId="209" />
  <row Id="2003" PostId="1987" Score="0" Text="What's the paper you mentioned in your first sentence? It might give helpful context. Otherwise, it does sound like these are two mutually incompatible uses of the word &quot;fluence&quot;." CreationDate="2016-02-01T17:29:09.283" UserId="48" />
  <row Id="2004" PostId="1987" Score="0" Text="I made an edit to include a link to the paper. Is there a preferred format for referencing papers?" CreationDate="2016-02-01T18:00:59.183" UserId="2457" />
  <row Id="2007" PostId="1493" Score="1" Text="This should go somewhere in this thread, so adding it here :) Inigo Quilez's analytic sphere projection: https://www.shadertoy.com/view/XdBGzd" CreationDate="2016-02-02T12:11:49.200" UserId="2463" />
  <row Id="2008" PostId="1995" Score="0" Text="The only thing I can think of is doing the perspective projection on the CPU, and using an ortho matrix... Does that work?" CreationDate="2016-02-03T14:07:19.980" UserId="2500" />
  <row Id="2009" PostId="1995" Score="4" Text="why not use glsl? You are going to need to learn to program a shader eventually why not now." CreationDate="2016-02-03T14:54:31.353" UserId="137" />
  <row Id="2010" PostId="1995" Score="4" Text="It should be possible to do this using projective texture mapping, which can be done in fixed-function by providing 4-dimensional texcoords. I'd have to sit down and do the math to work out the details though." CreationDate="2016-02-03T16:48:34.523" UserId="48" />
  <row Id="2021" PostId="1999" Score="0" Text="Awh shoot, just realized [Geometry shaders aren't supported in WebGL](http://stackoverflow.com/questions/8641119/webgl-geometry-shader-equivalent). :|" CreationDate="2016-02-04T05:57:24.597" UserId="71" />
  <row Id="2022" PostId="1999" Score="2" Text="Why not make that your answer. Then this question is not as purposeless as it seems. (or delete offcourse)" CreationDate="2016-02-04T08:26:01.440" UserId="38" />
  <row Id="2023" PostId="1999" Score="0" Text="@joojaa done and done :)" CreationDate="2016-02-04T09:29:33.060" UserId="71" />
  <row Id="2024" PostId="1997" Score="0" Text="@trichoplax which part of it you did not understand?" CreationDate="2016-02-04T23:53:47.483" UserId="2602" />
  <row Id="2025" PostId="1997" Score="0" Text="`transform bounding sphere radius and center of sphere in 0-1` doesn't make clear whether you want the radius normalised to 1, or transformed to some value between 0 and 1 (in which case, how is that value determined?). The same for the centre - is it to be moved to the origin or translated to somewhere within a distance 1 from the origin or something else?" CreationDate="2016-02-05T04:56:14.470" UserId="231" />
  <row Id="2026" PostId="1997" Score="0" Text="`somebody pointed out to me that its not possible to transform radius into 0-1` From the comments on your [initial question](http://computergraphics.stackexchange.com/questions/1996/calculate-bounding-sphere-radius-in-normalized-space) it doesn't appear that anyone is saying that it is not possible to transform the radius. We just need to know what you mean by &quot;0-1&quot;." CreationDate="2016-02-05T05:00:12.693" UserId="231" />
  <row Id="2027" PostId="1997" Score="0" Text="@trichoplax okay. I changed my algorithm. Can you check it" CreationDate="2016-02-05T05:35:09.910" UserId="2602" />
  <row Id="2028" PostId="2004" Score="0" Text="I'm accepting this answer because the paper itself is good and its &quot;Related work&quot; section seems pretty comprehensive. Even if I don't end up using this technique exactly, I'm sure I'll be able to tailor something for my use case from this and its references." CreationDate="2016-02-05T09:42:17.403" UserId="2041" />
  <row Id="2029" PostId="1963" Score="0" Text="How is define the pattern ? as an image of pixels, or as a noise function, a procedural field, etc, with some threshold to make in B&amp;W ? in the letter case you could adapt the value of the threshold.&#xA;&#xA;Anyway, we need more information about what is the texture to be able to answer." CreationDate="2016-02-06T06:25:19.577" UserId="1810" />
  <row Id="2030" PostId="1997" Score="1" Text="This seems clearer now. I've edited to add in links and hovertext for the acronyms. If this changes your intention please edit to correct this." CreationDate="2016-02-06T13:18:48.847" UserId="231" />
  <row Id="2031" PostId="2006" Score="0" Text="There's a detailed look into Quake's engine [in this article](http://fabiensanglard.net/quakeSource/index.php). You'll also find one on [Quake 2](http://fabiensanglard.net/quake2/index.php) on the same site. I think there's a link there somewhere to a GDC presentation talk by Michael Abrash where he briefly talks about the visibility problem and how it was solved using portals." CreationDate="2016-02-06T18:01:34.113" UserId="54" />
  <row Id="2032" PostId="2006" Score="0" Text="Ah found it! That's the video I mentioned: http://www.gdcvault.com/play/1014234/Excerpt-Quake-Postmortem-Optimizing-Level" CreationDate="2016-02-06T18:08:16.550" UserId="54" />
  <row Id="2035" PostId="1995" Score="0" Text="@DanielMGessel I think that's a good idea. I'll try." CreationDate="2016-02-07T05:05:58.510" UserId="2600" />
  <row Id="2036" PostId="1995" Score="0" Text="@ratchetfreak Actually it's a project requirement..." CreationDate="2016-02-07T05:06:37.673" UserId="2600" />
  <row Id="2037" PostId="2008" Score="2" Text="Maybe your model of the box has a wrong normal." CreationDate="2016-02-07T10:05:01.643" UserId="1888" />
  <row Id="2038" PostId="1995" Score="1" Text="Nathan's right - you can undo perspective projection with a 4th coordinate (I too would have to work out the math); look into texture coordinate generation. I have vague memories that on (very) old hardware precision can crop up as an issue (I think implementing stipple this way was error prone - I never had to do it personally) - so if old hardware is why you're not able to use shaders, you may bump into some oddities at the pixel level. But it is a better approach, especially if you have alot of geometry or if you are using vertex buffers." CreationDate="2016-02-07T12:54:25.723" UserId="2500" />
  <row Id="2039" PostId="2008" Score="2" Text="This might not be the error, but you have a uniform &quot;viewPos&quot; which is, I assume, the camera position. But you do all calculations in Camera-Space, so this should always be $(0.0, 0.0, 0.0)$ and thereby the **viewDir** becomes $-1 * fragPos$." CreationDate="2016-02-07T14:30:11.230" UserId="273" />
  <row Id="2040" PostId="2011" Score="1" Text="the first parameter of glVertexAttribPointer should be queried with glGetAttribLocation" CreationDate="2016-02-07T23:50:21.737" UserId="137" />
  <row Id="2042" PostId="2012" Score="0" Text="Do you see that working well when you have N grid cells, each of which may or may not have a sphere in it that you need to test your ray against?  Maybe a bvh would be better when the grid is mostly empty, but have a worse worst case? Hrm.." CreationDate="2016-02-08T02:40:57.633" UserId="56" />
  <row Id="2043" PostId="2012" Score="0" Text="I was thinking using it for the (real) sphere set only, after a first cut to the plate &quot;skin&quot;, as you said.&#xA;&#xA;Note that dynamics loops are not totally an issue as long as they are bounded; you can break/return/continue using an if inside. only the longest length for a given warp will be applied to all of its pixel.s The only issue is the code length, since loops and functions are unrolled." CreationDate="2016-02-08T03:13:24.527" UserId="1810" />
  <row Id="2044" PostId="2014" Score="0" Text="Is it possible that the solid red is a hole?" CreationDate="2016-02-08T14:12:08.473" UserId="137" />
  <row Id="2046" PostId="2014" Score="0" Text="No I have checked that, and it is not" CreationDate="2016-02-08T15:14:53.877" UserId="2630" />
  <row Id="2047" PostId="2014" Score="1" Text="Can you upload your code somewhere (gist, pastebin, etc.) and link it? We might be able to spot the error in the code." CreationDate="2016-02-08T15:37:48.797" UserId="310" />
  <row Id="2048" PostId="2008" Score="0" Text="@Dragonseel  Yup!! That was the problem! Thank you for taking out some time for this question!" CreationDate="2016-02-08T17:41:38.177" UserId="2096" />
  <row Id="2049" PostId="2007" Score="0" Text="I can not calculate min max corner in modelspace as after multiplying with view and projection matrices, min and max corner does not remains the same. that is why I am iterating over all the vertices.Previously I thought just using bounding box would suffice but looks like it is changing in clip coordinate space." CreationDate="2016-02-08T18:09:17.637" UserId="2602" />
  <row Id="2050" PostId="2014" Score="4" Text="Problem is most likely in your normal computation. The end is a singularity in the ubderlying parametric shape which can yeld bad results if you dont know what your doing. The visible artefact number 2 is pretty common in these cases." CreationDate="2016-02-08T22:25:12.420" UserId="38" />
  <row Id="2051" PostId="2007" Score="0" Text="when I checked NDC coordinates at the end in both ways, I get very different numbers. with iteration over vertices I get vertex cooridinates in range of minus thousands to plus thousands whereas with bounding boxes they are in range of -10 plus 10 approx." CreationDate="2016-02-09T00:48:54.703" UserId="2602" />
  <row Id="2053" PostId="2014" Score="0" Text="@joojaa, could you explain more on the second sentence?" CreationDate="2016-02-09T08:06:49.157" UserId="2630" />
  <row Id="2055" PostId="1995" Score="0" Text="@DanielMGessel I figured it out. Just use(sz,tz,0,z) and it works, where z is z-coordinate in eye space." CreationDate="2016-02-09T08:48:56.447" UserId="2600" />
  <row Id="2056" PostId="2016" Score="2" Text="The normal depth value is not linear and goes against 1 relatively quickly. Are you sure it is really white as in value 1.0? Could be that the values are just close to 1.0 so that you cannot visibly distinguish them? (there are only 255 white-gray-black values but far more values in a big depth-buffer)." CreationDate="2016-02-09T10:30:13.477" UserId="273" />
  <row Id="2057" PostId="1995" Score="0" Text="Cool - glad you sorted it out." CreationDate="2016-02-09T13:42:54.303" UserId="2500" />
  <row Id="2058" PostId="2018" Score="2" Text="Oh dear, 2 answers and no picture computer graphicists on their best." CreationDate="2016-02-09T14:03:32.957" UserId="38" />
  <row Id="2064" PostId="2020" Score="1" Text="@Nicol Thank you. I edited the answer accordingly." CreationDate="2016-02-09T23:54:48.630" UserId="273" />
  <row Id="2067" PostId="2023" Score="2" Text="The range of numbers on the axes change between the three diagrams, and the 2nd and 3rd have more vertices labelled. Are these transformed versions of the same cube, or three separate shapes?" CreationDate="2016-02-10T03:43:37.407" UserId="231" />
  <row Id="2068" PostId="2023" Score="0" Text="There is [information about syntax highlighting on Meta Stack Exchange](http://meta.stackexchange.com/questions/184108/what-is-syntax-highlighting-and-how-does-it-work) but it appears that Matlab is not supported." CreationDate="2016-02-10T03:54:42.653" UserId="231" />
  <row Id="2071" PostId="2023" Score="0" Text="The first shape is an example of one of the composite worldspace shapes in its local coordinates with normals drawn on. The middel and right hand plots is the same worldspace shape transformed" CreationDate="2016-02-10T11:38:40.393" UserId="2646" />
  <row Id="2072" PostId="2023" Score="3" Text="Just FYI, in a GPU, the back face culling is done (or can be considered to be done) on the post-projection values of the vertices, and thus only needs to consider the X&amp;Y values of the 3 triangle vertices. (There are a few reasons for this: avoiding problems with floating point rounding being one)" CreationDate="2016-02-10T11:53:46.063" UserId="209" />
  <row Id="2073" PostId="2026" Score="0" Text="Thanks for the comment, updated my post. This is my first post on stackoverflow so I appreciate all constructive criticism haha" CreationDate="2016-02-10T15:26:21.163" UserId="2651" />
  <row Id="2074" PostId="2026" Score="1" Text="This is a significant improvement! Note that [so] and [computergraphics.se] are different sites. They are both part of the Stack Exchange network but they have different rules about what is on topic. If there's any confusion feel free to drop in to [chat]." CreationDate="2016-02-10T15:37:30.790" UserId="231" />
  <row Id="2075" PostId="2029" Score="1" Text="In principle, I think automatic has an advantage: when using the same vertex shader with a variety of fragment shaders, some attributes may not propogate to the output of a given pixel shader and the compiler can detect this and inform the application, allowing optimization of the vertex layout. In practice, I don't take advantage of this (for a few reasons). I too am curious to hear what the downside to automatic is..." CreationDate="2016-02-11T05:21:48.507" UserId="2500" />
  <row Id="2076" PostId="2030" Score="1" Text="That's just a warning that the vram is getting over-committed and could cause more memory transfers." CreationDate="2016-02-11T11:06:37.830" UserId="137" />
  <row Id="2077" PostId="2014" Score="1" Text="Your question would definitely be more answerable if you included relevant parts of the code." CreationDate="2016-02-11T12:36:04.533" UserId="16" />
  <row Id="2078" PostId="2031" Score="2" Text="Where have you seen that term? Stereo 3D reconstruction is a special case of multi-view reconstruction as far as I know. I have never seen the term &quot;multi-view stereo&quot;." CreationDate="2016-02-11T14:23:38.730" UserId="273" />
  <row Id="2079" PostId="2022" Score="2" Text="I think some more information about your code and what you did would help us here to answer your question. Without seeing what you did it is pretty much guessing and it seems nobody here likes guessing. That's why there is no answer jet." CreationDate="2016-02-11T14:30:27.153" UserId="273" />
  <row Id="2080" PostId="2031" Score="1" Text="For example http://dl.acm.org/citation.cfm?id=1153518" CreationDate="2016-02-11T14:31:27.930" UserId="2670" />
  <row Id="2081" PostId="2016" Score="0" Text="I tried it from different view points and with different entities. But still all I can see is a wait blank texture." CreationDate="2016-02-11T14:59:10.137" UserId="2637" />
  <row Id="2082" PostId="2016" Score="0" Text="How do you check the framebuffer texture? And have you tried to leave out the 'out_color' from the fragment shader?" CreationDate="2016-02-11T15:16:53.333" UserId="273" />
  <row Id="2083" PostId="2030" Score="0" Text="@ratchetfreak: That's just a side effect then? I still don't understand why the program stops working though." CreationDate="2016-02-11T17:08:21.130" UserId="2666" />
  <row Id="2084" PostId="2030" Score="0" Text="Can you share OS + GPU details?" CreationDate="2016-02-12T02:19:05.593" UserId="2500" />
  <row Id="2086" PostId="2034" Score="6" Text="https://en.wikipedia.org/wiki/ClearType or more generally https://en.wikipedia.org/wiki/Subpixel_rendering" CreationDate="2016-02-12T08:44:20.430" UserId="457" />
  <row Id="2088" PostId="2014" Score="0" Text="What is the source of your teapot? Glut-teapot?" CreationDate="2016-02-12T09:57:09.787" UserId="2463" />
  <row Id="2090" PostId="2035" Score="0" Text="If you know the final display size, scaling your original, highest quality image to the display size with a good graphics application should produce as good as or better than letting the display system do it for you (which is likely to cut corners for performance/simplicity)." CreationDate="2016-02-12T15:08:23.783" UserId="2500" />
  <row Id="2091" PostId="2016" Score="0" Text="I'm using a gui system to show the texture and then I screenshot it to check it with better detail in a editor program. I removed the 'out_color' but still it is just blank white space." CreationDate="2016-02-12T15:35:00.743" UserId="2637" />
  <row Id="2092" PostId="2030" Score="0" Text="@DanielMGessel: Yes, I am using Linux Mint 17.2 and my GPU is a GTX 980Ti with 6GB of VRAM." CreationDate="2016-02-12T17:00:26.943" UserId="2666" />
  <row Id="2093" PostId="2037" Score="1" Text="The object in the right hand image does appear to be at a similar angle to the middle image. It's just that the axes are not the same. Is it only the axes you are asking about, or does the object itself have a problem?" CreationDate="2016-02-12T19:20:25.223" UserId="231" />
  <row Id="2094" PostId="2030" Score="0" Text="The message seems to be that a buffer in VRAM is getting straight up discarded - unless you are using a ton of VRAM for something else, or you have locked up the GPU - this could be a symptom what would be a TDR on windows - if you submit a command buffer that takes too long. But that should just kill your command buffer, not your data. You could try changing the bits passed to glBufferStorage to make it dynamic and see what happens. Crosspost on an NV message board and/or file a bug report." CreationDate="2016-02-12T20:02:26.917" UserId="2500" />
  <row Id="2095" PostId="2037" Score="0" Text="It's essentially the axes - I would have thought that the axes should be rotated to look like the central figure. Instead, it's slightly off, as in the right figure" CreationDate="2016-02-12T20:36:08.070" UserId="2646" />
  <row Id="2096" PostId="2016" Score="0" Text="Okay.  My last idea is that you try to bind a color attachment that you write a static random color into. Maybe there is some optimisation bug. I got my shadow mapping running with such a &quot;fake&quot; texture attached." CreationDate="2016-02-12T21:20:02.317" UserId="273" />
  <row Id="2097" PostId="2016" Score="0" Text="When you say GUI-system do you mean some software or a self written texture renderer with orthogonal projection? If self written you may have a slight error there an hour could give the relevant shader that reads the shadow map." CreationDate="2016-02-12T21:22:34.177" UserId="273" />
  <row Id="2101" PostId="2042" Score="0" Text="I was writing ray-tracing instead of raytracing, that was the error" CreationDate="2016-02-14T00:52:34.470" UserId="2684" />
  <row Id="2103" PostId="2041" Score="2" Text="As for the code, there are many examples online. Take a look at SO: http://stackoverflow.com/questions/1659440/32-bit-to-16-bit-floating-point-conversion" CreationDate="2016-02-14T16:57:40.007" UserId="54" />
  <row Id="2104" PostId="2041" Score="0" Text="Thanks - tighter than my versions. :)" CreationDate="2016-02-14T17:11:59.850" UserId="2500" />
  <row Id="2105" PostId="2045" Score="0" Text="Cool - it's nice to have an alternative to AVX. f11_f11_f10 is my priority right now, as I use it for all my linear space/HDR rendering." CreationDate="2016-02-14T22:52:45.997" UserId="2500" />
  <row Id="2108" PostId="2030" Score="0" Text="@DanielMGessel: I think it has something to do with how long my compute shader is working. When I am using a simple compute shader where I iterate through the whole buffer and compute an average over all the values inside, then it works with bigger buffer sizes. Are there any rules on how long a shader can be working? My error appeared when the shader was working for more than ~8 seconds." CreationDate="2016-02-15T07:55:24.873" UserId="2666" />
  <row Id="2109" PostId="2045" Score="1" Text="I've added some links to R11G11B10_Float conversion implementations." CreationDate="2016-02-15T08:29:16.037" UserId="93" />
  <row Id="2114" PostId="2045" Score="0" Text="Beautiful! Thanks!" CreationDate="2016-02-15T10:59:09.793" UserId="2500" />
  <row Id="2115" PostId="2043" Score="1" Text="&quot;The only difference is that half edge uses directional edge and winged edge uses undirectional edge.&quot; From my understanding, more like: [Half-edge](https://en.wikipedia.org/wiki/Doubly_connected_edge_list) is doubly linked (and each direction may contain additional information), while [winged edge](https://en.wikipedia.org/wiki/Winged_edge) is, most commonly, counter-clockwise only." CreationDate="2016-02-15T11:28:57.173" UserId="385" />
  <row Id="2116" PostId="88" Score="1" Text="The best way to do this is probably using atomic operations for the scattered accumulation operations (when multiple rays may hit the same point), but you could render your entire &quot;ray cloud&quot; as points and do the intersection and occlusion calculations in a vertex shader, use a passthrough pixel shader and let the HW blender do the accumulation." CreationDate="2016-02-15T13:55:00.150" UserId="2500" />
  <row Id="2117" PostId="2049" Score="0" Text="It is customary for the camera to face in the negative z direction. Perhaps you have view in positive direction? Sorry no time to debug." CreationDate="2016-02-15T20:57:51.090" UserId="38" />
  <row Id="2118" PostId="2049" Score="0" Text="I'm unsure if I got the camera facing in the right direction - my main thought was that the view frustum was still point towards the object from the origin so that should be satisfactory? Either way I dont think it transforms correctly...." CreationDate="2016-02-15T21:16:40.003" UserId="2646" />
  <row Id="2120" PostId="2046" Score="0" Text="Links aren't allowed to be answers by themselves,  but here is a link that explains it pretty well, and has simple working source code: http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2016-02-16T00:37:18.353" UserId="56" />
  <row Id="2121" PostId="2050" Score="1" Text="Follow your answer, I found out that the Modified Conjugate Gradient method is proposed to deal with the non-symmetric semi-positive-definite linear equation(section 5.2). Thanks a lot." CreationDate="2016-02-16T01:14:47.317" UserId="2593" />
  <row Id="2123" PostId="2043" Score="0" Text="So, you mean the way they use doubly linked simply to add more info explicitly? Because I think by using Half Edge there might be some performance gained for specific query from the mesh. But till now, I still cannot figure which query.." CreationDate="2016-02-16T07:28:43.660" UserId="2687" />
  <row Id="2124" PostId="2047" Score="0" Text="I don't think it is what I had to begin with - although on a brief glance it should seem that e &lt;=&gt; C and  x,y,z &lt;=&gt; U,V,N, when I change the code to use your method, the results are starting to seem more intuitively correct!" CreationDate="2016-02-16T10:13:23.190" UserId="2646" />
  <row Id="2125" PostId="2046" Score="0" Text="@AlanWolfe , that's a great article which explains much, but nevertheless it says about 16 control points for bicubic surface, which I'm trying to evade" CreationDate="2016-02-16T10:43:47.063" UserId="2694" />
  <row Id="2126" PostId="2052" Score="0" Text="I've performed the perspective divide - (see update) - and my result is now a cube that has coordinates of +/- 1 in all axes (seems right!) although the z plane has been reflected such that the far plane is now at +1 and the near plane at -1 - is this correct? Thanks very much" CreationDate="2016-02-16T10:58:02.583" UserId="2646" />
  <row Id="2127" PostId="2047" Score="2" Text="In your matrix you have a &quot;c&quot; superscript on the x, y and z components (camera space). I added a description of how to derive the eye local space based on the gluLookat parameters. I think that pyramid in front is pretty confusing; I'd use one box to start with. Try plotting looking down the axi first, make sure you get the correct rectangles. Also, anything that can let you animate changing the parameters will help clear things up immensely. A single wireframe picture is always hard to decode..." CreationDate="2016-02-16T12:31:51.497" UserId="2500" />
  <row Id="2128" PostId="2046" Score="0" Text="Ah, sorry for missing that part" CreationDate="2016-02-16T14:25:53.657" UserId="56" />
  <row Id="2129" PostId="2052" Score="0" Text="Additionally - should the aspect ratio of the view volume be taken into account in the size - or does this simply lead to a slight deformation of the shape rather than the [1, -1] cube deformation?" CreationDate="2016-02-16T15:16:48.743" UserId="2646" />
  <row Id="2130" PostId="2048" Score="0" Text="thank you. Do I understand you correctly, that in my case using NURBS the user could first specify points on surface, and then fine-tune the surface by fine-tuning section curves? And I still cannot get how I can make Bezier (or NURBS) patch with control points and knots only on edges. Can you show your second image with control points and edges?" CreationDate="2016-02-16T16:43:55.223" UserId="2694" />
  <row Id="2131" PostId="2048" Score="0" Text="@morodeer theres are infinite ways of doing this. But i was drawing a image for you allready just dont have time to complete it" CreationDate="2016-02-16T17:19:11.770" UserId="38" />
  <row Id="2132" PostId="2052" Score="1" Text="@davidhood2 That sounds right, yes. Post-projective space is left-handed (+x = right, +y = up, +z = into screen). As for the aspect ratio, that should be handled by x and y scale factors in the projection matrix. Post-projection, x and y should be in [−1, 1] regardless of aspect ratio." CreationDate="2016-02-16T17:28:29.150" UserId="48" />
  <row Id="2133" PostId="2048" Score="0" Text="I think, I understood almost everything: should I use the same algorithm for mid-points? PS: I think there are some mistakes in points and lengths, but image makes it clear." CreationDate="2016-02-16T18:40:20.657" UserId="2694" />
  <row Id="2134" PostId="2048" Score="0" Text="@morodeer yes same algo works fine" CreationDate="2016-02-16T19:00:10.883" UserId="38" />
  <row Id="2135" PostId="2048" Score="0" Text="ok, and the algorithm you described looks more like blending between 3 curves, but not as interpolating between curve network, right? Do you know equally elegant way to interpolate mid-points in case of curve network?" CreationDate="2016-02-16T19:02:51.053" UserId="2694" />
  <row Id="2136" PostId="2048" Score="0" Text="@morodeer Thats what you asked. It is forming a interpolation, curve networks just split up and insert knots in the underlying surface least if its a square case," CreationDate="2016-02-16T19:27:37.363" UserId="38" />
  <row Id="2137" PostId="2043" Score="0" Text="While we're on edge-representations, this is a great paper, generalising a lot of them: http://graphics.cs.ucdavis.edu/~joy/ecs178/Unit-9/resources/Tahoe-Paper.pdf" CreationDate="2016-02-16T19:56:33.423" UserId="2463" />
  <row Id="2138" PostId="2058" Score="0" Text="Though in general it is far easier to just use the construction joojaa explained in his answer." CreationDate="2016-02-16T23:27:35.177" UserId="273" />
  <row Id="2139" PostId="2060" Score="3" Text="I don't know a lot about voxel-based rendering, but I always got the impression voxel-based raytracers intersected the rays directly with the voxels, instead of constructing a polygonal representation of the voxels first." CreationDate="2016-02-17T15:14:39.980" UserId="16" />
  <row Id="2140" PostId="2060" Score="0" Text="Neither am I, I just read about rendering using voxelization so I might have a wrong idea on the technique." CreationDate="2016-02-17T15:26:39.277" UserId="116" />
  <row Id="2141" PostId="1861" Score="0" Text="The problem for me is projecting the actual beam, like if the light is passing through fog, from the source to the floor... The simplest way i think would be &quot;projecting&quot; a fake beam, a cone of a fixed lenght, made of triangles, from the lens of the light, towards the direction in which the light is facing, with some alpha blending." CreationDate="2016-02-17T17:28:01.917" UserId="2388" />
  <row Id="2142" PostId="1861" Score="0" Text="But this way the beam would pass through the objects, so i need a way to stop it at the first intersection... a very simple way would be raytracing the distance from the center of the lens to the floor, and then making the cone of that lenght. but its not a good way..." CreationDate="2016-02-17T17:34:00.240" UserId="2388" />
  <row Id="2143" PostId="2053" Score="0" Text="Where the lens are located in your model? Onto the screen plane? Where is the aperture size?" CreationDate="2016-02-17T18:15:08.913" UserId="2684" />
  <row Id="2144" PostId="2059" Score="0" Text="Thank you very much for your answer, could you tell me more about stratified sampling and low-discrepancy sequence ?" CreationDate="2016-02-17T18:38:36.007" UserId="2372" />
  <row Id="2145" PostId="2029" Score="0" Text="The accepted answer on this post argues for the pro-explicit side, maybe you could take a look at why he thinks so.&#xA;http://stackoverflow.com/questions/4635913/explicit-vs-automatic-attribute-location-binding-for-opengl-shaders" CreationDate="2016-02-17T19:30:35.047" UserId="2651" />
  <row Id="2146" PostId="2029" Score="0" Text="Reusing VAOs with multiple shaders that use the same vertex layout. If you let the compiler decide, you have potentially need a VAO for each VB for each compiled program, even if they use exactly the same attributes. Sounds good to me! I should look up what Vulkan does, now that the spec is out..." CreationDate="2016-02-18T02:01:04.087" UserId="2500" />
  <row Id="2147" PostId="2063" Score="0" Text="What transformations are you applying? Or are these coordinates directly in clip space with no transformations? And how are you triangulating the square?" CreationDate="2016-02-18T04:21:46.547" UserId="48" />
  <row Id="2148" PostId="2063" Score="0" Text="No transformations, I edited my post to show code" CreationDate="2016-02-18T04:25:14.960" UserId="2651" />
  <row Id="2157" PostId="2068" Score="0" Text="Wouldn't `norm` be better at normalizing also you can write `R[U, 0; V, 0; -N, 0; 0, 0, 0, 1]`" CreationDate="2016-02-18T22:23:28.900" UserId="38" />
  <row Id="2158" PostId="2068" Score="1" Text="You are right about norm, but sqrt(dot(x,x)), is the same (I do not know octave/matlab very good, so I use math). Regarding matrix - not really, because U, V, N are columns, correct way would be R = [U', 0; V', 0; -N', 0; 0,0,0,1] :-) but I tried to keep original formatting." CreationDate="2016-02-18T22:34:29.107" UserId="43" />
  <row Id="2159" PostId="2068" Score="0" Text="Its the same but really ease of reading would make the code much better... As the original code is a totall mess. Which is mostly why there were not many takers. And the fact that its matlab..." CreationDate="2016-02-18T22:47:25.633" UserId="38" />
  <row Id="2160" PostId="2062" Score="0" Text="Thanks for the great answer! Would it be possible to allocate texture or geometry data to just individual cards then? The link seemed to indicate that computing could be distributed, which is makes sense, but I'm still confused as to how or if geometry or textures could be split." CreationDate="2016-02-19T02:12:22.923" UserId="2707" />
  <row Id="2161" PostId="2062" Score="0" Text="@aces Yes, you can allocate geometry or textures to just individual cards—you'd just create those resources only on one node. Every time you create a resource, you specify which node to put it on, so you have total control over which GPUs get copies of which resources." CreationDate="2016-02-19T02:20:00.133" UserId="48" />
  <row Id="2162" PostId="2062" Score="0" Text="That makes sense, but would that be feasible in a game? I would think you would have to synchronize all the fragments if you did that somehow." CreationDate="2016-02-19T02:22:04.977" UserId="2707" />
  <row Id="2163" PostId="2062" Score="0" Text="@aces &quot;Synchronize all the fragments&quot;? You've lost me. :)" CreationDate="2016-02-19T02:22:49.977" UserId="48" />
  <row Id="2164" PostId="2062" Score="0" Text="Sorry :). In a simple example, if you have one model with a certain texture on one GPU and another model with a different texture on another GPU, they would both need to be rasterized and shaded, so the frame buffer would need to contain both of this information in the end for depth tests. I guess I'm just confused on how that could utilize VRAM stacking." CreationDate="2016-02-19T02:25:53.217" UserId="2707" />
  <row Id="2165" PostId="2062" Score="2" Text="@aces OK, yeah. If you distribute the rendering across GPUs in some way, you have to put the results back together somehow afterward. So in your example, after rendering, you could copy the color &amp; depth buffers from GPU2 back to GPU1, and use GPU1 to composite the two frames together. That would take some extra time, which eats into the time you saved by distributing the rendering in the first place, so it might or might not be an overall perf win depending on circumstances." CreationDate="2016-02-19T02:33:24.280" UserId="48" />
  <row Id="2166" PostId="2062" Score="0" Text="Ah ok, so I guess if the bus latency/bandwidth isn't too bad that could work. Thanks for the clarification!" CreationDate="2016-02-19T02:36:16.770" UserId="2707" />
  <row Id="2167" PostId="2071" Score="0" Text="Ah cool neat idea. And yeah, I'm just doing nearest neighbor sampling. I'll edit the question to reflect that." CreationDate="2016-02-19T05:14:05.223" UserId="56" />
  <row Id="2168" PostId="2071" Score="1" Text="Given you are storing 0-1 values where, probably, having high precision near 0 is of little value, the exponents of the f16 may be wasted space. Something to think about if you are looking to tighten things up as much as possible. Bit packing can be especially useful in vertex data before any interpolation happens; in one case, I store texture array coordinates in 4 bytes and use the alpha channel to add extra bits to x &amp; y; this gives me texel precise addressing for up to a 4k x 4k x 256 texture." CreationDate="2016-02-19T05:29:18.677" UserId="2500" />
  <row Id="2169" PostId="2071" Score="0" Text="Good to know! In my case unfortunately all I have is a full screen pixel shader, I don't have geometry. But, on the plus side, the buffer can store values outside of 0 to 1.  It's a full 16 bit floating point number which is nice." CreationDate="2016-02-19T05:33:37.603" UserId="56" />
  <row Id="2170" PostId="2071" Score="0" Text="When you talk about the exponent bits being wasted are you talking about doing something like storing two values in a single float, where you make them be of different scales and add then, so that you can use mod and div to get the values out again? Do you have any working details of that by chance?" CreationDate="2016-02-19T05:52:11.697" UserId="56" />
  <row Id="2171" PostId="2072" Score="0" Text="Slerp is a possibility for normals. It stands for &quot;spherical lerp&quot;—unit vectors lie on a sphere as much as unit quaternions do." CreationDate="2016-02-19T07:08:04.263" UserId="196" />
  <row Id="2172" PostId="2071" Score="1" Text="Basically, the exponent scales an otherwise fixed point value (the mantissa) with the size of the value. If you don't benefit from that, then fixed point numbers (ints or GL &quot;norms&quot;) utilize the bits better. It may not apply in your case for any number of reasons (does WebGL support integers? I'm out of date). I do use mod and div to unpack when I need to (also, before GLSL supported ints)." CreationDate="2016-02-19T13:28:07.083" UserId="2500" />
  <row Id="2173" PostId="2071" Score="1" Text="I can rant on floats, so I won't go there. Fixed point can be a better choice (ask if more precision near 0 is useful - with positional values fp is often more of a convenience; in a linear color space, fp matches our perceptual system beautifully). Just something to consider when concerned with bandwidth." CreationDate="2016-02-19T13:56:30.313" UserId="2500" />
  <row Id="2174" PostId="2072" Score="1" Text="I've found out that nlerp is a decent method: just interpolate your components and re-normalize.  I can do a bicubic version of this by doing each component individually.  Found some decent links too: https://keithmaggio.wordpress.com/2011/02/15/math-magician-lerp-slerp-and-nlerp/  and http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/ ." CreationDate="2016-02-19T14:50:07.500" UserId="56" />
  <row Id="2175" PostId="2072" Score="1" Text="If you know your grid vectors don't vary more than, say, 120 degrees  from grid point to grid point (i.e. trying to avoid problems with &quot;which way do you interpolate), why wouldn't bicubic interpolation with renormalisation be &quot;good enough&quot;?" CreationDate="2016-02-19T14:55:46.257" UserId="209" />
  <row Id="2176" PostId="2069" Score="0" Text="I am currently using the code to provide 2d rendering support for sprites that position the image pixel perfect to the desired position on the screen in pixel coordinates instead of opengl coordinates. As such I currently do not plan to allow batch rendering of multiple quads but instead use the quad as a &quot;stamp&quot; to render the image to the screen. As such each quad drawn has of course a different texture or different texture coordinates to read from. As such I also never know how many quads are drawn beforehand." CreationDate="2016-02-19T19:19:33.343" UserId="2623" />
  <row Id="2177" PostId="2069" Score="1" Text="@salbeira OK. You could still combine sprites in an atlas to allow instancing. Even if you don't do that, though, you can still batch the uniform buffer updates. And you can batch even while not knowing how many quads are drawn beforehand—I added a paragraph to the answer about that." CreationDate="2016-02-19T20:08:47.280" UserId="48" />
  <row Id="2178" PostId="2069" Score="0" Text="I can imagine how this should work but it still seems a bit like a mystery to me. Like when I supply a method to &quot;render a single spirte&quot; that has a texture attached to it and information about where to find the sprite within that texture, I'd need to pile that data up and store it in a huge prepared array. When I want to draw that I'd still need to call a drawElements for each sprite, and bind each texture that contains that sprite sparately. Do I magically combine each texture with a framebuffer to a huge atlas before rendering? Or should I just say to the application dev to use an atlas?" CreationDate="2016-02-20T03:04:03.583" UserId="2623" />
  <row Id="2179" PostId="2074" Score="0" Text="After you fix the main problem, if you want better image quality you should try bicubic interpolation instead of bilinear.  http://blog.demofox.org/2015/08/15/resizing-images-with-bicubic-interpolation/" CreationDate="2016-02-20T05:41:31.877" UserId="56" />
  <row Id="2180" PostId="2069" Score="0" Text="@salbeira Yeah, designing the interface for this sort of thing is tricky. You could supply a method to &quot;add a sprite to the render list&quot; and another method to &quot;render all the sprites on the list&quot;, so the user would be responsible for calling that when the rendering was finished. If the rendering order of the sprites doesn't matter, you could also sort them by texture, so that you don't have to switch textures so many times and you can use instancing in a group of sprites with the same texture. Then your system would work with or without a user-provided atlas (but faster with)." CreationDate="2016-02-20T07:45:05.573" UserId="48" />
  <row Id="2181" PostId="2068" Score="0" Text="Thanks very much - I know the original code is a mess - please accept my apologies! I've tried to do it using only simple matlab functions because I will be porting it to an FPGA (in VHDL) - and am simply trying to model the maths!" CreationDate="2016-02-20T18:26:26.127" UserId="2646" />
  <row Id="2185" PostId="2060" Score="0" Text="You don't want to do voxel rendering, right ? since you would no longer need triangles. You wan't to do an optimisation structure sorting the scene triangles withing regular grid cells, right ? (but there are several variants, e.g. spliting the triangles or not. which is yours ?)" CreationDate="2016-02-21T00:49:57.720" UserId="1810" />
  <row Id="2186" PostId="2069" Score="0" Text="I guess this will also not work since I'd like the user of these functions to be able to apply the painters algorithm if needed and also the bound framebuffer, color modifiers and stencil information all need to be applied in order" CreationDate="2016-02-21T04:17:07.203" UserId="2623" />
  <row Id="2187" PostId="2081" Score="0" Text="how the objects in the scene are defined? In ray tracing you calculate intersections using the implicit function, ie, sphere, plane, cone, etc, how is it in ray marching? also can you expaling the meaining of the variables that you wrote in your post?" CreationDate="2016-02-21T04:21:20.363" UserId="2684" />
  <row Id="2188" PostId="2081" Score="0" Text="I mimicked your very definition, which did not defined meshes :-). For volume rendering, the scene is made of density stored in each voxel, i.e. each cell of the 3D grid." CreationDate="2016-02-21T09:39:25.080" UserId="1810" />
  <row Id="2189" PostId="2081" Score="0" Text="I modified the equation and completed the definition. Now for such basic definitions, you should rather google or open a book. ;-) https://en.wikipedia.org/wiki/Volume_rendering" CreationDate="2016-02-21T09:42:41.673" UserId="1810" />
  <row Id="2191" PostId="2078" Score="0" Text="What you are saying may be a valid thing I haven't heard of before, but for the purposes of high performance, I've always heard of scan line conversion working like you convert 3d object to 2d screenspace objects and then use something like bressenham (for triangles, drawing a line down each edge, one y step at a time) to know where to start and end on the x axis for the current y.  In your case I could see similar working, where you do that N times, where each index of N is a slice of the Z axis.  I haven't ever done what you are trying though so can't say for sure if that is a good idea." CreationDate="2016-02-21T21:31:20.850" UserId="56" />
  <row Id="2192" PostId="2083" Score="1" Text="NB:  they don't answer emails about these questions :-)" CreationDate="2016-02-21T21:45:19.160" UserId="1810" />
  <row Id="2193" PostId="2078" Score="0" Text="Whenever it comes to scanline conversion for 2D graphics, definitely it is not a good idea. But in a particular technology of 3d printing, this type of printing head manuevoring has been deemed to effective to generate better printing outputs. This is why I want to find out intersection between scan line segment [(x1,y1)------(x2,y2)] parallel to x-axis or y-axis and polygon edges." CreationDate="2016-02-21T23:33:27.227" UserId="2712" />
  <row Id="2194" PostId="2073" Score="0" Text="Thanks for taking the time, on both the answer and the ShaderToy example!" CreationDate="2016-02-22T14:16:06.993" UserId="457" />
  <row Id="2196" PostId="397" Score="0" Text="Why do you think Wavelet based Denoising would be best?" CreationDate="2016-02-23T07:04:54.733" UserId="234" />
  <row Id="2204" PostId="2088" Score="0" Text="Even if we know the order theres till a infinite number of curves that fit trough the points. Even if we add additional constraints, then the open ends are problematic as their tangent orientation can be arbitrary. A [picture here](http://i.imgur.com/nlWqkRT.png)" CreationDate="2016-02-23T16:17:59.463" UserId="38" />
  <row Id="2207" PostId="2088" Score="0" Text="@joojaa Yes, you are right. But since the packing of points is very dense, I don't expect it to be exact. If I do get to have the right order, I was planning to connect the sequence of points as a polyline." CreationDate="2016-02-23T16:41:32.440" UserId="2562" />
  <row Id="2209" PostId="2088" Score="0" Text="In the code that needs to order the points, are you even aware of the parametric form of the curve? (If not, I'll delete my first answer, because it requires you to know the parametric form.)" CreationDate="2016-02-23T16:49:33.153" UserId="16" />
  <row Id="2210" PostId="2088" Score="0" Text="@MartinBüttner Yes, I do have access to the parametric form of the curve, if it's needed." CreationDate="2016-02-23T16:51:17.707" UserId="2562" />
  <row Id="2211" PostId="2088" Score="0" Text="@andrea.al i do secondary curve fitting for cad data all the time and I was not entirely happy with the results every second time. Thats why I have started to inject normal's to the starting points for the fitting. Problem is that curves tend to lose it in certain situations." CreationDate="2016-02-23T17:29:49.320" UserId="38" />
  <row Id="2214" PostId="397" Score="0" Text="@poor, What make you think that (Not that I think the other way, I'm just wondering)?" CreationDate="2016-02-24T06:42:29.020" UserId="234" />
  <row Id="2215" PostId="2088" Score="1" Text="Please show a typical point set !" CreationDate="2016-02-24T09:26:57.800" UserId="1703" />
  <row Id="2217" PostId="2078" Score="0" Text="Your question is still unclear but I figure that you are trying to fill a polygon with a Hilbert curve, right ? So you are asking about an algorithm that finds the useful portions of a Hilbert curve inside a given polygon ? If yes, do you want whole segments only or precise clipping to the polygonal window ?" CreationDate="2016-02-24T11:03:19.857" UserId="1703" />
  <row Id="2218" PostId="397" Score="0" Text="@Drazick I've read this somewhere 2-3 years ago. Not sure, but I guess it's probably a mixture of different algorithms for different situations. May I ask why you are interested in this? Is there any better approach? *Note: I'm not a physicist. However I'm using denoisers for video very frequently and I'm just curious how they work* :) Also see: http://dsp.stackexchange.com/questions/20086/denoise-images-with-wavelets" CreationDate="2016-02-24T12:26:51.637" UserId="18" />
  <row Id="2219" PostId="2078" Score="0" Text="You guessed right!. I am looking for the whole segments that reside inside the given polygon contour. In the usual scan line conversion we have parallel scan lines that fill the inside  region and in my specific case I am using Hilbert curve to fill the inside region.  Here goes a sample [image](http://i.imgur.com/NTtPYoB.jpg) that shows the pattern. As you can see that intersection test has to be performed with hilbert line segments and polygon outer and inner borders to define the inner region and I need tips in intersection test." CreationDate="2016-02-24T21:07:50.960" UserId="2712" />
  <row Id="2220" PostId="397" Score="0" Text="I would also guess Wavelets, but here - https://ni.neatvideo.com/overview/how-does-it-work the imply something else. Regarding the approach, I'm not sure, Non Local Means should also be good but harder to tune and slower to run." CreationDate="2016-02-25T06:56:27.483" UserId="234" />
  <row Id="2221" PostId="397" Score="0" Text="@Drazick That's neat image, a stand alone app to denoise *images*." CreationDate="2016-02-25T10:46:52.927" UserId="18" />
  <row Id="2222" PostId="397" Score="0" Text="It is the same for Video. But you deleted the comment which creates the context." CreationDate="2016-02-25T11:02:13.980" UserId="234" />
  <row Id="2224" PostId="2094" Score="0" Text="Everything looks ok. Bugs like these are the worst. :( Your quad coordinates are ok. DX Clip space is from (-1.0, 1.0). 2 suggestions: First, enable a debug Device using D3D11_CREATE_DEVICE_DEBUG. And see if anything interesting pops up in the output. Second, try capturing a frame in RenderDoc: https://github.com/baldurk/renderdoc Look at the call graph and see if anything is fishy." CreationDate="2016-02-25T23:28:56.167" UserId="310" />
  <row Id="2225" PostId="2096" Score="0" Text="Jason Allen is also correct. You will need to do both to get your program rendering." CreationDate="2016-02-26T00:25:59.260" UserId="2752" />
  <row Id="2226" PostId="2095" Score="0" Text="Fixed it, no change. See my comment on icStatic's answer" CreationDate="2016-02-26T01:41:21.753" UserId="2726" />
  <row Id="2227" PostId="2096" Score="0" Text="I added more code that is in my project. My D3D11 Initialization and my swap chain resize. Initialization creates rasterization states and also sets one of them. Swap chain makes/remakes the viewport according to screen current size." CreationDate="2016-02-26T01:42:39.307" UserId="2726" />
  <row Id="2228" PostId="2094" Score="0" Text="Have you tried temporarily turning off back-face culling (`D3D11_CULL_NONE` instead of `D3D11_CULL_BACK`) to see if there's a winding order problem?" CreationDate="2016-02-26T04:00:33.473" UserId="48" />
  <row Id="2229" PostId="2094" Score="0" Text="just did, didn't work" CreationDate="2016-02-26T04:04:04.400" UserId="2726" />
  <row Id="2233" PostId="2094" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:30.087" UserId="2726" />
  <row Id="2234" PostId="2096" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:34.900" UserId="2726" />
  <row Id="2238" PostId="2099" Score="0" Text="Its a bit hard to say what something looks like depends on where your camera is and whet Leese you have going on in the scene." CreationDate="2016-02-26T14:20:59.543" UserId="38" />
  <row Id="2239" PostId="2099" Score="0" Text="There is only this one cube in the scene. It is slowly rotating around its z axis and slowly going into the distance (this seems to work fine)." CreationDate="2016-02-26T14:39:16.877" UserId="2508" />
  <row Id="2240" PostId="2099" Score="1" Text="Yes, but that does not help us if we cant see your matrix, etc. You dont have any code that i can help you with. Your supposed to give minimal code to reproduce the problem otherwise theres no debugging to be had. Yes cubes can look very elongated if they are very close to the camera plane." CreationDate="2016-02-26T14:58:51.237" UserId="38" />
  <row Id="2241" PostId="2099" Score="0" Text="@joojaa sorry. I was a bit reluctant to provide any additional code because I am writing all of this in assembly. I supposed that would not help if I posted it here :)" CreationDate="2016-02-26T15:52:36.163" UserId="2508" />
  <row Id="2245" PostId="2104" Score="2" Text="To expand a little on the solution you found: with values like 1366 and 768 in `glFrustum`, you were effectively setting an extremely wide field of view (nearly 180 degrees), which caused extreme perspective distortion; that's why the cube looked weirdly stretched out. You might prefer to use `gluPerspective` instead of `glFrustum`, as the parameters there are more intuitive." CreationDate="2016-02-27T01:22:06.027" UserId="48" />
  <row Id="2246" PostId="2103" Score="0" Text="I don't get it. What is the relation between the MERL database (and the mystery of it's references for colors) and BRDF viewer ? Or do you think there is a special mode dedicated to MERL ? but just browsing the BRDF does not require to accurately depict the colors, right ?" CreationDate="2016-02-27T02:55:41.603" UserId="1810" />
  <row Id="2247" PostId="2103" Score="0" Text="I hoped they would interpret the colours giving you a hint.  They also use two databases so the might have to interpret to unify. Sounds like they don't." CreationDate="2016-02-27T03:01:03.483" UserId="2748" />
  <row Id="2248" PostId="2093" Score="0" Text="What do you consider ligtweight and why not use opengl or matplotlib?" CreationDate="2016-02-27T09:49:06.657" UserId="38" />
  <row Id="2249" PostId="2106" Score="0" Text="How are the objects represented? Surfaces modeled with triangles?" CreationDate="2016-02-28T03:44:26.493" UserId="2500" />
  <row Id="2250" PostId="2106" Score="0" Text="Yes, they are triangular poly meshes." CreationDate="2016-02-28T04:18:50.750" UserId="2748" />
  <row Id="2251" PostId="2105" Score="0" Text="Are you working with integers or floating point numbers?" CreationDate="2016-02-28T04:34:39.430" UserId="56" />
  <row Id="2252" PostId="1662" Score="0" Text="Not a full answer but the smaller amount of memory you use the better, as it will be more likely to fit in caches and have fewer cache misses.  If you have interpolatable values, like you are baking out points on a curve into textures, you might check this out as a way to get higher quality curve lookup tables with less memory: http://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2016-02-28T04:51:46.943" UserId="56" />
  <row Id="2253" PostId="2105" Score="0" Text="Floating point numbers." CreationDate="2016-02-28T11:44:50.393" UserId="2712" />
  <row Id="2254" PostId="2105" Score="0" Text="ah ok, assumed so.  The answer I gave is appropriate for your usage case then.  If using integer coordinates there would be a way to get an EXACT answer without thresholding, but with floating point it's basically unavoidable." CreationDate="2016-02-28T21:04:44.877" UserId="56" />
  <row Id="2255" PostId="2108" Score="0" Text="The above explanation is focused on clarity, but there are some other optimizations you can do if you need it to be even faster.  For instance, instead of using a distance threshold, you can use a squared distance threshold, to avoid a square root.  You could also avoid the normalization step, and clamp between 0 and 1 instead of from 0 to length.  The above explanation should be plenty fast for most needs though!" CreationDate="2016-02-28T21:06:22.633" UserId="56" />
  <row Id="2256" PostId="2107" Score="0" Text="This is for the LUT, but it tells nothing for the gamut, right ? or does it ? (moreover I understood that their capture device was more home-made than the standard camera - and not all camera captors have the same raw gamut anyway)." CreationDate="2016-02-28T21:19:20.050" UserId="1810" />
  <row Id="2257" PostId="2106" Score="0" Text="approximate (conservative or not ?) or exact ? GPU assisted of pure CPU ? rigid object or deformable ? are the 2 objects arbitrary complex or at least one of both has a simple shape ?" CreationDate="2016-02-28T22:02:38.030" UserId="1810" />
  <row Id="2258" PostId="2106" Score="0" Text="Minkowski portal refinement and also GJK  are algorithms commonly used for this." CreationDate="2016-02-29T04:47:15.370" UserId="56" />
  <row Id="2260" PostId="2103" Score="0" Text="If the tool you mention can be used to provide an answer, it would help to explain how. As it stands, this is more of an additional question than an answer." CreationDate="2016-02-29T12:05:49.693" UserId="231" />
  <row Id="2262" PostId="2108" Score="0" Text="You are suggesting a dot product between a point and vector. Should it not be between two vectors. In that case, we have to derive vector from point. We can do it by generating a vector between the point and the start point of the line segment" CreationDate="2016-02-29T16:15:17.363" UserId="2712" />
  <row Id="2265" PostId="2108" Score="0" Text="You are right, good eye.  Fixed!" CreationDate="2016-02-29T16:23:50.907" UserId="56" />
  <row Id="2266" PostId="2093" Score="0" Text="@joojaa Saying &quot;lightweight&quot;, I mean not a game framework, because I don't need a lot of features just to visualize models. Matplotlib took several seconds to render the model; mayavi does it fast, but I don't need a frame with image on it -- I need just a matrix with colors. I started meeting with PyOpenGL and didn't find, how to render triangulated model and get buffer as a matrix instead of displaying it." CreationDate="2016-02-29T17:09:13.360" UserId="2750" />
  <row Id="2267" PostId="2117" Score="0" Text="Here is the texture I'm using: &#xA;http://i.stack.imgur.com/SEi0G.png" CreationDate="2016-02-29T19:19:59.807" UserId="2775" />
  <row Id="2268" PostId="2119" Score="1" Text="I've started experimenting with some of this already.  I've found that sampling in a + sign (5 reads) instead of the full 9 showed no differences in my testing, but im sure with more complex situations, there would be differences.  Doing a full x jfa and then a full y jfa does make lots of errors.&#xA;&#xA;I'll be interested to hear more details/info if you have it, but accepting your answer :P" CreationDate="2016-02-29T20:43:56.783" UserId="56" />
  <row Id="2269" PostId="2119" Score="1" Text="Forgot, here's link to one of my experiments:&#xA;https://www.shadertoy.com/view/Mdy3D3" CreationDate="2016-02-29T20:44:08.737" UserId="56" />
  <row Id="2270" PostId="2119" Score="0" Text="Interesting that it works apparently just as well with only 5 reads - especially since they can't be parallelised. Since the paper lists the cases that lead to error maybe you could deliberately set these up and see if 5 jump directions is still as good." CreationDate="2016-02-29T20:49:41.167" UserId="231" />
  <row Id="2271" PostId="2119" Score="0" Text="Sounds like you are ready to post your own answer..." CreationDate="2016-02-29T20:50:13.827" UserId="231" />
  <row Id="2272" PostId="2118" Score="1" Text="Thanks, I'll try this. It was one of my original methods of building the polygon, however much more complex mathematically so I went with the simpler method. I'll give it another shot and report back." CreationDate="2016-02-29T20:58:35.013" UserId="2775" />
  <row Id="2273" PostId="2119" Score="0" Text="my info supplements yours :P" CreationDate="2016-02-29T21:07:27.703" UserId="56" />
  <row Id="2274" PostId="2108" Score="0" Text="Dont we have to normalize the vector &quot;PRelative&quot; ?" CreationDate="2016-02-29T21:10:42.860" UserId="2712" />
  <row Id="2275" PostId="2108" Score="0" Text="No, you don't and shouldn't!  The length of PRelative is what is projected onto the direction vector (direction must be normalized though).  Are you having problems getting it working still?  If easily done you might try visualizing the various steps of finding the closest point on the line to see where it's breaking down." CreationDate="2016-02-29T21:14:21.913" UserId="56" />
  <row Id="2276" PostId="2108" Score="0" Text="I am defining the vector between closestPoint and P and then getting the length of the vector . If the length of the vector is less than equal to std::numerical_limit&lt;float&gt;::epsilon() , I return true, else return false. Is there anything worng with the concept ? I am still getting the wrong answer." CreationDate="2016-02-29T22:25:58.587" UserId="2712" />
  <row Id="2277" PostId="2108" Score="0" Text="i would make the epsilon larger than that.  The error that creeps in from floating point calculations is likely much larger than the smallest possible difference between two floating point numbers.  I'd try a larger than needed number to start out, and then shrink it down to the smallest value that feels right." CreationDate="2016-02-29T22:27:40.800" UserId="56" />
  <row Id="2280" PostId="2123" Score="3" Text="Don't apologise, I think it's a fair question, seeing that GLM's function naming is quite misleading here. I expect this could be a useful (and concise) reference in the future." CreationDate="2016-03-01T09:52:48.017" UserId="16" />
  <row Id="2282" PostId="2108" Score="0" Text="It works!, Some of the issues are not clear though. When to or not to normalize vectors. Since the normalization of a vector does not change its direction other than clamping its value between zero and one, it would be nice if some one elaborate more over this issue of when to normalize." CreationDate="2016-03-01T13:54:24.103" UserId="2712" />
  <row Id="2283" PostId="2108" Score="1" Text="You might consider asking some new questions about that, perhaps even on the math stack exchange sites.  You might also give dot product / vector projection a Google and read up on that a little bit, there is some great info on the net on that stuff.  Congratulations though, I'm really glad to hear you got it working (:" CreationDate="2016-03-01T14:53:03.910" UserId="56" />
  <row Id="2284" PostId="2117" Score="1" Text="I assume the issue is the shearing of the image? I see this happens where you have inconsistent spacing between the vertices of the triangular subdivision. My guess is you are using consistent steps for your uv mapping, giving you the shearing effect." CreationDate="2016-03-01T23:15:41.800" UserId="2500" />
  <row Id="2285" PostId="2128" Score="3" Text="I don't really think you can be more efficient than a equality check for one coordinate of two points. And as far as I see this check is correct. So... What is your issue with this solution?" CreationDate="2016-03-02T11:57:06.680" UserId="273" />
  <row Id="2286" PostId="2128" Score="2" Text="It may (or may not) be *slightly* faster to subtract the points and check whether one component of the difference is zero. Dragonseel has a valid question though." CreationDate="2016-03-02T12:26:55.480" UserId="16" />
  <row Id="2288" PostId="2128" Score="1" Text="@MartinBüttner Yes, its also easier to deal with floating point inaccuracy that way. Possibly more readable algo... Sounds like premature optimization to me." CreationDate="2016-03-02T13:01:06.953" UserId="38" />
  <row Id="2289" PostId="2130" Score="0" Text="Just thinking aloud... Could you model the negative part of a filter separately to generate two sets of samples, one to be treated as positive and the other as negative? Would this allow arbitrary filters for your second approach (generate in the shape of a filter)?" CreationDate="2016-03-02T20:02:08.893" UserId="231" />
  <row Id="2290" PostId="2073" Score="0" Text="Lerp and nlerp seem “good enough” for angles &lt;90°, but when you start getting higher (especially around 180°), the inaccuracies of lerp/nlerp become extremely pronounced.  I modified your ShaderToy example to have the vectors be 178.2° (180° * 0.99) apart from each other and the lerp/nlerp results are garbage: https://www.shadertoy.com/view/4sGGWd  If you can guarantee that the angle delta will be &lt;90° lerp/nlerp may be a fair optimization; if not, I think using slerp in combination with heavy memoization or a pre-computed lookup table is a safer and fast-enough way to go." CreationDate="2016-03-02T20:23:53.287" UserId="2785" />
  <row Id="2291" PostId="2073" Score="0" Text="I think you missed the fact that you can click the mouse to change the vector angles! and yeah, it gets real bad at larger angles.  You can even see that in the top image in my answer!" CreationDate="2016-03-02T20:40:26.713" UserId="56" />
  <row Id="2292" PostId="2073" Score="0" Text="@AlanWolfe: Side-note: I can write up an answer than pre-computes the `slerp` results into a texture and looks-up the values at runtime, if you're interested.  Given that the color value is the output normal, and if we can assume both lookup vectors are normal-or-nearly-normal-length, I believe the lookup table can be reduced to only plausible inputs, without covering the whole input space (4-dimensional, 2 for each input vector) and without resorting to using vector angles as inputs (2-dimensional, but still incurs the `atan2` cost)." CreationDate="2016-03-02T20:40:34.917" UserId="2785" />
  <row Id="2293" PostId="2073" Score="0" Text="@AlanWolfe Ha, yah, I totally did.  You're one step ahead of me!  _Deleting my shadertoy clone (link above is now dead)._" CreationDate="2016-03-02T20:41:45.603" UserId="2785" />
  <row Id="2294" PostId="2130" Score="0" Text="Maybe? Lemme fiddle with it for a bit" CreationDate="2016-03-02T20:47:44.223" UserId="310" />
  <row Id="2295" PostId="2130" Score="1" Text="Ok, if you track the zeros of the function, you can abs() the output into the pdf. Then when sampling, you can check if you're negative. Sample code here: https://gist.github.com/RichieSams/aa7e71a0fb4720c8cb41" CreationDate="2016-03-02T22:31:26.867" UserId="310" />
  <row Id="2296" PostId="2117" Score="0" Text="I actually fixed that shortly after posting this by making the subdivisions consistent and smaller, it fixes the shearing somewhat (although not completely). However the texture is still very distorted. I'm working on @nathan suggestion below, which at the very least will allow for a much finer subdivision." CreationDate="2016-03-03T00:06:08.263" UserId="2775" />
  <row Id="2297" PostId="2135" Score="0" Text="Try a game engine like unity, etc... not sure this is the best place for this question though" CreationDate="2016-03-03T12:37:20.770" UserId="38" />
  <row Id="2298" PostId="2135" Score="0" Text="Thanks, I have thought of Unity already. But I think it would be overkill for my purpose. I was hoping to find something not so complex." CreationDate="2016-03-03T14:11:00.490" UserId="2789" />
  <row Id="2299" PostId="2134" Score="2" Text="Are you looking to measure details smaller than a pixel by looking at several different instances of the pattern that have different offsets from the pixel grid? Or are you looking to use the arrangement of the red, green and blue subpixels to improve accuracy with a single instance of the pattern?" CreationDate="2016-03-03T14:48:16.543" UserId="231" />
  <row Id="2300" PostId="2137" Score="0" Text="Have you tried tessellating the environment map and associating depth with each vertex? Then crossfading as you move from one point to the other." CreationDate="2016-03-03T15:43:11.610" UserId="2500" />
  <row Id="2301" PostId="2132" Score="0" Text="Thanks! These are great resources. So, in the end, there are 3 methods? 1. Generate and Weigh with splatting 2. Generate and Weigh without splatting 3. Generate in the Shape of a Filter" CreationDate="2016-03-03T15:54:38.817" UserId="310" />
  <row Id="2302" PostId="2132" Score="0" Text="Do you know of any papers, blogs, etc. that explore how to parallelize Generate and Weight *with* splatting? Off the top of my head, you could have a mutex per tile, or make each pixel atomic." CreationDate="2016-03-03T15:57:59.273" UserId="310" />
  <row Id="2304" PostId="2131" Score="0" Text="It isn't clear why static objects and empty cells should allow the deletion of rows and columns. Are you setting these rows and columns to zero or removing them altogether to give a smaller matrix?" CreationDate="2016-03-03T17:13:24.183" UserId="231" />
  <row Id="2305" PostId="2131" Score="0" Text="In case the problem is somewhere other than where you guess, it would help to see the code, if this is something you are happy to share. Ideally an [MCVE](http://stackoverflow.com/help/mcve)" CreationDate="2016-03-03T17:14:50.840" UserId="231" />
  <row Id="2307" PostId="2131" Score="0" Text="Hey trichoplax. A matrix with an all zero row or column would be singular, as far as I know, so I instead remove them from the matrix to make a smaller matrix (as well as their corresponding entries in the b vector)." CreationDate="2016-03-03T17:57:59.703" UserId="2786" />
  <row Id="2308" PostId="2131" Score="0" Text="I will edit an MCVE in tonight when I am near my computer with the source." CreationDate="2016-03-03T17:58:37.703" UserId="2786" />
  <row Id="2309" PostId="2131" Score="0" Text="I also suspected that I was maybe making a wrong assumption somewhere else in the code, however this only pertains to the matrix structure (and whether or not it's singular). The only thing I can think of is what qualifies as a &quot;surface cell&quot; vs an air cell or a liquid cell. If this is a liquid cell adjacent to an air cell, is there something different that I should be doing with its corresponding columns/rows?" CreationDate="2016-03-03T18:00:50.033" UserId="2786" />
  <row Id="2310" PostId="2132" Score="2" Text="@RichieSams I don't know why you'd use &quot;generate and weigh without splatting&quot;, actually—that seems like it would be worse in any case than filter importance sampling. I was assuming that &quot;generate and weigh&quot; implies splatting. As for parallelization of splatting, off the top of my head, one way would be to split the image into tiles, but give each tile a 2‒3 pixel border to catch splats that cross the tile edge. Then in a final pass, additively composite the bordered tiles together into the final image." CreationDate="2016-03-03T18:20:25.620" UserId="48" />
  <row Id="2311" PostId="2131" Score="0" Text="I edited the question to include my code for generating the matrix. It is in Processing/Java." CreationDate="2016-03-03T19:49:29.360" UserId="2786" />
  <row Id="2312" PostId="2134" Score="0" Text="I'm trying to measure details smaller than a pixel so by several instance you mean shifting it to left or right and then match .Thanks for your response." CreationDate="2016-03-04T03:24:09.633" UserId="2788" />
  <row Id="2314" PostId="2135" Score="0" Text="if it's really for trivial shapes and to play with positions and orientations, why base OpenGL is not good ?" CreationDate="2016-03-04T07:06:06.737" UserId="1810" />
  <row Id="2315" PostId="2139" Score="0" Text="I don't see a problem, doubles can represent numbers as small as 10^-308." CreationDate="2016-03-04T09:00:09.867" UserId="1703" />
  <row Id="2316" PostId="2128" Score="0" Text="You will get the most appropriate answers by tell us **why** you want to detect such segments." CreationDate="2016-03-04T09:03:50.460" UserId="1703" />
  <row Id="2317" PostId="2134" Score="0" Text="By several instances I mean an image with a repeat pattern as described [here](http://computergraphics.stackexchange.com/questions/1880/what-is-the-state-of-the-art-on-using-computers-to-clean-up-images/1881#1881). By red, green and blue subpixels I mean taking advantage of the placement of different colours within a single pixel (the pixel geometry) as described [here](http://computergraphics.stackexchange.com/questions/424/subpixel-rendering-for-a-ray-tracer). It sounds like you probably mean the first approach." CreationDate="2016-03-04T12:30:21.587" UserId="231" />
  <row Id="2319" PostId="2118" Score="0" Text="Thank you. I was able to modify your suggestion into a solution. See my edit above." CreationDate="2016-03-04T17:16:09.480" UserId="2775" />
  <row Id="2320" PostId="2131" Score="0" Text="I added more information to the post." CreationDate="2016-03-04T18:02:45.977" UserId="2786" />
  <row Id="2321" PostId="2139" Score="1" Text="Perhaps your problem might stem from the order of ops in your calculation. You might have something like  (BigValue + SmallValue) - BigValue, and instead of getting &quot;SmalValue&quot; you get 0." CreationDate="2016-03-04T18:29:46.107" UserId="209" />
  <row Id="2322" PostId="2139" Score="1" Text="Just occurred to me that you seem to, effectively, have a rotation, R, followed a translation T.  If you can keep these separate, to get Inverse(R*T) you can just do Inverse(T)*Inverse(R), both of which are trivial. Would that help?" CreationDate="2016-03-04T18:46:03.690" UserId="209" />
  <row Id="2323" PostId="2139" Score="0" Text="Simon, you're right about the problem stemming from multiplying numbers of different magnitudes. The matrix is more complex than an R*T; it's also a scale, and can be a multiplication of any affine.. I managed to fix the problem, however, see below. Thanks for your help." CreationDate="2016-03-04T21:10:12.203" UserId="2792" />
  <row Id="2324" PostId="2139" Score="0" Text="You should ask on the math forum !" CreationDate="2016-03-04T21:16:23.010" UserId="1810" />
  <row Id="2325" PostId="2144" Score="0" Text="But the other line x = x1 is an infinite line parallel to the y-axis. Is not there a significant difference between a line and a line segment where both are parallel to y-axis ?" CreationDate="2016-03-04T21:55:48.563" UserId="2712" />
  <row Id="2326" PostId="2144" Score="0" Text="woops, you want segments. then verify that y match, and that $\lambda$ match." CreationDate="2016-03-05T00:47:07.520" UserId="1810" />
  <row Id="2329" PostId="2140" Score="0" Text="Is it known which line segment is axis parallel and which axis it is parallel to? Are you given two line segments, where one of them is parallel to one of the axes but you don't know which line segment or which axis, or are you given the first line segment, knowing it is always parallel to the x axis, and the second one may be any arbitrary line segment?" CreationDate="2016-03-05T18:04:10.747" UserId="231" />
  <row Id="2330" PostId="2140" Score="0" Text="These seemingly subtle differences may make a large difference to the approach, and so answerers will need to know which is the case." CreationDate="2016-03-05T18:05:11.483" UserId="231" />
  <row Id="2331" PostId="2140" Score="0" Text="It is confirmed that one of the line segment is either parallel to x-axis or y-axis and the other line segment may be or may be not parallel to either of the axis." CreationDate="2016-03-05T18:28:54.393" UserId="2712" />
  <row Id="2332" PostId="2133" Score="0" Text="24 bit shifters are used in single precision floating point to align mantissas, so the compiler might generate a few, but I don't think you'll see 30." CreationDate="2016-03-05T18:34:46.297" UserId="2500" />
  <row Id="2333" PostId="1893" Score="0" Text="I'm betting the 0.12 is in &quot;uv space&quot; meaning it's 12% of the image size (on each axis)." CreationDate="2016-03-07T04:11:23.850" UserId="56" />
  <row Id="2337" PostId="2139" Score="0" Text="@solendil sorry yes I meant to mention scaling as well, but missed the edit timeout. Scaling is, obviously, also trivial to include particularly if it's the same for both dimensions. FWIW I used this trick in an API for some early PC graphics chips. Anyway, glad you've resolved your issues." CreationDate="2016-03-07T08:52:34.917" UserId="209" />
  <row Id="2338" PostId="316" Score="0" Text="Of course I don't have all the details, but it seems to me that this person merely used a pseudo-distance field in place of a regular one, which has already been demonstrated in a 2006 paper by Qin, McCool and Kaplan, &quot;Real-time texture-mapped vector glyphs&quot;, which is also referenced in the Valve paper. It only affects the miters of outlines and does nothing to improve the appearance of corners. I suspect the reason it looks sharp is because he uses unpractically large distance field textures. I might be wrong though." CreationDate="2016-03-07T12:21:17.253" UserId="2811" />
  <row Id="2339" PostId="2151" Score="4" Text="Great first answer, welcome to the Computer Graphics SE! :) Is your thesis publicly available? (Or will it be after you've finished said paper?) If so it would probably be very helpful to link to that, too." CreationDate="2016-03-07T13:57:41.343" UserId="16" />
  <row Id="2340" PostId="2151" Score="0" Text="It is supposed to be publicly available, but it seems the school hasn't put it up yet. Anyway, I would prefer not to spread it right now, since the article I'm writing will really explain the important parts much better and focus on how to implement it, and it should be complete very soon." CreationDate="2016-03-07T14:26:47.413" UserId="2811" />
  <row Id="2341" PostId="2151" Score="0" Text="@Detheroc Please notify here and on the gamedev Q when you are done with the article. Explanation's still not 100% clear for me. I would suggest showing the composition step by step in images." CreationDate="2016-03-07T15:09:17.600" UserId="101" />
  <row Id="2342" PostId="2151" Score="1" Text="would love to be able to replicate your current results even if they are not as good as your future results, +1 to sharing whatever details you can.  very exciting.  Have you considered either technique's application towards ray marching (sphere tracing)?  In volume textures or similar..." CreationDate="2016-03-07T19:36:45.700" UserId="56" />
  <row Id="2343" PostId="2151" Score="0" Text="Also, obviously would love to see how it compares to just having a single channel texture that has 3x as many pixels (;" CreationDate="2016-03-07T23:18:19.107" UserId="56" />
  <row Id="2344" PostId="2150" Score="1" Text="You might try asking on the math stack exchange site if you don't get an answer here." CreationDate="2016-03-08T04:43:30.923" UserId="56" />
  <row Id="2345" PostId="2150" Score="2" Text="I must say that i dont understand what the inaccuracy is. So I cant help. Basically your saying Im doing A but A does not work. Without explaining what thing that does not work is." CreationDate="2016-03-08T07:17:27.507" UserId="38" />
  <row Id="2346" PostId="2150" Score="0" Text="@joojaa, When i fit single surface from a set of sampled points (Each sampled point was sampled from one of the source surface), the resulting surface fails to achieve good accuracy (In sense of maximum deviation between resulting surface and the set of sampled points). So I'm asking if there is some another method to do same thing (Get single surface from different trimmed surfaces) with smaller loss in accuracy, because least squares method gives too rough results." CreationDate="2016-03-08T11:37:15.227" UserId="2644" />
  <row Id="2347" PostId="2150" Score="1" Text="Make a picture showing the error." CreationDate="2016-03-08T11:42:02.893" UserId="38" />
  <row Id="2348" PostId="2150" Score="0" Text="@joojaam, it's just an error of NLib library, which tells that i cannot approximate set of points with desired tolerance. It's programming related, not from CAD package, sorry, i forgot to mention this." CreationDate="2016-03-08T16:02:01.437" UserId="2644" />
  <row Id="2350" PostId="2150" Score="1" Text="Even if the error is not clearly visible, it may help to include in the question an image of a curved surface for which the accuracy is unacceptable, and the (possibly textual) evidence that the accuracy is not sufficient. This will give answerers a better idea of what you are dealing with." CreationDate="2016-03-08T20:00:50.473" UserId="231" />
  <row Id="2351" PostId="2153" Score="2" Text="What are you trying to do with the array and loop? I'm asking because this somehow sounds like an [XY Problem](http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem) to me. Since the best way to use conditions and loops on the GPU is to refrain from using them, maybe there are even better ways instead of using arrays and loops in your case." CreationDate="2016-03-08T21:38:13.670" UserId="127" />
  <row Id="2352" PostId="2153" Score="0" Text="I am implementing a screenspace subsurface scattering effect which currently work. But I have some doubts about the way I use the kernel according to performances. I've choose to do a maximum array size and fill only a part and use a dynamic loop with a dynamic number of iteration which is related to the currently used array content.&#xA;I think that there are things to do or know when programming shaders according to performances for example. And in my opinion loops is a common performance topic which might follow some rules and maybe &quot;good practices&quot; but I didn't found any good answer about it." CreationDate="2016-03-09T07:38:31.413" UserId="2372" />
  <row Id="2354" PostId="2156" Score="0" Text="Out of curiosity, what was the precision before? Low or Medium?" CreationDate="2016-03-09T09:33:26.220" UserId="209" />
  <row Id="2355" PostId="2152" Score="0" Text="Doh:  actually I'm wrong about the method not being uniform...working on a rewrite." CreationDate="2016-03-09T10:57:04.507" UserId="2831" />
  <row Id="2356" PostId="2152" Score="0" Text="OK, I put up a first revision." CreationDate="2016-03-09T12:17:30.033" UserId="2831" />
  <row Id="2363" PostId="2156" Score="0" Text="I found a document saying it's low for samplers and high for float/int by default on ES. Oh, ints depend on shader type I've added link to my answer." CreationDate="2016-03-09T21:16:58.943" UserId="2840" />
  <row Id="2364" PostId="2163" Score="0" Text="To be clear, are you looking for an existing software renderer for vector graphics, or are you looking to learn how to write a software renderer for vector graphics on your own?" CreationDate="2016-03-10T04:58:27.120" UserId="48" />
  <row Id="2365" PostId="2163" Score="0" Text="i am looking to write a software renderer for vector graphics on my own" CreationDate="2016-03-10T04:59:42.887" UserId="2853" />
  <row Id="2366" PostId="2163" Score="0" Text="Are you trying to make a program that outputs an image file? Or are you trying to make a program that shows the vector shapes on the screen and lets you edit them in real time?  What operating system are you using?" CreationDate="2016-03-10T06:19:25.380" UserId="56" />
  <row Id="2367" PostId="2152" Score="0" Text="Added the area-distortion note" CreationDate="2016-03-10T09:13:11.253" UserId="2831" />
  <row Id="2368" PostId="2159" Score="0" Text="The scene graph technique that I am looking forward to implement is very similar to the http://www.openscenegraph.org/ . I have used it for a while and I am familiar to it. The scene that you created with it can be stored as a trivial file format which can be processed further by this library." CreationDate="2016-03-10T10:53:03.073" UserId="2712" />
  <row Id="2369" PostId="2163" Score="0" Text="I'm afraid it is not possible for us to narrow the question down for you, as we do not know specifically what you want. Is there a similar application that already exists for comparison, so we can see what you intend to build? What will the finished program be used for? Will a shape be displayed as a wire frame, a solid, with flat shading or realistic lighting? Do you want to produce images in real time or slowly generate high quality images?" CreationDate="2016-03-10T12:08:33.670" UserId="231" />
  <row Id="2370" PostId="2156" Score="0" Text="OK. Was just wondering if you'd tried mediump as there could be performance (and power) benefits." CreationDate="2016-03-10T12:30:13.953" UserId="209" />
  <row Id="2371" PostId="2163" Score="0" Text="i wan't to create a 3D program where the only thing it does is showing a box with 8 vertex points , 12 edges in wire frame . This program will be an executable for windows operating system 10 . I wan't to create this 'sample' structure program without using OpenGL, DirectX or Vulkan , only with custom code for the entire input , output of the program . That means creating the class methods for vertex and edges and a ouput system to the screen with vector graphics. It may be in CLI or MFC , it does not matter for me." CreationDate="2016-03-10T14:03:53.507" UserId="2853" />
  <row Id="2372" PostId="2168" Score="0" Text="The option to queue the frame but not wait on it, would that be considered tripple (and higher) buffering?" CreationDate="2016-03-10T14:19:45.650" UserId="56" />
  <row Id="2373" PostId="2168" Score="0" Text="@AlanWolfe possibly, you need at least 3 buffers for a non-tearing and non-blocking view, one to display, one as the next to display and one that's being rendered to." CreationDate="2016-03-10T14:21:21.993" UserId="137" />
  <row Id="2375" PostId="2159" Score="0" Text="I would either look into the javascript library http://osgjs.org/, which also is based on openscenegraph.org. But if you want to have more control over the interaction with the library i would suggest trying to compile the openscenegraph using emscripten (as it is open source), or leaving the compiled library as is and creating javascript to c++ bindings for it (also possible with emscripten iirc)." CreationDate="2016-03-10T16:31:28.303" UserId="64" />
  <row Id="2376" PostId="2162" Score="0" Text="Thanks for your suggestion. Must try it, there are a good few demos on youtube, but I had never seen any mentioning live imput from mike." CreationDate="2016-03-10T19:23:51.333" UserId="2848" />
  <row Id="2377" PostId="2151" Score="0" Text="@Detheroc This is great. I'm also interested to get notified when you release it publicly." CreationDate="2016-03-11T00:55:40.090" UserId="250" />
  <row Id="2378" PostId="2163" Score="0" Text="what is your reasoning for not wanting to use a graphics API? Without a graphics API you will be doing CPU software rendering which is slower and not the modern way to do graphics. The graphics APIs utilize the graphics card's hardware, that is all.  You really are best off using opengl, directx or similar, unless you have a really strange reason why you can't use them." CreationDate="2016-03-11T03:37:11.020" UserId="56" />
  <row Id="2379" PostId="2163" Score="0" Text="i want to learn the work flow and understand the logic in details. It's only for educational purpose . There is not any big work going on or idea . You can say that i am a maniac in knowing this things how they work down in detail and code. I can use a API and i know how , it's just for my own personal reason." CreationDate="2016-03-11T05:30:16.603" UserId="2853" />
  <row Id="2380" PostId="2163" Score="0" Text="So I have re-interpreted the question as follows: *How to make the rasterizer of vector graphics yourself?* Does that sound about right? It hard to find a modern operating system functionality that allows you to manage everything yourself. Even the lowest level modern apis know how to draw lines for you." CreationDate="2016-03-11T08:42:40.920" UserId="38" />
  <row Id="2382" PostId="2170" Score="1" Text="Will you want to repeatedly test many different angles of ray from the same starting point, against the same polyline? Or will the starting point and angle both be variable? (What I'm getting at is it's probably possible to build an acceleration structure that would speed up these queries, but which structure is best will depend on how it's going to be used.)" CreationDate="2016-03-11T21:45:17.817" UserId="48" />
  <row Id="2386" PostId="2172" Score="0" Text="Putting online and use the google image advanced search ? :-p ( beside kidding, they might have publish (white) papers on that )." CreationDate="2016-03-12T00:10:26.250" UserId="1810" />
  <row Id="2387" PostId="2175" Score="1" Text="I really like this idea, I'll have to try it out. It makes perfect sense. Thanks!" CreationDate="2016-03-12T02:49:48.353" UserId="31" />
  <row Id="2388" PostId="2170" Score="0" Text="Nathan, I will be testing many from a single starting point. And then test multiple angles from another starting point and another and etc. I actually have segments of two contour lines and am trying to draw a line in between the two of them. My current strategy is to draw a series of lines at semi-regular intervals between the two lines. I can then create a polyline between all the short crossing lines.  I'm currently working on how to create the lines that cross between the two contours, which is surprisingly difficult due to how curvy and convoluted some of them are." CreationDate="2016-03-12T03:41:19.340" UserId="2863" />
  <row Id="2390" PostId="2172" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it." CreationDate="2016-03-12T11:25:11.537" UserId="231" />
  <row Id="2391" PostId="2172" Score="0" Text="As it stands, this question does not specify what types of images will need to be considered, so I am closing as unclear. The question may be reopened if it can be edited to clarify what types of non-photographic images will be presented, and to request an algorithm rather than an off site resource recommendation." CreationDate="2016-03-12T11:28:02.647" UserId="231" />
  <row Id="2392" PostId="2172" Score="0" Text="See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:29:43.333" UserId="231" />
  <row Id="2394" PostId="2135" Score="0" Text="Not sure if this is what you're looking for, but [OpenSceneGraph](http://www.openscenegraph.org/) or [Vtk](http://www.vtk.org/) are [scene graph toolkits](https://en.m.wikipedia.org/wiki/Scene_graph). You may want to check licensing. I think they are LGPL but I'm not sure." CreationDate="2016-03-05T13:39:03.330" UserId="2562" />
  <row Id="2395" PostId="2135" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:38:51.463" UserId="231" />
  <row Id="2397" PostId="1758" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T12:23:28.780" UserId="231" />
  <row Id="2399" PostId="2170" Score="0" Text="So your triong to find the closest point on the curve?" CreationDate="2016-03-12T15:51:57.763" UserId="38" />
  <row Id="2400" PostId="2170" Score="0" Text="I tested out closest point on the curve and while it can work there are too many cases where it doesn't work at all or works poorly. The approach I'm currently considering is based on angles. When a line is drawn between two other lines 4 angles are created. I'm thinking the &quot;best&quot; line is created when all 4 angles are as close to 90 degrees as possible. So at a given position I intend to draw every possible line (intervals of 5 or 10 degrees or similar) and select the best line based on a ranking of the 4 angles." CreationDate="2016-03-12T16:51:03.637" UserId="2863" />
  <row Id="2401" PostId="2163" Score="0" Text="joojaa , yes that is about what i wan't ." CreationDate="2016-03-12T17:02:06.673" UserId="2853" />
  <row Id="2402" PostId="2170" Score="0" Text="Would you be interested in solutions that use precomputed acceleration structures? Bsp trees and possibly vornoi diagrams seem likely useful (:" CreationDate="2016-03-12T17:26:06.243" UserId="56" />
  <row Id="2403" PostId="2163" Score="0" Text="Ah ok Roger.  To help you out in googling or asking further questions, this might be referred to as software rendering or software rasterization." CreationDate="2016-03-12T20:08:41.263" UserId="56" />
  <row Id="2406" PostId="2178" Score="1" Text="If you take a screenshot using Alt and printscreen, instead of printscreen alone, it will just copy the current window instead of the whole screen, so you can show just the relevant part of your screen." CreationDate="2016-03-13T13:51:29.133" UserId="231" />
  <row Id="2407" PostId="2178" Score="1" Text="Thanks for the suggestion!" CreationDate="2016-03-13T13:55:44.587" UserId="2096" />
  <row Id="2408" PostId="1614" Score="0" Text="Hi cupe, is it possible to take a look to the code somewhere? Ps: are you checking also for multiple entries?" CreationDate="2016-03-13T18:35:18.960" UserId="1561" />
  <row Id="2409" PostId="1983" Score="0" Text="@trichoplax 1. Why now, after all this time? 2. Moreover, the response to the [meta-question](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic/212#212), while not firm, was leaning towards allowing the question. 3. Finally, as I have explained there, the argument about &quot;opinionated answers and spam&quot; is completely inapplicable in the case of requests for reputable sources for a specific fact. Either there is such a source or not." CreationDate="2016-03-13T20:29:40.783" UserId="2574" />
  <row Id="2412" PostId="1983" Score="0" Text="If anyone disagrees that off site resource requests should be off topic, they can have their say in [Are questions asking for off site resources on topic?](http://meta.computergraphics.stackexchange.com/questions/146/are-questions-asking-for-off-site-resources-on-topic) If anyone thinks requests for reputable sources should be an exception to the off site resources rule, they can have their say in [Are requests for reputable sources on topic?](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic)" CreationDate="2016-03-14T00:10:10.030" UserId="231" />
  <row Id="2415" PostId="2181" Score="0" Text="You might also want to read up on signed distance fields in general by the way. Seems relevant as you might possibly be able to skip the distance transform step and just draw sdf's" CreationDate="2016-03-14T03:12:19.650" UserId="56" />
  <row Id="2416" PostId="2177" Score="0" Text="Thank you for the suggestions Nathan, I'll look into BVH. Delaunay triangulation could be useful, but I'm seeing potential issues in the example you posted. I'm having the greatest issues where one contour is significantly longer than the other. On the ENE ridge in the image the triangles are spanning the same contour, which is the type of location that I really need the lines to span both contours to interpolate a smooth curve. I may have to go with a ray tracing type approach." CreationDate="2016-03-14T17:05:09.320" UserId="2863" />
  <row Id="2417" PostId="2183" Score="0" Text="Thank you for the response!" CreationDate="2016-03-14T18:25:40.887" UserId="2096" />
  <row Id="2418" PostId="2183" Score="0" Text="I got the problem!" CreationDate="2016-03-14T18:26:00.437" UserId="2096" />
  <row Id="2420" PostId="2177" Score="0" Text="@MikeBannister, yes, there are places where triangles span the same contour, but you could detect those and filter them out, and only look at triangles that span two adjacent contours." CreationDate="2016-03-14T21:17:01.287" UserId="48" />
  <row Id="2421" PostId="2156" Score="0" Text="mediump doesn't help in my case, and also I don't notice any significant performance difference either." CreationDate="2016-03-14T21:22:20.267" UserId="2840" />
  <row Id="2422" PostId="2181" Score="0" Text="Thanks, it seems to work quite well. Using OpenCV, applying a distance transform, thresholding the image and finally normalizing it back to 0 to 255 values seems to give quite a good result. Some jagged lines are really visible though. I tried a Gaussian blur which kind of works but I would really like to know if there is a better solution." CreationDate="2016-03-15T02:54:12.533" UserId="2319" />
  <row Id="2423" PostId="2181" Score="0" Text="There is a better solution.  The topic kind of warrants it's own question but here's a short answer.  First step is to make it fade from white to black over a specific distance.  Like if 10 was the distance that went from black to white, you could make it fade between 9 and 11 where you use the distance value to figure out how light or dark to make the pixel based on distance.  Next, you take that pixel shade, which should be between 0 and 1, and put it through the smoothstep function.  That will turn it from a linear fade to something a lot more appealing. Adjust fade distance to taste (:" CreationDate="2016-03-15T03:10:46.917" UserId="56" />
  <row Id="2424" PostId="2181" Score="0" Text="Here is the details of the smoothstep function:  https://en.m.wikipedia.org/wiki/Smoothstep  for a better or more detailed answer you might ask how to do anti aliasing when rendering using a distance field." CreationDate="2016-03-15T03:12:21.340" UserId="56" />
  <row Id="2425" PostId="2181" Score="0" Text="oh sorry, to address more towards your original question.  I believe the curve just says how to turn the linear distance into a pixel shade.  like if your width was 10 pixels, and you were 5 pixels away (either on the positive or negative side), that it would take that to mean you were at 50% distance, and do a lookup on the curve at 50% to see how white or black to make the output pixel." CreationDate="2016-03-15T03:47:51.517" UserId="56" />
  <row Id="2426" PostId="2189" Score="2" Text="You can (and probably should) include images straight in the post (which I've done for your post now). Just hit Ctrl+G while editing the post and drag your image file into the browser. That way it will be hosted on imgur (on a specific Stack Exchange subdomain), which is a lot less likely to be subject to link rot at some point in the future (plus, people don't have to follow a link to your image to see what the problem)." CreationDate="2016-03-15T15:37:43.417" UserId="16" />
  <row Id="2427" PostId="2189" Score="0" Text="@MartinBüttner oh, thanks! I didn't know that I can do that here :)" CreationDate="2016-03-15T15:40:11.260" UserId="2508" />
  <row Id="2428" PostId="2188" Score="0" Text="Given this and your previous answers is there a reason why you do not use nurbs and B-Rep?" CreationDate="2016-03-15T17:35:16.007" UserId="38" />
  <row Id="2429" PostId="1963" Score="0" Text="I just came across this paper from SIGGRAPH 2006 that talks about a way to do this using wang tiles.&#xA;Recursive Wang Tiles for Real-Time Blue Noise&#xA;http://johanneskopf.de/publications/blue_noise/" CreationDate="2016-03-15T18:53:54.283" UserId="56" />
  <row Id="2430" PostId="2181" Score="0" Text="Sorry if my comment was a bit unclear.&#xA;&#xA;here is an [image](http://i.imgur.com/RKxyKtH.png) of the result I get.&#xA;&#xA;As you can see in the result, jagged strips are quite visible.&#xA;Like I mentioned, I used the distance transform + threshold + normalize to &quot;make it fade from white to black over a specific distance&quot;.&#xA;&#xA;You mentioned &quot;anti aliasing when rendering using a distance field&quot;. Is this what corresponds to the issue here?" CreationDate="2016-03-15T20:24:52.497" UserId="2319" />
  <row Id="2431" PostId="2181" Score="0" Text="Did you put the shade color through smoothstep to make the blending non linear? If so that is really weird. Probably worth another question about just that specifically. (Using terminology of rendering using a distance field)" CreationDate="2016-03-15T20:28:58.600" UserId="56" />
  <row Id="2432" PostId="2188" Score="0" Text="Yes. Whatever I build must then be uploaded in CSG format to a very old piece of software that then does raytracing on it. And the old software is setting up some STRONG restrictions on functions. However I seem to have found a workaround. As soon as I get approval on the figures I'll post my current solution." CreationDate="2016-03-16T08:14:08.810" UserId="2858" />
  <row Id="2433" PostId="2188" Score="0" Text="Yes but if you could tell what the old ray tracer is then we can come up with better solutions. I mean first you tell use it has to be torus and sphere cylinder etc. than you relax it to hull and Malinowski functions. Neither one of those alone are perfect solutions. But given artificial restrictions that are there because you think you are restricted in this way is not effective communication.   B-Rep is for most part CSG." CreationDate="2016-03-16T08:28:55.827" UserId="38" />
  <row Id="2434" PostId="2188" Score="0" Text="You are indeed correct. The final object must be constructed in Craig E. Kolb's Rayshade program and it must be inputted as a text file. I am still not allowed to use commands such as Hull or Minkowski since they are not available in Rayshade. I was looking for a way to simulate them. Up to now I've had (in my opinion) relative success by taking three projections of the objects and then intersecting them.   Documentation about Rayshade can be found in here: http://graphics.stanford.edu/~cek/rayshade/doc/guide/guide.html" CreationDate="2016-03-16T09:19:58.120" UserId="2858" />
  <row Id="2435" PostId="2190" Score="1" Text="I've been thinking about this myself and I'm not entirely sure, but here's some inspiration that might or might not be valid... So can there be a continuity? Ontologically no. Phenomenologically yes." CreationDate="2016-03-16T09:26:56.747" UserId="385" />
  <row Id="2436" PostId="2188" Score="0" Text="Howabout using sweptsph that gives you quite some options." CreationDate="2016-03-16T10:32:09.303" UserId="38" />
  <row Id="2437" PostId="2188" Score="0" Text="It turns out that intersecting the projections from three different views gives me an acceptable quality. I will check swepthsph if I need extra smoothness in the figures but for now the method I'm using is enough. Thank you very much for taking the time to help." CreationDate="2016-03-16T12:58:44.057" UserId="2858" />
  <row Id="2438" PostId="2190" Score="2" Text="BRDF space is definitely continuous. The classes of BRDFs you mention are simplified slices of BRDF space and it would take careful analysis of the formulas to decide if there is a parameter for specular in Cook-Torrence that gives a lambertian result when used in a physically based renderer. I think of a surface with both specular and diffuse reflection as being &quot;layered&quot; - like a gloss coating on a magazine, a polished outermost layer (specular) with crevices - all plastics seem translucent, underneath a shiny layer, light bounces around before being reemitted. Thus the divided model." CreationDate="2016-03-16T13:31:28.293" UserId="2500" />
  <row Id="2439" PostId="2193" Score="0" Text="Thank you for the response!" CreationDate="2016-03-16T20:11:46.887" UserId="2096" />
  <row Id="2440" PostId="2193" Score="0" Text="And this discrete nature of depth map is created because of floating point precision. Right?" CreationDate="2016-03-16T20:12:31.963" UserId="2096" />
  <row Id="2441" PostId="2193" Score="1" Text="No its created because images are discrete as in have only one value for a area that varies." CreationDate="2016-03-16T21:00:08.700" UserId="38" />
  <row Id="2442" PostId="2193" Score="0" Text="Images are different in camera and light space?" CreationDate="2016-03-16T21:04:51.713" UserId="2096" />
  <row Id="2443" PostId="2176" Score="0" Text="thanks a lot for your help in this subject . It covers the basics that i need to continue what im searching for . i am grateful  for the help." CreationDate="2016-03-17T00:03:55.267" UserId="2853" />
  <row Id="2444" PostId="2176" Score="0" Text="Glad to hear! Sometimes it's hard to know what to search for." CreationDate="2016-03-17T00:14:55.893" UserId="56" />
  <row Id="2445" PostId="2189" Score="2" Text="I'd suggest to proceed step by step: try to display the framebuffer (directly with a quad) to validate its content first. Also, it looks like the UV on your cube might be wrong, so you may want to display your cube with its texture coordinates (`gl_FragColor = vec4(gl_TexCoord[0].xy, 0., 1.);`)." CreationDate="2016-03-17T03:01:52.287" UserId="182" />
  <row Id="2446" PostId="2187" Score="0" Text="Great question, I'm also curious." CreationDate="2016-03-17T03:48:40.693" UserId="56" />
  <row Id="2447" PostId="2187" Score="0" Text="I suggest you do a test by outputting a grey scale band, and see how it looks on all your available platforms. then try the same thing with `pow(c, 1/2.2)` at the end of the pipeline. Your trained eye will immediately see which is good and which is over-done. Over-done gamma should result in banding." CreationDate="2016-03-17T08:23:56.873" UserId="1614" />
  <row Id="2448" PostId="2174" Score="0" Text="Yeah. I see how detecting toy story images could classify, but Octane render/maxwell render or any path traced image is just going to shit in the fan." CreationDate="2016-03-17T08:26:35.730" UserId="1614" />
  <row Id="2449" PostId="2187" Score="0" Text="I came here *because* of the test I've done :-) https://www.shadertoy.com/view/4stSRN&#xA;The real world of webGLSL is incredibly messy and unrobust: behaviors can depend on driver, browser, OS, settings (Angle vs native OpenGL, display settings - soft and hard), plus the versions of all these. ( Of course here I had to trust people telling there system is well calibrated. )" CreationDate="2016-03-17T08:28:42.067" UserId="1810" />
  <row Id="2450" PostId="2181" Score="0" Text="Smoothstep does not solve the jagged lines problem which is not really surprising. It remaps the values in a smoother curve but it does not fix the aliasing in any way. I will address another question targeting this specific issue." CreationDate="2016-03-17T11:51:21.250" UserId="2319" />
  <row Id="2451" PostId="2193" Score="0" Text="Ok is that zigzag line represents depth map?" CreationDate="2016-03-17T14:14:02.127" UserId="2096" />
  <row Id="2452" PostId="2193" Score="0" Text="It represents the function of the depth map, the dashed lines represent the pixel samples of the depth map." CreationDate="2016-03-17T14:16:55.647" UserId="38" />
  <row Id="2453" PostId="2197" Score="0" Text="One can also be more clever and render mid distance maps." CreationDate="2016-03-17T14:18:53.710" UserId="38" />
  <row Id="2454" PostId="2193" Score="0" Text="Ok so when we sample from a depth map..Its not necessary you will sample from the exact same position..but the different one which can be either higher or lower than the depth map?" CreationDate="2016-03-17T14:25:36.870" UserId="2096" />
  <row Id="2455" PostId="2197" Score="1" Text="I would have thought it's called Peter Panning [because some films depict Peter Pan's shadow as having a mind of its own and detaching itself from Peter](http://movies.stackexchange.com/q/13552/15400)." CreationDate="2016-03-17T14:27:12.473" UserId="16" />
  <row Id="2456" PostId="2181" Score="0" Text="I'm looking forward to seeing the details in the new question. The artifacts you are seeing is strange. Fwiw the smoothstep-ing of the fade is a common technique. It might not help the specific issue you are hitting but it is useful / widely used. Just wanted to let you know I didn't just make it up :p" CreationDate="2016-03-17T15:04:59.550" UserId="56" />
  <row Id="2457" PostId="2197" Score="0" Text="@MartinBüttner Well, yes. That seems to be a sensible reason to call it that way. The tutorial I refered to uses the explanation that I gave." CreationDate="2016-03-17T16:12:43.823" UserId="273" />
  <row Id="2458" PostId="2181" Score="0" Text="Don't  worry! I believe you haha! I read about it and it seems to be used in many situations. This is simply not useful in this case since I need to preserve the linear interpolation. Here is [another post](http://dsp.stackexchange.com/questions/530/bitmap-alpha-bevel-algorithm) I found that has similar request and technology in use. I think the artifacts could be related to the implementation of distance transform in OpenCV." CreationDate="2016-03-17T18:27:42.700" UserId="2319" />
  <row Id="2459" PostId="2187" Score="0" Text="You cold add that to your post ;) Unfortunately this is all just a case of futility." CreationDate="2016-03-17T20:23:01.877" UserId="38" />
  <row Id="2460" PostId="2198" Score="0" Text="Do you want the insets to travel on the surface itself?" CreationDate="2016-03-17T21:41:28.000" UserId="38" />
  <row Id="2461" PostId="2198" Score="0" Text="Yes, on the surface.&#xA;&#xA;I've found papers for insetting 3D shapes in 3D, but I need to adhere to the surface." CreationDate="2016-03-17T22:35:39.817" UserId="2896" />
  <row Id="2462" PostId="2193" Score="0" Text="Yes you got it." CreationDate="2016-03-18T05:39:50.807" UserId="38" />
  <row Id="2465" PostId="2188" Score="0" Text="The reason nobody suggest the 3 projection boolean is because it does in fact not work for nearly any sensible case." CreationDate="2016-03-18T10:07:37.767" UserId="38" />
  <row Id="2467" PostId="2198" Score="0" Text="This is easier for cad applications as they have mathematically better surfaces than polygon meshes. But yes you could extrude tubes of varying sizes along edges then repeat and you'd get a good approximation. Its just that defining what distance along internal polygons is shortest and valid one is a bit hard." CreationDate="2016-03-18T18:37:10.940" UserId="38" />
  <row Id="2468" PostId="2201" Score="0" Text="OpenGL doesn't seem like it would help here; it's a computational geometry problem, not rendering. Can you edit the question and define your terms better? What do you mean by a &quot;maximum continual convex patch&quot; exactly?" CreationDate="2016-03-19T02:22:18.800" UserId="48" />
  <row Id="2469" PostId="2194" Score="0" Text="What is a &quot;dioptre material&quot;? AFAIK a dioptre is a unit of measurement of optical power. :)" CreationDate="2016-03-19T02:25:40.427" UserId="48" />
  <row Id="2471" PostId="2194" Score="1" Text="a dioptre between material 1 / material 2 is the surface between  2 optical materials of different index of refraction.  You confuse with the dioptry." CreationDate="2016-03-19T03:28:32.193" UserId="1810" />
  <row Id="2472" PostId="2194" Score="0" Text="Diopter/dioptre is [a unit of measurement](https://en.wikipedia.org/wiki/Dioptre). The surface between two materials is usually called an &quot;interface&quot;, AFAIK...I've never heard any term similar to &quot;diopter&quot; for that..." CreationDate="2016-03-19T03:30:54.830" UserId="48" />
  <row Id="2473" PostId="2194" Score="1" Text="possibly a problem of translation, then. The french wikipedia offer no english equivalent: https://fr.wikipedia.org/wiki/Dioptre  . And online translation suggest to use the same world is english. :-/ . Ok, I replace by &quot;interface&quot; but this world is less precise." CreationDate="2016-03-19T04:42:14.507" UserId="1810" />
  <row Id="2475" PostId="2189" Score="0" Text="@JulienGuertault I tried rendering the FB directly to screen and I've attached the output of this to my original post (at the end). It seems to be drawing to the FB just fine - I'm still not getting what is wrong here. Also, I switched to quads (2 triangles) and put in the coords by hand just to be sure - so the coords are now right, but the texture, when applied to the second quad, seems to be stretched badly in the Y direction - the square looks like a very tall rectangle. I don't get it..." CreationDate="2016-03-19T14:57:51.800" UserId="2508" />
  <row Id="2477" PostId="2201" Score="0" Text="@nathanreed opengl will be used to render the objects, processing algorithms are to be done in C++. I have to apply different colors to the different convex patches on the object to signify the selection. Say I have a sphere then the whole sphere is one maximal convex patch. Any portion of the sphere surface will be a convex patch, by maximal I mean the maximum continuous convex patch that can be found. Well in the rendering, depending on the viewing angles, the maximal convex patches visible to the viewer will have to colored." CreationDate="2016-03-20T04:01:00.113" UserId="2898" />
  <row Id="2478" PostId="2201" Score="0" Text="@nathanreed Below I have posted an answer that should work, can you improve upon it, or can you provide a better solution?" CreationDate="2016-03-20T04:02:01.553" UserId="2898" />
  <row Id="2483" PostId="2204" Score="0" Text="If a patch ends, and not all triangles have been visited/tested, you have to start anew with another random triangle that is not part of a patch yet. So you find all convex patches and not just the one you happen to begin with. Then chose the biggest one. (Note: Patches can be just a single triangle)" CreationDate="2016-03-20T13:13:42.493" UserId="273" />
  <row Id="2484" PostId="1983" Score="1" Text="@trichoplax You yourself were uncertain whether this question is really a request for off-site resources, and you say that uncertainty remains. Is it a standard practice on this stackexchange that, when there is uncertainty about whether a question is off-topic, to err on the side of it being off-topic?" CreationDate="2016-03-20T14:23:28.137" UserId="2574" />
  <row Id="2490" PostId="2204" Score="0" Text="@dragonseel well for practical purposes I think single triangle patches should be ruled out, as for us to determine convexity, it has to span across several triangles.&#xA;well we do get all patches, but when a convex patch continues further, it's size keeps on increasing by including the neighbouring triangles that satisfy convexity, until the patch reaches the maximum size it can attain." CreationDate="2016-03-20T16:41:13.333" UserId="2898" />
  <row Id="2492" PostId="2204" Score="0" Text="Yea. I just wanted to emphazise that in order to find the biggest convex patch, you have to keep sampling until you tested everything, since after the first one there might be a even bigger one disconnected that you haven't found jet." CreationDate="2016-03-20T20:30:55.647" UserId="273" />
  <row Id="2493" PostId="2206" Score="0" Text="It is perfectly OK if your material looks dark gray if you illuminate it with weak light source. What is the brightness of the light source?" CreationDate="2016-03-20T23:14:39.753" UserId="2479" />
  <row Id="2494" PostId="2206" Score="0" Text="@ivokabel but my materials should look white and light gray. The SPD of the illuminant used is the D65. Do i need to tweak the spd of the light in some way?" CreationDate="2016-03-20T23:17:02.907" UserId="2237" />
  <row Id="2495" PostId="2206" Score="0" Text="@ivokabel Do i need to define a brightness paramter and use it somewhere?" CreationDate="2016-03-20T23:17:50.843" UserId="2237" />
  <row Id="2496" PostId="2206" Score="0" Text="If I am not mistaken, D65 only defines the shape of the spectrum, not the intensity. Therefore, you will really have to add a parameter telling the amount of emitting radiance, or something similar. Related topic is the renderer exposure value, but I saw that you take 1 as the limit value, so you don't have to bother with this one." CreationDate="2016-03-20T23:37:29.820" UserId="2479" />
  <row Id="2497" PostId="2206" Score="0" Text="Thank you @ivokabel for the suggestion about the parameter radiance. Could it be just a constant that will be multiplied with the spd of the illuminant during the tracing of rays? Or do i need to multiply the spd of the illuminant during the conversion from spd to cie xyz? Also I don't understand what you mean with renderer exposure value. Where do I take 1 as its value?" CreationDate="2016-03-20T23:45:26.890" UserId="2237" />
  <row Id="2498" PostId="2206" Score="0" Text="Yes, multiplying the SPD of your illuminant with a value (whether constant or variable) during or before the ray tracing is the way to go." CreationDate="2016-03-20T23:55:55.353" UserId="2479" />
  <row Id="2499" PostId="2206" Score="0" Text="@ivokabel what about the render exposure value? Where do i take 1?" CreationDate="2016-03-20T23:58:54.817" UserId="2237" />
  <row Id="2500" PostId="2206" Score="0" Text="...and sorry for the confusion about exposure. What I meant is the image value which maps onto maximum value in the resulting picture, 255 in your case. Don't worry about that at this point." CreationDate="2016-03-20T23:58:58.063" UserId="2479" />
  <row Id="2501" PostId="2206" Score="0" Text="@ivokabel can you just write a response to my question? In this way your answer an the other (if someone else would response) will remain as reference, and the user will not have to search in the  :) thank you." CreationDate="2016-03-21T00:05:41.957" UserId="2237" />
  <row Id="2502" PostId="2206" Score="0" Text="I might get to it tomorrow..." CreationDate="2016-03-21T00:11:36.470" UserId="2479" />
  <row Id="2503" PostId="2206" Score="0" Text="After re-reading your code, it is unclear to me what is the relation between scene-&gt;light-&gt;spectrum and material-&gt;le in `PathBRDF::shade`. Is it the same value? If yes, do you allow just one light source in your scene? Moreover, why do you normalize your XYZ values with illuminant luminance in `CIE1931XYZ::tristimulusValues()`?" CreationDate="2016-03-21T10:01:09.643" UserId="2479" />
  <row Id="2504" PostId="2206" Score="0" Text="@ivokabel scene-&gt;light-&gt;spectrum and material-&gt;le contain the same value, the D65 SPD. In the scene init I give to the material of light object the SPD of the scene light. My scenes supports only one light. The normalization in CIE1931XYZ::tristimulusValues() follows the standard conversion from spd to CIE XYZ that you can found here http://www.scratchapixel.com/old/lessons/3d-basic-lessons/lesson-5-colors-and-digital-images/color-spaces/ or on the wiki CIE XYZ page. Is this passage not correct? I don't think so because my engine supports also the whitted ray tracing model that seems to be ok." CreationDate="2016-03-21T10:09:52.457" UserId="2237" />
  <row Id="2505" PostId="2206" Score="0" Text="This is an example scene generated with a different illuminant SPD and the Whitted model https://raw.githubusercontent.com/chicio/Spectrum-Clara-Lux-Tracer/master/Screenshots/03_scene4_whittedSpectrum_fl9.png" CreationDate="2016-03-21T10:10:25.363" UserId="2237" />
  <row Id="2506" PostId="2206" Score="0" Text="In fact if I multiply the material-&gt;le with a multiplier the scene become more bright. Here are some rendered images https://drive.google.com/drive/u/1/folders/0BxeVnHLvT8-7Ty1jTVM5U1JJdms. They are not totally correct (as I expect the floor to be white), but maybe i just need a higher multiplier. Do you see any error in code that could avoid this multiplier (some error in the pdf/BRDF calculation)? Thank you very much again @ivokabel." CreationDate="2016-03-21T10:13:59.067" UserId="2237" />
  <row Id="2508" PostId="2206" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/37281/discussion-between-ivokabel-and-fabrizio-duroni)." CreationDate="2016-03-21T12:29:26.397" UserId="2479" />
  <row Id="2509" PostId="2211" Score="1" Text="Not to mention it needs to render the image from 2 slightly different perspectives to accommodate 3D." CreationDate="2016-03-22T09:09:30.180" UserId="2933" />
  <row Id="2510" PostId="2211" Score="2" Text="@Tom.Bowen89 I'd file that under needs higher resolution or higher framerate. Given that you are either using geom shader to emit 2 sets of vertices to the rasterizer or simply rendering twice." CreationDate="2016-03-22T09:12:49.417" UserId="137" />
  <row Id="2513" PostId="2210" Score="0" Text="My laptop which has dual 980m cards is inferior too fyi. The mobile cards add latency apparently due to Intel optimus technology. I was very sad to find that out." CreationDate="2016-03-22T16:49:55.247" UserId="56" />
  <row Id="2514" PostId="2160" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-22T17:19:54.807" UserId="231" />
  <row Id="2517" PostId="1983" Score="0" Text="You make a good point, which I've taken some time to think about. In the absence of any community support for excluding such questions, I have reopened this one." CreationDate="2016-03-23T15:34:44.407" UserId="231" />
  <row Id="2519" PostId="2222" Score="0" Text="This question is a cross-post from http://gamedev.stackexchange.com/questions/118744/how-to-convert-non-axis-aligned-bounding-boxes-to-aabb" CreationDate="2016-03-23T21:30:58.277" UserId="2954" />
  <row Id="2521" PostId="1720" Score="0" Text="The historical terms are &quot;that's the difference between flat and gouraud's shading&quot;. In practice this relates to normals like explained by joojaa" CreationDate="2016-03-24T05:17:13.580" UserId="1614" />
  <row Id="2522" PostId="13" Score="0" Text="I believe this answer is completely false. You are free to multisample outgoing rays and weight their result when combining. You obtain the same truth than the russian roulette technique, but it's generally accepted that the former method is more expensive." CreationDate="2016-03-24T05:27:59.740" UserId="1614" />
  <row Id="2523" PostId="1983" Score="0" Text="This is allmost certainly never going to be answered. Besides what does it have to do with 3d graphics." CreationDate="2016-03-24T05:29:00.690" UserId="38" />
  <row Id="2524" PostId="2223" Score="0" Text="This is (too) many questions rolled into one. Should it possibly be split up?" CreationDate="2016-03-24T05:33:11.800" UserId="38" />
  <row Id="2525" PostId="2223" Score="0" Text="They are all algorithms which apply to the same topic in computational geometry, specifically, computation concerning non-convex enclosed polyhedrons. (Thank you, @nathan-reed, for adding that tag.)" CreationDate="2016-03-25T01:28:16.253" UserId="2957" />
  <row Id="2526" PostId="1983" Score="0" Text="@joojaa 1. you are probably right that if it hadn't been answered up to this point, it's has low probability of ever being answered. But (a) &quot;low&quot; is not zero, and (b) there was no way to estimate this probability before posting the question." CreationDate="2016-03-25T02:07:53.613" UserId="2574" />
  <row Id="2527" PostId="1983" Score="0" Text="@joojaa 2. I have explained my rationale for submitting this question to this stackexchange in the post itself, section **The reason I'm posting in Computer Graphics stackexchange**. Moreover, a book (_Multiple View Geometry in Computer Vision_), which isn't quite computer graphics but is surely quite related to it, does come pretty close to giving the answer, I would suggest that this is a reasonably strong argument that the question was indeed appropriate (see section **Two sources that come close**)." CreationDate="2016-03-25T02:09:05.650" UserId="2574" />
  <row Id="2528" PostId="1983" Score="0" Text="@trichoplax Thank you!" CreationDate="2016-03-25T02:12:48.827" UserId="2574" />
  <row Id="2529" PostId="2093" Score="0" Text="Can i ask you how you defined colors for each triangle when you have already a variable defined in 3D-space ? Cheers, Aurel" CreationDate="2016-03-24T22:51:04.317" UserId="2962" />
  <row Id="2530" PostId="2232" Score="3" Text="`See the number of pixels you need to update grows exponetially when the sides grow.` Quadratically, i think." CreationDate="2016-03-25T11:45:34.313" UserId="2970" />
  <row Id="2531" PostId="2232" Score="0" Text="&quot;Copying the data over from the CPU to the graphics card is a relatively slow operation.&quot; This is true, but irrelevant. Copying over a few million pixels at 60 fps is easily achievable over even modest PCI-E links (it would only require a few hundred megabytes per second.)" CreationDate="2016-03-25T13:06:31.973" UserId="2972" />
  <row Id="2532" PostId="2234" Score="0" Text="My laptop's APU has 256 shader cores @ 686MHZ while my tablet has 192." CreationDate="2016-03-25T13:38:36.217" UserId="2964" />
  <row Id="2533" PostId="2232" Score="0" Text="So APIs make things faster by letting the GPU do the work?" CreationDate="2016-03-25T13:42:15.433" UserId="2964" />
  <row Id="2534" PostId="2232" Score="0" Text="Yes - the GPU is capable of doing hundreds of times as many calculations as the CPU." CreationDate="2016-03-25T13:44:59.973" UserId="2971" />
  <row Id="2535" PostId="2232" Score="0" Text="@pjc50 its not so much that the cpu is faster per core it just has extrenely many cores. Since pixels pushing is a embarassingly parallel business it works." CreationDate="2016-03-25T14:19:36.793" UserId="38" />
  <row Id="2536" PostId="2232" Score="0" Text="@Coxy yes but on top of that copying you need to prepare that image. So it limits what i can prepare. Nothing stops you from doing this, with opengl. Prepare image upload prepare upload." CreationDate="2016-03-25T14:21:10.750" UserId="38" />
  <row Id="2537" PostId="2232" Score="2" Text="@pjc50 That's not wrong, but also not exactly true. A GPU is specialized to run a single program (usually a shader) in parallel on a large amount of data. So, you need to perform the same operations on lots of data to actually use the computing power of a GPU. If your program doesn't, you better run the program on the CPU." CreationDate="2016-03-25T14:22:53.633" UserId="127" />
  <row Id="2538" PostId="2189" Score="1" Text="Other debugging techniques: Use a non-FBO texture to see if it shows up as you'd expect. Make sure that your FBO is complete by calling [`glCheckFramebufferStatus`](http://docs.gl/gl3/glCheckFramebufferStatus)" CreationDate="2016-03-25T14:57:08.820" UserId="197" />
  <row Id="2539" PostId="1983" Score="0" Text="Have you tried posting this in some approriate journal. This should immediately bring oout the trolls that know it has been addressed." CreationDate="2016-03-25T15:25:50.633" UserId="38" />
  <row Id="2540" PostId="2212" Score="0" Text="Did this solve the problem?" CreationDate="2016-03-25T17:06:21.783" UserId="2479" />
  <row Id="2541" PostId="2232" Score="0" Text="@Cthulhu, quadratic can equal exponential (think about it, exponential = `y=x^2` and quadratic = `y=x^2+2x+1`, they looks the same, but they're in different places on the graph)" CreationDate="2016-03-25T18:00:36.997" UserId="2977" />
  <row Id="2542" PostId="2234" Score="0" Text="Hey, the Titan X has 5760 cores in it." CreationDate="2016-03-25T18:02:04.110" UserId="2977" />
  <row Id="2543" PostId="2232" Score="1" Text="@Daniel $x^2$ is a quadratic function. An exponential function would be $2^x$. And exponential functions grow way faster than quadratic functions." CreationDate="2016-03-25T18:14:13.497" UserId="127" />
  <row Id="2544" PostId="2232" Score="0" Text="Gah, my bad - this is what I get for not waking up fully before typing." CreationDate="2016-03-25T18:14:43.663" UserId="2977" />
  <row Id="2545" PostId="2232" Score="0" Text="many words, don't go to the point. @pjc50 answer is most to the point and understandable" CreationDate="2016-03-25T20:18:55.960" UserId="2981" />
  <row Id="2546" PostId="2234" Score="0" Text="@Daniel Are there any games that the Titan X goes &lt;  30fps at ultra high .The Titan X is very powerful" CreationDate="2016-03-26T02:33:38.857" UserId="2964" />
  <row Id="2547" PostId="2234" Score="0" Text="Ummm, One of these, perhaps: http://www.maximumpc.com/10-most-graphically-demanding-pc-games/" CreationDate="2016-03-26T02:46:36.110" UserId="2977" />
  <row Id="2548" PostId="2231" Score="0" Text="In general doing one operation on a lot of things is more efficient and easier to think about than doing it on every thing individually." CreationDate="2016-03-26T03:29:37.357" UserId="2988" />
  <row Id="2549" PostId="2231" Score="2" Text="Because every game would need to be rewritten for every graphics card. Unless they didn't use the graphics card, but then they'd be slow." CreationDate="2016-03-26T04:18:47.940" UserId="2316" />
  <row Id="2550" PostId="2231" Score="0" Text="I think that GPU companies have to create their own DirectX drivers" CreationDate="2016-03-26T04:35:21.333" UserId="2964" />
  <row Id="2551" PostId="2235" Score="0" Text="So not good for excel :)" CreationDate="2016-03-26T05:31:50.900" UserId="38" />
  <row Id="2552" PostId="2093" Score="0" Text="@AurelienSanchez sure. I want to use normal map as a color of pixels, and it's another problem -- to find normal vectors fast. Now I use glfw to render the model and planning to use glReadPixels to achieve matrix with pixels, and calculating normal vectors by myself" CreationDate="2016-03-26T06:35:48.780" UserId="2750" />
  <row Id="2553" PostId="2232" Score="0" Text="You talk about copying and moving, but what's more important is the actual image generation. You have to determine which object will be visible at which point, how it will be lighted, what will be the effects of smoke, etc, etc. GPUs are highly optimized to perform these operations fast and in parallel. APIs make it easy to express common operations." CreationDate="2016-03-26T06:47:47.153" UserId="2312" />
  <row Id="2554" PostId="2232" Score="0" Text="@IMil yesIi commented about this. It is more meant to be a lies to children type of answer than a this is the exact reason. 3D is by no reason the only reason we use graphics accelerators." CreationDate="2016-03-26T07:32:34.387" UserId="38" />
  <row Id="2555" PostId="2232" Score="0" Text="The operating system could a API which allows everything to be drawn by pixels" CreationDate="2016-03-26T07:48:40.713" UserId="2964" />
  <row Id="2556" PostId="2232" Score="0" Text="@SuiciDoga there is an api for that both directX and opengl can do this as can direct2D, GDI and numerous others. Just because the api can do more complex things does not mean it cant do simple things too. Just you do not write directly to the buffer on screen somebody else does that for you." CreationDate="2016-03-26T07:58:57.253" UserId="38" />
  <row Id="2557" PostId="2232" Score="0" Text="@joojaa what do you mean by 'lies to the children'? There are two options. 1) you calculate every pixel by hand, 2) you say: here are the coordinates, here is the texture, here are the light sources, go ahead and draw them. Option 2 is easier, and it allows graphic chip designers and driver developers to make even old games faster." CreationDate="2016-03-26T08:11:28.833" UserId="2312" />
  <row Id="2558" PostId="2232" Score="0" Text="@IMil Lies to children is a simplified reality, i know full and well that the answer is not this simple. Its not granted that the CPU is slower than the GPU. It can be faster IF the image you make is not parallelizable. Its just that in general pixels are independent of each other, if this is not the case then it does not hold true. So it is not certain and definitive that this is fastest in a general sense. But since it is there you rarely see other kinds of things. Too many dimensions to consider." CreationDate="2016-03-26T08:27:29.920" UserId="38" />
  <row Id="2559" PostId="2234" Score="0" Text="@Daniel The developers of those games must have need about 2-4 Titan X cards :)" CreationDate="2016-03-26T13:30:40.113" UserId="2964" />
  <row Id="2560" PostId="2234" Score="0" Text="Yep.  I'm going to start developing in UE4, and that engine is a beast.  For max settings in a big world you basically need 4 Titan X's to get 60fps @ 1080p." CreationDate="2016-03-26T14:48:27.263" UserId="2977" />
  <row Id="2561" PostId="2231" Score="0" Text="&quot;But why would we need all these frameworks and GPU features when we could just draw everything pixel by pixel?&quot; *that's* how it was done in good ol' days. Wolfenstein 3D, Doom, Duke Nukem 3D, Quake and most other games of late '90 used pure software rendering (Quake offered OpenGL renderer as an option)." CreationDate="2016-03-26T17:07:22.950" UserId="2997" />
  <row Id="2563" PostId="2222" Score="0" Text="If the current answer is sufficient you can close the question otherwise you can point at possible shortcomings or aspects to elude more deeply." CreationDate="2016-03-26T17:59:48.630" UserId="2287" />
  <row Id="2564" PostId="1983" Score="0" Text="@joojaa No, but I would be thankful indeed if your can think of some journal which might be appropriate---I'm certainly willing to try...  By the way, I'm not familiar with journal websites offering this sort of discussion as an option... do you happen to know of examples?" CreationDate="2016-03-26T19:52:22.037" UserId="2574" />
  <row Id="2566" PostId="2240" Score="1" Text="Are you sure you want N·H, or do you want R·L i.e. reflection vector dotted with light source vector? The latter would be the classic Phong equation (N·H is Blinn), and the figure appears to mark the R·L angle: 15 degrees." CreationDate="2016-03-27T06:01:07.813" UserId="48" />
  <row Id="2568" PostId="2240" Score="0" Text="Should the question not point out which Bidirectional Reflectance Distribution Function (BRDF) you need to use? You have Phong, Blinn-Phong, Modified Phong, Modified Blinn-Phong and a whole variety of physically-based ones." CreationDate="2016-03-27T10:17:51.400" UserId="2287" />
  <row Id="2569" PostId="2240" Score="0" Text="@NathanReed how can i find R.L if i dont know the vectors? Do i just use 15 degrees?" CreationDate="2016-03-27T13:28:42.073" UserId="2359" />
  <row Id="2570" PostId="2240" Score="1" Text="@user2976568 The dot product is the cosine of the angle between the vectors (for unit vectors). Just as you remarked that N·L = cos(theta). :)" CreationDate="2016-03-27T17:24:35.710" UserId="48" />
  <row Id="2571" PostId="2234" Score="0" Text="I am a beginner with developing games and creating some Unity games using my laptop's APU.I haven't experienced much lag because my laptop has a dedicated class APU which almost as good as a dedicated video card.My APU is a AMD A8-4500M.I do not think that this laptop can use UE4 without a external GPU card (using USB3)." CreationDate="2016-03-28T02:44:14.047" UserId="2964" />
  <row Id="2572" PostId="2234" Score="0" Text="And to get 4K they must need about 8 Titan X GPUs" CreationDate="2016-03-28T02:46:51.953" UserId="2964" />
  <row Id="2573" PostId="2231" Score="1" Text="@MatthewRock You're not being helpful. You can certainly bundle an OS with a game inside a docker container and distribute the container. That way, the user doesn't need to install library dependencies for their distro." CreationDate="2016-03-28T03:55:44.500" UserId="2968" />
  <row Id="2574" PostId="67" Score="1" Text="FWIW, I've heard of displays (usually very large displays in stadiums) that use a delta-nabla configuration. (Named for the Greek delta letter &quot;Δ&quot; and the Hebrew nabla letter &quot;∇&quot; because the pixels were alternating triangles with the point going up, then down, then up, then down.) One example is the Philips Vidiwall." CreationDate="2016-03-28T05:07:44.823" UserId="3003" />
  <row Id="2575" PostId="1741" Score="0" Text="@user3531082 Maya does not in fact solve this problem" CreationDate="2016-03-28T08:52:08.703" UserId="38" />
  <row Id="2576" PostId="1892" Score="0" Text="@Marqin Could you convert your last comment into an aswer so we can get this post out of the unanswered queue?" CreationDate="2016-03-28T09:03:15.333" UserId="38" />
  <row Id="2578" PostId="1892" Score="0" Text="@joojaa But frankly it's not valid answer. Some time ago I've reorganized my shader, changed types/order of  struct fields and now it &quot;miraculously&quot; work. And I still want to know why it was not working the old way - maybe I just made some error with padding? I cannot find that bug :/" CreationDate="2016-03-28T09:36:20.210" UserId="2413" />
  <row Id="2579" PostId="1892" Score="0" Text="@Marqin then answer that, the question does not contain your code so theres no way to verify your bug from this anyway." CreationDate="2016-03-28T09:41:19.163" UserId="38" />
  <row Id="2580" PostId="2243" Score="0" Text="Thanks for the reference !  In case of web application, this adds one more level of difficulties. The fact is that as shadertoy shows, not all people see the same result. Plus in all API already providing a texture loading, I guess we can just hope a linearization is done." CreationDate="2016-03-28T10:43:15.020" UserId="1810" />
  <row Id="2581" PostId="2243" Score="0" Text="Beside, when preparing a slide on googleDoc comprising imported images, exporting in pdf, and displaying via acroread on the same screen, I already don't have the same gamma for the 2 copies of the image !" CreationDate="2016-03-28T10:44:21.600" UserId="1810" />
  <row Id="2583" PostId="2240" Score="0" Text="@NathanReed sounds like the answer to me" CreationDate="2016-03-29T08:07:37.823" UserId="38" />
  <row Id="2584" PostId="2246" Score="0" Text="They may do some edge checking to maintain the crispness" CreationDate="2016-03-29T12:54:16.693" UserId="137" />
  <row Id="2585" PostId="2247" Score="1" Text="What level of authenticity to early-90s games are you looking for? You could just alpha-blend the sprites over the background and it will look &quot;right&quot;. If it must be done using palettes, it's going to be a lot more difficult and limited. Or are you asking about how to do alpha blending? If so, we'll need more info about how your renderer works, which API(s) you're using, etc." CreationDate="2016-03-29T16:26:05.513" UserId="48" />
  <row Id="2586" PostId="2247" Score="0" Text="Alpha blending should work(it's not 100% implemented yet), by the way of 50% blending every two color combination in the palette, finding the best fit color from the palette and writing that to an array. This array is then indexed by the background color and the new pixel's color to get the color that is then put on the screen." CreationDate="2016-03-29T17:37:58.307" UserId="3020" />
  <row Id="2587" PostId="2247" Score="0" Text="OK, assuming you have a palette that has close enough fits for all the blended colors, that could work. But then, what's the question about? Sounds like you already have a plan for how to do it." CreationDate="2016-03-29T17:44:44.280" UserId="48" />
  <row Id="2588" PostId="2247" Score="0" Text="Whether to first apply lighting to the sprite texel and then blend it with the background or first blend with the background and then apply lighting. (Or possibly something else entirely?)" CreationDate="2016-03-29T19:56:43.433" UserId="3020" />
  <row Id="2589" PostId="1741" Score="0" Text="...that you know of." CreationDate="2016-03-29T21:15:07.007" UserId="2091" />
  <row Id="2590" PostId="2234" Score="0" Text="Further to [pcj50's answer](http://computergraphics.stackexchange.com/a/2234/3007), you had to account for the peculiarities of every graphics card out there. While they claimed to support standards like EGA, VGA &amp; VESA VBE, there were peculiarities &amp; limitations with their implementations that you had to account for. I remember several graphics programs (not games) didn't work properly on my ATI Mach 32 card because ATI's implementation of VGA (or was that VESA VBE) was notoriously dodgy." CreationDate="2016-03-27T12:23:36.130" UserId="3007" />
  <row Id="2592" PostId="2030" Score="0" Text="It would be useful to include the additional information (such as the comment mentioning OS and GPU) in the question so it is more accessible. Also, comments are not intended to last long term. If there's any information you want to add to the question, just [edit]." CreationDate="2016-03-30T02:52:37.057" UserId="231" />
  <row Id="2593" PostId="2252" Score="0" Text="Thanks. After reading of Vulkan doc I could find the similar for directx [fine](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446950(v=vs.85).aspx) and [coarse](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446948(v=vs.85).aspx) derivatives." CreationDate="2016-03-30T12:00:36.940" UserId="386" />
  <row Id="2595" PostId="2256" Score="1" Text="Sorry wrote this quickly on my phone. Need to add links and stuff. Maybe organize and a few rounds of google. Feel free to fix my typos." CreationDate="2016-03-30T21:37:12.947" UserId="38" />
  <row Id="2596" PostId="1892" Score="0" Text="thanks, fixed the link to code" CreationDate="2016-03-30T22:06:43.913" UserId="2413" />
  <row Id="2597" PostId="1983" Score="0" Text="Not sure if this helps, but the book [3D Engine Design for Virtual Globes](http://www.virtualglobebook.com/) includes a chapter on GPU ray-casting the globe, which involves computing the horizon in the fragment shader.  Disclaimer, I know the authors of this book, after they wrote it they went on to be the founders of [Cesium](http://cesiumjs.org)." CreationDate="2016-03-31T21:08:16.023" UserId="1908" />
  <row Id="2600" PostId="2030" Score="0" Text="There are usually two limits, one having &quot;native&quot; in the name. Beyond native limits the driver is likely to emulate behavior in software. MB of shader data storage is never good. I had similar problem in DX11 a few years back. You should try move that data to a buffer object of some kind or divide the issue into more passes." CreationDate="2016-03-31T18:53:43.617" UserId="3041" />
  <row Id="2601" PostId="2030" Score="0" Text="Why is a buffer object better than a shader data storage? I thought it was the other way around." CreationDate="2016-03-31T23:39:04.643" UserId="2666" />
  <row Id="2602" PostId="2030" Score="0" Text="@gartenriese Better? How do you mean?" CreationDate="2016-04-01T06:07:30.627" UserId="3041" />
  <row Id="2603" PostId="2030" Score="0" Text="@Andreas: If you look [here](https://www.opengl.org/wiki/Shader_Storage_Buffer_Object), three out of four points are in favor of SSBOs over UBOs." CreationDate="2016-04-01T12:08:45.787" UserId="2666" />
  <row Id="2604" PostId="2201" Score="0" Text="The comments on your answer suggest some confusion over what is meant by &quot;maximal&quot;. I believe you are using &quot;maximal&quot; to mean a convex surface which is not a subset of a larger convex surface, rather than to mean the largest convex surface that exists in the triangle mesh. This is covered in your question but due to the confusion it might be worth editing to clarify." CreationDate="2016-04-02T07:51:33.137" UserId="231" />
  <row Id="2605" PostId="2201" Score="0" Text="It would be helpful to know what the purpose is, to give a better idea of exactly what is required. For example, does a convex patch need to be strictly convex (never flat), or does it count as convex provided it is nowhere concave? A cylinder is nowhere concave. It is convex everywhere, but it is not strictly convex in every direction (it is flat in the axial direction). Do you want the surface of a cylinder to count as one convex patch?" CreationDate="2016-04-02T07:59:20.887" UserId="231" />
  <row Id="2606" PostId="2259" Score="0" Text="Could you clarify what you mean by &quot;360 degree stereo video&quot;? You can create a stereo image by taking two images from  different positions (approximating one from each eye). However, if you take two 360 degree images then the stereo effect will be strongest in the directions perpendicular to the offset between the images, and zero in the directions parallel to the offset. To get a stereo effect in all directions I would expect two images in each of a wide variety of directions to be necessary. Answering your question will require knowing more about the input that is provided." CreationDate="2016-04-02T08:12:36.327" UserId="231" />
  <row Id="2608" PostId="2259" Score="0" Text="In the sense of what Kolor Eyes uses. You have a video, e.g. 1920x1080, split into two halves (side by side), each half (960x1080) then needs to be mapped onto a sphere and then you render the viewpoint of a viewer at the centre of that sphere looking out in a particular direction." CreationDate="2016-04-03T00:51:46.103" UserId="3038" />
  <row Id="2611" PostId="2201" Score="0" Text="@trichoplax thanks for the suggestion! yes, a convex surface which is not a subset of a larger convex surface, is what is meant. And yes, it needs to be strictly convex. For a cylinder, there would be one convex patch in the whole figure which is maximal, and this patch would not include the axial flat surfaces." CreationDate="2016-04-03T08:43:49.370" UserId="2898" />
  <row Id="2613" PostId="2259" Score="0" Text="Kolor Eyes appears to provide 360 degree video, but not in stereo. Are you looking to make your 360 degree stereo video by combining two 360 degree videos from two similar viewpoints, or by combining a large number of 360 degree videos from many similar viewpoints?" CreationDate="2016-04-03T10:54:20.407" UserId="231" />
  <row Id="2614" PostId="2259" Score="0" Text="You mention rendering in your comment. Are you working with recorded real world video here, or a rendered artificial scene?" CreationDate="2016-04-03T10:54:59.813" UserId="231" />
  <row Id="2615" PostId="2259" Score="0" Text="Note that requests for software recommendations and off site resources are off topic. See [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic) I recommend editing to remove the request for existing software or libraries, keeping just the request for how to program this yourself, in order to avoid the question being closed." CreationDate="2016-04-03T11:05:03.730" UserId="231" />
  <row Id="2616" PostId="2201" Score="0" Text="I meant that the curved surface of the cylinder is itself flat in the direction parallel to its central axis. Depending on whether you move to the next triangle around the cylinder or along the cylinder, it will appear to be either strictly convex (moving around), or flat (moving along). Do you require the surface to be strictly convex in every direction, or only one direction?" CreationDate="2016-04-03T11:15:01.443" UserId="231" />
  <row Id="2617" PostId="2204" Score="0" Text="A possible problem with this approach is that for some surfaces there will exist a path between two adjacent triangles that is made up only of convex steps, even though the two adjacent triangles touch along a concave edge. In sufficiently smooth examples this may not be a problem as the inaccuracy will only tend to be one triangle wide, but it is possible to construct examples with arbitrarily large concavities that are accessible by purely convex paths. For example, imagine the shape made by pushing a pin into a balloon so that the surface dips inwards (assuming it doesn't burst)." CreationDate="2016-04-03T11:30:31.910" UserId="231" />
  <row Id="2618" PostId="2201" Score="0" Text="It may help to consider a torus. Do you require an algorithm that makes the outer half of the torus a single convex patch, with the triangles on the inner half (around the hole) not counted as being in any convex patch? Or do you require an algorithm that makes the whole surface of the torus a single convex patch, since even on those parts of the surface that are concave in one direction, they are still convex in the perpendicular direction?" CreationDate="2016-04-03T11:38:01.053" UserId="231" />
  <row Id="2620" PostId="2189" Score="1" Text="Assuming that the content of the FBO is correct, and based on your comment and the look of the cube on your screenshot, it sounds to me your texture coordinates are simply incorrect. You could use a regular texture as Mokosha suggests, and switch to FBO only after you have the cube textured correctly." CreationDate="2016-04-03T12:32:00.377" UserId="182" />
  <row Id="2622" PostId="2266" Score="1" Text="You can actually turn Photoshop effects to be profile  aware its just not on by default because of backwards compatibility and least surprise for old users. Mind you though its not perfect. I would say that in general ALL of our color correction workflow is totally whacked because if the tacked on by later date nature. so for 5 i propose (d) because its hard and time consuming to rebuild a new all encompassing standard that can replace the old ones." CreationDate="2016-04-04T07:12:00.370" UserId="38" />
  <row Id="2623" PostId="2201" Score="0" Text="@trichoplax strictly convex is what I am looking for. In the example of the Torus, I require an algorithm that makes the outer half of the torus a single convex patch, with the visible (the inner surface of the other side of the torus will be visible to the viewer) triangles on the inner half (around the hole) , that forms a concave lateral arc to the viewer, it should show convex strips perpendicular to the concave arc. I hope I was able to communicate the idea effectively." CreationDate="2016-04-04T07:57:35.103" UserId="2898" />
  <row Id="2624" PostId="2204" Score="0" Text="@trichoplax I am myself not sure that the algorithm I have posted is optimal or the best one. If you have a better algorithm, please do share. I would love receive it! If another answer turns out to be better, I will unaccept my answer the accept the better one. And for your example, as I said above in coment to the question, if there are local convex strips, they should be shown as local convex strips." CreationDate="2016-04-04T08:00:49.620" UserId="2898" />
  <row Id="2627" PostId="2201" Score="0" Text="Your description of how the torus should be divided is perfectly clear, but I can't see a way of translating that into a rigorous requirement that would apply to other types of surface. The strips are distinct from each other due to concave edges, but they all seem to be connected to the large outer convex patch, and therefore part of it. I think the main question here is how to unambiguously define what a maximal convex patch is." CreationDate="2016-04-04T12:39:02.793" UserId="231" />
  <row Id="2629" PostId="2204" Score="0" Text="I understand that your answer is just an example to get things started. I was just giving some feedback in the hope that it will trigger an idea for improvement from someone." CreationDate="2016-04-04T12:40:59.857" UserId="231" />
  <row Id="2630" PostId="2204" Score="0" Text="@trichoplax thanks :)" CreationDate="2016-04-04T13:43:22.513" UserId="2898" />
  <row Id="2634" PostId="2272" Score="0" Text="Get it! You are a great guy. Thanks a lot." CreationDate="2016-04-05T09:40:31.677" UserId="3058" />
  <row Id="2635" PostId="2271" Score="0" Text="For VR you need super low latency, any type of network hop will be too long." CreationDate="2016-04-05T10:18:00.380" UserId="137" />
  <row Id="2636" PostId="2271" Score="0" Text="Only if the cloud is very close to the vr gear like, within tens of meters. But given that everything and their dad is going to be a computer that is certainly possible." CreationDate="2016-04-05T12:59:35.223" UserId="38" />
  <row Id="2639" PostId="2272" Score="0" Text="There are two causes of tearing: rendering to the visible buffer (front buffer rendering) and swapping to the back buffer in the middle of a display refresh. Vsync assumes you are doing at least double buffered rendering and holds the swap until vblank - there is no tearing whatsoever. Since rendering rarely completes during vsync, two buffers can be held up waiting on display (one currently displayed, one waiting for vsync) thus requiring triple+ buffering to keep rendering continuous..." CreationDate="2016-04-05T17:50:14.567" UserId="2500" />
  <row Id="2644" PostId="2271" Score="0" Text="@ratchetfreak,  I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T00:59:47.590" UserId="3058" />
  <row Id="2645" PostId="2273" Score="0" Text="I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T01:00:01.760" UserId="3058" />
  <row Id="2646" PostId="2271" Score="0" Text="@HaoZhang well if your calculations that the total time to display a frame to the user from the cloud is 15.5ms then things should work out. In order to have a game running at 60FPS you need to display a frame on the screen every 16.6ms. Are you sure its going to take 15.5ms to display both frames(one for each screen of the VR system)?" CreationDate="2016-04-06T07:18:05.233" UserId="204" />
  <row Id="2647" PostId="2271" Score="0" Text="I give it 5ms for transmission to OLED display of 2K*2@200Hz, that would need at least 40Gbps interface and cable, as display port 1.4. (https://en.wikipedia.org/wiki/DisplayPort) supports. Yes it would be a little costly but it is achievable." CreationDate="2016-04-06T08:45:59.250" UserId="3058" />
  <row Id="2648" PostId="2272" Score="0" Text="Yes, I heard triple buffer technology. That's cool. Thanks." CreationDate="2016-04-06T09:00:12.287" UserId="3058" />
  <row Id="2650" PostId="2275" Score="1" Text="The reason they're using a FPGA, is they require hardware features that the GPU does not have. So they emulate a GPU using a FPGA." CreationDate="2016-04-06T14:24:19.277" UserId="310" />
  <row Id="2653" PostId="2008" Score="3" Text="@Dragonseel: since it was the correct observation, could you turn the comment into an answer?" CreationDate="2016-04-07T02:53:05.380" UserId="182" />
  <row Id="2654" PostId="2276" Score="0" Text="Hi Nathan, nice to meet you here. I have read some of your presentations on Nvidia Gameworks VR. &quot;Timewarp&quot; can reduce the latency but with image quality loss. And I think, if the head rotated very fast,  it's very hard to warp the 2D image to cheat the eyes. Motion blurring may be help in this case. Local client or remote server could send an unclear image to the display in very low latency. You mentioned Cloud could do some offloadings on VR. That's a cool. What's the requirement for the network bandwidth and latency? Does it still need high bandwidth and low latency? Thanks." CreationDate="2016-04-07T06:08:09.030" UserId="3058" />
  <row Id="2655" PostId="2277" Score="0" Text="The second paper use &quot;ray casting&quot; -- a simplified version of  &quot;ray tracing&quot;. It could reduce the computing power dramatically. I think the rendering quality is not as good as full ray tracing rendering. But, is it acceptable in most cases? And, as what you said, frameless rendering, or racing the beam, could be implemented as spliting the screen into several blocks and &quot;racing the block&quot; instead of &quot;racing the beam&quot;. Of course it need modify the GPU and display. It seems wonderful, does it?" CreationDate="2016-04-07T06:31:30.850" UserId="3058" />
  <row Id="2656" PostId="2275" Score="0" Text="It seems that in second paper the authors used &quot;ray casting&quot; instead of rasterization. So they used FPGA directly, not emulate a GPU? And it also seems they didn't support any API like OpenGL or DirectX. Do you mean the authors just need FPGA to &quot;race the beam&quot;?" CreationDate="2016-04-07T06:38:59.827" UserId="3058" />
  <row Id="2657" PostId="2276" Score="0" Text="You are right, current internet access are mostly based on dialup or DSL. But, in the near future, e.g. 5-10 years later, could something be changed? Google has deployed 1Gbps fiber in some cities and I heard Google also tried to introduce 10Gbps to home." CreationDate="2016-04-07T06:48:39.943" UserId="3058" />
  <row Id="2658" PostId="2276" Score="0" Text="haha, Nathan, I got some information from your blog: (http://www.reedbeta.com/blog/2014/04/03/vr-and-multi-gpu/#more-591), &quot;That being said, what if the high-spec dGPUs were far away from you? Cloud-based rendering is a topic that’s been getting interest lately, and the same kind of latency reduction strategies could be applicable there. Imagine having your VR headset driven by the little GPU in your phone, but streaming down source frames or shading cache updates from a dGPU in the cloud (or your home PC)! &quot;" CreationDate="2016-04-07T07:23:32.787" UserId="3058" />
  <row Id="2660" PostId="2279" Score="0" Text="I can see shadows. Therefore i do not concurr with your analysis. However secondary light might have too high reflectivity. Could you try using a simpler scene." CreationDate="2016-04-07T13:10:39.740" UserId="38" />
  <row Id="2661" PostId="2279" Score="1" Text="@joojaa edited the description with a target reference image" CreationDate="2016-04-07T13:31:07.520" UserId="3069" />
  <row Id="2662" PostId="2279" Score="0" Text="Using a simpler scene helps you debug the effect. To me it looks like the shadows are there your scene is just not color corrected and you bounce too nuch energy." CreationDate="2016-04-07T13:57:06.897" UserId="38" />
  <row Id="2663" PostId="2275" Score="0" Text="As Nathan mentioned in his anwer, in order to &quot;race the beam&quot;, you need a very tight synchronization of the &quot;graphics hardware&quot; and the OS/CPU. Current GPUs and Graphics APIs don't have this support. So the authors created the hardware themselves using a FPGA. In theory, the hardware they created in FPGA could be manufactured into &quot;real&quot; hardware." CreationDate="2016-04-07T14:59:28.613" UserId="310" />
  <row Id="2664" PostId="2008" Score="0" Text="@JulienGuertault yes. Done" CreationDate="2016-04-07T14:59:52.257" UserId="273" />
  <row Id="2665" PostId="2008" Score="0" Text="@Dragonseel: thank you." CreationDate="2016-04-07T15:41:55.693" UserId="182" />
  <row Id="2666" PostId="2271" Score="0" Text="Although this question as phrased appears to be broad and opinion based, the answers show that the possibilities are not as arbitrary as they may at first appear, and objective restrictions can be placed on what is likely to happen." CreationDate="2016-04-07T16:21:10.270" UserId="231" />
  <row Id="2667" PostId="2279" Score="0" Text="@joojaa I just posted some of the code because I am using a much more complex framework. That's why I was avoiding rendering more simple scenes, because it requires me some effort finding out how exactly the scene loader works.  Ok, may you suggest me code-wisely what I could change to color the scene correctly?" CreationDate="2016-04-07T16:27:09.340" UserId="3069" />
  <row Id="2668" PostId="2281" Score="0" Text="First of all thanks for your answer :)&#xA;&#xA;So, the method to check the intersection between the ray and the square light is right. I have already been asking about it cause of a small error I couldn't find. The method and the relative solution can both be found here:&#xA;&#xA;http://stackoverflow.com/questions/36180741/intersection-of-ray-and-rectangle-in-c/36186088#36186088&#xA;&#xA;I also edited the question adding a picture that shows the result in case I omit the **depth &gt; 1** check. Maybe it can help understanding where the problem is.. thanks once more" CreationDate="2016-04-07T19:52:56.443" UserId="3069" />
  <row Id="2669" PostId="2279" Score="0" Text="It might help to restrict your image to a very small region in which you expect to have a very obvious shadow. Then you can get a more detailed look at that region in a much shorter rendering time, to see if it really is zero shadow, or just a less obvious softened shadow." CreationDate="2016-04-07T23:24:54.983" UserId="231" />
  <row Id="2670" PostId="2275" Score="0" Text="You say &quot;real hardware&quot;, does it mean ASIC?" CreationDate="2016-04-08T00:31:58.077" UserId="3058" />
  <row Id="2671" PostId="2283" Score="1" Text="Pro tips, unrelated to your question: you can expect `(foo * foo)` to be a lot faster than `pow(foo, 2.0f)`; if your data is organized row by row (as opposed to column by column), you should swap the for loops to traverse data in a more coherent order." CreationDate="2016-04-08T02:02:55.560" UserId="182" />
  <row Id="2672" PostId="1519" Score="0" Text="Do you have sources to recommend on the fact that metals do transmit light? What is the order of magnitude of the contribution to the perceived color?" CreationDate="2016-04-08T08:03:34.527" UserId="182" />
  <row Id="2673" PostId="2283" Score="0" Text="Thanks for the tip ;)" CreationDate="2016-04-08T12:16:55.040" UserId="2372" />
  <row Id="2674" PostId="2289" Score="1" Text="Great answer, thanks a lot!" CreationDate="2016-04-08T12:19:37.890" UserId="182" />
  <row Id="2675" PostId="2284" Score="1" Text="Thanks, it works perfectly !" CreationDate="2016-04-08T12:38:59.000" UserId="2372" />
  <row Id="2676" PostId="1519" Score="0" Text="@JulienGuertault It's fairly standard knowledge, and Google returns useful results. E.g. [this](https://www.reddit.com/r/askscience/comments/37ktye/would_it_be_possible_to_make_a_thin_enough_sheet/crnsmgw) &quot;Below 50 nm thickness, the light transmitted through gold is green (imagine all the colours that aren't reflected are coming through . . .).&quot; Probably appearance depends. Metals reflect a lot, even with thin coatings. The amount refracted depends on the extinction coefficient and thickness." CreationDate="2016-04-08T16:25:32.930" UserId="523" />
  <row Id="2680" PostId="2294" Score="0" Text="Thanks for the response! Yeah i will try that! Thanks!" CreationDate="2016-04-09T00:34:45.790" UserId="2953" />
  <row Id="2682" PostId="2288" Score="1" Text="Could you narrow things down a bit. Like what have you tried. What is your level of expertise. Think of it thisway, if I were to answer you as briegly as you ask would you be happy?" CreationDate="2016-04-09T05:14:46.607" UserId="38" />
  <row Id="2683" PostId="2288" Score="0" Text="I am pretty much a newbie, never touch cg before, just try to get some keywords so that I can google around and move forward. But details are welcomed too." CreationDate="2016-04-09T10:27:03.123" UserId="3074" />
  <row Id="2684" PostId="2288" Score="1" Text="Problem is that since your question is high level you also get a high level answer that pobably does not help you much. The first step is hard. Not the what to do once you have it done." CreationDate="2016-04-09T10:54:49.477" UserId="38" />
  <row Id="2685" PostId="2030" Score="0" Text="@gartenriese (Sry for delay, didnt have rep to comment) Oh I see. SSBO have additional features, and that is &quot;better&quot;. Ok. My idea of a &quot;buffer storage&quot; was not SSBO/UBO. This post mentions &quot;texture buffer objects&quot; that are quite large without the requirement to be writeable: http://stackoverflow.com/questions/7954927/glsl-passing-a-list-of-values-to-fragment-shader" CreationDate="2016-04-09T15:15:36.510" UserId="3041" />
  <row Id="2687" PostId="2294" Score="0" Text="Did you get to try this? Did it work?" CreationDate="2016-04-10T07:33:33.293" UserId="3083" />
  <row Id="2691" PostId="2275" Score="0" Text="No, just any chip. It could be a modified GPU. When I say 'real', I just mean that it's silicon logic, rather than programmed logic, ie. a FPGA." CreationDate="2016-04-10T15:17:58.693" UserId="310" />
  <row Id="2692" PostId="2294" Score="0" Text="I just did it now. I originally had the pos defined in the 4x4 view matrix, but i took it out and did it at the end, and it worked! Thankyou!" CreationDate="2016-04-10T16:05:34.780" UserId="2953" />
  <row Id="2693" PostId="2291" Score="0" Text="your answer was very useful, thanks first of all ! You where right, **distFromLight** and  **distFromObj** where wrongly calculated. I fixed it. Something has been improved but the problem is not fixed yet. I posted one picture showing the new results. I thought maybe the error relies on too soft shadows due to a too big light, therefore I tried to lower the dimension of the light. No matter how increase the power of the light, but in this case results are bad (posted a second picture about it). Hope it helps you. In the meantime I am working on the .obj loader for a simpler scene." CreationDate="2016-04-10T17:15:00.353" UserId="3069" />
  <row Id="2694" PostId="2294" Score="0" Text="Cool, glad you got it solved. Can you accept my answer then?" CreationDate="2016-04-10T19:12:27.187" UserId="3083" />
  <row Id="2695" PostId="2300" Score="0" Text="Looks fine to me. Is anything not working as it should, or do you have any specific questions about the code or Euler angles? Asking for &quot;your opinion about this approach&quot; is very vague and will probably lead to questions like this being closed on this site." CreationDate="2016-04-10T19:45:35.710" UserId="48" />
  <row Id="2696" PostId="2300" Score="1" Text="Thanks @NathanReed for your reply and reviewing my code. You were right about the 'open endedness' of the question, thus, rephrased it." CreationDate="2016-04-10T20:40:28.353" UserId="3098" />
  <row Id="2697" PostId="2291" Score="1" Text="A smaller light means more of the rays will never reach the light, so you will need to increase the number of rays to give a better quality image. This will also increase the time it takes to render. It might be worth instead making the light as large as possible (an infinite or very large plane). This way, if the problem is transparent surfaces, you should see even more evidence for it, with the added benefit that the largest possible light will reduce the graininess of the image." CreationDate="2016-04-10T22:45:39.950" UserId="231" />
  <row Id="2698" PostId="2291" Score="0" Text="I've added an additional final paragraph about putting the light underground as a test (see edited answer) that may help if you are unable to change the scene, but able to move the light." CreationDate="2016-04-10T22:49:17.140" UserId="231" />
  <row Id="2699" PostId="2279" Score="1" Text="Seeing your updated screenshot (3rd edit), is it still rendered with 16spp? With a only small area light, and no bidirectional path tracing or next event estimation, it's expected to have a lot of noise. Have you tried a much higher number, like 1000spp?" CreationDate="2016-04-11T01:55:11.087" UserId="182" />
  <row Id="2700" PostId="2301" Score="3" Text="Your question looks like it would belong to StackOverflow rather than here." CreationDate="2016-04-11T04:40:07.833" UserId="182" />
  <row Id="2701" PostId="2030" Score="0" Text="@Andreas: Thanks for the link, very informative. However the top answer says that SSBOs are even larger than TBOs, so I don't think this should be the problem. But I can try anyways!" CreationDate="2016-04-11T06:24:15.123" UserId="2666" />
  <row Id="2702" PostId="2030" Score="0" Text="@gartenriese That's not what I read. SSBO are limited to 16MB. TBO is limited to VRAM. And you do have more VRAM than that, right? In any case you should check those parameters for you platform (glGetInteger)." CreationDate="2016-04-11T06:34:53.510" UserId="3041" />
  <row Id="2703" PostId="2300" Score="1" Text="[Meta discussion about review questions.](http://meta.computergraphics.stackexchange.com/q/223/16)" CreationDate="2016-04-11T07:04:09.383" UserId="16" />
  <row Id="2704" PostId="2030" Score="1" Text="@Andreas: No, they are not limited to 16MB. 16MB is the minimal size it is guaranteed to have. Usually they are limited to VRAM. If you look at my question I already checked the size with `GL_MAX_SHADER_STORAGE_BLOCK_SIZE` and it's 2GB (which isn't actually my VRAM size, but still big enough)." CreationDate="2016-04-11T09:53:47.163" UserId="2666" />
  <row Id="2705" PostId="2030" Score="0" Text="@gartenriese Oh right, my bad." CreationDate="2016-04-11T09:55:06.207" UserId="3041" />
  <row Id="2706" PostId="2279" Score="0" Text="@JulienGuertault yes it is still rendered with 16 spp. No I didn't try with 1000spp because it would take a huge amount of time and this project is about a research topic which aims to good quality images with low number of spp." CreationDate="2016-04-11T12:59:48.033" UserId="3069" />
  <row Id="2707" PostId="2291" Score="0" Text="Ok, I followed your suggestion. I tried to put the light beneath the scene and nothing is lit up. No light passes through surfaces and therefore surfaces are not transparent. At least we can exclude this possibility.. so it can be that shadows are too soft or washed out by too much light reflected from the rest of the scene. How do you suggest me to proceed?" CreationDate="2016-04-11T13:06:12.773" UserId="3069" />
  <row Id="2708" PostId="2291" Score="0" Text="Have you tried rendering just a small section of the image yet? For example, the bottom right sixteenth of the image contains are strong shadow in the target image. Rendering just that small image would allow you to use a much higher number of samples per pixel, to give a clearer image and more idea of what is going on." CreationDate="2016-04-11T14:03:53.410" UserId="231" />
  <row Id="2709" PostId="2291" Score="0" Text="just did it, shadows are not there.. you can find the piece of image above at EDIT 4" CreationDate="2016-04-11T14:38:21.090" UserId="3069" />
  <row Id="2710" PostId="2291" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/38237/discussion-between-tarta-and-trichoplax)." CreationDate="2016-04-11T15:39:07.487" UserId="3069" />
  <row Id="2736" PostId="2302" Score="3" Text="I would just add that Euler angles work very well for FPS-style or orbiting cameras, where you can only rotate on 2 axes, and controls map directly to the yaw and pitch angles. There are no gimbal lock problems then. If your camera can rotate on all 3 axes, then quaternions may make more sense." CreationDate="2016-04-13T06:06:54.480" UserId="48" />
  <row Id="2738" PostId="2287" Score="0" Text="But how does the OS ask the GPU for memory to write to when you plugged in an HDMI port. Do OSes manage their own framebuffer when doing &quot;naive&quot; drawing operations and send the 1920*1080 pixel big framebuffer to the GPU with some arbitrary and common standard that can be implemented or does one need a driver for each GPU type so that one can send a simple text message over HDMI ? How does  a tty or xserver send it's content through HDMI to the screen?" CreationDate="2016-04-13T10:06:14.970" UserId="2623" />
  <row Id="2739" PostId="2287" Score="0" Text="The video card in a PC will handle encoding VRAM to a video signal, the CPU is never involved in this. For unaccelerated OS's they will keep a system memory based frame buffer and use the CPU to draw into it, and then send regions of it over the PCIE bus to the card (typically 'dirty' regions or just areas of the screen that need updating), so some GPU driver/motherboard driver is required to do this." CreationDate="2016-04-13T10:19:22.433" UserId="3073" />
  <row Id="2740" PostId="2287" Score="0" Text="So implementing this in my very own OS will not really be possible using open and known standards?" CreationDate="2016-04-13T10:34:27.023" UserId="2623" />
  <row Id="2741" PostId="2305" Score="1" Text="If you plan to immediately answer your own question please at least mention it in the question so others don't waste their time." CreationDate="2016-04-13T12:50:45.033" UserId="457" />
  <row Id="2742" PostId="2305" Score="2" Text="Every good question should have 2 or 3 good answers. You did not waste your time and I really appreciated your effords. I need this thread to explain the logic to a project partner and your post will help him alot." CreationDate="2016-04-13T12:53:50.467" UserId="361" />
  <row Id="2743" PostId="2279" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/38357/discussion-on-question-by-tarta-path-tracer-not-rendering-shadows)." CreationDate="2016-04-13T22:48:22.993" UserId="231" />
  <row Id="2744" PostId="2279" Score="0" Text="@SlippD.Thompson and Tarta, this is not an appropriate place to have a discussion about behaviour on an external site. You are both very welcome on this site, but complaints relating to other sites will need to be addressed there, not here. If you use the chat room created to continue discussing the original path tracing question, please do so respectfully. Heated conflict over differences tends to decrease understanding rather than increase it." CreationDate="2016-04-13T23:00:09.740" UserId="231" />
  <row Id="2745" PostId="2307" Score="0" Text="Thanks for your reply.&#xA;Idea with writing directly to color buffer sounds great. I will give it a try." CreationDate="2016-04-14T05:13:19.750" UserId="3123" />
  <row Id="2746" PostId="2309" Score="0" Text="Yes this tracing works, but if the object was sealed in the first place wouldt the contour have a sidedness?" CreationDate="2016-04-14T09:17:09.850" UserId="38" />
  <row Id="2747" PostId="2309" Score="0" Text="I'm not sure I follow. Isn't the point of the question to determine this sidedness? It's not like you magically get it as a parameter after the CSG slice." CreationDate="2016-04-14T09:23:23.287" UserId="2817" />
  <row Id="2748" PostId="2309" Score="1" Text="You do get the sidedness of  the model, 3d already has a sidedness which is almost allways the case. When you slice the model you know which side is outwards so for each loop you also know which side of it is inside (jsut record normal in 2D also). This means that if you loop around the contour clockwise (from above) and the predominate cross product of each edge is away from you then its a interior edge. But the trace trick can be faster as its a O(log(n)) operations while this test is n." CreationDate="2016-04-14T09:43:29.437" UserId="38" />
  <row Id="2749" PostId="2309" Score="0" Text="Ah, so you basically mean using the surface normal for that? I can see that working, but only if you have a way of knowing what &quot;away&quot; means. :) And while it's trivial for convex meshes (centroid should be fair enough), concave ones will be tricky. The ray trace method sidesteps this issue." CreationDate="2016-04-14T10:37:14.187" UserId="2817" />
  <row Id="2750" PostId="2311" Score="0" Text="Yes i know this, but wondering if there could be a alternate strategy to avoid division by 0." CreationDate="2016-04-14T10:37:54.963" UserId="38" />
  <row Id="2751" PostId="2309" Score="1" Text="that's whats i was alluding winding rules can handle this for you but tracing is less work." CreationDate="2016-04-14T10:40:02.013" UserId="38" />
  <row Id="2752" PostId="2287" Score="0" Text="You could have a look here for some more specifics: http://wiki.osdev.org/VGA_Hardware" CreationDate="2016-04-15T06:26:36.063" UserId="3073" />
  <row Id="2759" PostId="2305" Score="1" Text="@5chdn [Self answering is highly encouraged](http://blog.stackoverflow.com/2012/05/encyclopedia-stack-exchange/). Thanks for adding this answer. If you have an answer in mind at the time you post a question, there is a tick box below the question which will allow you to write your answer before posting the question, so both will appear at the same time. This is just to let you know in case it is useful - you certainly don't need to do this and self answers are still very welcome at any interval of time after posting the question." CreationDate="2016-04-16T09:48:50.590" UserId="231" />
  <row Id="2761" PostId="2319" Score="0" Text="This is what I was looking for! Thanks!!" CreationDate="2016-04-17T00:18:11.207" UserId="3144" />
  <row Id="2762" PostId="2311" Score="0" Text="You could avoid divide by zero by pushing in or out values that would cause one." CreationDate="2016-04-17T00:24:11.360" UserId="56" />
  <row Id="2763" PostId="2318" Score="0" Text="Also look into GJK and MPR (minkowski portal refinement)" CreationDate="2016-04-17T01:05:03.843" UserId="56" />
  <row Id="2764" PostId="2291" Score="0" Text="EDIT 6 contains the final explanation" CreationDate="2016-04-17T10:22:27.487" UserId="3069" />
  <row Id="2765" PostId="2310" Score="0" Text="Imagine having a 3d object INSIDE your eye. How would that look like in your opinion? What do your buddies think?" CreationDate="2016-04-17T11:06:14.913" UserId="3041" />
  <row Id="2766" PostId="2310" Score="0" Text="@Andreas yes but that only how the linear algebra trick works we could use numerous other tricks." CreationDate="2016-04-17T11:17:16.863" UserId="38" />
  <row Id="2767" PostId="2310" Score="0" Text="What other tricks?" CreationDate="2016-04-17T11:21:15.167" UserId="3041" />
  <row Id="2768" PostId="2310" Score="0" Text="@Andreas we could do spherical projections for example. Just because we have one mathematical model that works does not mean there are no other ways to solve problems." CreationDate="2016-04-17T11:50:04.330" UserId="38" />
  <row Id="2772" PostId="2310" Score="0" Text="Removed my answer" CreationDate="2016-04-17T14:59:11.590" UserId="3041" />
  <row Id="2773" PostId="2311" Score="0" Text="@joojaa Raytracing does not divide by zero." CreationDate="2016-04-17T15:01:07.537" UserId="3041" />
  <row Id="2774" PostId="2311" Score="0" Text="@Andreas Yes i know that neither does reality ;)" CreationDate="2016-04-17T15:01:44.980" UserId="38" />
  <row Id="2776" PostId="2326" Score="0" Text="Temporal algorithms ought to help, by reusing information from frame to frame, thus amortizing rendering costs over time." CreationDate="2016-04-18T04:45:53.690" UserId="56" />
  <row Id="2777" PostId="2323" Score="0" Text="Ok now we are getting somewhere. I am not really concerned in how most rasterizers work but rather if there is a novel way to do it that does not have this problem. I am thinking of accepting this answer as it hints at the answer, and would avoid me having to show that in fact one can do this by approaching the problem differently than how current pipelines do it. Its not a feature of projection but rather the matrix operations and how we read the data in the projection." CreationDate="2016-04-18T07:18:21.897" UserId="38" />
  <row Id="2778" PostId="2332" Score="1" Text="Are you looking to replace regions of pure red with pure blue? Or are you looking to change the red component of each colour to blue (so an orange region becomes cyan)? Somewhere in between these two approaches, you could identify regions of the image that are &quot;sufficiently red&quot; and only apply the change to those regions. If you edit the question to describe more specifically what you require, we will be better able to suggest algorithms." CreationDate="2016-04-18T09:33:19.480" UserId="231" />
  <row Id="2779" PostId="2332" Score="0" Text="I want to change pure red with pure blue maintaing the lightning and shades of color" CreationDate="2016-04-18T09:40:28.387" UserId="2383" />
  <row Id="2780" PostId="2332" Score="0" Text="This sounds like you want to change more than just one precise colour. In this [picture of an apple](https://upload.wikimedia.org/wikipedia/commons/2/20/Civni-Rubens_apple.jpg) would you want all of the reddish parts to become bluish? Or would you want only the very reddest parts to become blue, and the rest of the reddish parts to remain reddish?" CreationDate="2016-04-18T11:13:05.583" UserId="231" />
  <row Id="2781" PostId="2332" Score="0" Text="i want to all red to become blue? for both wouldn't the algorithm differ only in the threshold taken for red" CreationDate="2016-04-18T11:33:21.457" UserId="2383" />
  <row Id="2782" PostId="2332" Score="0" Text="Do you have an example before and after image?" CreationDate="2016-04-18T13:31:29.027" UserId="231" />
  <row Id="2783" PostId="2167" Score="0" Text="Very interesting post. Could you post some images using different constants?" CreationDate="2016-04-18T18:07:41.437" UserId="3041" />
  <row Id="2785" PostId="2334" Score="0" Text="What I am trying to do is Color Replacement Tool in Photoshop. I donot know what Channel Mixer does. I am trying to replicate the Color Replacement Tool." CreationDate="2016-04-19T03:36:26.743" UserId="2383" />
  <row Id="2786" PostId="2332" Score="0" Text="What i am trying to do is done in this tool http://explorug.net/coloranything/   I want to know what are the best methods to achieve this. This is same as PhotoShop's Color Replacement Tool." CreationDate="2016-04-19T03:37:48.830" UserId="2383" />
  <row Id="2788" PostId="2343" Score="1" Text="some type of [linear regression](https://en.wikipedia.org/wiki/Linear_regression) would help find the parameters" CreationDate="2016-04-19T14:04:13.627" UserId="137" />
  <row Id="2789" PostId="2342" Score="0" Text="Do you have any physical interpretation for the two normal maps? e.g. one is high-frequency and one is low-frequency and applied &quot;on top of it&quot; (which is the Reoriented Normal Mapping case)?&#xA;&#xA;I would expect that you would need to feed the mesh's normal/tangent/bitangent into the equation somehow. You might consider simply transforming the world space normal into tangent space, applying RNM, then transforming the result to world space—that's only one extra 3x3 mul." CreationDate="2016-04-19T14:17:41.263" UserId="196" />
  <row Id="2790" PostId="2342" Score="0" Text="Thanks for your answer @JohnCalsbeek but using baked tangent space normal doesn't provide good enough results that's why I am using a world space normal, especially a world space bent normal map, and I try to apply on top of if the regular normal map." CreationDate="2016-04-19T19:24:20.670" UserId="2372" />
  <row Id="2791" PostId="2342" Score="0" Text="I've added some screenshot to illustrate the issue I am having." CreationDate="2016-04-19T20:01:02.903" UserId="2372" />
  <row Id="2793" PostId="2346" Score="2" Text="Maybe a silly question, but you're setting up two separate VAOs and two separate arrays of VBOs for the two meshes, right? If you're accidentally re-using them, that would explain why setting-up B screws up the rendering of A." CreationDate="2016-04-19T22:46:37.597" UserId="48" />
  <row Id="2794" PostId="2341" Score="0" Text="Hi, we try to avoid link only answers. Can you summarize the contents of the link. Thisway your answer is not nearly as prone to link rot." CreationDate="2016-04-20T04:58:39.273" UserId="38" />
  <row Id="2795" PostId="2341" Score="0" Text="@joojaa I've corrected the answer." CreationDate="2016-04-20T05:44:47.307" UserId="2372" />
  <row Id="2797" PostId="2350" Score="0" Text="What Icetigris said, with the correction that although nails aren't always reflective, they have a high Fresnel constribution, especially on the tops of the lengthwise ridges. These ridges tend to be polished through normal wear, even if the valleys of the ridges are more diffuse." CreationDate="2016-04-20T17:58:44.800" UserId="3184" />
  <row Id="2798" PostId="2328" Score="0" Text="Thanks for your answer Julien! I do agree with you, most likely there is some error in the code.. I am not sure whether I am doing some wrong calculation or there is some error at the level of the algorithm itself. That's why I posted my code.. unfortunately I tried to implement some variance reduction algorithm like next event prediction.. the results are not better. Indeed those algorithm should give back a &quot;better&quot; image, but something is wrong at the base" CreationDate="2016-04-21T07:53:12.640" UserId="3069" />
  <row Id="2800" PostId="2208" Score="0" Text="Questions asking us to **recommend or find a book, tool, software, tutorial or other off-site resource** are off-topic for Computer Graphics as they tend to attract opinionated answers and spam. I have edited to remove that request, so that the question need not be closed. If this changes your intention in other ways, please edit to correct this." CreationDate="2016-04-21T11:20:42.770" UserId="231" />
  <row Id="2801" PostId="2355" Score="0" Text="There is [rotating calipers](https://en.wikipedia.org/wiki/Rotating_calipers) but that only works for convex polygons. Otherwise you can use it to as a base for a brute force solution." CreationDate="2016-04-22T09:51:58.693" UserId="137" />
  <row Id="2802" PostId="2355" Score="2" Text="Well if O(n^2) isnt a problem then test all point pairs" CreationDate="2016-04-22T09:52:26.363" UserId="38" />
  <row Id="2804" PostId="2355" Score="0" Text="Yes, diameter, intra-shape width, not sure if theres a proper name for it. E.g. how'd you measure the maximum length of a 5-point star shape?" CreationDate="2016-04-22T10:18:05.417" UserId="3204" />
  <row Id="2805" PostId="2355" Score="0" Text="was hoping for a simpler solution than this: https://gis.stackexchange.com/questions/32552/how-to-calculate-the-maximum-distance-within-a-polygon-in-x-direction-east-west" CreationDate="2016-04-22T11:18:34.263" UserId="3204" />
  <row Id="2806" PostId="2355" Score="1" Text="Actually it's a bit more involved: imagine 2 rooms connected by a narrow corridor. The largest diameter will end on the walls in the different rooms and won't end on any points." CreationDate="2016-04-22T11:19:25.323" UserId="137" />
  <row Id="2807" PostId="2355" Score="1" Text="Are you looking for an algorithm that works in the most general case or can it be restricted to e.g. the 2D case? This might be easier to solve with some more information or restrictions about the input. You use the word polygon which may hint at 2D-only, also the question you linked suggests the 2D case. Also, is it enough to consider vertex-vertex distances or do you need correct results for cases like ratchet freak mentioned in [his comment](http://computergraphics.stackexchange.com/questions/2355/find-the-longest-straight-line-between-two-points-on-surface-of-polygon#comment2806_2355)?" CreationDate="2016-04-22T20:02:12.510" UserId="127" />
  <row Id="2808" PostId="2355" Score="0" Text="@Nero 2D is fine. Shapes are mostly organic. Cancer nodules in ultrasound. I suppose vertex vertex is OK." CreationDate="2016-04-23T05:26:46.540" UserId="3204" />
  <row Id="2810" PostId="2355" Score="0" Text="Is your raw data raster (pixel) data rather than a list of vertices? If so a raster approach which cuts out the intermediate step of converting to vertices may increase accuracy." CreationDate="2016-04-23T11:58:07.300" UserId="231" />
  <row Id="2811" PostId="2309" Score="0" Text="Let me see if I have understood you correctly. Ray tracing parallel to the slicing planes seems to not only help me get the inside/ouside of the contour , but also pave a way to implement GPU slicer for 3D model made of triangular meshes." CreationDate="2016-04-23T13:04:45.360" UserId="2712" />
  <row Id="2812" PostId="2355" Score="0" Text="@trichoplax yes, but I thought that would be slower and messier. The ROI was drawn in paint by hand. Convert to vertexes was just noting the coordinates of each white pixel." CreationDate="2016-04-23T14:10:34.943" UserId="3204" />
  <row Id="2813" PostId="2320" Score="0" Text="In your `Sample` function, there is the statement `float ndotl = dot(I.getNormal(), L);`. Not sure, but `I.getNormal()` looks a bit like it denotes the normalized `I` vector, instead of the surface normal at position `I`." CreationDate="2016-04-25T08:03:36.617" UserId="1835" />
  <row Id="2814" PostId="2212" Score="0" Text="Hi @ivokabel, sorry for the late reply. You advice was good, and point me in the right direction, but was not enough. Indeed adding a brightness parameter to the light to manage the spectrum of the light. But i also added another parameter to manage the total luminance of a pixel: the tristimulus value obtained from the path tracer is multiply with the total radiance of the pixel (sum of each SPD wavelenght of the pixel). In this way the image brightness is good." CreationDate="2016-04-26T11:49:18.877" UserId="2237" />
  <row Id="2815" PostId="2212" Score="0" Text="This is an example: https://drive.google.com/open?id=0BxeVnHLvT8-7MFNuN1pDcG1DZjg. Anyway thank you again. I hope to find a completely physically based solution before the end of my thesis time." CreationDate="2016-04-26T11:49:24.143" UserId="2237" />
  <row Id="2816" PostId="2151" Score="0" Text="@Detheroc Looks really inteestingif you could notify me too when  you release publicly." CreationDate="2016-04-26T14:18:21.603" UserId="204" />
  <row Id="2817" PostId="2151" Score="0" Text="@UriPopov Well, I have released the source code publicly, please see my new answer." CreationDate="2016-04-26T15:28:18.787" UserId="2811" />
  <row Id="2818" PostId="2360" Score="5" Text="You can use any standard image downsampling method. There's nothing special about raytraced images that requires a different method." CreationDate="2016-04-26T17:29:20.133" UserId="48" />
  <row Id="2819" PostId="2360" Score="1" Text="Is part of the question whether or not downsampling should be done in linear or gamma corrected space? Linear space is always appropriate - GPUs can apply a degamma-regamma when resolving gamma-corrected multisample buffers." CreationDate="2016-04-26T18:08:32.773" UserId="2500" />
  <row Id="2820" PostId="2326" Score="0" Text="The question states &quot;acoording to some sources, less than 1% of computers in use...&quot;. Could you be more precise in what sources you are thinking of?" CreationDate="2016-04-26T18:50:18.367" UserId="3041" />
  <row Id="2821" PostId="2326" Score="1" Text="@Andreas http://goo.gl/YqMpwQ&#xA;http://goo.gl/2oQBPw&#xA;&#xA;I did not believe that this statement was relevant enough to the question to require a source. Especially considering that (while the actual numbers are debatable), it is obvious and well known that VR is performance heavy and thus won't run on slower PCs." CreationDate="2016-04-26T19:29:28.087" UserId="3156" />
  <row Id="2822" PostId="2360" Score="0" Text="I'm not asking about whether it should be done in linear or gamma space (I know that it should be done in linear space). @NathanReed could you suggest an easy to implement one?" CreationDate="2016-04-26T19:42:32.240" UserId="1924" />
  <row Id="2823" PostId="2326" Score="0" Text="I was not sure if &quot;some&quot; were some guys you met on reddit rambling or, as the links points out, quite respectable companies and news networks. As for obvious well that is matter of today and tomorrow. Today is 1%. 2020 is estimated 8%. The latter may well cover everyone still interested in VR by that time so it may or may not be a problem." CreationDate="2016-04-26T19:55:03.180" UserId="3041" />
  <row Id="2825" PostId="2362" Score="0" Text="is `y2 - y1` close to 0?" CreationDate="2016-04-27T10:53:42.743" UserId="137" />
  <row Id="2826" PostId="2362" Score="0" Text="it is already checked that y2 != y1. One more issue may be to check how the algorithm behaves when both the line segments overlap. I checked that the algorithm does not filter it out. Should it be a intersection in this case ?" CreationDate="2016-04-27T11:28:13.740" UserId="2712" />
  <row Id="2827" PostId="2362" Score="1" Text="subtracting 2 nearly equal values and then dividing with it is generally bad for precision." CreationDate="2016-04-27T11:37:38.387" UserId="137" />
  <row Id="2829" PostId="2362" Score="0" Text="Initially it just returns if the two line segments does intersect or not and the algorithm is edited. If there any intersection I proceed with next part of the algorithm to find the point where it intersects and then I use getXIntersection(..) function. I am afraid that the issue of two overlapped line segments are not identified in the initial part of the algorithm . What do you think ? Do you suggest any changes in the initial part ?" CreationDate="2016-04-27T12:07:49.303" UserId="2712" />
  <row Id="2834" PostId="2362" Score="0" Text="I tried with several point values with the help of geogebra and I get valid result as you asked." CreationDate="2016-04-27T12:22:56.343" UserId="2712" />
  <row Id="2839" PostId="2362" Score="0" Text="I've deleted my answer now that I understand that the initial step checks for segment intersection, not just line intersection. Sorry for the confusion. I've checked the pseudocode in the book and your code seems to match it. So your code for both steps looks correct to me. Since I can't see what is wrong, could you include the values you are testing with, showing the input values and returned result? The segment end points in the code appear to be identical (the same segment twice, rather than distinct segments): (5,1),(5,3) and (5,1),(5,3) again." CreationDate="2016-04-27T21:23:03.610" UserId="231" />
  <row Id="2840" PostId="2362" Score="0" Text="The result shows that it has the intersection with those points you mentioned - in other words overlapping line segments intersects. In my case loverlapping line segments must not be decided as intersection and I , believe that I have to add some extra condition to it. Need some hint on those conditions. At last but not the least, once the initial test mentions that there is intersection between the line segments, Do I have test if the calculated intersection point reside on both the line segments ?" CreationDate="2016-04-27T21:34:02.133" UserId="2712" />
  <row Id="2841" PostId="2361" Score="0" Text="I wonder if there are any high performance libraries out there instead of stb? Would halide support doing something like this?" CreationDate="2016-04-27T21:52:01.653" UserId="1924" />
  <row Id="2842" PostId="2362" Score="0" Text="Could you share the input values that result in your X-intersection value of -32768?" CreationDate="2016-04-27T22:25:30.363" UserId="231" />
  <row Id="2843" PostId="2362" Score="0" Text="No, if the initial test confirms that the segments intersect, then the intersection point automatically lies on both segments." CreationDate="2016-04-27T22:26:07.070" UserId="231" />
  <row Id="2844" PostId="2362" Score="0" Text="Lets name the line segments. Surrogate Segment - (0,1762)---(1057,1762) and Axis Aligned Segment - (1066,1762)----(1038,1762). As you can see both the segments overlap each other. The current algorithm does not deal with these type of scenarios" CreationDate="2016-04-27T22:48:44.470" UserId="2712" />
  <row Id="2845" PostId="2356" Score="2" Text="_&quot;Matlab is not the choice of language for this particular task&quot;_... nor mine. I'm not familiar with the language nor have access to Matlab.&#xA;&#xA;Having said that, I see you have the eigenvectors of the covariance matrix and are taking the dot product of each vertex against each of those vectors. The mins and maxs give the bounds of a box.&#xA;&#xA;However, I don't think this is guaranteed to give you a *good* bound. Imagine a dense central cluster of vertices with a few outliers.  The directions of your axes are going to governed by the dense cluster but your box really only depends on the outliers." CreationDate="2016-04-28T08:14:31.777" UserId="209" />
  <row Id="2846" PostId="2362" Score="0" Text="Does your code work correctly for segments that do not overlap (that is, segments that do not share the same line)? If so, then the original question is solved and you can ask about detecting overlapping segments as a new question. If not, then overlapping segments is best left until the code is fixed - let's get it working before adding extra exceptions. Please don't change the aim of a question - feel free to ask a new question for a new problem." CreationDate="2016-04-28T08:50:19.533" UserId="231" />
  <row Id="2847" PostId="2356" Score="1" Text="There seems to be a problem with matlab on this site, every time somebody puts up a question with matlab code it seems to rot forever. I have matlab installed but honestly would not care to fire it up for debugging" CreationDate="2016-04-28T08:54:03.000" UserId="38" />
  <row Id="2848" PostId="2356" Score="0" Text="@SimonF Surely though, as a bounding box, it still has to include those outliers - unless of course you have multiple bounding boxes. However, I'm only attempting to model the mathematics and the shape in question is just one I quickly drew up while I had the time..." CreationDate="2016-04-28T12:24:01.427" UserId="2646" />
  <row Id="2849" PostId="2356" Score="0" Text="Well, yes, it will still include the outliers, but one aim of OBBs over AABBs is to get a much smaller bounding volume. I just meant to point out that using PCA might not give a good result.  I must admit, though, I don't really have an efficient scheme for doing better :-|" CreationDate="2016-04-28T13:09:56.963" UserId="209" />
  <row Id="2850" PostId="2361" Score="0" Text="@Decave I don't know off the top of my head, but I did find [this image-resizing sample](https://github.com/halide/Halide/blob/master/apps/resize/resize.cpp) in the Halide repository." CreationDate="2016-04-28T15:54:44.777" UserId="48" />
  <row Id="2851" PostId="2361" Score="0" Text="Very cool, thanks @NathanReed" CreationDate="2016-04-28T19:11:08.077" UserId="1924" />
  <row Id="2852" PostId="2151" Score="1" Text="The thesis is publicly available here: https://dspace.cvut.cz/bitstream/handle/10467/62770/F8-DP-2015-Chlumsky-Viktor-thesis.pdf" CreationDate="2016-04-28T22:00:30.783" UserId="3255" />
  <row Id="2853" PostId="2370" Score="1" Text="Could you tell also where you found the &quot;mentions of using Interval Arithmetics&quot;?" CreationDate="2016-04-29T12:11:46.873" UserId="2074" />
  <row Id="2854" PostId="2374" Score="0" Text="Yes! Thanks! This solved the problem. As an additional note, &quot;Deterministic  Monte Carlo&quot; is (now) known as &quot;Brute Force&quot; in Vray. I thought I tried this already, but suspect I may have failed to set it in both my primary and secondary GI engines" CreationDate="2016-04-29T16:38:53.410" UserId="3251" />
  <row Id="2855" PostId="2370" Score="0" Text="@rych [In this paper.](http://www.cs.utah.edu/~knolla/dagstuhlHijazi.pdf)" CreationDate="2016-04-29T16:55:01.940" UserId="3233" />
  <row Id="2856" PostId="2327" Score="1" Text="Thanks! You were right, but I should also make note for future readers that I had misunderstood the inverse of the matrix. What I got from that inverse matrix (variable called x) was actually 3 scalars, not a coordinate. Think about that and everything else will click once this is realized. The first value (x.x) is actually the scalar distance from the start vector along the direction vector where the intersection occurs!" CreationDate="2016-04-29T22:44:42.077" UserId="3153" />
  <row Id="2857" PostId="2372" Score="0" Text="Two lines cannot partially overlap, but two line segments can." CreationDate="2016-04-30T00:42:17.940" UserId="231" />
  <row Id="2858" PostId="2364" Score="0" Text="It would be helpful to see your code in this question to make the post self-contained. I'll add an answer that refers to your code but it will make more sense if the code is visible in the question." CreationDate="2016-04-30T00:52:51.550" UserId="231" />
  <row Id="2859" PostId="2372" Score="0" Text="Ah, good point. I was thinking infinite lines. I'll update when I have a minute." CreationDate="2016-04-30T01:03:54.483" UserId="3003" />
  <row Id="2860" PostId="2366" Score="1" Text="It seems you have the answer you need, but if you ever want to look up the mathematics for anything else, the 2d donut-like shape you describe is called an [annulus](https://en.wikipedia.org/wiki/Annulus_%28mathematics%29)" CreationDate="2016-04-30T01:21:28.413" UserId="231" />
  <row Id="2861" PostId="2377" Score="1" Text="I've seen both, unfortunately. I'm not fond of ray per second as meaning exclusively primary rays and I'd suggest &quot;paths per second&quot; or  &quot;samples per second&quot; instead. &quot;Complete ray&quot; is not a name you'll find elsewhere: a ray is an unbounded line segment. &quot;Rays per second&quot; is ill specified for a path tracer: do shadow rays count, for instance? It's a useful metric for an acceleration framework (i.e. Embree or OptiX) but not a renderer." CreationDate="2016-04-30T21:20:15.810" UserId="3075" />
  <row Id="2862" PostId="2377" Score="0" Text="Also, be aware that samples per second still isn't a great metric of actual performance since sample quality will vary wildly depending on implementation details. It's probably the best thing you can do starting out though, as the better solutions involve fairly complex variance estimates." CreationDate="2016-04-30T21:30:43.067" UserId="3075" />
  <row Id="2863" PostId="2377" Score="0" Text="@KarlSchmidt I think you should post those comments as an answer ;)" CreationDate="2016-04-30T22:05:09.677" UserId="48" />
  <row Id="2864" PostId="2377" Score="0" Text="Probably a good idea, yes. :)" CreationDate="2016-04-30T22:56:45.753" UserId="3075" />
  <row Id="2865" PostId="2381" Score="0" Text="Can the pieces be of arbitrary shape, or do you require concave pieces to be decomposed into convex pieces?" CreationDate="2016-05-01T08:26:58.217" UserId="231" />
  <row Id="2866" PostId="2381" Score="2" Text="When you say &quot;clipped&quot;, can we assume that the inside of the square is discarded, and we are counting the number of disconnected pieces left outside the square?" CreationDate="2016-05-01T08:28:36.027" UserId="231" />
  <row Id="2867" PostId="2365" Score="0" Text="I can't find where it says there are two hardware queues in the programming guide. Could you post a quote from the document? Mention which chapter that says there are two queues? Is number of hardware queues queryable in runtime using OpenCL?" CreationDate="2016-05-01T12:06:03.263" UserId="3041" />
  <row Id="2868" PostId="2359" Score="0" Text="Can you be more precise about what your problem is (time, memory, logic)? What is stopping you from &quot;just do it&quot;?" CreationDate="2016-05-01T12:16:20.813" UserId="3041" />
  <row Id="2869" PostId="2380" Score="0" Text="Great answer. To add to it, since it is hard to come up with one metric that is universally meaningful, I'd suggest you pick one that makes most sense / is most honest for your usage case, and make sure you explain what you mean by your terminology." CreationDate="2016-05-01T13:59:28.150" UserId="56" />
  <row Id="2870" PostId="2365" Score="0" Text="I've updated my post. It does say *possible* execution, but if it can do a few why can't it do them all? Also the OpenCL runtime has no notion of hardware queue, so it's not something that you can query." CreationDate="2016-05-01T16:05:48.387" UserId="197" />
  <row Id="2872" PostId="2375" Score="1" Text="Thank you for the answer, also sorry for being so vague in my question. The goal is to create diverging lens using the CSG difference operator \ in the following manner: (C \ A) \ B. You've made it much more clearer to me on the general approach, but I'm still struggling to implement \. Can you please give an example for this operator as well?" CreationDate="2016-05-01T19:19:19.040" UserId="3233" />
  <row Id="2873" PostId="2359" Score="1" Text="Hey @Andreas I've updated the question with how am I doing it right now. The main issues are listed. I was wondering if a &quot;canonical&quot; best way of solving this problem is known. Typically when I cannot find them in google is because i am searching for &quot;the wrong keywords&quot;." CreationDate="2016-05-02T09:03:18.050" UserId="3225" />
  <row Id="2876" PostId="2387" Score="0" Text="A image might do wonders." CreationDate="2016-05-03T04:43:22.337" UserId="38" />
  <row Id="2880" PostId="2375" Score="1" Text="(Hoping I've got this right) Would it be easier for if you constructed the difference operator from *Intersection* and *Not* i.e.  A \ B  =  A ^ ~B.  To generate ~B you just need the &quot;gaps&quot;. For example, if the intersection along a ray for B is  { [b1, b2] }, then for ~B it'd be { [-infinity, b1] , [b2, +inf] }." CreationDate="2016-05-03T10:50:38.390" UserId="209" />
  <row Id="2881" PostId="2388" Score="0" Text="I'd upvote if there was also an example of a (possibly unrelated) soft shadow technique for contrast, as part of the question was what is the difference between them." CreationDate="2016-05-03T14:56:57.397" UserId="231" />
  <row Id="2882" PostId="2375" Score="0" Text="That did it, thanks a lot!" CreationDate="2016-05-03T17:48:02.430" UserId="3233" />
  <row Id="2883" PostId="2388" Score="1" Text="Soft shadows with shadow volumes makes me think of Instant Radiosity - essentially approximate area light sources as multiple point light sources and accumulate the results. Many shadow (100+) passes are doable in real time on a modern GPU with stacked memory if the shadow casting geometry can be limited." CreationDate="2016-05-03T19:09:00.507" UserId="2500" />
  <row Id="2884" PostId="2311" Score="0" Text="@joojaa But have you ever tried looking at something that's been pushed through your eyeball? :-)" CreationDate="2016-05-04T08:00:46.650" UserId="209" />
  <row Id="2885" PostId="2311" Score="0" Text="@SimonF Actually I have, although i tried to not concentrate on it.. But the projectile geometry actually pushes everything that crosses your view plane trough one point which reality does not ;)" CreationDate="2016-05-04T08:04:12.513" UserId="38" />
  <row Id="2886" PostId="2367" Score="0" Text="Hey thanks a lot for sharing ur knowledge. I wrote a code but it seems to have some error ive edites the post with an image. I would appreciate if u cab give me help with that. Im constantly running into problems so i searched for ur contact number cuz i cant post image in a comment lol." CreationDate="2016-05-04T10:24:59.313" UserId="3248" />
  <row Id="2887" PostId="2367" Score="0" Text="@quinnavery ...did you just copy/paste the shadertoy into your code? :D You're going to need a bit more plumbing than that. It's written in GLSL, not C++, which is where most of the errors are coming from. If you're not familiar with using shaders in Cocos2d-x, you should look up some tutorials on the subject first, such as [this one](https://www.raywenderlich.com/10862/how-to-create-cool-effects-with-custom-shaders-in-opengl-es-2-0-and-cocos2d-2-x)." CreationDate="2016-05-04T16:17:46.320" UserId="48" />
  <row Id="2888" PostId="2392" Score="0" Text="AMD's GCN will execute graphics and compute concurrently even when both are issued on the graphics queue, but generally not across multiple command buffers (multiple draw calls might even be sketchy). The driver (or application - I think in DX12 or Vulkan) must check for data dependencies and block between draw (graphics) and dispatch (compute) if needed. Multiple command queues would probably be useful if you have compute that is truly asynchronous from graphics (like physics for the next frame), but I have no direct experience with this." CreationDate="2016-05-05T00:22:33.503" UserId="2500" />
  <row Id="2892" PostId="2397" Score="1" Text="You could also have white bars. You can also use some image synthesis function to fill the pixels you can overlay a picture with itself  one that is cropped and one that stretches for example. here simply is a infinite amounts of ways to deal with no data. I think most of us just assumed its going to be a nonuniform scale." CreationDate="2016-05-06T08:23:02.803" UserId="38" />
  <row Id="2893" PostId="2394" Score="0" Text="Thanks for writing an answer! I'll take a look into BVH! Yes determining whether the voxels are inside/outside/intersected by the mesh is important for me. Could you elaborate a bit or point to a resource for using ray casting to determine inside/outsideness? From where to where do the rays go?" CreationDate="2016-05-06T11:27:18.987" UserId="3225" />
  <row Id="2895" PostId="2397" Score="0" Text="Very true. I assumed the original poster specifically didn't want to fill with a solid color and was interested only in the scaling aspect, since that's what they asked about. But there are lots of other fill methods. There are also non-linear scaling methods, where the center maintains its aspect ratio and the edges are stretched out, for example." CreationDate="2016-05-06T15:44:30.103" UserId="3003" />
  <row Id="2896" PostId="2394" Score="0" Text="@gnzlbg To test if a point P is inside the mesh, you can fire a ray starting at P in any direction, to infinity. If P is inside, it will hit the mesh an odd number of times; if P is outside, the ray will hit the mesh an even number of times (counting zero as an even number). [This Wikipedia article](https://en.wikipedia.org/wiki/Point_in_polygon#Ray_casting_algorithm) explains it for the 2D case, but it's the same idea in 3D." CreationDate="2016-05-06T19:43:02.843" UserId="48" />
  <row Id="2897" PostId="2403" Score="0" Text="This is a quick answer over the phone, spelling corrections and tex formulas welcome." CreationDate="2016-05-06T20:15:31.890" UserId="38" />
  <row Id="2900" PostId="2404" Score="0" Text="And the neat geometric interpretation is what I used. Personally I prefer to build the matrixes with a few number of atomic operations because then I only need to remember 3 rules of construction." CreationDate="2016-05-07T06:05:53.883" UserId="38" />
  <row Id="2901" PostId="2395" Score="0" Text="This is a great and informative answer, but Daniel's answer talks more about the tesselation in a historical context, which is what my question was really about. But thanks your answer. :)" CreationDate="2016-05-07T10:07:40.883" UserId="88" />
  <row Id="2905" PostId="2409" Score="0" Text="I'm not familiar with the framework, but wouldn't it be more efficient to create an acceleration structure for each model?" CreationDate="2016-05-07T12:39:30.493" UserId="457" />
  <row Id="2908" PostId="2402" Score="0" Text="explain your API. I would expect to see something like vertices A1,B1, A2,B2. Beside, what about the case they are not even aligned ? what kind of error threshold do you plan ?" CreationDate="2016-05-08T10:17:12.200" UserId="1810" />
  <row Id="2909" PostId="2412" Score="1" Text="The lines in plane $y = y_0$ are not all parallel." CreationDate="2016-05-08T13:06:10.150" UserId="137" />
  <row Id="2910" PostId="2412" Score="0" Text="Of course, I need a group of parallel lines in plane $y=y_0$ that their vanishing point is some point, say $(x_0,0,1)$ (the camera position and direction and the view plane are defined in the question)" CreationDate="2016-05-08T13:07:41.200" UserId="3305" />
  <row Id="2911" PostId="2414" Score="0" Text="I have tough about that, so if my camera in $(0,0,0)$ and it's looking to $(0,0,1)$, the view plane is $z=1$ and lets say that the vanishing point is $(10,0,1)$. Then the line from the camera to the vanishing point is $(0,0,0)+t(10,0,1)$. So all the parallel lines that are going to that vanishing point will be $(a_1,a_2,a_3) + t(10,0,1)$? And then all the lines in plane $y=y_0$ that have that vanishing point will be $(a_1,y_0,a_3) + t(10,0,1)$?" CreationDate="2016-05-08T13:58:57.273" UserId="3305" />
  <row Id="2912" PostId="2414" Score="1" Text="stop focusing on the $y = y_0$ plane, just having the vanishing point on the view plane and the camera position is enough." CreationDate="2016-05-08T14:08:22.040" UserId="137" />
  <row Id="2913" PostId="2407" Score="0" Text="What about the case where one corner of the square is inside the polygon and all others are outside?" CreationDate="2016-05-08T14:08:51.740" UserId="310" />
  <row Id="2914" PostId="2414" Score="0" Text="OK, so $(a_1,a_2,a_3)+t(10,0,1)$ are all the lines that their vanishing point is $(10,0,1)$?" CreationDate="2016-05-08T14:09:50.400" UserId="3305" />
  <row Id="2915" PostId="2407" Score="1" Text="@RichieSams its the same case as splits into one shard, it exits 2 times and results in one concave shard." CreationDate="2016-05-08T14:56:47.270" UserId="38" />
  <row Id="2916" PostId="2367" Score="0" Text="hi, Im having some trouble with my developer.... he really doesnt know how to code (Just my opinion) I'm considering to hire somebody else to create the game for me. You really seem like you can code since u shared your knowledge in less than a day. I think it will be quite easy for you? ^^ I'm trying to create for android platform and dont mind what program u use cocos, unity...., Will you be interested? I do pay upfront LOL" CreationDate="2016-05-08T17:03:50.183" UserId="3248" />
  <row Id="2917" PostId="2409" Score="0" Text="Okay. Yes, but then I have to switch out which mesh is the currently active one... I think that would be doable... switching just works by setting a variable in the graphics memory I think. I would have to try if this influences perfomance in a big way. Thank you for your suggestion." CreationDate="2016-05-08T17:10:12.533" UserId="273" />
  <row Id="2918" PostId="2367" Score="0" Text="if u are interested, plz let me know and we can proceed to further negotiations via email or something. I can pay you via paypal just to let ya know~" CreationDate="2016-05-08T17:10:20.763" UserId="3248" />
  <row Id="2919" PostId="2411" Score="1" Text="BTW, what is the &quot;*&quot; notation in this formula? I've never seen that in the rendering equation before." CreationDate="2016-05-08T18:32:01.207" UserId="48" />
  <row Id="2920" PostId="2411" Score="0" Text="@NathanReed p* is just a point from another surface, L(p*,-wi) is just the radiance from point p in direction wi" CreationDate="2016-05-08T18:53:54.177" UserId="2359" />
  <row Id="2923" PostId="2380" Score="0" Text="Yeah right, thank you for clearing my thoughts! Currently I choose the &quot;samples per second&quot; since it is easy to understand and hard to be misunderstood IMO. I'll eventually implement something that can calculate variance and measure rendering performance based on that." CreationDate="2016-05-08T19:42:39.173" UserId="3267" />
  <row Id="2924" PostId="2407" Score="1" Text="@trichoplax right i can add that. There shouldn't be less,  but see i added the or less for your comment.  I am not sure i have proven there isn't a smaller case (i could), there shouldn't be but i haven't laid out the proof very well for that case, not rigorous enough. I have however proven that there are no more cases than 4 and since i can find a 4 it proves that is maximum which is enough to answer the question at hand." CreationDate="2016-05-08T20:11:39.083" UserId="38" />
  <row Id="2925" PostId="2407" Score="0" Text="@joojaa fair point." CreationDate="2016-05-08T20:24:26.147" UserId="231" />
  <row Id="2926" PostId="2398" Score="0" Text="The C version to the above code is in the link at description of question." CreationDate="2016-05-09T01:32:42.353" UserId="2383" />
  <row Id="2928" PostId="2409" Score="0" Text="@Rotem I tried to just create everything before displaying, and then only switching it out... It turns out each time tell Optix to use another model this takes nearly half a second and my &quot;animation&quot; looks really unsmooth." CreationDate="2016-05-09T11:41:06.550" UserId="273" />
  <row Id="2929" PostId="2402" Score="0" Text="If integers are 32 bits, the multiplies in orientation could overflow if p1's coordinates are large negatives and p2, p3's coordinates are large positives. Other than that, this looks correct - cross products are zero, h's vertexes are within the bounding box of c." CreationDate="2016-05-09T13:40:06.563" UserId="2500" />
  <row Id="2931" PostId="2402" Score="0" Text="will it solve the issue if d1, d2 , d3 and d3 are stored as 64-bit integer type instead ?" CreationDate="2016-05-09T15:40:48.973" UserId="2712" />
  <row Id="2932" PostId="2402" Score="0" Text="&quot;orientation&quot; would have to compute with 64 bit integers too; the math there is currently promoted to integers, but they won't quite hold your worst case multiply result (if 32 bits). If you know your original data is limited (e.g. I think if you only had positive values - effectively positive 15 bit values, assuming 2's compliment integers), this may not be a problem. Start with a test that fails due to overflow, then work modify to fix. That is, prove my thinking isn't wrong before using the extra bits (or another strategy)... ;)" CreationDate="2016-05-09T17:46:49.560" UserId="2500" />
  <row Id="2934" PostId="2398" Score="0" Text="I have removed the &quot;//&quot; and C# implementation of above code is this much only" CreationDate="2016-05-10T00:05:01.570" UserId="2383" />
  <row Id="2935" PostId="2406" Score="0" Text="`combination of a quite dark texture with very bright lighting or an extremely overexposed camera setting can show banding in the final frame` very good insight. But we may note that gamma encoding is here precisely to mitigate this point. If he has the problem why not try a superior gamma exponent ? that would inhibit usage of hardware sRGB samplers though." CreationDate="2016-05-10T00:50:33.420" UserId="1614" />
  <row Id="2936" PostId="2419" Score="3" Text="The magic keyword to search for is &quot;polygon offsetting&quot;. The curves you're trying to generate are called offset curves. [Here is a StackOverflow question](http://stackoverflow.com/questions/1109536/an-algorithm-for-inflating-deflating-offsetting-buffering-polygons) with some good background and links to some libraries that can do it. You can also find plenty of material with a web search." CreationDate="2016-05-10T02:16:25.833" UserId="48" />
  <row Id="2937" PostId="2398" Score="0" Text="Thanks for confirming. That's much clearer now." CreationDate="2016-05-10T05:32:49.343" UserId="231" />
  <row Id="2938" PostId="2406" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines." CreationDate="2016-05-10T07:32:58.720" UserId="2736" />
  <row Id="2939" PostId="2420" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines. I've updated my original post." CreationDate="2016-05-10T07:33:27.423" UserId="2736" />
  <row Id="2940" PostId="2406" Score="0" Text="@KristofferHelander Good to know, but I think what I wrote probably applies just as well to offline rendering. But I admit that I don't have much direct experience in that area." CreationDate="2016-05-10T07:39:04.373" UserId="48" />
  <row Id="2941" PostId="2406" Score="0" Text="@NathanReed Yes, you most certainly made some really good points there :)" CreationDate="2016-05-10T07:54:49.367" UserId="2736" />
  <row Id="2942" PostId="2401" Score="0" Text="This doesn't really seem to adress the actual question much at all." CreationDate="2016-05-10T09:51:32.520" UserId="6" />
  <row Id="2943" PostId="2401" Score="0" Text="I added it as a comment and the original questioner asked me to promote it to an answer. See questioner's comment below..." CreationDate="2016-05-10T14:28:51.530" UserId="2500" />
  <row Id="2944" PostId="2422" Score="0" Text="I didn't read your code because I saw this:&#xA;&#xA;*&quot;For example, when one of the vertices is behind the camera, the texture is stretched.&quot;*&#xA;&#xA;You *have* to clip your polygons to a Z=constant plane in front of the camera or else all sorts of chaos will ensue :-)" CreationDate="2016-05-10T15:47:29.307" UserId="209" />
  <row Id="2945" PostId="2422" Score="0" Text="@SimonF I am already clipping on the Z axis, see _ZCLIP_ in the 3rd code." CreationDate="2016-05-10T16:32:41.590" UserId="3330" />
  <row Id="2946" PostId="2422" Score="0" Text="My apologies! I saw that opening paragraph and it &quot;raised a red flag&quot;.    So given *&quot;I am guessing that it has something to do with the texture mapping not being perspective-correct.&quot;* .. I tried to understand your &quot;map&quot; function but got lost. For example, I don't understand why you need a square root.  IIRC there's an initial set up from triangle coords to create 9 params, a,b,c d e f  p q r, then @ pixel x,y,  U= (a x +by + c)/(px+qy+r)  and V = (d x +ey + f)/(px+qy+r).   To determine the abc..pqr requires adjoint of a simplified &quot;3x3&quot; matrix (i.e. inverse w/out det) and mat muls." CreationDate="2016-05-10T16:55:31.523" UserId="209" />
  <row Id="2947" PostId="2416" Score="1" Text="Hi headbanger, image warping transformations are certainly possible but it's a bit unclear from your question what you're looking for. An example before/after screenshot or diagram would help." CreationDate="2016-05-10T17:08:59.257" UserId="48" />
  <row Id="2948" PostId="2422" Score="0" Text="@SimonF My *map()* function is adapted from [this StackoverFlow answer](http://stackoverflow.com/questions/808441/inverse-bilinear-interpolation). I thought it was perspective-correct, but I guess it isn't. Does all perspective-correct texture mapping techniques require to break down the polygon into triangles? Is there any formula that is not in a matricial form? Thanks!" CreationDate="2016-05-10T17:49:48.373" UserId="3330" />
  <row Id="2949" PostId="2423" Score="1" Text="Wow, thanks @LarryGritz! Considering you work for Sony Pictures Imageworks, I'll take you as a very reliable source! :)&#xA;&#xA;But how do you capture these textures? Most cameras only shoot in 14bit RAW files, do you have special cameras with 16bit linear sensors? Or do you take multiple exposures for textures, just like one does for HDRI imaged based lighting? Or do you simply capture with 14bit camera RAW and saves it as &quot;16bit&quot;? Ow, and what file format do you use, .tiff (.tx, .tex) or .exr?&#xA;Thanks again for your input!! :)" CreationDate="2016-05-10T19:27:15.013" UserId="2736" />
  <row Id="2950" PostId="2401" Score="1" Text="My question was about why a seemingly underused feature like tesselation has gotten so much API attention. RichieSams' answer talks more about what tesselation is, rather than explaining how it got to be so important. The above answer is the best answer I've gotten so far." CreationDate="2016-05-11T04:52:39.783" UserId="88" />
  <row Id="2951" PostId="2422" Score="0" Text="Why on earth do people want to avoid the matrix calculations. Triangles make it easy to transform the coordinates." CreationDate="2016-05-11T10:22:36.163" UserId="38" />
  <row Id="2952" PostId="2425" Score="0" Text="But how can I use this with the Rendering Equation which involves radiance ?" CreationDate="2016-05-11T10:36:33.913" UserId="3339" />
  <row Id="2953" PostId="2422" Score="0" Text="@joojaa My reason is that creating a matrix object, calling and updating members is going to take longer to the JVM than simply executing some bytecode. Okay, I understand. Does it work w/o breaking it down into triangles though?" CreationDate="2016-05-11T10:38:23.343" UserId="3330" />
  <row Id="2954" PostId="2426" Score="0" Text="Hi @sajis997, it's hard to understand what you're talking about from words alone. Could you add a screenshot or diagram to show what you mean by inner and outer loops and tree depth?" CreationDate="2016-05-11T16:43:11.213" UserId="48" />
  <row Id="2955" PostId="2401" Score="0" Text="Another aspect is that geometry shaders (flexible enough to implement tessellation, and available in DX10), are extremely hard to implement efficiently in HW. My understanding is the main issue is the in-order processing of the output from each input primitive means executing geometry shaders in parallel requires substantial buffering. A fixed function tessellator turned out to be far more efficient." CreationDate="2016-05-11T16:52:41.040" UserId="2500" />
  <row Id="2956" PostId="2426" Score="0" Text="Hi @NathanReed, The initial question is edited with some image snapshot. I hope it wil be clear now." CreationDate="2016-05-11T19:24:10.857" UserId="2712" />
  <row Id="2957" PostId="2424" Score="0" Text="In short, the number of photos you'll encounter drops as you get farther from a point light source. You'll encounter 1/(distance^2) in fact (;" CreationDate="2016-05-12T03:28:08.710" UserId="56" />
  <row Id="2959" PostId="2416" Score="0" Text="I think the question is pretty clear in intention. Just dont expect people to publish code for you as the question sounds more like do my work for me the way its presented." CreationDate="2016-05-12T08:38:12.673" UserId="38" />
  <row Id="2960" PostId="2422" Score="0" Text="*&quot;Does it work w/o breaking it down into triangles though&quot;*. If your surface is planar and the texturing, in world space, is linear, then &quot;yes&quot;. You just need 3 non-colinear points on that surface along with their matching UVs." CreationDate="2016-05-12T14:01:57.077" UserId="209" />
  <row Id="2961" PostId="2431" Score="0" Text="The code appears to return a point uniformly selected from the *interior volume* of the unit sphere. This will give different results from the surface of the sphere. Is this what you intend? I ask because you mention picking a point &quot;on&quot; the light, which sounds like a point on the surface." CreationDate="2016-05-12T17:04:48.740" UserId="231" />
  <row Id="2963" PostId="2431" Score="0" Text="But.. _centerOfSphere + Vec3&lt;float&gt;(x, y, z) * radius;_ should give me a point on the surface of the sphere! right? cause I move from the center towards the surface of _radius_" CreationDate="2016-05-12T20:01:05.323" UserId="3069" />
  <row Id="2966" PostId="2431" Score="1" Text="If the points are already on the surface of the sphere, then multiplying by radius will give the surface of a sphere of that radius. If the points are already in the interior volume of the sphere, then multiplying by radius will put them in the interior volume of a sphere of that radius. Multiplying by, for example, radius 2, will give a sphere twice as large. However, whether the points are on the surface or in the volume of this larger sphere depends on whether they were on the surface or in the volume of the original unit sphere" CreationDate="2016-05-12T22:18:24.223" UserId="231" />
  <row Id="2968" PostId="2436" Score="0" Text="I believe you are correct. I will check my PBRT book when I get home and correct the code. IIRC, I need to add a square root." CreationDate="2016-05-12T23:09:45.017" UserId="310" />
  <row Id="2969" PostId="2426" Score="0" Text="Do you have access to the tree shown in the image? If so my answer would be &quot;the outer loops are at even depths, the inner loops are at odd depths&quot;. Is your requirement to produce this tree, or to interpret its results?" CreationDate="2016-05-13T01:01:46.353" UserId="231" />
  <row Id="2970" PostId="2436" Score="2" Text="Yes, @trichoplax is right, sampling both angles uniformly will lead to points bunching at the poles. The simplest way to fix it is sample theta and Y uniformly, then set the XZ radius = $\sqrt{r^2 - y^2}$. Which, BTW, is exactly what you're doing in the second code snippet when you sample `cosPhi` (which is proportional to Y) uniformly. See also [this MathWorld article](http://mathworld.wolfram.com/SpherePointPicking.html)." CreationDate="2016-05-13T02:01:09.303" UserId="48" />
  <row Id="2971" PostId="2436" Score="0" Text="Fixed and updated." CreationDate="2016-05-13T02:36:22.383" UserId="310" />
  <row Id="2972" PostId="2424" Score="0" Text="By photos I meant photons by the way" CreationDate="2016-05-13T04:21:23.637" UserId="56" />
  <row Id="2973" PostId="2426" Score="0" Text="After the slicing operation, I have closed loops . I want separate into separate layer parts when each layer part contain an outer loop, one/several loops and infill pattern. I want to use Tree data structure for this purpose as mentioned in the paper." CreationDate="2016-05-13T07:38:12.677" UserId="2712" />
  <row Id="2975" PostId="2438" Score="1" Text="You could pass the color data in a constant buffer rather than vertex data. If they are all the same." CreationDate="2016-05-13T11:55:50.033" UserId="310" />
  <row Id="2979" PostId="2441" Score="0" Text="Do you require a neighbourhood that extends beyond immediate neighbours or would you be interested in approximations based on repeated application of a nearest neighbour approach?" CreationDate="2016-05-13T16:18:50.413" UserId="231" />
  <row Id="2980" PostId="2441" Score="0" Text="@trichoplax As long as the number of iterations required to reach a 10-hop standard deviation isn't going to be prohibitive. (I guess it wouldn't be, because at least in the 1D case a 10-hop deviation can be reached by mixing nearest neighbors 100 times.) I just wouldn't know how to weight the neighbors in this case." CreationDate="2016-05-13T17:11:29.177" UserId="3294" />
  <row Id="2981" PostId="2438" Score="1" Text="If GL ES 3.0 is a possibility, you could also use geometry instancing, but it doesn't seem to be available in GL ES 2.0 as far as I can tell from some googling." CreationDate="2016-05-13T18:02:20.797" UserId="48" />
  <row Id="2985" PostId="2438" Score="0" Text="@Ratchet freak OK I re-strategised my plan! Thank you for letting me know.  If you are willing I made a new one because I am stuck on that one, I need to get good at vertex buffers somehow." CreationDate="2016-05-14T00:12:35.043" UserId="2308" />
  <row Id="2986" PostId="2438" Score="0" Text="http://stackoverflow.com/questions/37221027/how-to-write-complicated-vertex-buffers" CreationDate="2016-05-14T00:12:37.637" UserId="2308" />
  <row Id="2990" PostId="2441" Score="0" Text="@joojaa It is separable when applied on a flat surface/grid but is it still separable when you have edges going off in arbitrary directions from each vertex?" CreationDate="2016-05-14T11:56:59.197" UserId="3294" />
  <row Id="2991" PostId="2443" Score="0" Text="Yes, MIS is an important production-verified technique, which helps a lot and I employ it in my solution (I guess, I should have stated that more clearly in the question).&#xA;&#xA;However, the overall performance of a MIS-based estimator depends on the quality of its partial sampling strategies. What I am trying to do here is to improve one of the the sub-strategies to improve the overall performance of the estimator. In my experience, it is usually more efficient to use less high-quality samples than may be more expensive to generate than more easily-generated low-quality ones." CreationDate="2016-05-14T12:28:28.730" UserId="2479" />
  <row Id="2992" PostId="2444" Score="0" Text="Thank you. My mention of the 3D Euclidean distance may have done more harm than good. I only meant it as an expensive (and conditional) approximation that is simple to explain. I wouldn't pursue the approximation if it is less attainable than the ideal answer." CreationDate="2016-05-14T23:28:35.730" UserId="3294" />
  <row Id="2993" PostId="2444" Score="0" Text="What I am ideally after is what you call &quot;using 2D distance&quot;. On a regular grid (with consistent local geometry) it is easy because the same symmetrical 1-hop-neighborhood kernel can be applied (repeatedly) throughout the grid. My real problem is determining the kernel-weights of 1-hop neighbors around *each* vertex *from the local geometry* of that 1-hop neighborhood." CreationDate="2016-05-14T23:28:54.640" UserId="3294" />
  <row Id="2994" PostId="2444" Score="0" Text="Each vertex has valence 4 but edge lengths and directions vary. (Alternatively (if for some reason planar faces make the problem tractable) each quadrilateral face can be broken along its shortest diagonal into two (now planar) triangles, in which case vertex valence varies from 4 to 8, and edge lengths become even more varied.)" CreationDate="2016-05-14T23:29:34.767" UserId="3294" />
  <row Id="2995" PostId="2444" Score="0" Text="Ultimately, I have a vertex with a ring of neighbors at different distances and angles, and I need to know the local kernel weights over these neighbors. Constraining the mean/centroid of the kernel to lie &quot;in the center&quot; (which in this case presumably means along the mean curvature normal's span from the vertex) already constrains 2DoF in the kernel's weights. Choosing some variance provides another constraint. But the system is still underdetermined since valence&gt;3. How to choose the weights?" CreationDate="2016-05-14T23:30:08.583" UserId="3294" />
  <row Id="2998" PostId="2444" Score="0" Text="Yes, &quot;using 2D distance&quot; is a little vague. I guess we should distinguish between &quot;shortest edgewise distance&quot; and &quot;shortest distance within the surface&quot; (which won't necessarily stay on edges). Fortunately the difference becomes less relevant the more times you apply the blur (even a box filter will approximate a Gaussian blur if applied several times). If you can determine whether you need a Gaussian blur with a precise parameter or just a blur that is Gaussian with an adjustable parameter, then you'll be able to decide whether a repeated box-style filter will be sufficient." CreationDate="2016-05-15T00:44:22.903" UserId="231" />
  <row Id="3001" PostId="2444" Score="0" Text="Well I did say in the question that I am after the Gaussian blur &quot;over the surface&quot;, and explicitly that the 3D Euclidean distance thing is an approximation accurate under certain conditions. What I was trying to explain in the comments is that (while the approximation is acceptable in accuracy and simple to explain) if an easier/cheaper algorithm can give the correct answer then I obviously wouldn't go to any extra lengths just to get back to the approximation." CreationDate="2016-05-15T07:31:47.510" UserId="3294" />
  <row Id="3003" PostId="2444" Score="0" Text="If the blur is to be performed by repeated convolution with a local (1-hop-neighborhood) kernel, that kernel (or &quot;box&quot;) would have to vary from vertex to vertex, because the valence and/or neighborhood geometry may vary from vertex to vertex. I think what matters most is that the (per-vertex) kernels' means/centroids and variances/moments of inertia must be consistent. These constraints would fully determine a valence-3 vertex's kernel's weights, but in cases of higher valence perhaps any kernel complying with these constraints will eventually converge to the effect of a Gaussian kernel." CreationDate="2016-05-15T07:42:21.683" UserId="3294" />
  <row Id="3004" PostId="2446" Score="0" Text="1. Does glGetError return anything else than no error? 2. &quot;Blown up&quot; does not really mean anything to me. Maybe it's just me but I'm having hard time figuring out what the problem is and/or expected behavior." CreationDate="2016-05-15T11:30:47.090" UserId="3041" />
  <row Id="3005" PostId="2444" Score="0" Text="I see. Rereading the question I see I misread the start and thought the main focus was a 3D distance approach. I'll have another look and edit the answer in a while." CreationDate="2016-05-15T11:38:20.457" UserId="231" />
  <row Id="3007" PostId="2446" Score="0" Text="As far as I can understand, the images each show some aspect working as it is intended. It isn't clear to me which one (if any) shows the problem you are having. Could you edit to either add an image showing the problem, or indicate which existing image already shows the problem?" CreationDate="2016-05-15T11:42:12.197" UserId="231" />
  <row Id="3008" PostId="2441" Score="0" Text="@Museful Ok whereabout  iterative smaller blurs that are a bigger that way you can do it only over edge connections and let the iterations propagete the effect. This is used in FEM and fluid sims to high efficiency" CreationDate="2016-05-15T15:49:26.570" UserId="38" />
  <row Id="3009" PostId="2446" Score="0" Text="@Andreas so sorry this wasn't clear! I have made some edits that hopefully make things a bit easier to understand.  The simulation I am doing is just a bunch of moving particles that are going to leave a trail behind them. Thankyou so much for looking into this." CreationDate="2016-05-15T16:18:39.363" UserId="2308" />
  <row Id="3010" PostId="2446" Score="0" Text="@trichoplax Same thing for you, I have edited it to make that bit more clear." CreationDate="2016-05-15T16:18:58.380" UserId="2308" />
  <row Id="3011" PostId="2446" Score="0" Text="@Andreas also glGetError does not return anything, except for one error every time I create a shader.  That has not proven to be significant.  Also this error happens on the code apple wrote for their template so It is probably a non-issue." CreationDate="2016-05-15T16:24:14.013" UserId="2308" />
  <row Id="3012" PostId="2446" Score="0" Text="This is a little clearer. If I understand correctly, you want the texture shown in image 1 to cover the image, but without being scaled up as in image 2. So you want to tile the image with the texture shown in image 1?" CreationDate="2016-05-15T17:35:22.787" UserId="231" />
  <row Id="3013" PostId="2441" Score="0" Text="@joojaa Yes I think that is the best approach but how exactly does the small blur work, as the connections/edges of a mesh are not generally regular, like they are between, say, pixels in an image. The mesh geometry should somehow influence how &quot;conductive&quot; each edge is relative to other edges in its neighborhood. How?" CreationDate="2016-05-15T17:38:45.307" UserId="3294" />
  <row Id="3014" PostId="2441" Score="1" Text="Im not sure, im not so deeply invested in the lore of FEM. All I know that they can solve the diffusion over irregular meshes and that is entirely analogous with gaussian blur." CreationDate="2016-05-15T18:04:02.593" UserId="38" />
  <row Id="3015" PostId="2449" Score="0" Text="Thank you **very** much for your explanation. This is maybe off-topic, but I'll ask anyway; in some source codes that compute noise, people use vector vec3(1, 57, 113) to compute dot product with current coordinate (I suppose the aim is also to obtain a hash). Why this particular choice of constants (57 is approx. 1 radian in degrees, 133 = approx. 2*radian in degrees)? Is it because of periodicity in trig functions? I'm unable to google this." CreationDate="2016-05-15T18:55:16.090" UserId="3365" />
  <row Id="3016" PostId="2449" Score="3" Text="@sarasvati I'm not really sure, but a guess is that 57 and 113 are chosen because they're prime-ish numbers. (113 is prime; 57 isn't, but it's 3*19, so still kinda primey...if that's a thing.) Multiplying or modding by a prime-ish number tends to jumble up the bits, so it's not an uncommon ingredient in hashes." CreationDate="2016-05-15T19:15:58.503" UserId="48" />
  <row Id="3017" PostId="2446" Score="0" Text="@trichoplax No, I am thinking that the texture in the FBO is not big enough." CreationDate="2016-05-15T19:19:35.480" UserId="2308" />
  <row Id="3018" PostId="2449" Score="0" Text="Wow, I didn't think about it this way. Thanks." CreationDate="2016-05-15T19:20:40.767" UserId="3365" />
  <row Id="3019" PostId="2446" Score="1" Text="As far as opengl errors go all indicate something did not have desired effect. It matters not who wrote the code. Fix it. Btw what does the FBO look like in the debugger? Anything unusual?" CreationDate="2016-05-15T19:55:22.667" UserId="3041" />
  <row Id="3020" PostId="2449" Score="0" Text="What are the magic numbers?" CreationDate="2016-05-15T20:41:06.323" UserId="3367" />
  <row Id="3021" PostId="2449" Score="0" Text="@cat Not sure how Mikkel came up with them. They may just be random / arbitrarily chosen values." CreationDate="2016-05-15T21:04:08.753" UserId="48" />
  <row Id="3022" PostId="2449" Score="0" Text="@NathanReed Doesn't GLSL have an RNG / PRNG? Why not use that or hardware entropy over hardcoded arbitrary constants?" CreationDate="2016-05-15T21:05:05.290" UserId="3367" />
  <row Id="3023" PostId="2449" Score="0" Text="@cat Well any PRNG or hash function has hardcoded constants in its implementation. And anyway a PRNG is not what you want for this application; you want a hash function. You could use a PRNG as a hash by re-seeding it every time, but a lot of PRNGs don't give good results when used that way; they're primarily designed to give good results in the sequence generated from one seed." CreationDate="2016-05-15T21:10:23.537" UserId="48" />
  <row Id="3024" PostId="2446" Score="0" Text="@Andreas Yeah for some reason the &quot;Depth Attachment&quot; appears to also have a renderbuffer." CreationDate="2016-05-15T21:21:42.247" UserId="2308" />
  <row Id="3025" PostId="2449" Score="1" Text="@cat I doubt GLSL has a PRNG, given that GLSL programs are deterministic." CreationDate="2016-05-15T22:20:40.880" UserId="2316" />
  <row Id="3026" PostId="2449" Score="0" Text="@immibis Shows what I know, I thought GLSL was at least turing-complete." CreationDate="2016-05-15T22:32:31.670" UserId="3367" />
  <row Id="3028" PostId="2449" Score="1" Text="Looks like there are several potential new questions in this comment thread..." CreationDate="2016-05-16T00:22:52.760" UserId="231" />
  <row Id="3029" PostId="2449" Score="0" Text="@cat Being deterministic does not preclude being Turing-complete." CreationDate="2016-05-16T01:28:36.453" UserId="2316" />
  <row Id="3030" PostId="2449" Score="0" Text="@immibis I think I misinterpreted your comment as &quot;it's decidable whether a given program will halt&quot; not &quot;deterministic side-effects&quot;, because I can read. I'll shut up now :P" CreationDate="2016-05-16T01:33:17.547" UserId="3367" />
  <row Id="3031" PostId="2444" Score="0" Text="I have a feeling that [this discretization of the Laplace-Beltrami operator](http://computergraphics.stackexchange.com/a/1721/3294) (&quot;cotan formula&quot;) may give the right weights to attribute to the edges in the mesh. When applied to (ambient space) coordinates of the surface itself, the operator yields the mean curvature normal, but in general when applied to some function defined over the surface, the operator yields the (generalized) Laplacian, or divergence of the function's gradient on that surface, I think." CreationDate="2016-05-16T18:02:46.193" UserId="3294" />
  <row Id="3033" PostId="2444" Score="0" Text="Once we have the Laplacian we can diffuse the initial function over the surface just like heat i.e. $\frac{\partial f}{\partial t}=-\Delta_S f$. Doing so with explicit Euler method may be tantamount to the local mixing we had in mind. Not sure what time-step to choose for good numerical properties. A large time-step means higher variance per iteration and therefore fewer iterations needed, but it would also mean we aren't staying true to the PDE, and I'm not sure at what step-size that begins to matter." CreationDate="2016-05-16T18:14:58.950" UserId="3294" />
  <row Id="3034" PostId="2441" Score="0" Text="@joojaa Thanks for helping me realize it's just diffusion ala $\frac{\partial f}{\partial t}=-\nabla_S f$ with the Laplace-Beltrami operator. I'm thinking of running explicit Euler using [this discretization of the operator](http://computergraphics.stackexchange.com/a/1721/3294). Not sure what time-step to use." CreationDate="2016-05-16T18:26:40.580" UserId="3294" />
  <row Id="3035" PostId="2451" Score="0" Text="Haha, when I posted the answer, SE asked me if I'm a human or a robot, the site wasn't sure :D I hope it is not because of the length of the answer, It got a little bit out of hand." CreationDate="2016-05-16T20:33:39.170" UserId="1613" />
  <row Id="3036" PostId="2451" Score="0" Text="you want to make my brain melt, don't you. ;-) BTW: I already managed to read two of the papers/presentations so I'll hopefully extend the question or write a superficial answer at the end of this week. And now, GoT FTW!" CreationDate="2016-05-16T20:39:10.357" UserId="2479" />
  <row Id="3037" PostId="2449" Score="0" Text="The math speak here is: 57 and 113 are coprimes.  GCD(57,133)=1.  And 57 is square free." CreationDate="2016-05-17T09:11:15.923" UserId="2831" />
  <row Id="3038" PostId="2436" Score="0" Text="Also in MathWorld article: Marsaglia (1972) - point in disk method should trump trig based." CreationDate="2016-05-17T09:18:09.863" UserId="2831" />
  <row Id="3039" PostId="2453" Score="0" Text="Exactly how do you &quot;know&quot; that? Reference maybe?" CreationDate="2016-05-17T10:16:01.747" UserId="3041" />
  <row Id="3040" PostId="2453" Score="0" Text="@Andreas the linked article states this, although doesn't provide a reference. However, it does provide evidence that this affects performance, unless that can be explained in terms of some other cause." CreationDate="2016-05-17T10:44:45.463" UserId="231" />
  <row Id="3041" PostId="2453" Score="0" Text="@trichoplax My bad. Did not read question properly." CreationDate="2016-05-17T10:47:51.343" UserId="3041" />
  <row Id="3042" PostId="2453" Score="0" Text="Also I saw several times that instead of fullscreen quad people use big triangle that covers the screen." CreationDate="2016-05-17T10:48:44.857" UserId="386" />
  <row Id="3044" PostId="2453" Score="0" Text="@trichoplax I saw this explanation several times. For example https://www.reddit.com/r/gamedev/comments/2j17wk/a_slightly_faster_bufferless_vertex_shader_trick/." CreationDate="2016-05-17T11:17:57.233" UserId="386" />
  <row Id="3045" PostId="2453" Score="1" Text="@nikitablack Oh I see - I thought you meant 2 triangles instead of a quad - but you mean 1 oversized triangle that encompasses the whole screen. That's interesting. I apologise - that is very much related to this question :)" CreationDate="2016-05-17T11:20:34.220" UserId="231" />
  <row Id="3046" PostId="2436" Score="0" Text="I would need to do some profiling, but I'm not entirely convinced that rejection sampling would beat trig." CreationDate="2016-05-17T11:56:21.650" UserId="310" />
  <row Id="3047" PostId="2454" Score="1" Text="Enabling GL_POLYGON_SMOOTH would cause multiple rasterization." CreationDate="2016-05-17T16:55:41.177" UserId="3041" />
  <row Id="3048" PostId="2456" Score="0" Text="A geometry shader may add more primitives than found in the original mesh. Does this do it for you?" CreationDate="2016-05-17T17:41:03.227" UserId="3041" />
  <row Id="3049" PostId="2450" Score="0" Text="I believe you need to define the &quot;edge&quot; more in detail. How does it differ from a simple wireframe? In cifz answer there is a good description of screen space post-process, but from your question it is difficult to determine if it is applicable." CreationDate="2016-05-17T19:07:58.727" UserId="3041" />
  <row Id="3050" PostId="2391" Score="0" Text="I have one question. Is the second picture generated only by sampling the EM? Or is it MISed version of sampling cosine and sampling EM? I really hope that it is the MISed version, because if so, then I might have a remedy for the high noise in the shadowy part." CreationDate="2016-05-17T20:42:02.350" UserId="1613" />
  <row Id="3051" PostId="2391" Score="0" Text="No @tom, it uses sperical EM sampling only, ignoring both the (Lambert) BRDF and the cosine factor. 64 samples were used and no image-space filtering applied, just averaging over pixel area. When MIS is applied to combine the EM sampling with the cosine sampling, the noise in the shadow decreases a lot, but slightly increases in the sunlit part." CreationDate="2016-05-17T22:17:39.087" UserId="2479" />
  <row Id="3052" PostId="2450" Score="0" Text="Well, with edge I mean the &quot;creases&quot; and &quot;ridges&quot; that form the solids. A wireframe would display all triangle faces, which is not what I want." CreationDate="2016-05-18T07:27:05.180" UserId="3377" />
  <row Id="3053" PostId="2452" Score="0" Text="Very nice explanation cifz, but what if no gradient is visible in a certain situation? For example, there is no light source at the back of the cube and therefore, no gradient is visible. Then this image based edge detection process you describe wouldn't work, am I right?" CreationDate="2016-05-18T07:31:15.297" UserId="3377" />
  <row Id="3054" PostId="2436" Score="0" Text="super nice answer! thanks a lot.. not giving back my opinion yet about it because I still have to really check and implement your answer.. I will get back to you soon.. but in the meantime a big thanks!" CreationDate="2016-05-18T09:28:37.203" UserId="3069" />
  <row Id="3055" PostId="2450" Score="0" Text="Ok, exactly what I asked for :-) I would generate a wireframe from those creases and ridges. The tricky part is still to determine what a crease/ridge is. Do you have any idea of how to do that?" CreationDate="2016-05-18T10:44:20.217" UserId="3041" />
  <row Id="3057" PostId="2460" Score="0" Text="How do area lights create sampling bias?" CreationDate="2016-05-18T14:32:42.377" UserId="3385" />
  <row Id="3058" PostId="2436" Score="0" Text="Everything is more than clear, thanks a lot for your awesome explanation!" CreationDate="2016-05-18T14:36:19.800" UserId="3069" />
  <row Id="3059" PostId="2436" Score="0" Text="If I may ask you, if you feel like of course, I have another question that might definitely solve my project.. Thanks in any case for your precious help so far:&#xA;http://computergraphics.stackexchange.com/questions/2461/resulting-probabilty-density-in-path-tracer-for-paths-using-next-event-estimatio" CreationDate="2016-05-18T14:38:39.343" UserId="3069" />
  <row Id="3060" PostId="2460" Score="0" Text="@Jake: In the same way as the rest: you have to cast more than one ray, so you need to decide what distribution to use." CreationDate="2016-05-18T19:34:14.567" UserId="196" />
  <row Id="3061" PostId="2460" Score="0" Text="I suppose I thought area lights where directional so that light was only emitted along the normal of the light surface. Is that not how area lights work?" CreationDate="2016-05-18T22:27:16.833" UserId="3385" />
  <row Id="3062" PostId="2460" Score="1" Text="@Jake No, area lights generally emit light into the entire hemisphere of directions at each point on their surface. You can see this if you imagine what a real-world area light looks like (such as a computer display showing solid white). You can see the emitted light from any direction." CreationDate="2016-05-18T23:24:53.520" UserId="48" />
  <row Id="3063" PostId="2460" Score="3" Text="I'd add that [distribution ray tracing](https://en.wikipedia.org/wiki/Distributed_ray_tracing) is sort of a middle ground between recursive ray tracing and path tracing. It allows one to _somewhat_ have things like glossy reflection or depth of field in the recursive framework. The limitation is it's easy to run into problems with the branching factor: imagine if every ray that hits a surface spawns 10-100 secondary rays; the ray count would grow exponentially with each bounce! So path tracing is much more efficient if you want to do more than a very limited amount of stochastic phenomena." CreationDate="2016-05-18T23:42:16.707" UserId="48" />
  <row Id="3064" PostId="2460" Score="0" Text="Yea the laptop example really clears things up. Of course it's hemispherical from each point now that you put it that way. Thanks so much!" CreationDate="2016-05-19T00:07:55.837" UserId="3385" />
  <row Id="3065" PostId="2452" Score="0" Text="If you use  a deferred renderer you can do the same but on the normal buffer or again, if you have a prepass you could apply the algorithm to the depth. &#xA;Still you are right, a screen space approach might be not ideal in all cases :) What solution is viable depends a lot on what is your budget for this effect, how complex are your scene." CreationDate="2016-05-19T07:24:06.327" UserId="100" />
  <row Id="3066" PostId="2452" Score="0" Text="Potentially if this is really central for your game, a very easy, but potentially taxing, would be computing a gradient on your neighboring normals for each vertex (offline at load time) and pass this factor as additional vertex attribute." CreationDate="2016-05-19T07:40:19.280" UserId="100" />
  <row Id="3067" PostId="2462" Score="1" Text="The standard approach is to use a transformation $T$ that does the conversion form space to space. In this case Homogeneous coordinates could be used. The process is pretty well described [here](http://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/building-basic-perspective-projection-matrix). Feel free to write a proper answer so that the question wont hang here forever." CreationDate="2016-05-19T08:12:02.633" UserId="38" />
  <row Id="3068" PostId="2452" Score="0" Text="Thangs again cifz for your help.&#xA;Well, what I want to achieve is to develop a CAD/CAM solution that looks similar to this: https://www.youtube.com/watch?v=-qTJZtYUDB4&#xA;And I really would like to know how they managed it to render all the black edges, creases and ridges.&#xA;cifz, do you think as a graphics programming specialis that they achieved this look by using one of your screen space approaches? Or maybe by computing a gradient on neighboring normals? I really want to know how this can be done. Thanks again!" CreationDate="2016-05-19T09:01:21.427" UserId="3377" />
  <row Id="3069" PostId="2430" Score="0" Text="Thanks for this Joojaa.  It does sound like do my work for me doesn't it? But it was genuinely just as idle thought on the train.  You'll given me some cool clues to get started with (I've never fiddled with image manipulation like this before (which is why it sounds a bit 'DMWFM' - I didn't have a starting point.  Now, thanks to you, I do)).  I'll probably fiddle with this in idle moment - and I may be back with further questions (and my own source code!) later.  Thanks again!" CreationDate="2016-05-19T09:18:53.697" UserId="3319" />
  <row Id="3070" PostId="2452" Score="0" Text="As a premise, I have didn't look that closely to the video as I am at work and super busy :) But judging from the cylindrical green shape at the beginning of the video, they are not using a screen space approach on the final image, as you don't see edges highlighted between that green shape and the white background. Not sure if that is a problem for you, as a first try I'd try a screen space edge detection combining information from depth and normals that should be quite fast to prototype, and if it is failing you go for the offline version or keep experimenting in SS :)" CreationDate="2016-05-19T09:48:16.243" UserId="100" />
  <row Id="3071" PostId="2452" Score="0" Text="Also, update your question to a reference image of what you are looking for :) Maybe it will attract someone with a precise answer. i'll try and think about it and have a better look at the video if I ever stop being so busy :(" CreationDate="2016-05-19T09:59:06.440" UserId="100" />
  <row Id="3072" PostId="2452" Score="0" Text="Thank's very much cifz for your hints. The thing is that I'm a total noob regarding OpenG, GLSL and Computer Graphics in general and don't know how to start. I know you haven't that much time but I would be very thankful if you could help me starting programming the shaders. Of course I would gladly pay you for your efforts." CreationDate="2016-05-19T15:16:49.877" UserId="3377" />
  <row Id="3074" PostId="2463" Score="0" Text="Thanks! I have been doing what you've said at the beginning, using geometry shaders to create geometry around the points and so enable to draw pixels around the point. It works, of course, but its a more cumbersome solution. I got particularly interested in the last bits of what you've said: if I understood you correctly, what you mention is one could render the whole scene and then use a screen shader aimed at the area of interest and then do a pass over the rendered frame in order to work on how many pixels one wants around the points in the center?" CreationDate="2016-05-20T00:06:26.130" UserId="3383" />
  <row Id="3075" PostId="60" Score="0" Text="I seriously doubt the ROP adds any performance overhead if blending is disabled." CreationDate="2016-05-20T03:33:03.143" UserId="3386" />
  <row Id="3076" PostId="60" Score="0" Text="@GroverManheim Depends on the architecture! The output merger/ROP step also has to deal with ordering guarantees even if blending is disabled. With a full-screen triangle there aren't any actual ordering hazards, but the hardware may not know that. There might be special fast paths in hardware, but knowing for certain that you qualify for them…" CreationDate="2016-05-20T06:05:40.787" UserId="196" />
  <row Id="3077" PostId="2465" Score="0" Text="There are also cases where a read-only buffer will do better than storing 4kb of read-only data in shared local memory. For example, storing it in local memory may mean that there is a unique copy of your data for every thread group. If the buffer fits in cache, it's quite possible that the cache performs better than local memory for read-only access patterns." CreationDate="2016-05-20T06:17:13.907" UserId="196" />
  <row Id="3078" PostId="2463" Score="0" Text="You could render the first pass in a texture. Then in the second pass on full screen, in each pixel you could inspect the texture content in a disk around the pixel and do what you want (e.g. color according to the closest filled pixel, or paint white only if distance is below some threshold). Of course for just drawing disks, this is far from being the most efficient way ;-)" CreationDate="2016-05-20T07:12:31.913" UserId="1810" />
  <row Id="3079" PostId="2463" Score="0" Text="Sure, I just gave the simples example I could think of. But yes, I think what you say is in line with what I've been thinking, i.e. rendering to a texture in one pass and then later we have the information for, in other passes, alter pixels in relation to what lays in surrounding pixels. Many thanks." CreationDate="2016-05-20T09:04:10.657" UserId="3383" />
  <row Id="3080" PostId="2452" Score="1" Text="Don't need to pay anyone, just start reading few tutorials online, buy some books and study hard :) It will be very rewarding at the end!" CreationDate="2016-05-20T13:41:32.590" UserId="100" />
  <row Id="3081" PostId="2450" Score="0" Text="The cad programs do not do this with a shader mostly. Instead they know the hard edges from the model and draw a line on top of the mesh form that info." CreationDate="2016-05-20T14:51:40.600" UserId="38" />
  <row Id="3082" PostId="2469" Score="2" Text="Additionally, the code for `vSize()` does not return very precise results as the square root of an integer is not necessarily also an integer, but the method returns an `int`. Actually, in most cases the given method will return imprecise results." CreationDate="2016-05-20T15:36:14.207" UserId="127" />
  <row Id="3083" PostId="2469" Score="0" Text="It might be used as a hash of some kind. Or maybe they are using a integer quantized field." CreationDate="2016-05-20T20:22:29.413" UserId="38" />
  <row Id="3084" PostId="2469" Score="0" Text="Basically the integer is 64-bit. Will it make any difference in that case ?" CreationDate="2016-05-21T09:56:39.100" UserId="2712" />
  <row Id="3085" PostId="2469" Score="0" Text="64 bit integers allow for a [larger maximum number](https://en.wikipedia.org/wiki/9223372036854775807) than 32 bit integers, but they can still only take whole number values. You could ask why integers are used as a separate question, but that would require access to more of the code and some context on what it is used for. Without that context it is difficult to judge whether this would be on topic for Computer Graphics." CreationDate="2016-05-21T12:48:31.720" UserId="231" />
  <row Id="3086" PostId="2471" Score="0" Text="There are several different meanings for &quot;OSG&quot;. Please could you specify which one this question refers to?" CreationDate="2016-05-21T13:04:33.680" UserId="231" />
  <row Id="3087" PostId="2471" Score="0" Text="I dont think there is a good equation for a prism as such." CreationDate="2016-05-21T13:54:29.767" UserId="38" />
  <row Id="3088" PostId="2474" Score="1" Text="Thank you for your help, successfully created the prism. :)&#xA;&#xA;I think, set the 8 points manually and draw with triangles is easier solution in this case. I believed it could be solved with a similar parametric equation, like cylinder.&#xA;&#xA;Like you said, first create a cube, and scale it to the proper size solved the problem." CreationDate="2016-05-21T16:19:53.773" UserId="3408" />
  <row Id="3089" PostId="2479" Score="0" Text="Many thanks for your answer! So, in the meanwhile I was able to come with the exact same solution for the cross-shape problem, but in a messier way. So, I am glad I was not thinking it wrong, but happy to see a cleaner solution. Thanks! About the performance, it's currently terrible: ~14ms now, which is of course unacceptable for real time simulations. I got interested in both your suggestions. Would care to elaborate a bit more? (just a PS: I am trying to implement the intensity of halos of point lights depending on the aggregation of how many point lights are close to each other)." CreationDate="2016-05-22T01:45:46.643" UserId="3410" />
  <row Id="3090" PostId="2479" Score="0" Text="By down-sampling (with which I would be totally fine and would be even welcome) you mean something like this http://stackoverflow.com/questions/14366672/how-can-i-improve-this-webgl-glsl-image-downsampling-shader or something simpler? And I the spatial hierarchy was what I though in the first place, but I though there wasn't a way to save an array of arrays from one pass to the next that would hold the quad-tree. Isn't that the case? I don't see how I would populate the quad-tree in the first pass but then store the info for the second pass (unless using a compute shader)." CreationDate="2016-05-22T01:47:52.310" UserId="3410" />
  <row Id="3091" PostId="2472" Score="0" Text="What do you mean by &quot;random sampling&quot;? Does it mean &quot;uniform random sampling&quot; or &quot;cosine-weighted random sampling&quot;? What does `createRandomReflect` do? How do you compute the PDF of the generated sample? How does your Monte Carlo estimator look like?" CreationDate="2016-05-22T13:05:05.427" UserId="2479" />
  <row Id="3093" PostId="2472" Score="0" Text="Ohh... Sorry,&#xA;&quot;random sampling&quot; means &quot;uniform random sampling&quot;. createRandomReflect creates sample on a hemisphere for uniform random sampling. As for the PDF, I did some integral(on paper) and google/wolfram|alpha search. I'm using a basic Monte Carlo estimator which could be found in any basic Path Tracer(eg. smallpt/smallpaint). It's a loop that throws samples at the scene and averages whatever I get back from the samples." CreationDate="2016-05-22T13:20:25.433" UserId="2448" />
  <row Id="3094" PostId="2472" Score="0" Text="Of course, but in a naive path tracer you usually use a simple Monte Carlo estimation, which consists of picking the direction, evaluating the BRDF for that direction and dividing the result by probability density of generating the direction. You showed us the sampling and evaluation of the BRDF, but we cannot see how you compute the PDF and how you use the generated sample later on. We need more code to understand your implementation." CreationDate="2016-05-22T15:49:06.213" UserId="2479" />
  <row Id="3095" PostId="2479" Score="0" Text="@GiovannS For this I think you would start with a simple 2x box filter downsample. It can be done by just copying a texture into a 2x smaller render target, using bilinear filtering; that will average together 2x2 pixels in the source to 1 pixel in the destination. For the spatial hierarchy approach, if the points are generated on the GPU then you probably want to build the data structure on the GPU. It's going to be a more complicated endeavor with append buffers, compute shaders, etc. If you google &quot;build quadtree on GPU&quot; or similar, there's a number of articles and papers on it." CreationDate="2016-05-22T21:02:41.577" UserId="48" />
  <row Id="3096" PostId="2481" Score="0" Text="WOW! That last link hit the spot! Things are looking great.  However the code they have makes no sense to me as I am a glsl programmer.  Thankfully I found two pass code somewhere." CreationDate="2016-05-23T06:44:48.920" UserId="2308" />
  <row Id="3097" PostId="2481" Score="0" Text="My confusion is the following, how exactly do you increase/decrease the size of the blur? Increasing how many texels away each sample is, or increasing the amount of texels sampled? And have you seen any good code for this working inside of a for loop based on glow radius?" CreationDate="2016-05-23T06:46:00.707" UserId="2308" />
  <row Id="3098" PostId="2481" Score="1" Text="_&quot;I would think rendering to a downsampled FBO would be much faster than generating mipmaps. glGenerateMipmap will generate the complete mipmap pyramid, which seems wasteful:&quot;_&#xA;Surely that might depend on whether there is custom hardware or not? Also, given that the total MIP map chain is a 33% overhead on the original image, but the 1st MIP map level is 25% of that, the storage for the remainder is only an additional 8%." CreationDate="2016-05-23T12:13:15.337" UserId="209" />
  <row Id="3099" PostId="2483" Score="1" Text="This is interesting but I'm confused by one point. Could you clarify what it means to &quot;divide the result for that direction by probability of picking the direction&quot;? If it is not a binary choice but a direction chosen from a continuous distribution, won't the probability be zero?" CreationDate="2016-05-23T15:00:26.530" UserId="231" />
  <row Id="3100" PostId="2483" Score="0" Text="Thank you! This helps a lot. The method you describe will work for ideal Fresnel surfaces. And I just discovered Walter et. al. [2007] and Burley [2015], which can be used for rough BSDFs." CreationDate="2016-05-23T15:10:04.173" UserId="310" />
  <row Id="3101" PostId="2483" Score="1" Text="@trichoplax: Yes it would, but in that paragraph I was describing the sampling technique just for a (dielectric) Fresnel BSDF - ideally smooth surface, which is a sum of two Dirac delta functions. In such case you are picking one of the directions with some discrete probability. In case of a non-delta (finite) BSDF, you generate directions according to a probability density function. Unfortunately, delta and non-delta cases have to be handled separately, which makes the code a little messy. More details on sampling microfacet BSDFs can be found, for example in the Walter et. al. [2007] paper." CreationDate="2016-05-23T16:37:09.823" UserId="2479" />
  <row Id="3102" PostId="2483" Score="2" Text="@RichieSams: Walter et. al. [2007] is basically still the state-of-the art for dielectric rough surfaces, but to make it work well you need a good sampling which was published just recently by Heitz and D'Eon the 2014 paper &quot;Importance Sampling Microfacet-Based BSDFs using the&#xA;Distribution of Visible Normals&quot;. And note that it is a single-scattering model which neglects inter-reflections between microfacets making it visibly dark for higher roughness values. See my question &quot;Compensation for energy loss in single-scattering microfacet BSDF models&quot; for more details." CreationDate="2016-05-23T16:53:04.740" UserId="2479" />
  <row Id="3103" PostId="2483" Score="0" Text="Thank you that makes perfect sense now." CreationDate="2016-05-23T18:13:07.927" UserId="231" />
  <row Id="3104" PostId="2483" Score="3" Text="Just wanted to point out that if you choose probability = fresnel() as the question suggested, then when you divide by the probability, you cancel out the Fresnel factor that would normally be multiplied in. So (in the discrete, two-Dirac case) you end up with the ray contribution not including any Fresnel factor at all. It's standard importance-sampling theory, but I thought I'd point that out as a potentially confusing issue." CreationDate="2016-05-23T18:22:23.363" UserId="48" />
  <row Id="3105" PostId="2485" Score="0" Text="As an example two edge sharing triangles may perfectly splitting the same pixel in two but an OpenGL implementation shall guarantee only one of the triangles cover the pixel center. Which one is implementation dependent." CreationDate="2016-05-23T18:27:22.187" UserId="3041" />
  <row Id="3106" PostId="2483" Score="0" Text="@Nathan, that's actually a very good point. I'm glad you mentioned it." CreationDate="2016-05-23T18:32:49.610" UserId="2479" />
  <row Id="3107" PostId="2212" Score="0" Text="@FabrizioDuroni, does it mean, that adjusting the power of your light source with the brightness parameter doesn't make the resulting picture bright enough? I think I am missing something and I'm afraid you are adding to much alchemy into it ;-) Anyway, I'm glad it helped at least partially." CreationDate="2016-05-23T20:04:32.153" UserId="2479" />
  <row Id="3108" PostId="2481" Score="0" Text="Good point, I spoke too broadly there.  It is possible that generating the chain is faster on some systems." CreationDate="2016-05-24T03:43:14.057" UserId="3412" />
  <row Id="3109" PostId="2481" Score="0" Text="To increase the size of the blur you increase the amount of pixels sampled.  See here for a 7x7 blur: https://software.intel.com/en-us/blogs/2014/07/15/an-investigation-of-fast-real-time-gpu-based-image-blur-algorithms" CreationDate="2016-05-24T03:48:38.897" UserId="3412" />
  <row Id="3110" PostId="2487" Score="0" Text="Oh awesome, I guess I should have dug a little deeper on the corporation websites.  I assumed that since apitrace didn't offer these details, there was some sort of reversing going on.  Thanks!" CreationDate="2016-05-24T06:06:11.660" UserId="3412" />
  <row Id="3111" PostId="2483" Score="2" Text="@Nathan, I incorporated your notice into the answer." CreationDate="2016-05-24T11:27:33.883" UserId="2479" />
  <row Id="3112" PostId="2490" Score="0" Text="That first case sounds more like the  non-zero rule (https://en.wikipedia.org/wiki/Nonzero-rule) (except using  the normal rather than 2D edge direction)" CreationDate="2016-05-24T16:40:01.747" UserId="209" />
  <row Id="3114" PostId="2212" Score="0" Text="I was missing something too, and in fact i remove that strange &quot;alchemy&quot; that i though was the right thing to do (web is not always a good source :D) Just a brightness multipler is good. This parameter could be substituted with more physically correct values, like the light power (using a Rienmann sum to calculate the integrate the light SPD ) and its area. But for my thesis has been decided that the brightness parameter is enough." CreationDate="2016-05-24T20:06:11.123" UserId="2237" />
  <row Id="3115" PostId="2212" Score="0" Text="This is an example of a final rendered image that will be included in the thesis https://drive.google.com/open?id=0BxeVnHLvT8-7TVBZZE44VTRuT1k Keep watching my github repo for the final result ;)" CreationDate="2016-05-24T20:06:16.093" UserId="2237" />
  <row Id="3116" PostId="2212" Score="0" Text="@fabrizio, but that is almost exactly what I suggested you to do. ;-) And yes, having a more physically meaningful parameter (e.g. power) in a physically-based renderer would be definitely a nicer thing than some unitless brightness parameter. Especially when you have a spectral renderer (it is not clear how to do it in a colour-space renderer). The image looks pretty good to me. Good luck with the thesis!" CreationDate="2016-05-24T20:39:39.613" UserId="2479" />
  <row Id="3118" PostId="2493" Score="0" Text="Requests for off site resources are off topic, but by removing the final sentence this is now an on topic question. Feel free to override my edit if I've changed your intent too much." CreationDate="2016-05-25T10:26:30.260" UserId="231" />
  <row Id="3119" PostId="2498" Score="0" Text="I don't understand, can you be more precise ?" CreationDate="2016-05-25T16:28:13.513" UserId="3339" />
  <row Id="3120" PostId="2499" Score="0" Text="Does that mean that the geometry term cancels the cosine term ?" CreationDate="2016-05-25T17:35:20.943" UserId="3339" />
  <row Id="3121" PostId="2499" Score="0" Text="No. The geometry term is a normalization factor that accounts for the fact that the microfacets will shadow and mask each other. The 'visibility' term I mention in answer is the integral itself. The integral adds up all *visible* incoming light." CreationDate="2016-05-25T17:45:59.950" UserId="310" />
  <row Id="3122" PostId="2499" Score="0" Text="However, in the book Real Time Rendering I read that for the Blinn-Phong model, the geometry term is (n.v)(n.l) which cancels the denominator." CreationDate="2016-05-25T20:06:31.837" UserId="3339" />
  <row Id="3123" PostId="2500" Score="0" Text="&quot;As you can see this causes an issue because of the segments.&quot;. Could you describe what problem you are seeing? What about the picture should be different?" CreationDate="2016-05-25T21:28:56.693" UserId="231" />
  <row Id="3124" PostId="2500" Score="0" Text="@Trichoplax the line has clear segments inside of it. The line should be smooth." CreationDate="2016-05-25T21:30:22.467" UserId="2308" />
  <row Id="3125" PostId="2502" Score="0" Text="What does n mean? Why 0.95?" CreationDate="2016-05-25T21:31:44.270" UserId="2308" />
  <row Id="3126" PostId="2500" Score="0" Text="Do you mean that there should be no visible colour difference between the segments?" CreationDate="2016-05-25T21:50:14.087" UserId="231" />
  <row Id="3127" PostId="2499" Score="1" Text="@Livetrack: The part (n.v)(n.l) is not usually called &quot;geometry factor&quot;, that is the G in the formula. The only part which gets cancelled when this BRDF is placed into the rendering equation is (n.l)." CreationDate="2016-05-25T21:54:58.507" UserId="2479" />
  <row Id="3128" PostId="2500" Score="0" Text="@trichoplax I just want the color to be smooth. I want the new segment to math nicely with the previous faded one.  Just so that everything looks continuous in both shape and color." CreationDate="2016-05-25T22:22:25.927" UserId="2308" />
  <row Id="3129" PostId="2504" Score="1" Text="Related: [MSI GTX 950 2GD5T triangles per second](http://computergraphics.stackexchange.com/questions/1845/msi-gtx-950-2gd5t-triangles-per-second)" CreationDate="2016-05-25T22:40:12.580" UserId="231" />
  <row Id="3130" PostId="2495" Score="0" Text="Oh I had totally missed that this was already a result in the paper. That certainly clears it. :) I'll have to reread it to get a better grasp of how it fits within the BRDF though." CreationDate="2016-05-26T09:00:09.833" UserId="182" />
  <row Id="3131" PostId="2503" Score="0" Text="could you apply your thinking to my example as stated above? I did not get the grasp of your method." CreationDate="2016-05-26T09:00:55.963" UserId="2214" />
  <row Id="3133" PostId="2507" Score="0" Text="The second method is Phong shading model ? And could you elaborate more on how you conclude that in Gouraud Shading we cannot have brighter points and in the second method we can have brighter points ?" CreationDate="2016-05-26T09:17:50.953" UserId="2214" />
  <row Id="3134" PostId="2507" Score="0" Text="@johnjohn gouraud shading is taking a weighted average of the colors of the vertices. While with per pixel you can have a point light near the middle of a large surface. The vertices won't get much light but the middle should have a lot of light." CreationDate="2016-05-26T10:08:00.483" UserId="137" />
  <row Id="3135" PostId="2509" Score="0" Text="Thanks. And how big are these groups?" CreationDate="2016-05-26T12:10:58.630" UserId="386" />
  <row Id="3136" PostId="2509" Score="1" Text="@nikitablack depends on the hardware and is kinda impossible to extract naively from any marketing information." CreationDate="2016-05-26T12:13:00.820" UserId="137" />
  <row Id="3137" PostId="2509" Score="0" Text="And also a question. Lets's say one thread in a group reads from some buffer in a branch based on that thread's id. Lets's say the buffer size is 1, so only the first thread can read, all other threads will cause out of bounds read. Is it safe? And one more thing - the read from the system memory is very slow, but all threads will read from it, even if they don't need, right?" CreationDate="2016-05-26T12:16:15.593" UserId="386" />
  <row Id="3138" PostId="2509" Score="1" Text="@nikitablack that's going to depend on a few things; 1) if the driver is smart enough to forward the constant in the test into the then clause, 2) if the hardware can do a &quot;masked read&quot;, 3) what exactly happens on out of bounds reads" CreationDate="2016-05-26T13:12:26.167" UserId="137" />
  <row Id="3139" PostId="2507" Score="0" Text="@johnjohn just to add to ratchet's comment, WRT Gouraud, just consider two points on a 2D graph where height represents brightness and connect them by a straight line segment. No point along the segment can be higher (i.e. brighter) than the higher end point.  If you consider the edges of your triangle it shows that no point on an edge can be brighter than the end vertices.  Now for any point inside the triangle just draw, say, a line through the point from edge to edge - the point inside can't be brighter.&#xA;&#xA;With per-pixel shading, however, you don't have linear interpolation." CreationDate="2016-05-26T13:53:55.863" UserId="209" />
  <row Id="3140" PostId="2503" Score="0" Text="If fact I'm not sure what you meanhere by &quot;interpolate&quot;. If what you aim at is a mesh, your formula directly give the *position* where the isoval cross an edge. Then you obtain the mesh by connecting these points." CreationDate="2016-05-26T15:50:39.080" UserId="1810" />
  <row Id="3141" PostId="2502" Score="0" Text="opacity 0.05 means transparency 0.95, i.e. the intensity scaling. n is the number of passes (i.e. number of times you apply a 0.05 opacity)." CreationDate="2016-05-26T15:52:19.933" UserId="1810" />
  <row Id="3142" PostId="2498" Score="0" Text="The other answer state the same differently in its last paragraph :-)" CreationDate="2016-05-26T15:54:02.783" UserId="1810" />
  <row Id="3143" PostId="2509" Score="2" Text="@nikitablack The lockstep group size is 32 for NVIDIA and 64 for AMD (they call them &quot;warps&quot; and &quot;wavefronts&quot; respectively). For Intel the size is variable from 8 to 32, determined by the shader compiler. Also, out-of-bounds reads are defined to just return zero in DirectX, and I'm pretty sure it's the same for GL and the other APIs." CreationDate="2016-05-26T17:13:09.697" UserId="48" />
  <row Id="3144" PostId="2492" Score="0" Text="The diagrams are nice, but you might want to label the points (p0, p1, p2) to make them easier to understand. :)" CreationDate="2016-05-26T17:16:53.060" UserId="48" />
  <row Id="3145" PostId="2492" Score="1" Text="There are just sketches @NathanReed I threw away the sources, but yes i can do that." CreationDate="2016-05-26T18:29:57.653" UserId="38" />
  <row Id="3156" PostId="2509" Score="1" Text="For the reason Nathan just described, you should always make your work group size a multiple of 64 to avoid wasting shader invocations in the last warp / wavefront of each group. If you don't need to use shared variables, exactly 64 is usually a good size." CreationDate="2016-05-27T07:57:30.677" UserId="1937" />
  <row Id="3157" PostId="2503" Score="0" Text="What I do not understand, is where you put what in the formula. I know that in V1 and in V2 I should replace the values from the vertices but in P1 and P2 I have no clue what I should put there? Can you give me a hint how it works in the current example?" CreationDate="2016-05-27T08:08:52.733" UserId="2214" />
  <row Id="3158" PostId="2512" Score="1" Text="I do not understand what you mean though in the last paragraph. Could you elaborate more on that ? For example, what do you mean by 'negative' and 'positive' end ?" CreationDate="2016-05-27T09:02:37.070" UserId="2214" />
  <row Id="3159" PostId="2510" Score="1" Text="It looks to me that it's just finding runs of monotonic edges. Given a defined winding order (in this case anticlockwise), then you can identify P6 through P0 as a decreasing run, as is P2 through P4.  Since the *left most* vertex, P8, is on a decreasing run, the *decreasing* runs define left boundaries (and therefore increasing runs, right boundaries)" CreationDate="2016-05-27T09:59:49.447" UserId="209" />
  <row Id="3160" PostId="2503" Score="0" Text="P1 and P2 are the coordinates of the segment extremities." CreationDate="2016-05-27T11:18:50.863" UserId="1810" />
  <row Id="3161" PostId="2507" Score="0" Text="@SimonF So, to recap there are three methods of finding the brightness:&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle.&#xA;2) Interpolate normals then calculate the dot product with light vector for each point.&#xA;3) Interpolate gradient vectors then calculate the dot product with light vector.&#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-27T18:50:58.750" UserId="2214" />
  <row Id="3162" PostId="2514" Score="0" Text="what if we assume a directional&#xA;light, whose direction does not depend on the position on the surface. What happens then ?" CreationDate="2016-05-27T19:47:59.517" UserId="2214" />
  <row Id="3163" PostId="2514" Score="0" Text="@johnjohn same thing, the view direction is still affected by the surface." CreationDate="2016-05-27T20:35:11.993" UserId="38" />
  <row Id="3165" PostId="2498" Score="0" Text="I don't understand : Imagine that I have a single point source, then I can simplify the equation like this : $$ L = f(\omega_o, \omega_i) E(\omega_i) $$. Why don't I need to bother about the cosine term ?  PS : J'ai vu que vous êtes chercheur à l'Inria. Je ne veux pas trop vous déranger mais vous pourriez faire l'explication en français ? Merci." CreationDate="2016-05-27T21:02:36.193" UserId="3339" />
  <row Id="3166" PostId="2499" Score="0" Text="I don't understand how the integral (which is about incoming lights) can cancel the cosine term." CreationDate="2016-05-27T21:05:33.693" UserId="3339" />
  <row Id="3167" PostId="2515" Score="0" Text="So there are three methods which you can find the brightness .&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle. 2) Interpolate normals then calculate the dot product with light vector for each point. 3) Interpolate gradient vectors then calculate the dot product with light vector. &#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-28T08:00:48.460" UserId="2214" />
  <row Id="3168" PostId="2515" Score="1" Text="@johnjohn 2 and 3. 1 is Gouraud shading that is hardly used anywhere anymore because it does not look correct." CreationDate="2016-05-28T08:03:34.127" UserId="38" />
  <row Id="3169" PostId="2515" Score="0" Text="So I assume from what you said that the **order** of interpolation (comes first in Gouraud Shading or comes second in the other methods) can lead to better shading results, only for the last two." CreationDate="2016-05-28T08:07:32.857" UserId="2214" />
  <row Id="3170" PostId="2515" Score="1" Text="@johnjohn Gouraud shading is a trick to compensate for the fact that hardware was not fast enough for the job. Nobody even back whan it was conceived thought it was any good. (well obviously if you use Gouraud on a mesh that has one triangle per pixel theres no difference) Since the lightning is heavily dependent on where the local normal points it means that any method that is locally per pixel calculated can have hot spots in other than corners." CreationDate="2016-05-28T08:11:42.950" UserId="38" />
  <row Id="3171" PostId="1721" Score="0" Text="What Meyer et al. 2003 did not (perhaps explicitly) mention was how to compute the curvature for border vertices. Since they have taken an angle deficit approach, the Gaussian curvature for border vertices should read $K = (\pi - \sum_j{\theta_j})/A_{mixed}$." CreationDate="2016-05-28T09:33:15.520" UserId="3444" />
  <row Id="3173" PostId="2510" Score="1" Text="Not sure if it will help you but [Clipper](http://www.angusj.com/delphi/clipper.php) uses Vatti's algorithm. The [docs](http://www.angusj.com/delphi/clipper/documentation/Docs/Overview/_Body.htm) mention: &quot;A section in 'Computer graphics and geometric modeling: implementation and algorithms' by By Max K. Agoston (Springer, 2005) discussing Vatti Polygon Clipping was also helpful in creating the initial Clipper implementation.&quot;" CreationDate="2016-05-28T13:56:46.263" UserId="141" />
  <row Id="3175" PostId="2516" Score="0" Text="Does orthogonal in this context mean using only vertical and horizontal edges? Are you looking for a way to represent more than 4 edges meeting at a single vertex?" CreationDate="2016-05-28T14:49:28.780" UserId="231" />
  <row Id="3178" PostId="2522" Score="0" Text="So what should be the correct implementation ?. If i am not wrong i used the variable as defined in this video on the algorithim.  https://www.youtube.com/watch?v=TRbwu17oAYY&#xA;&#xA;Any idea how to implement a correct Bresenham's line drawing algorithm for slope &lt; 1." CreationDate="2016-05-28T22:50:00.063" UserId="3437" />
  <row Id="3179" PostId="2522" Score="0" Text="Although the algorithm is often stated using fractions, it can be implemented in integers only, by using the horizontal and vertical measures of the line to keep track of when to move horizontally and when to move diagonally." CreationDate="2016-05-29T02:06:55.973" UserId="231" />
  <row Id="3180" PostId="2524" Score="1" Text="Thanks that was on spot. also got the reason why it was wrong. excellent answer with great explanation." CreationDate="2016-05-29T05:18:06.737" UserId="3437" />
  <row Id="3181" PostId="2522" Score="1" Text="Interesting, I'll delete the lastparagraph. Nice explanation BTW." CreationDate="2016-05-29T07:12:11.497" UserId="2479" />
  <row Id="3183" PostId="2519" Score="0" Text="Thank you for your reply. I've implemented the formula you've supplied and I got identical results as with my own (when using the macrosurface normal). So it seems it's just a different form (I got it from: http://graphicrants.blogspot.nl/2013/08/specular-brdf-reference.html)&#xA;&#xA;I was confused about the half vector because the SIGGRAPH 2015 PBS math course specifically state the geometry function dependant on the view, light and half vectors. So this is an error in the slides?" CreationDate="2016-05-29T08:26:00.767" UserId="3424" />
  <row Id="3184" PostId="2519" Score="0" Text="@Erwin, now that you provided also the formula itself, it is much clearer. Next time do it right at the beginning, it helps. Yes, both version (mine and yours) are equivalent, but neither of them uses halfway vector for computing sine or tangent function. It uses $n\cdot v$ rather than $h\cdot v$ as you did in your implementation - that seems to be the mistake. I suspect you did the same mistake with the new implementation as well." CreationDate="2016-05-29T10:01:42.137" UserId="2479" />
  <row Id="3185" PostId="2526" Score="0" Text="There are a lot of code snippets on the linked page, but none of them appear to have the character &quot;`#`&quot;. Could you show a minimal example of code that you know gives this error message?" CreationDate="2016-05-29T14:26:59.780" UserId="231" />
  <row Id="3186" PostId="2526" Score="0" Text="@trichoplax my bad. I should have explained that there is an executable provided in order for you to run it. I will update the question with a minimal example." CreationDate="2016-05-29T14:50:16.080" UserId="3452" />
  <row Id="3188" PostId="2527" Score="1" Text="Yet as you can see by yourself Chapman's unedited algorithm is given in this exact form, and I do guarantee that it could run perfectly on this but also another computer (both having ATI graphics cards). I think it could have something to do with Vulkan and a more strict set of rules, but I have not been able to verify it yet." CreationDate="2016-05-29T20:56:59.567" UserId="3452" />
  <row Id="3189" PostId="2527" Score="0" Text="@green_leaf oh yes I'm not doubting that it did work - I just don't have an explanation for why. I guess the change was a &quot;correction&quot; even though it hasn't turned out to be helpful for you..." CreationDate="2016-05-29T22:10:04.893" UserId="231" />
  <row Id="3192" PostId="2527" Score="0" Text="that's very classic with ATI" CreationDate="2016-05-30T01:04:12.653" UserId="1614" />
  <row Id="3194" PostId="2487" Score="0" Text="oh I thought he wanted to write another perfkit. which the answer would be that it's not possible unless you make your own hardware." CreationDate="2016-05-30T01:30:37.823" UserId="1614" />
  <row Id="3196" PostId="2482" Score="0" Text="russian roulette" CreationDate="2016-05-30T01:46:15.387" UserId="1614" />
  <row Id="3197" PostId="2483" Score="0" Text="I got it to work! http://i.imgur.com/m1fYRdr.png" CreationDate="2016-05-30T02:09:01.033" UserId="310" />
  <row Id="3200" PostId="2512" Score="0" Text="Say your cube is at position 0 on each axis in your volume, then it will fill the space from position 0 to position 1. So the left side of your cube will be at X=0 and the right side will be at X=1, same for the other two axes. So you use P1 and V1 for the edge end with the lower coordinate value and P2 and V2 for the end with the higher value." CreationDate="2016-05-30T07:30:24.077" UserId="1937" />
  <row Id="3201" PostId="2518" Score="0" Text="The 'rotation by area mapping' algorithm described at http://www.leptonica.com/rotation.html looks like a good place to start" CreationDate="2016-05-30T07:39:46.800" UserId="1937" />
  <row Id="3202" PostId="2465" Score="0" Text="Thanks for the feedback guys. I've finished the project I was using this for now, and wound up just using a r8ui readonly buffer texture, which worked pretty nicely :)" CreationDate="2016-05-30T08:01:59.140" UserId="1937" />
  <row Id="3205" PostId="2483" Score="0" Text="@RichieSams: Cool, but does your glass attenuate light? If it doesn't, the object should not be visible under constant illumination I'm afraid." CreationDate="2016-05-30T12:34:54.060" UserId="2479" />
  <row Id="3206" PostId="2519" Score="0" Text="I did use N dot V in my new implementation, that gave me identical results to the second image I've posted. But I'm still not clear on why the PBS course slides state that the halfway vector should be used (See: http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf, Slide 88)." CreationDate="2016-05-30T15:24:04.623" UserId="3424" />
  <row Id="3207" PostId="2483" Score="0" Text="Currently, the BTDF just returns the surface Albedo, (which is float3(0.95) in this render). If I set it to float3(1.0), yes, it will disappear. Implementing Beer-Lambert is my next task." CreationDate="2016-05-30T15:50:47.180" UserId="310" />
  <row Id="3209" PostId="2423" Score="0" Text="@KristofferHelander :  Converting from 14 bit capture to a 16 bit representation of the 0-1 range is easily achieved by multiplication. But most of our textures are painted, not photographed -- sometimes they are painted directly in a 16 bit format, sometimes they are painted in sRGB and then converted to 16 bit when &quot;linearized&quot; for use as a texture. There's no need for HDR for albedo textures." CreationDate="2016-05-30T17:31:00.883" UserId="1781" />
  <row Id="3211" PostId="2423" Score="0" Text="@KristofferHelander : For albedo textures, we tend to use TIFF with 16 bit integer data (what we call .tx is just TIFF format but tiled and with MIP-map multiresolution stored as multiple subimages within the TIFF file). For true HDR data, like environment captures, we use OpenEXR. Renderer output also tends to be OpenEXR." CreationDate="2016-05-30T17:34:53.267" UserId="1781" />
  <row Id="3213" PostId="2531" Score="0" Text="What i understand is that you want to make User interface that will not be used in a game. right ?" CreationDate="2016-05-30T17:43:53.450" UserId="3437" />
  <row Id="3214" PostId="2527" Score="0" Text="@v.oddou indeed now that you mention it. It seems that they do that quite often." CreationDate="2016-05-30T18:19:32.347" UserId="3452" />
  <row Id="3215" PostId="2531" Score="0" Text="Yes, My current goal is to create my own pomodoro time management application, so I want to create a simple User Interface that can be used for that purpose." CreationDate="2016-05-30T18:57:16.693" UserId="3459" />
  <row Id="3216" PostId="2531" Score="0" Text="Check the answer i posted just now" CreationDate="2016-05-30T19:55:42.357" UserId="3437" />
  <row Id="3217" PostId="2533" Score="0" Text="This is perfect, Thank you so much!" CreationDate="2016-05-30T20:57:03.377" UserId="3459" />
  <row Id="3219" PostId="2519" Score="0" Text="Do I understand it correctly that using $h\cdot v$ instead of $n\cdot v$ was THE problem? Regarding the use of halfway vector in $G_1$: In fact it is used in both of the versions I posted (I made a mistake when constructing the LaTeX formula and wrote geomeotric normal into the first one, I'll fix it soon), but the point is that the halfway vector is not used to compute the cosine value (i.e. there is no $h\cdot v$ used)." CreationDate="2016-05-30T21:28:30.040" UserId="2479" />
  <row Id="3220" PostId="2533" Score="0" Text="@ArdenRasmussen So what are you using to make UI ?" CreationDate="2016-05-30T21:49:57.927" UserId="3437" />
  <row Id="3221" PostId="2534" Score="1" Text="I am unsure what you are asking here. What you posted as quote from the book is just the definition of a line, your title sounds unrelated. Are you asking how to find point on a line of which you have an implicit form like the one you posted?" CreationDate="2016-05-30T21:59:39.150" UserId="100" />
  <row Id="3222" PostId="2534" Score="0" Text="The midpoint algorithm uses the implicit equation of the line and I am looking forward to some proofs of the three cases mentioned in my initial post." CreationDate="2016-05-30T23:51:27.607" UserId="2712" />
  <row Id="3223" PostId="2534" Score="0" Text="This question should be on Math StackExchange." CreationDate="2016-05-31T04:51:05.573" UserId="3437" />
  <row Id="3226" PostId="2538" Score="0" Text="Are you doing coding by deferring to stackexhange, perchance?" CreationDate="2016-05-31T12:38:14.947" UserId="38" />
  <row Id="3228" PostId="2516" Score="0" Text="@trichoplax Exactly, and I do appreciate any help." CreationDate="2016-05-31T12:52:09.830" UserId="537" />
  <row Id="3229" PostId="2538" Score="0" Text="I totally missed your last question ?" CreationDate="2016-05-31T12:52:12.900" UserId="2712" />
  <row Id="3231" PostId="2538" Score="0" Text="if `xMax` and `xMin` are `float`s, why is `xDist` an `int`?" CreationDate="2016-05-31T14:32:39.117" UserId="457" />
  <row Id="3232" PostId="2538" Score="0" Text="All the declared variable are in short int . The only floating point issue may arise with xDist/scanSpacingStep." CreationDate="2016-05-31T14:47:44.910" UserId="2712" />
  <row Id="3233" PostId="2540" Score="0" Text="No. Constant buffers are just a chunk of memory. The DX spec and hlsl spec define the different types." CreationDate="2016-05-31T23:04:07.060" UserId="310" />
  <row Id="3234" PostId="2542" Score="0" Text="Will you always have a fixed number of floats to store per cell? If so, would using one texture for each float work? For example, using 3 textures to store 9 floats per cell (3 per texture using the R, G, and B channels)." CreationDate="2016-06-01T00:12:22.283" UserId="231" />
  <row Id="3235" PostId="2538" Score="0" Text="I'm not quite sure what you are asking. Are xMax and xMin pixel locations? If so are both inclusive? If xMin=2 and xMax=4, does that cover 2 columns of pixels or 3?" CreationDate="2016-06-01T00:22:07.583" UserId="231" />
  <row Id="3236" PostId="2543" Score="0" Text="I love the code review part. I'd add that in addition to renaming `startHueInRadians` the poster should also change it from an array to a `struct`. None of hue, saturation, or brightness represent the same type of value, so you shouldn't treat them as such!" CreationDate="2016-06-01T05:05:30.140" UserId="3003" />
  <row Id="3237" PostId="2543" Score="0" Text="@user1118321 Sadly there is nothing like a c++ struct in java, i need to create a special class for it." CreationDate="2016-06-01T06:02:35.160" UserId="3437" />
  <row Id="3239" PostId="2538" Score="0" Text="yes , it can be associated with pixel locations. (xMin,yMin) and (xMax,yMax) are inclusive. If xMin = 2 and xMax = 4 , and if both of them are inclusive , it should cover three columns, is not it ?" CreationDate="2016-06-01T11:04:27.273" UserId="2712" />
  <row Id="3240" PostId="2544" Score="0" Text="Can show some pictures and code ?" CreationDate="2016-06-01T17:22:46.613" UserId="3437" />
  <row Id="3241" PostId="2544" Score="0" Text="@ritwiksinha Done!" CreationDate="2016-06-01T17:24:52.747" UserId="2308" />
  <row Id="3242" PostId="2544" Score="0" Text="Are you using $sine/cosine$ as the oscillating function ?" CreationDate="2016-06-01T17:30:14.887" UserId="3437" />
  <row Id="3243" PostId="2544" Score="0" Text="Yes technically. However as you can see by the value v I am going to map it differently." CreationDate="2016-06-01T17:37:01.890" UserId="2308" />
  <row Id="3244" PostId="2516" Score="2" Text="Is the graph guaranteed to be planar so that, forgetting the orthogonal edges for a moment, the graph can be drawn without edges overlapping?" CreationDate="2016-06-01T18:36:59.830" UserId="2500" />
  <row Id="3245" PostId="2542" Score="0" Text="@trichoplax Thanks for your comment! No, I won't have a fixed number of floats, unfortunately. But even if I could fix it for those purposes, that solution wouldn't works since I will have high number of floats - which would result in great memory overhead due to the high number of textures to be used in order to implement your suggestion" CreationDate="2016-06-01T20:00:51.870" UserId="3410" />
  <row Id="3246" PostId="2542" Score="0" Text="This is very relevant information - I'd recommend editing the question to include this." CreationDate="2016-06-01T21:19:55.183" UserId="231" />
  <row Id="3247" PostId="2544" Score="1" Text="If you solved your own problem, you can also post an answer for anyone who sees the same problem in future. This is [actively encouraged](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2016-06-01T21:28:31.180" UserId="231" />
  <row Id="3248" PostId="1978" Score="0" Text="Is it correct to think of this model-matrix as a model-to-world matrix? in case I want to transform a point to the coordinate system of the model, then will I have to use the inverse of the model-matrix?" CreationDate="2016-06-02T05:24:49.527" UserId="116" />
  <row Id="3249" PostId="1978" Score="0" Text="Regarding the camera-matrix (built with cam position, lookat point and up vector), can I call it a camera-to-world matrix? Then, if I want to go from world to camera space, will I have to use the inverse of that camera-matrix?" CreationDate="2016-06-02T05:27:06.040" UserId="116" />
  <row Id="3250" PostId="2549" Score="0" Text="Yes, you are completely right, the direction vector from the camera has nothing to do with the world z-axis, my mistake. What confused me, I think it is silly but still, is [this image](http://www.scratchapixel.com/images/upload/perspective-matrix/camera2.png?) where the +z-axis enters the lens and I think you mean the +z-axis exits the lens ... what are the implications of using one or the other? This is what I cannot seem to understand" CreationDate="2016-06-02T06:02:47.743" UserId="116" />
  <row Id="3251" PostId="2549" Score="0" Text="@BRabbit27 your image is interesting, it does mention the **default** word which is important. I'm curious as to why the cam is pointing to `-z`, maybe a row matrix vs column matrix thing again ?" CreationDate="2016-06-02T06:07:12.227" UserId="1614" />
  <row Id="3252" PostId="2549" Score="0" Text="@BRabbit27 oh right, we can think of it this way: an identity camera matrix is the same as no view matrix. therefore we end up with the default basis of the API. In OpenGL we definitely see towards `-z` by default. In DirectX historically I've used LeftHanded systems I guess which made me reverse z." CreationDate="2016-06-02T06:09:42.617" UserId="1614" />
  <row Id="3253" PostId="2549" Score="0" Text="In [Scratchapixel](http://www.scratchapixel.com/index.php) the authors use row-major matrix and right-hand system. So actually when you say &quot;camera looking along&quot; it means the direction the lens points to? Maybe a mistake in the image? I think, if they say &quot;looking along -z&quot;, the direction vector is as you just said `lookat - campos` ?" CreationDate="2016-06-02T06:14:16.007" UserId="116" />
  <row Id="3254" PostId="2549" Score="0" Text="@BRabbit27 Yes that what it means, I think there is no mistake in their image. That just happens to be configurable by a `-1` in the projection matrix later on. So any image would still be right." CreationDate="2016-06-02T06:16:24.417" UserId="1614" />
  <row Id="3255" PostId="2549" Score="0" Text="Ok, now I think I am getting somewhere. So if I choose to have the direction as `camPos-lookat` to agree with the image, that's why the projection matrix has the `-1` in the (2,3) entry to reverse this, right?" CreationDate="2016-06-02T06:18:28.343" UserId="116" />
  <row Id="3256" PostId="2549" Score="0" Text="The other thing I still don't get, is how can I convince myself that the view-matrix = camera-to-world? What kind of exercise can I do in order to visualize it?" CreationDate="2016-06-02T06:22:52.727" UserId="116" />
  <row Id="3257" PostId="2549" Score="1" Text="@BRabbit27 it's world-to-camera because &quot;the view matrix changes space from world points to view points&quot;. If a point is far away, but your camera is also far away (close to the point) passing the point into the view matrix will make it close to zero. that is: a point visible in your view space. Another way to see it, is use the fact &quot;view matrix = inverse of world matrix for the camera&quot;. The wold matrix for the camera would be the matrix to move it from zero in world, to its place in world (away from origin)." CreationDate="2016-06-02T06:28:26.767" UserId="1614" />
  <row Id="3258" PostId="2516" Score="1" Text="General graph layout algorithms are anoying to program and implement. Even just finding proper ones from literature is pain. You should describe a bit more what you expect, draw a picture." CreationDate="2016-06-02T07:57:35.683" UserId="38" />
  <row Id="3259" PostId="2551" Score="2" Text="&quot;fails to compile&quot; is kinda useless without the error messages, check the `shaderInfoLog` and/or the `programInfoLog` after attempting the compile and link." CreationDate="2016-06-02T08:42:53.983" UserId="137" />
  <row Id="3260" PostId="2516" Score="0" Text="@DanielMGessel It is an electricity network and I have the real world map(coordinates) in hand, The graph is planar and maybe the edges cross each other, so its not a problem if the orthogonal edges cross each other, because in some case I believe there is no way to draw the graph without overlapping edges." CreationDate="2016-06-02T09:01:18.467" UserId="537" />
  <row Id="3261" PostId="2516" Score="0" Text="@joojaa I have GIS data of an electricity network and just like any electricity computation software(cyme, digsilent, pti,...) I just need to draw an arc-node structure in an orthogonal manner. As I described above for Daniel. I have extracted the hypernodes and their connections so the ortho graph could be drawn from sketch using these data. I will attach some picture to the question to increase the clearity, thanks for suggesting." CreationDate="2016-06-02T09:10:01.800" UserId="537" />
  <row Id="3262" PostId="2552" Score="2" Text="More annoyingly, this fails on only some drivers/cards, which makes it hard to debug in the wild. As best practice, never rely on implicit casts to float in GLSL, always specify the decimal dot." CreationDate="2016-06-02T12:33:37.630" UserId="457" />
  <row Id="3263" PostId="1978" Score="1" Text="The model-matrix is indeed a model-to-world matrix. If you can call the camera-matrix or often view-matrix a &quot;camera-to-world&quot; or &quot;world-to-camera&quot; matrix depends on how you build it i think. I aways use a approach of &quot;world-to-camera&quot; in which the matrix is composed with the coordinate system and the _negative_ position of the camera. Then multiplying by this takes a world coordinate into camera/view space. http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/ explains this in a much more detailed fashion and in the way I use the terms and matrices." CreationDate="2016-06-02T15:55:16.567" UserId="273" />
  <row Id="3264" PostId="2542" Score="1" Text="@trichoplax Thanks! I did it, in order to better clarify that particular detail" CreationDate="2016-06-03T00:20:28.447" UserId="3410" />
  <row Id="3265" PostId="2533" Score="0" Text="I am going to take a look at QT, as that uses c++ which I know best, and seems like a good multi platform utility." CreationDate="2016-06-03T03:48:22.027" UserId="3459" />
  <row Id="3266" PostId="2549" Score="0" Text="To complement the answer, check [this video](https://www.youtube.com/watch?v=mpTl003EXCY&amp;list=PL_w_qWAQZtAZhtzPI5pkAtcUVgmzdAP8g&amp;index=5)" CreationDate="2016-06-03T04:45:42.690" UserId="116" />
  <row Id="3267" PostId="2556" Score="2" Text="This is actually a very interesting question. I really hope this isn't considered off topic here." CreationDate="2016-06-03T08:25:39.840" UserId="2479" />
  <row Id="3268" PostId="2516" Score="1" Text="The vertexes already have a given position in 2D (or on a sphere), so it's more of an edge routing problem? The trivial solution is to route each edge first along the x, then along the y axis (or the other way 'round). If this is right, explaining the problems with this trivial solution might be helpful. I imagine a minimization problem by assigning penalties for the problems..." CreationDate="2016-06-03T16:54:26.387" UserId="2500" />
  <row Id="3269" PostId="2559" Score="0" Text="Thanks for your answer! Yes you are right, and I know them: they were introduced with the Compute Shaders and indeed they solve part of the problem because they allow saving values from one shader pass to the next. However, I don't think they solve the main problems I described: the array-of-arrays or array-of-lists issue" CreationDate="2016-06-03T20:05:16.150" UserId="3410" />
  <row Id="3270" PostId="2562" Score="2" Text="The things that stand out to me about the model are its symmetrical features. Not all of these are mismatched on all people, but most people have *some* asymmetry. This model has ears positioned at the same height. The wrinkles in general are symmetrical, particularly the frown lines on the left and right of the bridge of the nose. In addition to these permanent features, the expression is also very symmetrical. None of these things are impossible on a human face, but they invite closer inspection, making it more difficult to miss other subtle clues that the face is artificial." CreationDate="2016-06-04T00:38:34.417" UserId="231" />
  <row Id="3271" PostId="2562" Score="2" Text="The reason why i would not want to go there is that i know that the fact that it IS artificial makes you more sceptical and start to see things that may or may not be critical. Yes i agree its a archetypical character. But again since its not a blind test i would refrain from few of the points." CreationDate="2016-06-04T06:00:12.370" UserId="38" />
  <row Id="3273" PostId="2516" Score="0" Text="@DanielMGessel I have added some pictures and explanation, hope it clear the ambiguities." CreationDate="2016-06-04T21:41:29.730" UserId="537" />
  <row Id="3274" PostId="2516" Score="0" Text="@joojaa Please take a look at the added picture.Thanks" CreationDate="2016-06-04T21:42:15.963" UserId="537" />
  <row Id="3275" PostId="2558" Score="0" Text="Thank you, Dragonseel! I actually suspected that all went down to collision detection. Do you think AABB is enough?" CreationDate="2016-06-05T15:47:59.580" UserId="3472" />
  <row Id="3276" PostId="2558" Score="1" Text="A General BB should be fine but the axis alignment of the boxes prohibits you from calculation correct rotation of the vehicles. For the easier just up-down movment you could maybe make them work. But I think for the more general solution you need rotating bounding volumes." CreationDate="2016-06-05T23:42:31.653" UserId="273" />
  <row Id="3277" PostId="2559" Score="0" Text="Doh you have the same number of floats for all pixel? I mean just in a single frame the number could change between frames if necessary. If you have you could unpack the 2D data structure into a 1D array by index shifting. This could then be written as a buffer. If the amount of data varies you could write some raw data Ald just bytes and then an additional index structure which pixel has what amount of data... That would be just one not per pixel overhead." CreationDate="2016-06-05T23:46:16.750" UserId="273" />
  <row Id="3278" PostId="2554" Score="0" Text="I can't test this until I get home so can't fix your main problem right now, but there's a fair bit of unnecessary maths in your shader :" CreationDate="2016-06-06T06:15:01.990" UserId="1937" />
  <row Id="3279" PostId="2554" Score="1" Text="the W divide of vertexPositionHomogenous is pointless as only the projection matrix alters W ; you're normalizing lightDirection twice, once in the declaration and once in the calculation of cosTheta ; and the halfway vector calculation is cheaper as (lightDirection + viewDirection) * 0.5 .... could you post the fragment shader you're using with this vertex shader as well? Because other than that the maths look right." CreationDate="2016-06-06T06:27:01.367" UserId="1937" />
  <row Id="3280" PostId="2519" Score="0" Text="Yes, that was the problem. But my main question was: What IS the half vector used for, since it appears in the function definition. As far as I understand now it is only used in the check if H dot V is positive. Thank you for taking time to write the answers." CreationDate="2016-06-06T08:00:54.563" UserId="3424" />
  <row Id="3281" PostId="3568" Score="0" Text="I was familiar with tom sawyer  however yWorks was new and inspiring, But I asked for an algorithm. Thanks by the way." CreationDate="2016-06-06T08:47:24.423" UserId="537" />
  <row Id="3282" PostId="3568" Score="0" Text="@Iman you have read [this](https://cs.brown.edu/~rt/gdhandbook/chapters/orthogonal.pdf), thing is is will not suggest algorithms in this realm because i can only implement 2 of them and even they are not efficient." CreationDate="2016-06-06T09:19:22.980" UserId="38" />
  <row Id="3283" PostId="3568" Score="0" Text="Yes, I read some other PDF too but they where very theoretic and extracting an algorithm from these literature was almost impossible for a newbie, However I am testing my own made algorithm, and will publish it as soon as it result something, this algorithm I am talking about is to draw a grid, over the main map, then fit the nodes (all substations and middle nodes) over the grid, then use the connection table to draw edges via A* algorithm toward the grid! that would create an orthogonal graph for sure, but it needs a lot of adjustments." CreationDate="2016-06-06T09:25:56.010" UserId="537" />
  <row Id="3284" PostId="2519" Score="0" Text="That is my understanding as well. Glad to help." CreationDate="2016-06-06T09:28:04.990" UserId="2479" />
  <row Id="3285" PostId="3568" Score="0" Text="@Zich graph drawing is a hard problem it has not been solved yet. It took me about a year to implement a graphing algorithm that actually worked. Even then i have no idea why it works" CreationDate="2016-06-06T09:28:21.593" UserId="38" />
  <row Id="3286" PostId="3568" Score="0" Text="but I am sure it has been done in many commercial software and tom sawyer did it before, I agree that it would be hard but I do not think in is an original field and not solved yet." CreationDate="2016-06-06T09:32:27.280" UserId="537" />
  <row Id="3287" PostId="3568" Score="0" Text="@Zich its not solved yet mans we do not know what are good algorithms, we know passable algorithms that sort of work. Engineering is not beholden to perfect, but science is. Problem with many of these algorithms is that if you would try to optimize the layout of say a large pcb you wouldn't get optimal as searching for the optimal would take way too long with current algorithms and they are hideously slow for global optimum. They work fine for small number of nodes or strategically bisected graphs. Being not solved means there are not many easily available standard solutions to follow." CreationDate="2016-06-06T10:11:04.500" UserId="38" />
  <row Id="3288" PostId="2556" Score="0" Text="An oft-overlooked feature is how well (or poorly) the scene data was mapped to the display. It's common for high-dynamic-range scene data to be arbitrarily &quot;squashed&quot; into an sRGB range without careful mapping. This can create a lot of VERY subtle problems that cue the brain to be skeptical. A great (generic) discussion was started here: http://blender.stackexchange.com/questions/46825/render-with-a-wider-dynamic-range-in-cycles-to-produce-photorealistic-looking-im" CreationDate="2016-06-06T19:59:42.570" UserId="4494" />
  <row Id="3289" PostId="3569" Score="0" Text="This looks like a homework problem. Is this a homework problem?" CreationDate="2016-06-06T20:00:49.890" UserId="4494" />
  <row Id="3290" PostId="3569" Score="2" Text="@mHurley Question from an exam paper" CreationDate="2016-06-06T20:20:10.177" UserId="1971" />
  <row Id="3291" PostId="3569" Score="0" Text="What exactly don't you understand? You have n and you have VUP. And you have the formula for u. Just plug in the values. The x stands for the cross product which you can lookup if that makes you trouble. And the | | means length. / should be clear as division." CreationDate="2016-06-07T00:29:30.770" UserId="273" />
  <row Id="3292" PostId="3569" Score="0" Text="Should this be on Math.stackexchange ?" CreationDate="2016-06-07T03:36:12.037" UserId="3437" />
  <row Id="3293" PostId="3572" Score="0" Text="Thanks, i thought i need a translation matrix but i didn't know that i messed the order of operation." CreationDate="2016-06-07T06:32:58.537" UserId="3437" />
  <row Id="3294" PostId="2558" Score="0" Text="The collision check has to be done with all objects in the scene, correct? Or at least, the ones within a certain distance.&#xA;I wonder if calculating the vector between the center of the car and the center of the obstacle and adding it to the car's speed would suffice." CreationDate="2016-06-07T08:33:52.040" UserId="3472" />
  <row Id="3295" PostId="3569" Score="2" Text="@ritwiksinha questions can be on topic on more than one site." CreationDate="2016-06-07T08:47:23.357" UserId="231" />
  <row Id="3296" PostId="2558" Score="1" Text="The collision would have to be checked against all nearby objects that you want to be able to colide with. In general using that distance would not work. Think of a long object. It's center of mass would be far away from the object and the force you add would be much too large. What is physically correct to use is the penetration depth aka how deep is the object inside the obstacle (using the maximum depth). But I cannot say that in your special case it is impossible to find some plausible simplification. I just don't know enough about your project and goals." CreationDate="2016-06-07T08:53:46.657" UserId="273" />
  <row Id="3297" PostId="2538" Score="0" Text="Could you explain what &quot;tool-path&quot; and &quot;scanSpacingStep&quot; mean in this context?" CreationDate="2016-06-07T09:03:19.353" UserId="231" />
  <row Id="3298" PostId="2538" Score="0" Text="The tool-path is basically the path that follows a series of coordinate points over a grid cell. And the scanSpacingStep is the distance between two adjacent lines. Check the link http://imgur.com/NTtPYoB The fractal path in the image could be a tool path that is defined by series of coordinates and the in-between distance of each and every fractal line segments is the scanSpacingStep." CreationDate="2016-06-07T09:09:47.980" UserId="2712" />
  <row Id="3299" PostId="3570" Score="0" Text="Thank you very much! This is very very helpful" CreationDate="2016-06-07T14:21:52.043" UserId="310" />
  <row Id="3300" PostId="3570" Score="0" Text="How would you sample distances for non-monochromatic scattering/absorption coefficients? Randomly choose a channel, then divide by 1/3 (in the case of RGB or XYZ)?" CreationDate="2016-06-07T17:25:55.173" UserId="310" />
  <row Id="3301" PostId="3570" Score="1" Text="@RichieSams The recommendation I've seen for that case is to assign each ray a single wavelength or color channel. So you basically calculate the scattering for each channel separately. For instance, in atmospheric scattering, blue light scatters much more strongly than red and therefore needs a lot more scattering events, and the blue photons will follow much more convoluted paths than the red ones. So it makes some sense to simulate them separately—much like dispersion due to refraction. I've never really tried this myself though." CreationDate="2016-06-07T17:46:09.757" UserId="48" />
  <row Id="3302" PostId="3570" Score="0" Text="Ahh, that makes sense. Though, performance will suffer... No wonder everyone wants to estimate Monte-Carlo Participating media. Thanks for all the info!" CreationDate="2016-06-07T17:51:53.400" UserId="310" />
  <row Id="3303" PostId="2568" Score="0" Text="As to your last comment - it's not a requirement of course for CG to look like a photo, but the goal of a lot of animation is to look as life-like as possible. Maybe I picked a less-than-perfect example, but I've always been interested in how real-time rendering could be made more realistic, especially in games." CreationDate="2016-06-07T18:29:52.393" UserId="3477" />
  <row Id="3304" PostId="2568" Score="0" Text="Of course, in this case I was talking about the information provided in the question, so I was discussing a still image. For extra realistic look, I don't know what software you're using, but if it's Blender, there's a new LUT that can be replaced (not official yet) which makes Cycles rendering very close to real life filming quality, concerning material's reaction to light, lighting is a major aspect, specially when discussing photo real." CreationDate="2016-06-07T18:36:48.823" UserId="3487" />
  <row Id="3305" PostId="2568" Score="0" Text="So do you think it's often the case that while something is almost realistic, the CG is &quot;designed&quot; (if you will) with the idea that it shouldn't be picture-perfect?" CreationDate="2016-06-07T21:09:22.370" UserId="3477" />
  <row Id="3306" PostId="2568" Score="0" Text="I wouldn't use &quot;Should not&quot; or &quot;must not&quot; with anything related to an artistic matter, there is no should nor must in art, everything is subjective, what I meant is that (as I believe) a glance of non realism, even in realistic artworks, keeps a small window open to the personality of the artist, compared to stylized artworks which for this example would be no roof at all. It all depends on were you like to stand." CreationDate="2016-06-07T21:43:00.577" UserId="3487" />
  <row Id="3307" PostId="3575" Score="0" Text="If you can do something in one API, you can do it any other API, just the way of doing it will depend on the API." CreationDate="2016-06-07T21:46:02.013" UserId="3437" />
  <row Id="3309" PostId="3577" Score="0" Text="I would bet that if OSX would ever support Vulkan it will support only a small subset of it, and that would also become the Next Generation graphics api for web browsers, OpenGL is still ways simpler to use (to a certain degree of course) than Vulkan, what vulkan gain in simplicity over rendering pipeline it lose it in explicit handling of much other stuff" CreationDate="2016-06-08T08:04:14.783" UserId="4503" />
  <row Id="3310" PostId="3577" Score="1" Text="@DarioOO OpenGL immediate mode is way simpler to use than whatever-you-call-the-thing-that's-not-immediate-mode, yet it's not recommended." CreationDate="2016-06-08T08:56:08.637" UserId="2316" />
  <row Id="3312" PostId="3580" Score="0" Text="Excellent question! I've been wondering about this myself. It's not that the checkerboard pattern has a yellow tint... it's the grays that are incorrect. When I compare what I see on my laptop to my phone, where the checkerboard pattern seems to have roughly the same color as the grays, the gradient on my laptop appears distinctly more blue. Perhaps a better question would be to ask why do shades of gray look blue on most LCD computer screens." CreationDate="2016-06-08T14:11:16.993" UserId="3470" />
  <row Id="3313" PostId="3580" Score="0" Text="@Quinchilion Interesting perspective. I'm so used to how gray looks on my screen that I might not notice if there's some blue to it, though it looks really metallic to me. It might be relevant that blue is the complementary color of yellow." CreationDate="2016-06-08T14:28:41.600" UserId="4490" />
  <row Id="3314" PostId="3576" Score="1" Text="Good answer, reminded me of this: http://www.elise.com/quotes/heinlein_-_specialization_is_for_insects" CreationDate="2016-06-08T16:52:32.307" UserId="3473" />
  <row Id="3315" PostId="3582" Score="2" Text="It isn't clear to me which direction you are working. Are you looking to convert data into an image, or convert an image into data? Could you [edit] to mention what you already have, and what final result you want?" CreationDate="2016-06-08T18:43:48.997" UserId="231" />
  <row Id="3316" PostId="3586" Score="2" Text="I'm not aware of any distinction between &quot;GPU instancing&quot; and &quot;standard instancing&quot;. As far as I'm aware, there's only one kind of instancing and it is a GPU feature. Whatever this is, it's probably something specific to Unity (and perhaps a term made up by Unity marketing), and you might have to track down a Unity engineer to get an explanation of what the new feature really is." CreationDate="2016-06-09T07:50:32.520" UserId="48" />
  <row Id="3317" PostId="3586" Score="0" Text="By 'standard instancing', could they be referring to just having one copy of the mesh in system memory? That sounds rather elementary to any game engine. It also seems odd that they didn't have GPU instancing until now." CreationDate="2016-06-09T07:55:16.000" UserId="457" />
  <row Id="3318" PostId="3581" Score="0" Text="Vulkan is stable right now. Your GL 1.0 *vs* 2.0 analogy is apt. Starting with GL now would be like starting learning GL 1.0 six months after GL 2.0 comes out." CreationDate="2016-06-09T08:15:01.433" UserId="2041" />
  <row Id="3319" PostId="3585" Score="0" Text="re `new`, it looks more like c#, not c++, though OP has not explicitly specified." CreationDate="2016-06-09T08:17:30.277" UserId="457" />
  <row Id="3320" PostId="3585" Score="0" Text="Actually looks more like Java." CreationDate="2016-06-09T08:27:45.267" UserId="457" />
  <row Id="3321" PostId="3584" Score="0" Text="Are you writing a rasterizer in Java?" CreationDate="2016-06-09T08:48:18.887" UserId="3331" />
  <row Id="3322" PostId="3585" Score="0" Text="@Rotem I don't think there is any way of knowing the language except when i tell that it is Java." CreationDate="2016-06-09T09:06:21.463" UserId="3437" />
  <row Id="3323" PostId="3584" Score="0" Text="@Syntac_ Yes the language is Java." CreationDate="2016-06-09T09:06:54.213" UserId="3437" />
  <row Id="3324" PostId="3585" Score="0" Text="@ritwik In c# the `Math` methods are capitalized, e.g. `Round()`, not `round()`." CreationDate="2016-06-09T09:08:49.103" UserId="457" />
  <row Id="3325" PostId="3585" Score="0" Text="@Rotem ok that's legit." CreationDate="2016-06-09T09:09:53.553" UserId="3437" />
  <row Id="3326" PostId="3585" Score="0" Text="@Nathan reed i used the good old school maths first but not the way that you suggested. Thanks i will try your method." CreationDate="2016-06-09T09:12:20.440" UserId="3437" />
  <row Id="3327" PostId="3580" Score="1" Text="Monitors are frequently very poorly calibrated. That's the most likely cause of issues with images not looking grey.&#xA;&#xA;Further, assuming the display is meant to be sRGB, then (and this is from memory so please take with a grain of salt) the RGB values that should correspond to a B&amp;W chequer board are about 186.&#xA;&#xA;What you may find helpful is to repeat your experiment separately with each of R,G &amp; B using a Primary/Black pattern VS Black-&gt;Primary blend. The crossover points are ideally meant to be at the same position but frequently aren't." CreationDate="2016-06-09T10:34:52.397" UserId="209" />
  <row Id="3328" PostId="3586" Score="0" Text="@Rotem Those were exactly my thoughts and the origin of my question. See here in their roadmap what they say about it: https://unity3d.com/pt/unity/beta/unity5.4.0b1 And yes, the only thing I can think of is that their &quot;non-GPU&quot; instancing is a copy of mesh in RAM, which **is** quite a ppor way of doing that. As per the link I sent, it seems that now they are firstly introducing the possibility of instancing using shaders, i.e. directly in the GPU" CreationDate="2016-06-09T12:22:30.517" UserId="2061" />
  <row Id="3329" PostId="2567" Score="1" Text="What's an orthographic cuboid? &quot;Orthographic&quot; is a property of the projection, it's not a shape. Perhaps you could explain what effect you're hoping this projection will have." CreationDate="2016-06-09T13:04:08.070" UserId="2041" />
  <row Id="3330" PostId="3588" Score="0" Text="If you coded the calculation wouldn't it be easier to query the data before it gets turned into colours?" CreationDate="2016-06-09T13:14:31.240" UserId="273" />
  <row Id="3331" PostId="3576" Score="0" Text="This C++ analogy is inappropriate.  Vulkan is a new next generation API developed by the creators of OpenGL.  C++ is an established, mostly backwards-compatible competitor to C." CreationDate="2016-06-09T13:33:16.703" UserId="4531" />
  <row Id="3332" PostId="3588" Score="0" Text="I did not code it it is not an open source component I can not acces to surce code of 3d sun exposure component. But I want to convert result to data. Is it possible? Are there any component for this purpose? Or Do I have to code myself all workflows of analysis to achieve this?" CreationDate="2016-06-09T13:36:26.720" UserId="4513" />
  <row Id="3333" PostId="3576" Score="0" Text="Those specifics are irrelevant to the point of the analogy." CreationDate="2016-06-09T13:59:02.013" UserId="4494" />
  <row Id="3334" PostId="3583" Score="0" Text="&quot;*Another problem I have is with Vulkan's claims is how it claims to be better.*&quot; Vulkan does not claim to be anything. It's just an API; it cannot make claims." CreationDate="2016-06-09T15:18:32.767" UserId="2654" />
  <row Id="3335" PostId="3587" Score="0" Text="Please include a reference image with your question. Googling 'logic studio environment editor' for images gives me nothing useful." CreationDate="2016-06-09T15:20:50.717" UserId="457" />
  <row Id="3336" PostId="3581" Score="0" Text="Vulkan might be &quot;stable&quot; it doesn't mean things wont change, this is the nature of languages, which I presented 2 recent changes to languages within the last few years, so how do we know this wouldn't happen to Vulkan? You're basically saying that learning OpenGL is a waste, and that's false.  Making it seem like OpenGL is dead, and wont be used any more... Also false.  Besides, I'm not the only one who believes that Vulkan could have stability issues.  It might not, but with any new language, that's what you look for." CreationDate="2016-06-09T16:07:02.913" UserId="3420" />
  <row Id="3337" PostId="3581" Score="1" Text="Of course things will change, and the working group is already working towards new features for a next spec version (ssh, you didn't hear that from me). *Stable* means that those things will add to the spec, and the things you learn about it today will still be valid two years from now. OTOH, the things you learn about GL today are already a poor match for how GPUs work: just like learning about fixed-function pipelines in a programmable-shader world. If you read my answer, you see I don't think learning GL now is a waste. But &quot;Vulkan is too new&quot; is not a good reason to discount it." CreationDate="2016-06-09T16:13:32.113" UserId="2041" />
  <row Id="3338" PostId="3581" Score="0" Text="Libraries change with addons, but I'm talking about architecture changes, which is what happened in FX, Play!, OpenGL, and others.  How can we say in 2 years it will be the same?   The Fixed-Pipeline comment is what the issue with GL 1-2 was, so yes, why learn it if soon it could change, and we wouldn't even know?   Vulkan is too new isn't the full reason.  It's that it's subject to change, not many companies will adapt to using it right away, and how many people really want to change from OpenGL to Vulkan who are hardcore GL coders?  I think you should look at Vulkan though." CreationDate="2016-06-09T16:18:43.730" UserId="3420" />
  <row Id="3339" PostId="3590" Score="1" Text="Yup same exact result, i even went as far as calibrate the one not calibrated showing hue and poof gone." CreationDate="2016-06-09T16:30:50.033" UserId="38" />
  <row Id="3341" PostId="3581" Score="1" Text="@Lasagna: &quot;*It's that it's subject to change, not many companies will adapt to using it right away*&quot; Valve. Epic. Unity. All of them are heavily invested in making their engines work on Vulkan. CryTech and Id aren't exactly ignoring it either. So your statement is not commensurate with the facts." CreationDate="2016-06-09T17:37:22.600" UserId="2654" />
  <row Id="3342" PostId="3577" Score="1" Text="@Gavin: It should be noted that OpenGL versions 4.2 or greater aren't supported on OSX either. So if you want to use anything *recent* from OpenGL on OSX, you can't. And Apple is highly unlikely to adopt later OpenGL versions. The Apple platform is all-in on Metal, so cross-platform is a bust either way." CreationDate="2016-06-09T17:40:28.680" UserId="2654" />
  <row Id="3343" PostId="3576" Score="0" Text="Claiming specialization is inefficient or unmarketable is incredibly naive. A translator that knows all of five words in every single spoken language is useless, people will hire translators that mastered (a.k.a specialized in) a small number of languages. Now *over*-specializing *is* problematic. A translator that completely mastered one language and only knows that language is also not a useful translator. And devs (indie or otherwise) need to be careful not to spend all their time learning new tools. Eventually they need to actually make something to sell, lest they find themselves bankrupt" CreationDate="2016-06-09T18:32:26.773" UserId="4535" />
  <row Id="3344" PostId="3576" Score="0" Text="Just to be clear, I don't necessarily disagree with you in regards to OpenGL and Vulkan, This quite possibly is a case where learning both instead of just Vulkan is the better choice." CreationDate="2016-06-09T18:34:30.713" UserId="4535" />
  <row Id="3345" PostId="3576" Score="0" Text="I'm pretty sure that's what I implied by using the examples I did ;-) maybe not..." CreationDate="2016-06-09T18:38:41.940" UserId="4494" />
  <row Id="3346" PostId="3581" Score="0" Text="Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it, especially projects that are mid-way.  You might get a brand new project in Vulkan, but again, are you willing to hire new employees, train new ones, and be the one to test out a new platform?  I've done testing on new platforms, and it's a PITA sometimes working around bugs, reporting bugs, etc, when you're trying to just put out a product." CreationDate="2016-06-09T19:38:40.367" UserId="3420" />
  <row Id="3347" PostId="3586" Score="1" Text="If the mesh moves and needs to be drawn in many places then one could instantiate it by reusing the draw calls and not redo mesh deformations. Used to be done in ages past ages." CreationDate="2016-06-09T20:23:37.800" UserId="38" />
  <row Id="3348" PostId="3581" Score="1" Text="@Lasagna: &quot;*Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it*&quot; ... So 3D engines don't qualify as &quot;projects&quot;? Does DOTA2 count as a &quot;project&quot;? Because it's got support for Vulkan *right now*. Does Samsung's line of smartphones and tablets count as a &quot;project&quot;? Because they're looking at incorporating it into the UI. The reality is that *lots* of people are willing to &quot;be the one to test out a new platform&quot;, if that platform gets them what they need." CreationDate="2016-06-09T22:30:15.220" UserId="2654" />
  <row Id="3349" PostId="3587" Score="0" Text="https://macprovid.vo.llnwd.net/o43/hub/media/1074/8250/Arp_Env_Pic.png" CreationDate="2016-06-10T02:13:57.483" UserId="4528" />
  <row Id="3350" PostId="3587" Score="0" Text="http://media.soundonsound.com/sos/jan03/images/staudiomedia712.l.gif" CreationDate="2016-06-10T02:16:06.510" UserId="4528" />
  <row Id="3351" PostId="3587" Score="0" Text="http://www.pcrecording.com/extlinkexp.jpg" CreationDate="2016-06-10T02:16:39.340" UserId="4528" />
  <row Id="3352" PostId="3587" Score="0" Text="http://musicplayers.com/reviews/recording/2015/images/Eventide_patchbay.jpg" CreationDate="2016-06-10T02:16:58.610" UserId="4528" />
  <row Id="3353" PostId="3587" Score="0" Text="http://i40.tinypic.com/2dhwqp1.jpg" CreationDate="2016-06-10T02:17:44.490" UserId="4528" />
  <row Id="3355" PostId="3579" Score="0" Text="I somewhat disagree. Vulkan (and DX12)has proven to be very difficult to implement, for *experienced* devs. With all the power that Vulkan gives you, it's very very easy to shoot yourself in the foot. In a addition, Vulkan requires a lot more boilerplate code. For someone just learning about GPU programming, I tend to think that Vulkan will be very overwhelming." CreationDate="2016-06-10T12:09:34.540" UserId="310" />
  <row Id="3356" PostId="3593" Score="0" Text="Is there any reason why you use the old style openGL" CreationDate="2016-06-10T14:06:58.027" UserId="38" />
  <row Id="3357" PostId="3595" Score="0" Text="Looks like simple trigonometry to me, if you consider that the two triangles are mirror images of each other. The triangle (v1, v2, middle) is isosceles and you know both the length of v1-v2 and the angle defined by (v1, middle, v2), so you should be able to calculate both v1-middle and alpha." CreationDate="2016-06-10T14:45:32.137" UserId="3470" />
  <row Id="3358" PostId="3595" Score="0" Text="I don't know where middle is, though. I only have the vertices of the cube and a degree by which I want it bend." CreationDate="2016-06-10T15:00:36.320" UserId="4543" />
  <row Id="3359" PostId="3588" Score="0" Text="I have closed the [older question](http://computergraphics.stackexchange.com/questions/3582/reporting-results-of-3d-sun-exposure-analysis) as a duplicate of this one, since this one is more specific about what is required." CreationDate="2016-06-10T15:17:06.007" UserId="231" />
  <row Id="3360" PostId="3595" Score="0" Text="If you know where v1 and v2 are, you can calculate where 'middle' is from the requested angle. If you don't, can you clarify the problem? I see no cube in the picture." CreationDate="2016-06-10T15:45:36.743" UserId="3470" />
  <row Id="3361" PostId="3595" Score="0" Text="http://prntscr.com/ber3a7 that is input and result (with a 45° angle). Those are Screenshots from Cinema4D's bending tool. As you can see the 45° is the angle of the topmost edge of the cube. The bending process strectches the sides but not the 'spine' of the object. I'm trying to emulate the tool in code with a given Mesh." CreationDate="2016-06-10T15:55:16.443" UserId="4543" />
  <row Id="3362" PostId="3579" Score="0" Text="@RichieSams I don't think you meant &quot;implement&quot;. Only the GPU vendor has to implement Vulkan, and it's a lot easier than implementing OpenGL. (Believe me, I've done it!) But assuming you meant it's difficult to integrate with or program against, I've added a paragraph with some information I learned from the Vulkan WG." CreationDate="2016-06-10T16:01:46.880" UserId="2041" />
  <row Id="3363" PostId="3579" Score="0" Text="Correct. Implement is perhaps a poor word choice. I meant 'to use' in your program. But I like your edits. Well put" CreationDate="2016-06-10T16:06:08.887" UserId="310" />
  <row Id="3364" PostId="3589" Score="0" Text="I'm glad you posted this, because it makes very clear some ideas that I was hesitating to express in my answer: that learning the concepts is most important. I think where we differ is that you encourage GL because it's easier to get started, while I think that ease of getting started brings a risk of doing things in legacy ways. It's quite hard to know in GL what the &quot;modern GL idiom&quot; is, compared with programming in a legacy style. If nobody tells you to use VAOs instead of a separate draw call per primitive, it's a lot harder to unlearn that mistake later." CreationDate="2016-06-10T16:08:50.723" UserId="2041" />
  <row Id="3365" PostId="3589" Score="0" Text="@DanHulme: It's all a matter of using the right learning materials. There are plenty of such materials online that use modern idioms. Nobody should ever try learning graphics just by reading a reference manual or specification." CreationDate="2016-06-10T16:28:17.220" UserId="2654" />
  <row Id="3366" PostId="3597" Score="0" Text="how can I entegrate it to my software. I am thinking to use unity3d as a developing platform. Can I entegrate Nvidia Optix to it?" CreationDate="2016-06-10T17:16:25.420" UserId="4513" />
  <row Id="3367" PostId="3597" Score="0" Text="Depends. OptiX has a C/C++ API so if you use Unity with Cpp you should be fine. Just install CUDA and link against the OptiX library. The tricky part will be to get the Nvcc to do its thing. I use CMake for that but I don't know about Unity + CMake. Then just ist unity to visualize the output of the OptiX simulation." CreationDate="2016-06-10T17:20:06.220" UserId="273" />
  <row Id="3368" PostId="2006" Score="0" Text="When I first read about this (15-16 years ago), I wanted to try applying it to Wolfenstein 3D to see if it was a major improvement over ray casting, especially at higher resolutions and/or when using OpenGL rendering. Just using BSP trees as the SNES and Macintosh Wolf 3D and DOOM did are probably a win regardless, but I was hung up on wanting to learn this potential best method before actually implementing anything." CreationDate="2016-06-10T21:53:40.930" UserId="2608" />
  <row Id="3369" PostId="2006" Score="0" Text="I think that any and all (BSP-based, specifically, unless it's just that awesome) algorithms that meet the criteria of being suitable for solving visibility and rendering in Wolfenstein 3D / DOOM-style games, including their implementations being practical on those games' target systems, would be sufficient in answering the question, and not necessarily any algorithms that were invented or known by John Carmack." CreationDate="2016-06-10T22:03:16.923" UserId="2608" />
  <row Id="3370" PostId="3580" Score="0" Text="@SimonF 186 corresponds roughly to the middle of the bottom gradient, and that place also seems to match the intensity of the top rectangle." CreationDate="2016-06-10T23:30:52.850" UserId="4490" />
  <row Id="3371" PostId="3583" Score="0" Text="You do realise that Vulkan and GL are made by mostly the same people?" CreationDate="2016-06-11T00:03:25.313" UserId="2041" />
  <row Id="3372" PostId="3583" Score="0" Text="Yes I do. I still prefer GL" CreationDate="2016-06-11T00:04:18.247" UserId="4522" />
  <row Id="3373" PostId="3589" Score="0" Text="You make it sound really easy! Even so, I still see programmers getting started in graphics - some of them very competent in other fields - and using legacy features and learning nothing about the performance characteristics. It's really hard to get that wrong in Vulkan, because it makes expensive things *look* expensive. Anyway, we don't need to argue. I agree with your main point: learning the concepts is most important, and you can do that without Vulkan or GL." CreationDate="2016-06-11T00:08:47.103" UserId="2041" />
  <row Id="3374" PostId="3589" Score="0" Text="@DanHulme: &quot;*It's really hard to get that wrong in Vulkan*&quot; Because you're too busy getting *everything else* wrong ;) Also, it's still pretty easy to get performance wrong in Vulkan. Unnecessary synchonization. Using only the &quot;general&quot; image layout. Not taking advantage of subpasses. Frequent pipeline changes. And so forth. Not to mention, different vendors don't even agree on how best to use Vulkan." CreationDate="2016-06-11T00:23:03.147" UserId="2654" />
  <row Id="3375" PostId="3589" Score="0" Text="@DanHulme: As for how easy it is to use modern OpenGL, I [do my best](https://www.opengl.org/wiki/Main_Page) to [make it easy](http://alfonse.bitbucket.org/oldtut/). If people insist on learning from garbage sites like NeHe, or by reading random documentation, that's not something anyone can help. You can lead horses to water, but you can't make them drink." CreationDate="2016-06-11T00:25:05.303" UserId="2654" />
  <row Id="3376" PostId="2347" Score="0" Text="Right is not defined, if it works it works" CreationDate="2016-06-11T10:48:57.437" UserId="38" />
  <row Id="3377" PostId="3593" Score="0" Text="It is a computer graphics project in OpenGL and SDL." CreationDate="2016-06-11T14:58:30.990" UserId="3472" />
  <row Id="3378" PostId="3599" Score="0" Text="Thank you! I might as well try to reimplement some basic reader - but in the sample code that was given as example the calls didn't mess up things like this. I'm going to look at it again!" CreationDate="2016-06-11T15:00:05.277" UserId="3472" />
  <row Id="3379" PostId="3593" Score="0" Text="that's not a reason its a circular definition." CreationDate="2016-06-11T15:07:44.933" UserId="38" />
  <row Id="3380" PostId="3599" Score="0" Text="I see you're using the SDL surface's pixel buffer directly. You could get the debug version of the SDL library and the source, and inspect what's happening inside SDL. The core of the problem is that OpenGL expects the pixels to be laid out in memory differently than what is actually in the buffer." CreationDate="2016-06-11T16:29:33.877" UserId="2817" />
  <row Id="3381" PostId="3602" Score="0" Text="Could you also explain how this differs from standard instancing?" CreationDate="2016-06-11T16:55:58.717" UserId="231" />
</comments>